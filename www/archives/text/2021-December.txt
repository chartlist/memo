From ChristopherA at lifewithalacrity.com  Wed Dec  1 17:54:12 2021
From: ChristopherA at lifewithalacrity.com (Christopher Allen)
Date: Wed, 1 Dec 2021 09:54:12 -0800
Subject: [bitcoin-dev] New Portuguese & Spanish translations of Learning
	Bitcoin self-paced course
Message-ID: <CACrqygD7ANroeHF2wZbKLg8-7p4P=-KgayRQEyOjMaSB2jcaaA@mail.gmail.com>

Blockchain Commons has recently released two translations of our free,
self-paced, "Learning Bitcoin from the Command Line" course, into Spanish
and Portuguese:


   - Portuguese Translation:
   https://github.com/BlockchainCommons/Learning-Bitcoin-from-the-Command-Line/tree/master/pt
   - Spanish Translation:
   https://github.com/BlockchainCommons/Learning-Bitcoin-from-the-Command-Line/tree/master/es

Learning Bitcoin from the Command Line teaches about Bitcoin development
starting with bitcoin-cli and moving on to using computer languages to
access the RPC API. We?ve always intended that it provide a pathway for
developers to join the broader Bitcoin ecosystem, and we?ve seen personal
success toward that goal, with most of our international interns getting
their start with our course, and with many of them having since found
employment in the field.

Having more educated people in the field not only helps everyone looking
for developers, but it also will make it that much easier for us to make
the next big transition, such as the Taproot transition that we?re
currently working on.

With 460 million native speakers of Spanish and 230 million native speakers
of Portuguese, and with 29 different countries where one or both is an
official language, we think these new translations will considerably widen
the scope of Learning Bitcoin?s coverage and invite many new developers to
work together with all of us on Bitcoin, using the international language
of computer code. Of course, this year?s decision by El Salvador to adopt
Bitcoin as an official currency makes it even more obvious why these sorts
of translations are important.

Here?s what?s next for Learning Bitcoin from the Command Line.

   1. Learning Bitcoin from the Command Line v3.0

Our current iteration of Learning Bitcoin from the Command Line is now a
full year old, so we want to update it to talk about the newest Bitcoin
work, including Taproot, Schnorr signatures, miniscript, and more. Our
current outline for v3.0 is found here (though it?s likely to change some
as we dive fully into the latest bitcoin-core releases):

https://github.com/BlockchainCommons/Learning-Bitcoin-from-the-Command-Line/blob/master/TODO-30.md

We?d love your expertise on anything you think we?re missing, or getting
wrong, for the v3.0 update. Please feel free to respond here or write us an
issue, either telling us of any problems with the current course (including
things that have just gotten out of date) or things that we should have in
v3.0 that we?re not currently outlining.

https://github.com/BlockchainCommons/Learning-Bitcoin-from-the-Command-Line/issues

   1. Learning Bitcoin from the Command Line Seminars

We are considering offering some brief, weekly seminars in 2022, looking at
individual sections of Learning Bitcoin from the Command Line and answering
questions. If this interests you, or you?d like to help support it, please
let us know.

Thank you to everyone who worked on the translations of Learning Bitcoin:
Ian Culp, Maxi Goyheneche, Said Rahal, C?sar A. Vallero, and Javier Vargas
for our Spanish translation; Namcios, Korea, Luke Pavsky, and hgrams for
the Portuguese translation.

To continue this work, we are looking for monthly patronage to support
Learning Bitcoin. If you think increasing the pool of Bitcoin developers is
important, please consider becoming a patron of Blockchain Commons, and let
us know it?s because of your interest in this course.

https://github.com/sponsors/BlockchainCommons

Thanks for your interest!

-- Christopher Allen
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211201/eb429a8d/attachment.html>

From prayank at tutanota.de  Sat Dec  4 04:36:02 2021
From: prayank at tutanota.de (Prayank)
Date: Sat, 4 Dec 2021 05:36:02 +0100 (CET)
Subject: [bitcoin-dev] Pawn (chess piece) | Breaking bitcoin by playing chess
Message-ID: <Mq2iZ6U--7-2@tutanota.de>

Hello World,

Link with what, why and how: https://gist.github.com/prayank23/22763f48199ed106e59801be43ad4efc

Two related things that I found: 

1.Koala Studio tried chess on LN in 2019 but shutdown in August 2019
2.Etleneum still has chess but works differently

Primary goal of this project can be different and focus on testing Bitcoin transactions. Secondary goal is to have fun and contribute in increasing demand for block space. Maybe an app for developers to play chess, friendly competitions, learn and share new things. 

If chess sounds boring it can be replaced with any 2 player game that works for such setup and can be played with patience over few hours/days.

Spam? Sorry zero fee transactions do not work anymore. In fact, nothing below 1 sat/vbyte fee rate would work and all transactions will pay fees that are required long term. OP_RETURN is used by many projects and excluded from UTXO set. Let me know if something looks wrong. I won't be working on this as busy with another project and recently started contributing in Wasabi.


-- 
Prayank

A3B1 E430 2298 178F
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211204/b04a0d5c/attachment.html>

From willtech at live.com.au  Sat Dec  4 10:00:13 2021
From: willtech at live.com.au (LORD HIS EXCELLENCY JAMES HRMH)
Date: Sat, 4 Dec 2021 10:00:13 +0000
Subject: [bitcoin-dev] Pawn (chess piece) | Breaking bitcoin by playing
 chess
In-Reply-To: <Mq2iZ6U--7-2@tutanota.de>
References: <Mq2iZ6U--7-2@tutanota.de>
Message-ID: <PS2P216MB10891D91DDD52F784AB8E06F9D6B9@PS2P216MB1089.KORP216.PROD.OUTLOOK.COM>

The frivolous use of block space - ie. to increase the demand for block space -  is not encouraged. Although it is possible you may write chess moves on a wrap of dollar bills and send them to your friends, nowhere that I know of has this been recorded in a ledger as a valid past time.

KING JAMES HRMH
Great British Empire

Regards,
The Australian
LORD HIS EXCELLENCY JAMES HRMH (& HMRH)
of Hougun Manor & Glencoe & British Empire
MR. Damian A. James Williamson
Wills

et al.


Willtech
www.willtech.com.au
www.go-overt.com
duigco.org DUIGCO API
and other projects


m. 0487135719
f. +61261470192


This email does not constitute a general advice. Please disregard this email if misdelivered.
________________________________

From: bitcoin-dev <bitcoin-dev-bounces at lists.linuxfoundation.org> on behalf of Prayank via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>
Sent: Saturday, 4 December 2021 3:36 PM
To: Lightning Dev <lightning-dev at lists.linuxfoundation.org>
Cc: Bitcoin Dev <bitcoin-dev at lists.linuxfoundation.org>
Subject: [bitcoin-dev] Pawn (chess piece) | Breaking bitcoin by playing chess

Hello World,

Link with what, why and how: https://gist.github.com/prayank23/22763f48199ed106e59801be43ad4efc

Two related things that I found:

1.Koala Studio tried chess on LN in 2019 but shutdown in August 2019
2.Etleneum still has chess but works differently

Primary goal of this project can be different and focus on testing Bitcoin transactions. Secondary goal is to have fun and contribute in increasing demand for block space. Maybe an app for developers to play chess, friendly competitions, learn and share new things.

If chess sounds boring it can be replaced with any 2 player game that works for such setup and can be played with patience over few hours/days.

Spam? Sorry zero fee transactions do not work anymore. In fact, nothing below 1 sat/vbyte fee rate would work and all transactions will pay fees that are required long term. OP_RETURN is used by many projects and excluded from UTXO set. Let me know if something looks wrong. I won't be working on this as busy with another project and recently started contributing in Wasabi.


--
Prayank

A3B1 E430 2298 178F
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211204/0a21b13b/attachment.html>

From prayank at tutanota.de  Sat Dec  4 15:28:32 2021
From: prayank at tutanota.de (Prayank)
Date: Sat, 4 Dec 2021 16:28:32 +0100 (CET)
Subject: [bitcoin-dev] Pawn (chess piece) | Breaking bitcoin by playing
 chess
In-Reply-To: <PS2P216MB10891D91DDD52F784AB8E06F9D6B9@PS2P216MB1089.KORP216.PROD.OUTLOOK.COM>
References: <Mq2iZ6U--7-2@tutanota.de>
 <PS2P216MB10891D91DDD52F784AB8E06F9D6B9@PS2P216MB1089.KORP216.PROD.OUTLOOK.COM>
Message-ID: <Mq52uCs--3-2@tutanota.de>

Can you share the email address to get approval or permission for this type of bitcoin transactions? i.e. opening and closing of LN channels or OP_RETURN. I will keep that in Cc next time.

I can write chess moves on a dollar bill and send to my friends but it does not solve any of the problems. Bitcoin's blockchain or ledger is for transactions. As long as a transaction is valid, standard and paying fees nobody should have issues with what is being achieved with the transaction.

Thanks

-- 
Prayank

A3B1 E430 2298 178F



Dec 4, 2021, 15:30 by willtech at live.com.au:

> The frivolous use of block space - ie. to increase the demand for block space -? is not encouraged. Although it is possible you may write chess moves on a wrap of dollar bills and send them to your friends, nowhere that I know of has this been recorded in a ledger as a valid past time.
>
> KING JAMES HRMH 
> Great British Empire
>
> Regards,
> The Australian
> LORD HIS EXCELLENCY JAMES HRMH (& HMRH)
> of Hougun Manor & Glencoe & British Empire
> MR. Damian A. James Williamson
> Wills
>
> et al.
>
> ?
> Willtech
> www.willtech.com.au
> www.go-overt.com
> duigco.org DUIGCO API
> and other projects
> ?
>
> m. 0487135719
> f. +61261470192
>
>
> This email does not constitute a general advice. Please disregard this email if misdelivered.
>  
>
>
> From:>  bitcoin-dev <bitcoin-dev-bounces at lists.linuxfoundation.org> on behalf of Prayank via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>
>  
> Sent:>  Saturday, 4 December 2021 3:36 PM
>  > To:>  Lightning Dev <lightning-dev at lists.linuxfoundation.org>
>  > Cc:>  Bitcoin Dev <bitcoin-dev at lists.linuxfoundation.org>
>  > Subject:>  [bitcoin-dev] Pawn (chess piece) | Breaking bitcoin by playing chess>  > ?
> Hello World,
>
> Link with what, why and how: https://gist.github.com/prayank23/22763f48199ed106e59801be43ad4efc
>
> Two related things that I found: 
>
> 1.> Koala Studio tried chess on LN in 2019 but shutdown in August 2019
> 2.Etleneum still has chess but works differently
>
> Primary goal of this project can be different and focus on testing Bitcoin transactions. Secondary goal is to have fun and contribute in increasing demand for block space. Maybe an app for developers to play chess, friendly competitions, learn and share new things. 
>
> If chess sounds boring it can be replaced with any 2 player game that works for such setup and can be played with patience over few hours/days.
>
> Spam? Sorry zero fee transactions do not work anymore. In fact, nothing below 1 sat/vbyte fee rate would work and all transactions will pay fees that are required long term. OP_RETURN is used by many projects and excluded from UTXO set. Let me know if something looks wrong. I won't be working on this as busy with another project and recently started contributing in Wasabi.
>
>
> -- 
> Prayank
>
> A3B1 E430 2298 178F
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211204/0788929e/attachment.html>

From hcarpach at gmail.com  Mon Dec  6 09:54:30 2021
From: hcarpach at gmail.com (=?utf-8?Q?H=C3=A9ctor_Jos=C3=A9_C=C3=A1rdenas_Pacheco?=)
Date: Mon, 6 Dec 2021 06:54:30 -0300
Subject: [bitcoin-dev] Sending OP_RETURN via Bitcoin Lightning
Message-ID: <DD7D5A8B-F61F-4302-ACF4-CE731843D97D@gmail.com>

Hello all,

I?ve been thinking about how OP_RETURN is being used to create and trade NFTs on Bitcoin (think RarePepes, SoG and other new ones) and was wondering if it?s possible to make transactions with this opcode via Lightning.

More specific questions could be:
Can opcodes like OP_RETURN be inside a channel?s opening or closing transaction?
If so, could that OP_RETURN change hands within that channel or network of channels?
If possible, could the OP_RETURN be divisible? Could one person send a piece of a OP_RETURN just like one can do right now on the primary ledger or would it need to maintain the OP_RETURN code intact?
I?m assuming that, if possible, this would need a protocol layer parallel to Bitcoin/Lightning that stores and reads all Bitcoin transactions and the ones which involve the node's channels as well as the ones with the OP_RETURN, just like CounterParty does right now with the primary ledger.

Thank in advance.
??
H?ctor C?rdenas
@hcarpach

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211206/96193038/attachment.html>

From gmkarl at gmail.com  Mon Dec  6 10:20:55 2021
From: gmkarl at gmail.com (Karl)
Date: Mon, 6 Dec 2021 05:20:55 -0500
Subject: [bitcoin-dev] Sending OP_RETURN via Bitcoin Lightning
In-Reply-To: <DD7D5A8B-F61F-4302-ACF4-CE731843D97D@gmail.com>
References: <DD7D5A8B-F61F-4302-ACF4-CE731843D97D@gmail.com>
Message-ID: <CALL-=e5mF9TqbbD=Cf-bawbw4dq2PGjC9W_nqAQeHsB829ZpNg@mail.gmail.com>

Hi,

I'm not a bitcoin developer.

On Mon, Dec 6, 2021, 5:05 AM H?ctor Jos? C?rdenas Pacheco via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Hello all,
>
> I?ve been thinking about how OP_RETURN is being used to create and trade
> NFTs on Bitcoin (think RarePepes, SoG and other new ones) and was wondering
> if it?s possible to
>

Do you have a link to any of these protocols?

make transactions with this opcode via Lightning.
>
> More specific questions could be:
>
>    1. Can opcodes like OP_RETURN be inside a channel?s opening or closing
>    transaction?
>    2. If so, could that OP_RETURN change hands within that channel or
>    network of channels?
>
> OP_RETURNs do not have ownership according to the bitcoin network.  It is
not hard to define a protocol that associates an OP_RETURN with ownership,
and ownership could then be transferred via lightning by sending associated
currency via lightning.  Robustness improvements seem possible.


>    1. If possible, could the OP_RETURN be divisible? Could one person
>    send a piece of a OP_RETURN just like one can do right now on the primary
>    ledger or would it need to maintain the OP_RETURN code intact?
>
> OP_RETURNs themselves do not have ownership, but you can define a protocol
that gives them divisible ownership, including via lightning.

I?m assuming that, if possible, this would need a protocol layer parallel
> to Bitcoin/Lightning that stores and reads all Bitcoin transactions and the
> ones which involve the node's channels as well as the ones with the
> OP_RETURN, just like CounterParty does right now with the primary ledger.
>
> Thank in advance.
> ??
>
> *H?ctor C?rdenas*@hcarpach
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211206/f3253ec4/attachment.html>

From martin.habovstiak at gmail.com  Mon Dec  6 11:31:29 2021
From: martin.habovstiak at gmail.com (=?UTF-8?Q?Martin_Habov=C5=A1tiak?=)
Date: Mon, 6 Dec 2021 12:31:29 +0100
Subject: [bitcoin-dev] [Lightning-dev] Sending OP_RETURN via Bitcoin
	Lightning
In-Reply-To: <CALL-=e5mF9TqbbD=Cf-bawbw4dq2PGjC9W_nqAQeHsB829ZpNg@mail.gmail.com>
References: <DD7D5A8B-F61F-4302-ACF4-CE731843D97D@gmail.com>
 <CALL-=e5mF9TqbbD=Cf-bawbw4dq2PGjC9W_nqAQeHsB829ZpNg@mail.gmail.com>
Message-ID: <CALkkCJas_pf7Un45CJyFg8j9cBk8PtKN4iYAL81TtLSRNnKqeg@mail.gmail.com>

I recommend you researching RGB: https://rgb-org.github.io/

On Mon, Dec 6, 2021, 11:21 Karl <gmkarl at gmail.com> wrote:

> Hi,
>
> I'm not a bitcoin developer.
>
> On Mon, Dec 6, 2021, 5:05 AM H?ctor Jos? C?rdenas Pacheco via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> Hello all,
>>
>> I?ve been thinking about how OP_RETURN is being used to create and trade
>> NFTs on Bitcoin (think RarePepes, SoG and other new ones) and was wondering
>> if it?s possible to
>>
>
> Do you have a link to any of these protocols?
>
> make transactions with this opcode via Lightning.
>>
>> More specific questions could be:
>>
>>    1. Can opcodes like OP_RETURN be inside a channel?s opening or
>>    closing transaction?
>>    2. If so, could that OP_RETURN change hands within that channel or
>>    network of channels?
>>
>> OP_RETURNs do not have ownership according to the bitcoin network.  It is
> not hard to define a protocol that associates an OP_RETURN with ownership,
> and ownership could then be transferred via lightning by sending associated
> currency via lightning.  Robustness improvements seem possible.
>
>
>>    1. If possible, could the OP_RETURN be divisible? Could one person
>>    send a piece of a OP_RETURN just like one can do right now on the primary
>>    ledger or would it need to maintain the OP_RETURN code intact?
>>
>> OP_RETURNs themselves do not have ownership, but you can define a
> protocol that gives them divisible ownership, including via lightning.
>
> I?m assuming that, if possible, this would need a protocol layer parallel
>> to Bitcoin/Lightning that stores and reads all Bitcoin transactions and the
>> ones which involve the node's channels as well as the ones with the
>> OP_RETURN, just like CounterParty does right now with the primary ledger.
>>
>> Thank in advance.
>> ??
>>
>> *H?ctor C?rdenas*@hcarpach
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
> _______________________________________________
> Lightning-dev mailing list
> Lightning-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211206/d86e56cb/attachment.html>

From chrismoss411 at gmail.com  Mon Dec  6 12:38:30 2021
From: chrismoss411 at gmail.com (Christian Moss)
Date: Mon, 6 Dec 2021 12:38:30 +0000
Subject: [bitcoin-dev] Sending OP_RETURN via Bitcoin Lightning
In-Reply-To: <CALL-=e5mF9TqbbD=Cf-bawbw4dq2PGjC9W_nqAQeHsB829ZpNg@mail.gmail.com>
References: <DD7D5A8B-F61F-4302-ACF4-CE731843D97D@gmail.com>
 <CALL-=e5mF9TqbbD=Cf-bawbw4dq2PGjC9W_nqAQeHsB829ZpNg@mail.gmail.com>
Message-ID: <CANQKmgLaKisDFMGg2tOMNHmkvEtFoFfFxtm9VwOi_i-dx3Ry=w@mail.gmail.com>

Hi, it is not really possible in the way you think, mainly because
lightning relies on liquidity to work, i.,e. lots of bitcoin locked up in
channels to allow liquidity, NFTs are not liquid, so if you have 1 NFT then
it would be impossible to send on the network

I think the best off chain solution to NFTs on bitcoin is using Ruben
Somsens state chain protocol, which allows you to swap utxos off chain, and
those off chain utxos could harbour an op return/nft

On Mon, Dec 6, 2021 at 10:36 AM Karl via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Hi,
>
> I'm not a bitcoin developer.
>
> On Mon, Dec 6, 2021, 5:05 AM H?ctor Jos? C?rdenas Pacheco via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> Hello all,
>>
>> I?ve been thinking about how OP_RETURN is being used to create and trade
>> NFTs on Bitcoin (think RarePepes, SoG and other new ones) and was wondering
>> if it?s possible to
>>
>
> Do you have a link to any of these protocols?
>
> make transactions with this opcode via Lightning.
>>
>> More specific questions could be:
>>
>>    1. Can opcodes like OP_RETURN be inside a channel?s opening or
>>    closing transaction?
>>    2. If so, could that OP_RETURN change hands within that channel or
>>    network of channels?
>>
>> OP_RETURNs do not have ownership according to the bitcoin network.  It is
> not hard to define a protocol that associates an OP_RETURN with ownership,
> and ownership could then be transferred via lightning by sending associated
> currency via lightning.  Robustness improvements seem possible.
>
>
>>    1. If possible, could the OP_RETURN be divisible? Could one person
>>    send a piece of a OP_RETURN just like one can do right now on the primary
>>    ledger or would it need to maintain the OP_RETURN code intact?
>>
>> OP_RETURNs themselves do not have ownership, but you can define a
> protocol that gives them divisible ownership, including via lightning.
>
> I?m assuming that, if possible, this would need a protocol layer parallel
>> to Bitcoin/Lightning that stores and reads all Bitcoin transactions and the
>> ones which involve the node's channels as well as the ones with the
>> OP_RETURN, just like CounterParty does right now with the primary ledger.
>>
>> Thank in advance.
>> ??
>>
>> *H?ctor C?rdenas*@hcarpach
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211206/7533655c/attachment-0001.html>

From chrismoss411 at gmail.com  Mon Dec  6 16:35:19 2021
From: chrismoss411 at gmail.com (Christian Moss)
Date: Mon, 6 Dec 2021 16:35:19 +0000
Subject: [bitcoin-dev] [Lightning-dev] Sending OP_RETURN via Bitcoin
	Lightning
In-Reply-To: <CALkkCJas_pf7Un45CJyFg8j9cBk8PtKN4iYAL81TtLSRNnKqeg@mail.gmail.com>
References: <DD7D5A8B-F61F-4302-ACF4-CE731843D97D@gmail.com>
 <CALL-=e5mF9TqbbD=Cf-bawbw4dq2PGjC9W_nqAQeHsB829ZpNg@mail.gmail.com>
 <CALkkCJas_pf7Un45CJyFg8j9cBk8PtKN4iYAL81TtLSRNnKqeg@mail.gmail.com>
Message-ID: <CANQKmgLyaYAjL_=LziTCFT=Ahc2SXjJrWc+RO59pxd3mnJApfQ@mail.gmail.com>

As far as I understand it, RGB doesn't scale NFTs as each
transaction to transfer ownership of an NFT would require an onchain
transaction

On Mon, Dec 6, 2021 at 3:44 PM Martin Habov?tiak via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> I recommend you researching RGB: https://rgb-org.github.io/
>
> On Mon, Dec 6, 2021, 11:21 Karl <gmkarl at gmail.com> wrote:
>
>> Hi,
>>
>> I'm not a bitcoin developer.
>>
>> On Mon, Dec 6, 2021, 5:05 AM H?ctor Jos? C?rdenas Pacheco via bitcoin-dev
>> <bitcoin-dev at lists.linuxfoundation.org> wrote:
>>
>>> Hello all,
>>>
>>> I?ve been thinking about how OP_RETURN is being used to create and trade
>>> NFTs on Bitcoin (think RarePepes, SoG and other new ones) and was wondering
>>> if it?s possible to
>>>
>>
>> Do you have a link to any of these protocols?
>>
>> make transactions with this opcode via Lightning.
>>>
>>> More specific questions could be:
>>>
>>>    1. Can opcodes like OP_RETURN be inside a channel?s opening or
>>>    closing transaction?
>>>    2. If so, could that OP_RETURN change hands within that channel or
>>>    network of channels?
>>>
>>> OP_RETURNs do not have ownership according to the bitcoin network.  It
>> is not hard to define a protocol that associates an OP_RETURN with
>> ownership, and ownership could then be transferred via lightning by sending
>> associated currency via lightning.  Robustness improvements seem possible.
>>
>>
>>>    1. If possible, could the OP_RETURN be divisible? Could one person
>>>    send a piece of a OP_RETURN just like one can do right now on the primary
>>>    ledger or would it need to maintain the OP_RETURN code intact?
>>>
>>> OP_RETURNs themselves do not have ownership, but you can define a
>> protocol that gives them divisible ownership, including via lightning.
>>
>> I?m assuming that, if possible, this would need a protocol layer parallel
>>> to Bitcoin/Lightning that stores and reads all Bitcoin transactions and the
>>> ones which involve the node's channels as well as the ones with the
>>> OP_RETURN, just like CounterParty does right now with the primary ledger.
>>>
>>> Thank in advance.
>>> ??
>>>
>>> *H?ctor C?rdenas*@hcarpach
>>>
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev at lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>> _______________________________________________
>> Lightning-dev mailing list
>> Lightning-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev
>>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211206/c62a093a/attachment-0001.html>

From gloriajzhao at gmail.com  Tue Dec  7 17:24:33 2021
From: gloriajzhao at gmail.com (Gloria Zhao)
Date: Tue, 7 Dec 2021 17:24:33 +0000
Subject: [bitcoin-dev] A fee-bumping model
In-Reply-To: <8wtAeG1p6qyiOWW0pIJP06_h-3ro7UTBsNO-0BMxLnSKUU6xFBMEvhyQGhjsh3gvQAjDpFajGEC0C6NSQ0Nfj8KtT1cGlaQMW_nnEkAuozM=@protonmail.com>
References: <hBx6OYA5Mv9C_anoMQ-s-9l_XNwNFPfDVmOND9pXBJEBi7qsULF3bgPGpagtqjOsKDTXu8iOTVzvOjflz-M6EfnfwVH81Cu-nnai0kakouo=@protonmail.com>
 <CALZpt+F6h8uLw48e4FRrkjPe2ci6Uqy-o9H=++hu5fx7+bxOZw@mail.gmail.com>
 <8wtAeG1p6qyiOWW0pIJP06_h-3ro7UTBsNO-0BMxLnSKUU6xFBMEvhyQGhjsh3gvQAjDpFajGEC0C6NSQ0Nfj8KtT1cGlaQMW_nnEkAuozM=@protonmail.com>
Message-ID: <CAFXO6=KESTUAoHt0ZeizctEwPqFPaQs_e-NCG+i6-3RDbRPz+A@mail.gmail.com>

Hi Darosior and Ariard,

Thank you for your work looking into fee-bumping so thoroughly, and for
sharing your results. I agree about fee-bumping's importance in contract
security and feel that it's often under-prioritized. In general, what
you've described in this post, to me, is strong motivation for some of the
proposed changes to RBF we've been discussing. Mostly, I have some
questions.

> The part of Revault we are interested in for this study is the delegation
process, and more
> specifically the application of spending policies by network monitors
(watchtowers).

I'd like to better understand how fee-bumping would be used, i.e. how the
watchtower model works:
- Do all of the vault parties both deposit to the vault and a refill/fee to
the watchtower, is there a reward the watchtower collects for a successful
Cancel, or something else? (Apologies if there's a thorough explanation
somewhere that I haven't already seen).
- Do we expect watchtowers tracking multiple vaults to be batching multiple
Cancel transaction fee-bumps?
- Do we expect vault users to be using multiple watchtowers for a better
trust model? If so, and we're expecting batched fee-bumps, won't those
conflict?

> For Revault we can afford to introduce malleability in the Cancel
transaction since there is no
> second-stage transaction depending on its txid. Therefore it is
pre-signed with ANYONECANPAY. We
> can't use ANYONECANPAY|SINGLE since it would open a pinning vector [3].
Note how we can't leverage
> the carve out rule, and neither can any other more-than-two-parties
contract.

We've already talked about this offline, but I'd like to point out here
that even transactions signed with ANYONECANPAY|ALL can be pinned by RBF
unless we add an ancestor score rule. [0], [1] (numbers are inaccurate,
Cancel Tx feerates wouldn't be that low, but just to illustrate what the
attack would look like)

[0]:
https://user-images.githubusercontent.com/25183001/135104603-9e775062-5c8d-4d55-9bc9-6e9db92cfe6d.png
[1]:
https://user-images.githubusercontent.com/25183001/145044333-2f85da4a-af71-44a1-bc21-30c388713a0d.png

> can't use ANYONECANPAY|SINGLE since it would open a pinning vector [3].
Note how we can't leverage
> the carve out rule, and neither can any other more-than-two-parties
contract.

Well stated about CPFP carve out. I suppose the generalization is that
allowing n extra ancestorcount=2 descendants to a transaction means it can
help contracts with <=n+1 parties (more accurately, outputs)? I wonder if
it's possible to devise a different approach for limiting
ancestors/descendants, e.g. by height/width/branching factor of the family
instead of count... :shrug:

> You could keep a single large UTxO and peel it as you need to sponsor
transactions. But this means
> that you need to create a coin of a specific value according to your need
at the current feerate
> estimation, hope to have it confirmed in a few blocks (at least for now!
[5]), and hope that the
> value won't be obsolete by the time it confirmed.

IIUC, a Cancel transaction can be generalized as a 1-in-1-out where the
input is presigned with counterparties, SIGHASH_ANYONECANPAY. The fan-out
UTXO pool approach is a clever solution. I also think this smells like a
case where improving lower-level RBF rules is more appropriate than
requiring applications to write workarounds and generate extra
transactions. Seeing that the BIP125#2 (no new unconfirmed inputs)
restriction really hurts in this case, if that rule were removed, would you
be able to simply keep the 1 big UTXO per vault and cut out the exact
nValue you need to fee-bump Cancel transactions? Would that feel less like
"burning" for the sake of fee-bumping?

> First of all, when to fee-bump? At fixed time intervals? At each block
connection? It sounds like,
> given a large enough timelock, you could try to greed by "trying your
luck" at a lower feerate and
> only re-bumping every N blocks. You would then start aggressively bumping
at every block after M
> blocks have passed.

I'm wondering if you also considered other questions like:
- Should a fee-bumping strategy be dependent upon the rate of incoming
transactions? To me, it seems like the two components are (1) what's in the
mempool and (2) what's going to trickle into the mempool between now and
the target block. The first component is best-effort keeping
incentive-compatible mempool; historical data and crystal ball look like
the only options for incorporating the 2nd component.
- Should the fee-bumping strategy depend on how close you are to your
timelock expiry? (though this seems like a potential privacy leak, and the
game theory could get weird as you mentioned).
- As long as you have a good fee estimator (i.e. given a current mempool,
can get an accurate feerate given a % probability of getting into target
block n), is there any reason to devise a fee-bumping strategy beyond
picking a time interval?

It would be interesting to see stats on the spread of feerates in blocks
during periods of fee fluctuation.

> > In the event that you notice a consequent portion of the block is
filled with transactions paying
> > less than your own, you might want to start panicking and bump your
transaction fees by a certain
> > percentage with no consideration for your fee estimator. You might skew
miners incentives in doing
> > so: if you increase the fees by a factor of N, any miner with a
fraction larger than 1/N of the
> > network hashrate now has an incentive to censor your transaction at
first to get you to panic.

> Yes I think miner-harvesting attacks should be weighed carefully in the
design of offchain contracts fee-bumping strategies, at least in the future
when the mining reward exhausts further.

Miner-harvesting (such cool naming!) is interesting, but I want to clarify
the value of N - I don't think it's the factor by which you increase the
fees on just your transaction.

To codify: your transaction pays a fee of `f1` right now and might pay a
fee of `f2` in a later block that the miner expects to mine with 1/N
probability. The economically rational miner isn't incentivized if simply
`f2 = N * f1` unless their mempool is otherwise empty.
By omitting your transaction in this block, the miner can include another
transaction/package paying `g1` fees instead, so they lose `f1-g1` in fees
right now. In the future block, they have the choice between collecting
`f2` or `g2` (from another transaction/package) in fees, so their gain is
`max(f2-g2, 0)`.
So the equation is more like: a miner with 1/N of the hashrate, employing
this censorship strategy, gains only if `max(f2-g2, 0) > N * (f1-g1)`. More
broadly, the miner only profits if `f2` is significantly higher than `g2`
and `f1` is about the same feerate as everything else in your mempool: it
seems like they're betting on how much you _overshoot_, not how much you
bump.

In general, I agree it would really suck to inadvertently create a game
where miners can drive feerates up by triggering desperation-driven
fee-bumping procedures. I guess this is a reason to avoid
increasingly-aggressive feebumping, or strategies where we predictably
overshoot.

Slightly related question: in contracts, generally, the timelock deadline
is revealed in the script, so the miner knows how "desperate" we are right?
Is that a problem? For Revault, if your Cancel transaction is a keypath
spend (I think I remember reading that somewhere?) and you don't reveal the
script, they don't see your timelock deadline yes?

Again, thanks for the digging and sharing. :)

Best,
Gloria

On Tue, Nov 30, 2021 at 3:27 PM darosior via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Hi Antoine,
>
> Thanks for your comment. I believe for Lightning it's simpler with regard
> to the management of the UTxO pool, but harder with regard to choosing
> a threat model.
> Responses inline.
>
>
> For any opened channel, ensure the confirmation of a Commitment
> transaction and the children HTLC-Success/HTLC-Timeout transactions. Note,
> in the Lightning security game you have to consider (at least) 4 types of
> players moves and incentives : your node, your channel counterparties, the
> miners, the crowd of bitcoin users. The number of the last type of players
> is unknown from your node, however it should not be forgotten you're in
> competition for block space, therefore their block demands bids should be
> anticipated and reacted to in consequence. With that remark in mind,
> implications for your LN fee-bumping strategy will be raised afterwards.
>
> For a LN service provider, on-chain overpayments are bearing on your
> operational costs, thus downgrading your economic competitiveness. For the
> average LN user, overpayment might price out outside a LN non-custodial
> deployment, as you don't have the minimal security budget to be on your own.
>
>
> I think this problem statement can be easily generalised to any offchain
> contract. And your points stand for all of them.
> "For any opened contract, ensure at any point the confirmation of a (set
> of) transaction(s) in a given number of blocks"
>
>
> Same issue with Lightning, we can be pinned today on the basis of
> replace-by-fee rule 3. We can be also blinded by network mempool
> partitions, a pinning counterparty can segregate all the full-nodes  in as
> many subsets by broadcasting a revoked Commitment transaction different for
> each. For Revault, I think you can also do unlimited partitions by mutating
> the ANYONECANPAY-input of the Cancel.
>
>
> Well you can already do unlimited partitions by adding different inputs to
> it. You could malleate the witness, but since we are using Miniscript i'm
> confident you would only be able in a marginal way.
>
>
> That said, if you have a distributed towers deployment, spread across the
> p2p network topology, and they can't be clustered together through
> cross-layers or intra-layer heuristics, you should be able to reliably
> observe such partitions. I think such distributed monitors are deployed by
> few L1 merchants accepting 0-conf to detect naive double-spend.
>
>
> We should aim to more than 0-conf (in)security level..
> It seems to me the only policy-level mitigation for RBF pinning around the
> "don't decrease the abolute fees of a less-than-a-block mempool" would be
> to drop the requirement on increasing absolute fees if the mempool is "full
> enough" (and the feerate increases exponentially, of course).
> Another approach could be by introducing new consensus rules as proposed
> by Jeremy last year [0]. If we go in the realm of new consensus rules, then
> i think that simply committing to a maximum tx size would fix pinning by
> RBF rule 3. Could be in the annex, or in the unused sequence bits (although
> they currently are by Lightning, meh). You could also check in the output
> script that the input commits to this.
>
> [0]
> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-September/018168.html
>
>
> Have we already discussed a fee-bumping "shared cache", a CPFP variation ?
> Strawman idea: Alice and Bob commit collateral inputs to a separate UTXO
> from the main "offchain contract" one. This UTXO is locked by a multi-sig.
> For any Commitment transaction pre-signed, also counter-sign a CPFP with
> top mempool feerate included, spending a Commitment anchor output and the
> shared-cache UTXO. If the fees spike,  you can re-sign a high-feerate CPFP,
> assuming interactivity. As the CPFP is counter-signed by everyone, the
> outputs can be CSV-1 encumbered to prevent pinnings. If the share-cache is
> feeded at parity, there shouldn't be an incentive to waste or maliciously
> inflate the feerate. I think this solution can be easily generalized to
> more than 2 counterparties by using a multi-signature scheme. Big issue, if
> the feerate is short due to fee spikes and you need to re-sign a
> higher-feerate CPFP, you're trusting your counterparty to interact, though
> arguably not worse than the current update fee mechanism.
>
>
> It really looks just like `update_fee`. Except maybe with the property
> that you have the channel liquidity not depend on the onchain feerate.
> In any case, for Lightning i think it's a bad idea to re-introduce trust
> on this side post anchor outputs. For Revault it's clearly out of the
> question to introduce trust in your counterparties (why would you bother
> having a fee-bumping mechanism in the first place then?). Probably the same
> holds for all offchain contracts.
>
>
> > For Lightning, it'd mean keeping an equivalent amount of funds as the
> sum of all your
> channels balances sitting there unallocated "just in case". This is not
> reasonable.
>
> Agree, game-theory wise, you would like to keep a full fee-bumping
> reserve, ready to burn as much in fees as the contested HTLC value, as it's
> the maximum gain of your counterparty. Though perfect equilibrium is hard
> to achieve because your malicious counterparty might have an edge pushing
> you to broadcast your Commitment first by witholding HTLC resolution.
>
> Fractional fee-bumping reserves are much more realistic to expect in the
> LN network. Lower fee-bumping reserve, higher liquidity deployed, in theory
> higher routing fees. By observing historical feerates, average offchain
> balances at risk and routing fees expected gains, you should be able to
> discover an equilibrium where higher levels of reserve aren't worth the
> opportunity cost. I guess this  equilibrium could be your LN fee-bumping
> reserve max feerate.
>
> Note, I think the LN approach is a bit different from what suits a custody
> protocol like Revault,  as you compute a direct return of the frozen
> fee-bumping liquidity. With Revault, if you have numerous bitcoins
> protected, it's might be more interesting to adopt a "buy the mempool,
> stupid" strategy than risking fund safety for few percentages of interest
> returns.
>
>
> True for routing nodes. For wallets (if receiving funds), it's not about
> an investment: just users expectations to being able to transact without
> risking to lose their funds (ie being able to enforce their contract
> onchain). Although wallets they are much less at risk.
>
>
> This is where the "anticipate the crowd of bitcoin users move" point can
> be laid out. As the crowd of bitcoin users' fee-bumping reserves are
> ultimately unknown from your node knowledge, you should be ready to be a
> bit more conservative than the vanilla fee-bumping strategies shipped by
> default. In case of massive mempool congestion, your additional
> conservatism might get your time-sensitive transactions and game on the
> crowd of bitcoin users. First Problem: if all offchain bitcoin software
> adopt that strategy we might inflate the worst-case feerate rate at the
> benefit of the miners, without holistically improving block throughput.
> Second problem : your class of offchain bitcoin softwares might have
> ridiculous fee-bumping reserve compared
> to other classes of offchain bitcoin softwares (Revault > Lightning) and
> just be priced out bydesign in case of mempool congestion. Third problem :
> as the number of offchain bitcoin applications should go up with time, your
> fee-bumping reserve levels based from historical data might be always late
> by one "bank-run" scenario.
>
>
> Black swan event 2.0? Just rule n?3 is inherent to any kind of fee
> estimation.
>
> For Lightning, if you're short in fee-bumping reserves you might still do
> preemptive channel closures, either cooperatively or unilaterally and get
> back the off-chain liquidity to protect the more economically interesting
> channels. Though again, that kind of automatic behavior might be compelling
> at the individual node-level, but make the mempol congestion worse
> holistically.
>
>
> Yeah so we are back to the "fractional reserve" model: you can only
> enforce X% of the offchain contracts your participate in.. Actually it's
> even an added assumption: that you still have operating contracts, with
> honest counterparties.
>
>
> In case of massive mempool congestion, you might try to front-run the
> crowd of bitcoin users relying on block connections for fee-bumping, and
> thus start your fee-bumping as soon as you observe feerate groups
> fluctuations in your local mempool(s).
>
>
> I don't think any kind of mempool-based estimate generalizes well, since
> at any point the expected time before the next block is 10 minutes (and a
> lot can happen in 10min).
>
> Also you might proceed your fee-bumping ticks on a local clock instead of
> block connections in case of time-dilation or deeper eclipse attacks of
> your local node. Your view of the chain might be compromised but not your
> ability to broadcast transactions thanks to emergency channels (in the
> non-LN sense...though in fact quid of txn wrapped in onions ?) of
> communication.
>
>
> Oh, yeah, i didn't explicit "not getting eclipsed" (or more generally
> "data availability") as an assumption since it's generally one made by
> participants of any offchain contract. In this case you can't even have
> decent fee estimation, so you are screwed anyways.
>
>
> Yes, stay open the question on how you enforce this block insurance
> market. Reputation, which might be to avoid due to the latent
> centralization effect, might be hard to stack and audit reliably for an
> emergency mechanism running, hopefully, once in a halvening period. Maybe
> maybe some cryptographic or economically based mechanism on slashing or
> swaps could be found...
>
>
> Unfortunately, given current mining centralisation, pools are in a very
> good position to offer pretty decent SLAs around that. With a block space
> insurance, you of course don't need all these convoluted fee-bumping hacks.
> I'm very concerned that large stakeholders of the "offchain contracts
> ecosystem" would just go this (easier) way and further increase mining
> centralisation pressure.
>
> I agree that a cryptography-based scheme around this type of insurance
> services would be the best way out.
>
>
> Antoine
>
> Le lun. 29 nov. 2021 ? 09:34, darosior via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> a ?crit :
>
>> Hi everyone,
>>
>> Fee-bumping is paramount to the security of many protocols building on
>> Bitcoin, as they require the
>> confirmation of a transaction (which might be presigned) before the
>> expiration of a timelock at any
>> point after the establishment of the contract.
>>
>> The part of Revault using presigned transactions (the delegation from a
>> large to a smaller multisig)
>> is no exception. We have been working on how to approach this for a while
>> now and i'd like to share
>> what we have in order to open a discussion on this problem so central to
>> what seem to be The Right
>> Way [0] to build on Bitcoin but which has yet to be discussed in details
>> (at least publicly).
>>
>> I'll discuss what we came up with for Revault (at least for what will be
>> its first iteration) but my
>> intent with posting to the mailing list is more to frame the questions to
>> this problem we are all
>> going to face rather than present the results of our study tailored to
>> the Revault usecase.
>> The discussion is still pretty Revault-centric (as it's the case study)
>> but hopefully this can help
>> future protocol designers and/or start a discussion around what
>> everyone's doing for existing ones.
>>
>>
>> ## 1. Reminder about Revault
>>
>> The part of Revault we are interested in for this study is the delegation
>> process, and more
>> specifically the application of spending policies by network monitors
>> (watchtowers).
>> Coins are received on a large multisig. Participants of this large
>> multisig create 2 [1]
>> transactions. The Unvault, spending a deposit UTxO, creates an output
>> paying either to the small
>> multisig after a timelock or to the large multisig immediately. The
>> Cancel, spending the Unvault
>> output through the non-timelocked path, creates a new deposit UTxO.
>> Participants regularly exchange the Cancel transaction signatures for
>> each deposit, sharing the
>> signatures with the watchtowers they operate. They then optionally [2]
>> sign the Unvault transaction
>> and share the signatures with the small multisig participants who can in
>> turn use them to proceed
>> with a spending. Watchtowers can enforce spending policies (say, can't
>> Unvault outside of business
>> hours) by having the Cancel transaction be confirmed before the
>> expiration of the timelock.
>>
>>
>> ## 2. Problem statement
>>
>> For any delegated vault, ensure the confirmation of a Cancel transaction
>> in a configured number of
>> blocks at any point. In so doing, minimize the overpayments and the UTxO
>> set footprint. Overpayments
>> increase the burden on the watchtower operator by increasing the required
>> frequency of refills of the
>> fee-bumping wallet, which is already the worst user experience. You are
>> likely to manage a number of
>> UTxOs with your number of vaults, which comes at a cost for you as well
>> as everyone running a full
>> node.
>>
>> Note that this assumes miners are economically rationale, are
>> incentivized by *public* fees and that
>> you have a way to propagate your fee-bumped transaction to them. We also
>> don't consider the block
>> space bounds.
>>
>> In the previous paragraph and the following text, "vault" can generally
>> be replaced with "offchain
>> contract".
>>
>>
>> ## 3. With presigned transactions
>>
>> As you all know, the first difficulty is to get to be able to
>> unilaterally enforce your contract
>> onchain. That is, any participant must be able to unilaterally bump the
>> fees of a transaction even
>> if it was co-signed by other participants.
>>
>> For Revault we can afford to introduce malleability in the Cancel
>> transaction since there is no
>> second-stage transaction depending on its txid. Therefore it is
>> pre-signed with ANYONECANPAY. We
>> can't use ANYONECANPAY|SINGLE since it would open a pinning vector [3].
>> Note how we can't leverage
>> the carve out rule, and neither can any other more-than-two-parties
>> contract.
>> This has a significant implication for the rest, as we are entirely
>> burning fee-bumping UTxOs.
>>
>> This opens up a pinning vector, or at least a significant nuisance: any
>> other party can largely
>> increase the absolute fee without increasing the feerate, leveraging the
>> RBF rules to prevent you
>> from replacing it without paying an insane fee. And you might not see it
>> in your own mempool and
>> could only suppose it's happening by receiving non-full blocks or with
>> transactions paying a lower
>> feerate.
>> Unfortunately i know of no other primitive that can be used by
>> multi-party (i mean, >2) presigned
>> transactions protocols for fee-bumping that aren't (more) vulnerable to
>> pinning.
>>
>>
>> ## 4. We are still betting on future feerate
>>
>> The problem is still missing one more constraint. "Ensuring confirmation
>> at any time" involves ensuring
>> confirmation at *any* feerate, which you *cannot* do. So what's the
>> limit? In theory you should be ready
>> to burn as much in fees as the value of the funds you want to get out of
>> the contract. So... For us
>> it'd mean keeping for each vault an equivalent amount of funds sitting
>> there on the watchtower's hot
>> wallet. For Lightning, it'd mean keeping an equivalent amount of funds as
>> the sum of all your
>> channels balances sitting there unallocated "just in case". This is not
>> reasonable.
>>
>> So you need to keep a maximum feerate, above which you won't be able to
>> ensure the enforcement of
>> all your contracts onchain at the same time. We call that the "reserve
>> feerate" and you can have
>> different strategies for choosing it, for instance:
>> - The 85th percentile over the last year of transactions feerates
>> - The maximum historical feerate
>> - The maximum historical feerate adjusted in dollars (makes more sense
>> but introduces a (set of?)
>>   trusted oracle(s) in a security-critical component)
>> - Picking a random high feerate (why not? It's an arbitrary assumption
>> anyways)
>>
>> Therefore, even if we don't have to bet on the broadcast-time feerate
>> market at signing time anymore
>> (since we can unilaterally bump), we still need some kind of prediction
>> in preparation of making
>> funds available to bump the fees at broadcast time.
>> Apart from judging that 500sat/vb is probably more reasonable than
>> 10sat/vbyte, this unfortunately
>> sounds pretty much crystal-ball-driven.
>>
>> We currently use the maximum of the 95th percentiles over 90-days windows
>> over historical block chain
>> feerates. [4]
>>
>>
>> ## 5. How much funds does my watchtower need?
>>
>> That's what we call the "reserve". Depending on your reserve feerate
>> strategy it might vary over
>> time. This is easier to reason about with a per-contract reserve. For
>> Revault it's pretty
>> straightforward since the Cancel transaction size is static:
>> `reserve_feerate * cancel_size`. For
>> other protocols with dynamic transaction sizes (or even packages of
>> transactions) it's less so. For
>> your Lightning channel you would probably take the maximum size of your
>> commitment transaction
>> according to your HTLC exposure settings + the size of as many
>> `htlc_success` transaction?
>>
>> Then you either have your software or your user guesstimate how many
>> offchain contracts the
>> watchtower will have to watch, time that by the per-contract reserve and
>> refill this amount (plus
>> some slack in practice). Once again, a UX tradeoff (not even mentioning
>> the guesstimation UX):
>> overestimating leads to too many unallocated funds sitting on a hot
>> wallet, underestimating means
>> (at best) inability to participate in new contracts or being "at risk"
>> (not being able to enforce
>> all your contracts onchain at your reserve feerate) before a new refill.
>>
>> For vaults you likely have large-value UTxOs and small transactions (the
>> Cancel is one-in one-out in
>> Revault). For some other applications with large transactions and
>> lower-value UTxOs on average it's
>> likely that only part of the offchain contracts might be enforceable at a
>> reasonable feerate. Is it
>> reasonable?
>>
>>
>> ## 6. UTxO pool layout
>>
>> Now that you somehow managed to settle on a refill amount, how are you
>> going to use these funds?
>> Also, you'll need to manage your pool across time (consolidating small
>> coins, and probably fanning
>> out large ones).
>>
>> You could keep a single large UTxO and peel it as you need to sponsor
>> transactions. But this means
>> that you need to create a coin of a specific value according to your need
>> at the current feerate
>> estimation, hope to have it confirmed in a few blocks (at least for now!
>> [5]), and hope that the
>> value won't be obsolete by the time it confirmed. Also, you'd have to do
>> that for any number of
>> Cancel, chaining feebump coin creation transactions off the change of the
>> previous ones or replacing
>> them with more outputs. Both seem to become really un-manageable (and
>> expensive) in many edge-cases,
>> shortening the time you have to confirm the actual Cancel transaction and
>> creating uncertainty about
>> the reserve (how much is my just-in-time fanout going to cost me in fees
>> that i need to refill in
>> advance on my watchtower wallet?).
>> This is less of a concern for protocols using CPFP to sponsor
>> transactions, but they rely on a
>> policy rule specific to 2-parties contracts.
>>
>> Therefore for Revault we fan-out the coins per-vault in advance. We do so
>> at refill time so the
>> refiller can give an excess to pay for the fees of the fanout transaction
>> (which is reasonable since
>> it will occur just after the refilling transaction confirms). When the
>> watchtower is asked to watch
>> for a new delegated vault it will allocate coins from the pool of
>> fanned-out UTxOs to it (failing
>> that, it would refuse the delegation).
>> What is a good distribution of UTxOs amounts per vault? We want to
>> minimize the number of coins,
>> still have coins small enough to not overpay (remember, we can't have
>> change) and be able to bump a
>> Cancel up to the reserve feerate using these coins. The two latter
>> constraints are directly in
>> contradiction as the minimal value of a coin usable at the reserve
>> feerate (paying for its own input
>> fee + bumping the feerate by, say, 5sat/vb) is already pretty high.
>> Therefore we decided to go with
>> two distributions per vault. The "reserve distribution" alone ensures
>> that we can bump up to the
>> reserve feerate and is usable for high feerates. The "bonus distribution"
>> is not, but contains
>> smaller coins useful to prevent overpayments during low and medium fee
>> periods (which is most of the
>> time).
>> Both distributions are based on a basic geometric suite [6]. Each value
>> is half the previous one.
>> This exponentially decreases the value, limiting the number of coins. But
>> this also allows for
>> pretty small coins to exist and each coin's value is equal to the sum of
>> the smaller coins,
>> or smaller by at most the value of the smallest coin. Therefore bounding
>> the maximum overpayment to
>> the smallest coin's value [7].
>>
>> For the management of the UTxO pool across time we merged the
>> consolidation with the fanout. When
>> fanning out a refilled UTxO, we scan the pool for coins that need to be
>> consolidated according to a
>> heuristic. An instance of a heuristic is "the coin isn't allocated and
>> would not have been able to
>> increase the fee at the median feerate over the past 90 days of blocks".
>> We had this assumption that feerate would tend to go up with time and
>> therefore discarded having to
>> split some UTxOs from the pool. We however overlooked that a large
>> increase in the exchange price of
>> BTC as we've seen during the past year could invalidate this assumption
>> and that should arguably be
>> reconsidered.
>>
>>
>> ## 7. Bumping and re-bumping
>>
>> First of all, when to fee-bump? At fixed time intervals? At each block
>> connection? It sounds like,
>> given a large enough timelock, you could try to greed by "trying your
>> luck" at a lower feerate and
>> only re-bumping every N blocks. You would then start aggressively bumping
>> at every block after M
>> blocks have passed. But that's actually a bet (in disguised?) that the
>> next block feerate in M blocks
>> will be lower than the current one. In the absence of any predictive
>> model it is more reasonable to
>> just start being aggressive immediately.
>> You probably want to base your estimates on `estimatesmartfee` and as a
>> consequence you would re-bump
>> (if needed )after each block connection, when your estimates get updated
>> and you notice your
>> transaction was not included in the block.
>>
>> In the event that you notice a consequent portion of the block is filled
>> with transactions paying
>> less than your own, you might want to start panicking and bump your
>> transaction fees by a certain
>> percentage with no consideration for your fee estimator. You might skew
>> miners incentives in doing
>> so: if you increase the fees by a factor of N, any miner with a fraction
>> larger than 1/N of the
>> network hashrate now has an incentive to censor your transaction at first
>> to get you to panic. Also
>> note this can happen if you want to pay the absolute fees for the
>> 'pinning' attack mentioned in
>> section #2, and that might actually incentivize miners to perform it
>> themselves..
>>
>> The gist is that the most effective way to bump and rebump (RBF the
>> Cancel tx) seems to just be to
>> consider the `estimatesmartfee 2 CONSERVATIVE` feerate at every block
>> your tx isn't included in, and
>> to RBF it if the feerate is higher.
>> In addition, we fallback to a block chain based estimation when estimates
>> aren't available (eg if
>> the user stopped their WT for say a hour and we come back up): we use the
>> 85th percentile over the
>> feerates in the last 6 blocks. Sure, miners can try to have an influence
>> on that by stuffing their
>> blocks with large fee self-paying transactions, but they would need to:
>> 1. Be sure to catch a significant portion of the 6 blocks (at least 2,
>> actually)
>> 2. Give up on 25% of the highest fee-paying transactions (assuming they
>> got the 6 blocks, it's
>>    proportionally larger and incertain as they get less of them)
>> 3. Hope that our estimator will fail and we need to fall back to the
>> chain-based estimation
>>
>>
>> ## 8. Our study
>>
>> We essentially replayed the historical data with different deployment
>> configurations (number of
>> participants and timelock) and probability of an event occurring (event
>> being say an Unvault, an
>> invalid Unvault, a new delegation, ..). We then observed different
>> metrics such as the time at risk
>> (when we can't enforce all our contracts at the reserve feerate at the
>> same time), or the
>> operational cost.
>> We got the historical fee estimates data from Statoshi [9], Txstats [10]
>> and the historical chain
>> data from Riccardo Casatta's `blocks_iterator` [11]. Thanks!
>>
>> The (research-quality..) code can be found at
>> https://github.com/revault/research under the section
>> "Fee bumping". Again it's very Revault specific, but at least the data
>> can probably be reused for
>> studying other protocols.
>>
>>
>> ## 9. Insurances
>>
>> Of course, given it's all hacks and workarounds and there is no good
>> answer to "what is a reasonable
>> feerate up to which we need to make contracts enforceable onchain?",
>> there is definitely room for an
>> insurance market. But this enters the realm of opinions. Although i do
>> have some (having discussed
>> this topic for the past years with different people), i would like to
>> keep this post focused on the
>> technical aspects of this problem.
>>
>>
>>
>> [0] As far as i can tell, having offchain contracts be enforceable
>> onchain by confirming a
>> transaction before the expiration of a timelock is a widely agreed-upon
>> approach. And i don't think
>> we can opt for any other fundamentally different one, as you want to know
>> you can claim back your
>> coins from a contract after a deadline before taking part in it.
>>
>> [1] The Real Revault (tm) involves more transactions, but for the sake of
>> conciseness i only
>> detailed a minimum instance of the problem.
>>
>> [2] Only presigning part of the Unvault transactions allows to only
>> delegate part of the coins,
>> which can be abstracted as "delegate x% of your stash" in the user
>> interface.
>>
>> [3]
>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-May/017835.html
>>
>> [4]
>> https://github.com/revault/research/blob/1df953813708287c32a15e771ba74957ec44f354/feebumping/model/statemachine.py#L323-L329
>>
>> [5] https://github.com/bitcoin/bitcoin/pull/23121
>>
>> [6]
>> https://github.com/revault/research/blob/1df953813708287c32a15e771ba74957ec44f354/feebumping/model/statemachine.py#L494-L507
>>
>> [7] Of course this assumes a combinatorial coin selection, but i believe
>> it's ok given we limit the
>> number of coins beforehand.
>>
>> [8] Although there is the argument to outbid a censorship, anyone
>> censoring you isn't necessarily a
>> miner.
>>
>> [9] https://www.statoshi.info/
>>
>> [10] https://www.statoshi.info/
>>
>> [11] https://github.com/RCasatta/blocks_iterator
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211207/79ed7638/attachment-0001.html>

From jlrubin at mit.edu  Tue Dec  7 23:29:27 2021
From: jlrubin at mit.edu (Jeremy)
Date: Tue, 7 Dec 2021 15:29:27 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] What's Smart about Smart
	Contracts
Message-ID: <CAD5xwhjSZtm6X9J0w6uVg_ZDO7FuS=OCQ_kncURAcW_DuXq9HQ@mail.gmail.com>

Hi!

Over the next month I'm doing a one-a-day blog post series till Christmas,
and I think some of the posts might be appropriate for discussion here.

Unfortunately I forgot to start the calendar series syndicated here too...
The first few posts are less bitcoin development related and philosophical,
so I think we could skip them and start around Day 6 and I'll post the rest
up to Day 10 here today (and do every day starting tomorrow). You can see
an archive of all posts at https://rubin.io/archive/. Every post will have
[Bitcoin Advent Calendar] if you wish to filter it :(.

---------------------------------

Here's the day 6 post: https://rubin.io/bitcoin/2021/12/03/advent-6/, the
topic is why smart contracts (in extended form) may be a critical precursor
to securing Bitcoin's future rather than something we should do after
making the base layer more robust.

Cheers,

Jeremy Rubin

--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211207/8d8df500/attachment.html>

From jlrubin at mit.edu  Tue Dec  7 23:31:06 2021
From: jlrubin at mit.edu (Jeremy)
Date: Tue, 7 Dec 2021 15:31:06 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Review of Smart Contract
	Concepts
Message-ID: <CAD5xwhi=0iURq8tqWNaf4j65OdYxSVSRgYVngLqWmhF+2A02hA@mail.gmail.com>

This post covers some high-level smart contract concepts that different
opcodes or proposals could have (or not).

https://rubin.io/bitcoin/2021/12/04/advent-7/

Interested to hear about other properties that you think are relevant!

Best,

Jeremy

--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211207/278d94b2/attachment.html>

From jlrubin at mit.edu  Tue Dec  7 23:37:42 2021
From: jlrubin at mit.edu (Jeremy)
Date: Tue, 7 Dec 2021 15:37:42 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Sapio Primer
Message-ID: <CAD5xwhiO05jSNQwqRukQv7KfGUFpn7aFxD3FxFH+3xRgJqc-0A@mail.gmail.com>

This post covers a basic intro to Sapio and links to more complete docs.
https://rubin.io/bitcoin/2021/12/06/advent-9/

I've previously shared Sapio on this list, and there's been a lot of
progress since then! I think Sapio is a fantastic system to express Bitcoin
ideas in, even if you don't want to use it for your production
implementation. Most of the future posts in the series will make heavy use
of Sapio so it's worth getting comfortable with, at least for reading.

Cheers,

Jeremy

--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211207/b9350e90/attachment.html>

From jlrubin at mit.edu  Tue Dec  7 23:40:46 2021
From: jlrubin at mit.edu (Jeremy)
Date: Tue, 7 Dec 2021 15:40:46 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Vaults
Message-ID: <CAD5xwhhMsAeWBhg+kv+e_oTcUxDDLyD6WAT_RxYCWR21QW4Q9w@mail.gmail.com>

Last one for today -- sorry for the overload, I had meant to post as the
series kicked off...

This post covers building various vaults/better cold storage using sapio
https://rubin.io/bitcoin/2021/12/07/advent-10/.

In an earlier post I motivated why self-custody is so critical (see
https://rubin.io/bitcoin/2021/11/30/advent-3/); this post demonstrates how
Sapio + CTV can dramatically enhance what users can do.

Cheers, you'll see me in the inbox tomorrow,

Jeremy

--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211207/d0903acc/attachment.html>

From ZmnSCPxj at protonmail.com  Wed Dec  8 00:32:37 2021
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Wed, 08 Dec 2021 00:32:37 +0000
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] What's Smart about
	Smart Contracts
In-Reply-To: <CAD5xwhjSZtm6X9J0w6uVg_ZDO7FuS=OCQ_kncURAcW_DuXq9HQ@mail.gmail.com>
References: <CAD5xwhjSZtm6X9J0w6uVg_ZDO7FuS=OCQ_kncURAcW_DuXq9HQ@mail.gmail.com>
Message-ID: <4RdeDclGQpoDin2VLO5Ngmoghw03BZ_tvdO0vaIp_fNWWlKL9tHeIz1iQMpHxAww2pzjI4NXYtNFuND5Qkj7AmvLUajSp4AKxNg70VWr3Rw=@protonmail.com>

Good morning Jeremy,

>
> Here's the day 6 post: https://rubin.io/bitcoin/2021/12/03/advent-6/, the topic is why smart contracts (in extended form) may be a critical precursor to securing Bitcoin's future rather than something we should do after making the base layer more robust.


*This* particular post seems to contain more polemic than actual content.
This is the first post I read of the series, so maybe it is just a "breather" post between content posts?

In any case, given the subject line, it seems a waste not to discuss the actual "smart" in "smart" contract...

## Why would a "Smart" contract be "Smart"?

A "smart" contract is simply one that somehow self-enforces rather than requires a third party to enforce it.
It is "smart" because its execution is done automatically.

Consider the humble HTLC.
It is simply a contract which says:

* If B can provide the preimage for this hash H, it gets the money from A.
* If the time L arrives without B claiming this fund, A gets its money back.

Why would an HTLC self-enforce?
Why would a simple paper contract with the above wording, signed and notarized, be insufficient?

An HTLC self-enforces because given the Bitcoin network, it is not possible to violate and transfer the funds outside of the HTLC specification.
Whereas a paper contract can be mere ink on a page, if sufficient firepower is directed at the people (judges, lawyers, etc.) that would ensure its faithful execution.
You puny humans are notoriously squishy and easily destroyed.

But we must warn as well that the Bitcoin network is *also* run by people.
Thus, a "smart" contract is only "smart" to a degree, and that degree is dependent on how easily it is for the "justice system" that enforces the contract to be subverted.
After all, a "smart" contract is software, and software must run on some hardware in order to execute.

Thus, even existing paper contracts are "smart" to a degree, too.
It is simply that the hardware they run on top of --- a bunch of puny humans --- is far less reliable than cold silicon (so upgrade your compute substrate already, puny humans!).
Our hope with the Bitcoin experiment is that we might actually be able to make it much harder to subvert contracts running on the Bitcoin network.

It is that difficulty of subversion which determines the "smart"ness of a smart contract.
Bitcoin is effectively a massive RAID1 on several dozen thousands of redundant compute hardware, ensuring that the execution of every contract is faithful to the Bitcoin SCRIPT programming model.

This is why the reticence of Bitcoin node operators to change the programming model is a welcome feature of the network.
Any change to the programming model risks the introduction of bugs to the underlying virtual machine that the Bitcoin network presents to contract makers.
And without that strong reticence, we risk utterly demolishing the basis of the "smart"ness of "smart" contracts --- if a "smart" contract cannot reliably be executed, it cannot self-enforce, and if it cannot self-enforce, it is no longer particularly "smart".

## The N-of-N Rule

What is a "contract", anyway?

A "contract" is an agreement between two or more parties.
You do not make a contract to yourself, since (we assume) you are completely a single unit (in practice, humans are internally divided into smaller compute modules with slightly different incentives (note: I did not get this information by *personally* dissecting the brains of any humans), hence the "we assume").

Thus, a contract must by necessity require N participants.

This is of interest since in a reliability perspective, we often accept k-of-n.
For example, we might run a computation on three different pieces of hardware, and if only one diverges, we accept the result of the other two as true and the diverging hardware as faulty.

However, the above 2-of-3 example has a hidden assumption: that all three pieces of hardware are actually owned and operated by a single entity.

A contract has N participants, and is not governed by a single entity.
Thus, it cannot use k-of-n replication.

Contracts require N-of-N replication.
In Bitcoin terms, that is what we mean by "consensus" --- that all Bitcoin network participants can agree that some transfer is "valid".

Similarly, L2 layers, to be able to host properly "smart" contracts, require N-of-N agreement.
For example, a Lightning Network channel can properly host "smart" HTLCs, as the channel is controlled via 2-of-2 agreement.

Lesser L2 layers which support k-of-n thus have degraded "smartness", as a quorum of k participants can evict the n-k and deny the execution of the smart contract.
But with an N-of-N, *you* are a participant and your input is necessary for the execution of the smart contract, thus you can be *personally* assured that the smart contract *will* be executed faithfully.

Regards,
ZmnSCPxj

From jlrubin at mit.edu  Wed Dec  8 00:36:35 2021
From: jlrubin at mit.edu (Jeremy)
Date: Tue, 7 Dec 2021 16:36:35 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Contract Primitives and
	Upgrades to Bitcoin
Message-ID: <CAD5xwhgAJSvJE0pe6kGhc+n4fjEv-n6ACgOHuGDU8=GpsE86mw@mail.gmail.com>

This post is a mini high level SoK covering basic details of a number of
different new proposed primitives that folks might find useful -- I think
there's less to discuss around this post, since it is at a higher level and
the parts contained here could be discussed separately.

If something isn't on this list, it's an oversight by me and I'd love to
add it. The subjective criteria for inclusion/exclusion is if it seems
something the community is actively considering and is relatively well
researched.

Post here: https://rubin.io/bitcoin/2021/12/05/advent-8/

best,

Jeremy

(sorry it's out of order sent from the wrong email so it bounced, this is
day 8)

--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211207/2f71f61a/attachment.html>

From jlrubin at mit.edu  Wed Dec  8 01:11:55 2021
From: jlrubin at mit.edu (Jeremy)
Date: Tue, 7 Dec 2021 17:11:55 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] What's Smart about
	Smart Contracts
In-Reply-To: <4RdeDclGQpoDin2VLO5Ngmoghw03BZ_tvdO0vaIp_fNWWlKL9tHeIz1iQMpHxAww2pzjI4NXYtNFuND5Qkj7AmvLUajSp4AKxNg70VWr3Rw=@protonmail.com>
References: <CAD5xwhjSZtm6X9J0w6uVg_ZDO7FuS=OCQ_kncURAcW_DuXq9HQ@mail.gmail.com>
 <4RdeDclGQpoDin2VLO5Ngmoghw03BZ_tvdO0vaIp_fNWWlKL9tHeIz1iQMpHxAww2pzjI4NXYtNFuND5Qkj7AmvLUajSp4AKxNg70VWr3Rw=@protonmail.com>
Message-ID: <CAD5xwhgxaDzM3T7YiRjOzC7cjD65yyq2_Z=QZ0Ko-d3LGvB9jQ@mail.gmail.com>

--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>

Hi!

On Tue, Dec 7, 2021 at 4:33 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:

> Good morning Jeremy,
>
> >
> > Here's the day 6 post: https://rubin.io/bitcoin/2021/12/03/advent-6/,
> the topic is why smart contracts (in extended form) may be a critical
> precursor to securing Bitcoin's future rather than something we should do
> after making the base layer more robust.
>
>
> *This* particular post seems to contain more polemic than actual content.
> This is the first post I read of the series, so maybe it is just a
> "breather" post between content posts?
>

The series in general is intended to be a bit more on the approachable side
than hardcore detail.



>
> In any case, given the subject line, it seems a waste not to discuss the
> actual "smart" in "smart" contract...
>
>
Yeah maybe a better title would be "The Case for Enhanced Functionality in
Bitcoin" -- it's not really about smart contracts per se, but the thing
that people are calling smart contracts in the broader community. This gets
down to prescriptive v.s. descriptive lingo and it's not really a debate I
care much for :)




> ## Why would a "Smart" contract be "Smart"?
>
> A "smart" contract is simply one that somehow self-enforces rather than
> requires a third party to enforce it.
> It is "smart" because its execution is done automatically.
>

There are no automatic executing smart contracts on any platform I'm aware
of. Bitcoin requires TX submission, same with Eth.

Enforcement and execution are different subjects.


> Consider the humble HTLC.
> *<snip>*
> This is why the reticence of Bitcoin node operators to change the
> programming model is a welcome feature of the network.
> Any change to the programming model risks the introduction of bugs to the
> underlying virtual machine that the Bitcoin network presents to contract
> makers.
> And without that strong reticence, we risk utterly demolishing the basis
> of the "smart"ness of "smart" contracts --- if a "smart" contract cannot
> reliably be executed, it cannot self-enforce, and if it cannot
> self-enforce, it is no longer particularly "smart".
>

I don't think that anywhere in the post I advocated for playing fast and
loose with the rules to introduce any sort of unreliability.

What I'm saying is more akin to we can actually improve the "hardware" that
Bitcoin runs on to the extent that it actually does give us better ability
to adjudicate the transfers of value, and we should absolutely and
aggressively pursue that rather than keeping Bitcoin running on a set
mechanisms that are insufficient to reach the scale, privacy, self custody,
and decentralization goals we have.



> ## The N-of-N Rule
>
> What is a "contract", anyway?
>
> A "contract" is an agreement between two or more parties.
> You do not make a contract to yourself, since (we assume) you are
> completely a single unit (in practice, humans are internally divided into
> smaller compute modules with slightly different incentives (note: I did not
> get this information by *personally* dissecting the brains of any humans),
> hence the "we assume").



> Thus, a contract must by necessity require N participants


This is getting too pedantic about contracts. If you want to go there,
you're also missing "consideration".

Smart Contracts are really just programs. And you absolutely can enter
smart contracts with yourself solely, for example, Vaults (as covered in
day 10) are an example where you form a contract where you are intended to
be the only party.

You could make the claim that a vault is just an open contract between you
and some future would be hacker, but the intent is that the contract is
there to just safeguard you and those terms should mostly never execute. +
you usually want to define contract participants as not universally
quantified...

>
> This is of interest since in a reliability perspective, we often accept
> k-of-n.
> <snip>
> But with an N-of-N, *you* are a participant and your input is necessary
> for the execution of the smart contract, thus you can be *personally*
> assured that the smart contract *will* be executed faithfully.
>
>
Yes I agree that N-N or K-N have uses -- Sapio is designed to work with
arbitrary thresholds in lieu of CTV/other covenant proposals which can be
used to emulate arbitrary business logic :)


However, the benefit of the contracts without that is non-interactivity of
sending. Having everyone online is a major obstacle for things like
decentralized coordination free mining pools (kinda, the whole coordination
free part). So if you just say "always do N-of-N" you basically lose the
entire thread of"smart contract capabilities improving the four pillars
(covered in earlier posts) which solidifies bitcoin's adjudication of
transfers of value.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211207/4df0986a/attachment.html>

From jlrubin at mit.edu  Wed Dec  8 01:28:42 2021
From: jlrubin at mit.edu (Jeremy)
Date: Tue, 7 Dec 2021 17:28:42 -0800
Subject: [bitcoin-dev] Take 2: Removing the Dust Limit
Message-ID: <CAD5xwhid2OH0GzXPvqWgsMag4J9zidsewEquT-JoOweVD5pxZg@mail.gmail.com>

Bitcoin Devs (+cc lightning-dev),

Earlier this year I proposed allowing 0 value outputs and that was shot
down for various reasons, see
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-August/019307.html

I think that there can be a simple carve out now that package relay is
being launched based on my research into covenants from 2017
https://rubin.io/public/pdfs/multi-txn-contracts.pdf.

Essentially, if we allow 0 value outputs BUT require as a matter of policy
(or consensus, but policy has major advantages) that the output be used as
an Intermediate Output (that is, in order for the transaction to be
creating it to be in the mempool it must be spent by another tx)  with the
additional rule that the parent must have a higher feerate after CPFP'ing
the parent than the parent alone we can both:

1) Allow 0 value outputs for things like Anchor Outputs (very good for not
getting your eltoo/Decker channels pinned by junk witness data using Anchor
Inputs, very good for not getting your channels drained by at-dust outputs)
2) Not allow 0 value utxos to proliferate long
3) It still being valid for a 0 value that somehow gets created to be spent
by the fee paying txn later

Just doing this as a mempool policy also has the benefits of not
introducing any new validation rules. Although in general the IUTXO concept
is very attractive, it complicates mempool :(

I understand this may also be really helpful for CTV based contracts (like
vault continuation hooks) as well as things like spacechains.

Such a rule -- if it's not clear -- presupposes a fully working package
relay system.

I believe that this addresses all the issues with allowing 0 value outputs
to be created for the narrow case of immediately spendable outputs.

Cheers,

Jeremy

p.s. why another post today? Thank Greg
https://twitter.com/JeremyRubin/status/1468390561417547780


--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211207/1a309947/attachment-0001.html>

From bastien at acinq.fr  Wed Dec  8 08:34:32 2021
From: bastien at acinq.fr (Bastien TEINTURIER)
Date: Wed, 8 Dec 2021 09:34:32 +0100
Subject: [bitcoin-dev] Take 2: Removing the Dust Limit
In-Reply-To: <CAD5xwhid2OH0GzXPvqWgsMag4J9zidsewEquT-JoOweVD5pxZg@mail.gmail.com>
References: <CAD5xwhid2OH0GzXPvqWgsMag4J9zidsewEquT-JoOweVD5pxZg@mail.gmail.com>
Message-ID: <CACdvm3Oynv4gWdaGXATxc3SoYDD8kuiPq-d9F2itsmayP0qeZQ@mail.gmail.com>

Hi Jeremy,

Right now, lightning anchor outputs use a 330 sats amount. Each commitment
transaction has two such outputs, and only one of them is spent to help the
transaction get confirmed, so the other stays there and bloats the utxo set.
We allow anyone to spend them after a csv of 16 blocks, in the hope that
someone will claim a batch of them when the fees are low and remove them
from the utxo set. However, that trick wouldn't work with 0-value outputs,
as
no-one would ever claim them (doesn't make economical sense).

We actually need to have two of them to avoid pinning: each participant is
able to spend only one of these outputs while the parent tx is unconfirmed.
I believe N-party protocols would likely need N such outputs (not sure).

You mention a change to the carve-out rule, can you explain it further?
I believe it would be a necessary step, otherwise 0-value outputs for
CPFP actually seem worse than low-value ones...

Thanks,
Bastien

Le mer. 8 d?c. 2021 ? 02:29, Jeremy via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> a ?crit :

> Bitcoin Devs (+cc lightning-dev),
>
> Earlier this year I proposed allowing 0 value outputs and that was shot
> down for various reasons, see
> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-August/019307.html
>
> I think that there can be a simple carve out now that package relay is
> being launched based on my research into covenants from 2017
> https://rubin.io/public/pdfs/multi-txn-contracts.pdf.
>
> Essentially, if we allow 0 value outputs BUT require as a matter of policy
> (or consensus, but policy has major advantages) that the output be used as
> an Intermediate Output (that is, in order for the transaction to be
> creating it to be in the mempool it must be spent by another tx)  with the
> additional rule that the parent must have a higher feerate after CPFP'ing
> the parent than the parent alone we can both:
>
> 1) Allow 0 value outputs for things like Anchor Outputs (very good for not
> getting your eltoo/Decker channels pinned by junk witness data using Anchor
> Inputs, very good for not getting your channels drained by at-dust outputs)
> 2) Not allow 0 value utxos to proliferate long
> 3) It still being valid for a 0 value that somehow gets created to be
> spent by the fee paying txn later
>
> Just doing this as a mempool policy also has the benefits of not
> introducing any new validation rules. Although in general the IUTXO concept
> is very attractive, it complicates mempool :(
>
> I understand this may also be really helpful for CTV based contracts (like
> vault continuation hooks) as well as things like spacechains.
>
> Such a rule -- if it's not clear -- presupposes a fully working package
> relay system.
>
> I believe that this addresses all the issues with allowing 0 value outputs
> to be created for the narrow case of immediately spendable outputs.
>
> Cheers,
>
> Jeremy
>
> p.s. why another post today? Thank Greg
> https://twitter.com/JeremyRubin/status/1468390561417547780
>
>
> --
> @JeremyRubin <https://twitter.com/JeremyRubin>
> <https://twitter.com/JeremyRubin>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211208/58427839/attachment.html>

From rsomsen at gmail.com  Wed Dec  8 10:46:22 2021
From: rsomsen at gmail.com (Ruben Somsen)
Date: Wed, 8 Dec 2021 11:46:22 +0100
Subject: [bitcoin-dev] [Lightning-dev]  Take 2: Removing the Dust Limit
In-Reply-To: <CACdvm3Oynv4gWdaGXATxc3SoYDD8kuiPq-d9F2itsmayP0qeZQ@mail.gmail.com>
References: <CAD5xwhid2OH0GzXPvqWgsMag4J9zidsewEquT-JoOweVD5pxZg@mail.gmail.com>
 <CACdvm3Oynv4gWdaGXATxc3SoYDD8kuiPq-d9F2itsmayP0qeZQ@mail.gmail.com>
Message-ID: <CAPv7TjZBU2v2Nfw2_8Qz33rUWKJ=uJ7u+_5tFxjM94mk=RnmOA@mail.gmail.com>

Hi Jeremy,

I brought up the exact same thing at coredev, but unfortunately I came up
with a way in which the 0 sat output could still enter the UTXO set under
those rules:

- Parent P1 (0 sat per byte) has 2 outputs, one is 0 sat
- Child C1 spends the 0 sat output for a combined feerate of 1 sat per byte
and they enter the mempool as a package
- Child C2 spends the other output of P1 with a really high feerate and
enters the mempool
- Fees rise and child C1 falls out of the mempool, leaving the 0 sat output
unspent

For this to not be a problem, the 0 sat output needs to provably be the
only spendable output. As you pointed out to me a few days ago, having a
relative timelock on the other outputs would do the trick (and this happens
to be true for spacechains), but that will only be provable if all script
conditions are visible prior to spending time (ruling out p2sh and taproot,
and conflicting with standardness rules for transactions).

It's worth noting out that you can't really make a policy rule that says
the 0 sat output can't be left unspent (i.e. C1 can't be evicted without
also evicting P1), as this would not mirror economically rational behavior
for miners (they would get more fees if they evicted C1, so we must assume
they will, if the transaction ever reaches them).

This last example really points out the tricky situation we're dealing
with. In my opinion, we'd only want to relay 0 sat outputs if we can
guarantee that it's never economically profitable to mine them without them
getting spent in the same block.

Finally, here's a timestamped link to a diagram that shows where 0 sat
outputs would be helpful for spacechains (otherwise someone would have to
pay the dust up front for countless outputs):
https://youtu.be/N2ow4Q34Jeg?t=2556

Cheers,
Ruben




On Wed, Dec 8, 2021 at 9:35 AM Bastien TEINTURIER <bastien at acinq.fr> wrote:

> Hi Jeremy,
>
> Right now, lightning anchor outputs use a 330 sats amount. Each commitment
> transaction has two such outputs, and only one of them is spent to help the
> transaction get confirmed, so the other stays there and bloats the utxo
> set.
> We allow anyone to spend them after a csv of 16 blocks, in the hope that
> someone will claim a batch of them when the fees are low and remove them
> from the utxo set. However, that trick wouldn't work with 0-value outputs,
> as
> no-one would ever claim them (doesn't make economical sense).
>
> We actually need to have two of them to avoid pinning: each participant is
> able to spend only one of these outputs while the parent tx is unconfirmed.
> I believe N-party protocols would likely need N such outputs (not sure).
>
> You mention a change to the carve-out rule, can you explain it further?
> I believe it would be a necessary step, otherwise 0-value outputs for
> CPFP actually seem worse than low-value ones...
>
> Thanks,
> Bastien
>
> Le mer. 8 d?c. 2021 ? 02:29, Jeremy via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> a ?crit :
>
>> Bitcoin Devs (+cc lightning-dev),
>>
>> Earlier this year I proposed allowing 0 value outputs and that was shot
>> down for various reasons, see
>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-August/019307.html
>>
>> I think that there can be a simple carve out now that package relay is
>> being launched based on my research into covenants from 2017
>> https://rubin.io/public/pdfs/multi-txn-contracts.pdf.
>>
>> Essentially, if we allow 0 value outputs BUT require as a matter of
>> policy (or consensus, but policy has major advantages) that the output be
>> used as an Intermediate Output (that is, in order for the transaction to be
>> creating it to be in the mempool it must be spent by another tx)  with the
>> additional rule that the parent must have a higher feerate after CPFP'ing
>> the parent than the parent alone we can both:
>>
>> 1) Allow 0 value outputs for things like Anchor Outputs (very good for
>> not getting your eltoo/Decker channels pinned by junk witness data using
>> Anchor Inputs, very good for not getting your channels drained by at-dust
>> outputs)
>> 2) Not allow 0 value utxos to proliferate long
>> 3) It still being valid for a 0 value that somehow gets created to be
>> spent by the fee paying txn later
>>
>> Just doing this as a mempool policy also has the benefits of not
>> introducing any new validation rules. Although in general the IUTXO concept
>> is very attractive, it complicates mempool :(
>>
>> I understand this may also be really helpful for CTV based contracts
>> (like vault continuation hooks) as well as things like spacechains.
>>
>> Such a rule -- if it's not clear -- presupposes a fully working package
>> relay system.
>>
>> I believe that this addresses all the issues with allowing 0 value
>> outputs to be created for the narrow case of immediately spendable outputs.
>>
>> Cheers,
>>
>> Jeremy
>>
>> p.s. why another post today? Thank Greg
>> https://twitter.com/JeremyRubin/status/1468390561417547780
>>
>>
>> --
>> @JeremyRubin <https://twitter.com/JeremyRubin>
>> <https://twitter.com/JeremyRubin>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
> _______________________________________________
> Lightning-dev mailing list
> Lightning-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211208/7634150b/attachment-0001.html>

From darosior at protonmail.com  Wed Dec  8 14:51:25 2021
From: darosior at protonmail.com (darosior)
Date: Wed, 08 Dec 2021 14:51:25 +0000
Subject: [bitcoin-dev] A fee-bumping model
In-Reply-To: <CAFXO6=KESTUAoHt0ZeizctEwPqFPaQs_e-NCG+i6-3RDbRPz+A@mail.gmail.com>
References: <hBx6OYA5Mv9C_anoMQ-s-9l_XNwNFPfDVmOND9pXBJEBi7qsULF3bgPGpagtqjOsKDTXu8iOTVzvOjflz-M6EfnfwVH81Cu-nnai0kakouo=@protonmail.com>
 <CALZpt+F6h8uLw48e4FRrkjPe2ci6Uqy-o9H=++hu5fx7+bxOZw@mail.gmail.com>
 <8wtAeG1p6qyiOWW0pIJP06_h-3ro7UTBsNO-0BMxLnSKUU6xFBMEvhyQGhjsh3gvQAjDpFajGEC0C6NSQ0Nfj8KtT1cGlaQMW_nnEkAuozM=@protonmail.com>
 <CAFXO6=KESTUAoHt0ZeizctEwPqFPaQs_e-NCG+i6-3RDbRPz+A@mail.gmail.com>
Message-ID: <xa9H5xpIDvshinr4ql6G3F_EsCEQ3VqxC6GgXgHIABmJnUsXi8CjBVjTN3Oabgp5vjov2Y3PgZyVslRHjl4Y2XFw2OLOe10r4Bj9stXLeZA=@protonmail.com>

Hi Gloria,

I agree with regard to the RBF changes. To me, we should (obviously?) do ancestor feerate instead of requiring confirmed inputs (#23121).
However, we are yet to come with a reasonable policy-only fix to "rule 3 pinning".

Responses inline!

>> The part of Revault we are interested in for this study is the delegation process, and more
>> specifically the application of spending policies by network monitors (watchtowers).
>
> I'd like to better understand how fee-bumping would be used, i.e. how the watchtower model works:
> - Do all of the vault parties both deposit to the vault and a refill/fee to the watchtower, is there a reward the watchtower collects for a successful Cancel, or something else? (Apologies if there's a thorough explanation somewhere that I haven't already seen).
> - Do we expect watchtowers tracking multiple vaults to be batching multiple Cancel transaction fee-bumps?
> - Do we expect vault users to be using multiple watchtowers for a better trust model? If so, and we're expecting batched fee-bumps, won't those conflict?

Grossly, it could be described as "enforce spending policied on 10BTC worth of delegated coins by allocating 5mBTC to 3 different watchtowers".
Each participant that will be delegating coins is expected to run a number of watchtowers. They should ideally be replicated (for full disclosure if
it wasn't obvious providing replication is the business model of the company behind the Revault project :p).
You can't batch fee-bumps as you *could* (maybe not *would*) do with anchor outputs channels, since the latter use CPFP and we "sponsor"
(or whatever the name of "supplementing the fees of a transaction by adding inputs" is).
In the current model, watchtowers enforcing the same policy do compete in that they broadcast conflicting transactions since they attach different
fee-bumping inputs. Ideally we could sign the added feebumping inputs themselves with ACP so they are allowed to cooperate. However doing that
would allow anyone on the network to snip the added fees to perform a "RBF rule-3 pinning".
Finally, there could be concerns around game theory of letting others' watchtowers feebump for you. I'm convinced however that in our case the fee
is completely dwarfed by the value at stake. Trying to delay your own WT's fee-bump reaction to hope someone else will pay the 10k sats for enforcing
a 1BTC contract, hmmm, i wouldn't do that.

>> For Revault we can afford to introduce malleability in the Cancel transaction since there is no
>> second-stage transaction depending on its txid. Therefore it is pre-signed with ANYONECANPAY. We
>> can't use ANYONECANPAY|SINGLE since it would open a pinning vector [3]. Note how we can't leverage
>> the carve out rule, and neither can any other more-than-two-parties contract.
>
> We've already talked about this offline, but I'd like to point out here that even transactions signed with ANYONECANPAY|ALL can be pinned by RBF unless we add an ancestor score rule. [0], [1] (numbers are inaccurate, Cancel Tx feerates wouldn't be that low, but just to illustrate what the attack would look like)

Thanks for expliciting that, i should have mentioned it. For everyone reading the PR is at https://github.com/bitcoin/bitcoin/pull/23121 .

>> can't use ANYONECANPAY|SINGLE since it would open a pinning vector [3]. Note how we can't leverage
>> the carve out rule, and neither can any other more-than-two-parties contract.
>
> Well stated about CPFP carve out. I suppose the generalization is that allowing n extra ancestorcount=2 descendants to a transaction means it can help contracts with <=n+1 parties (more accurately, outputs)? I wonder if it's possible to devise a different approach for limiting ancestors/descendants, e.g. by height/width/branching factor of the family instead of count... :shrug:

I don't think so, because you want any party involved in the contract to be able to unilaterally enforce it. With >2 anchor outputs any 2-parties can
collude against the other one(s) by pinning the transaction using the first party's output to hit the descendant chain limit and the second one to trigger
the carve-out.

Ideally i think it'd be better that all contracts move toward using sponsoring ("tx malleation") when we can (ie for all transactions that are at the end of
the chain, or post-ANYPREVOUT any transaction really) instead of CPFP for fee-bumping because:
1. It's way easier to reason about wrt mempool DOS protections (the fees don't depend on a chain of childs, it's just a function of the transaction alone)
2. It's more space efficient (and thereby economical): you don't need to create a new transaction (or a set of new txs) to bump your fees.

Unfortunately, having to use ACP instead of ACP|SINGLE is a showstopper. Managing a fee-bumping UTxO pool is a massive burden.
On a side note, thinking back about ACP|SINGLE vs ACP i'm not so sure anymore the latter opens up more pinning vectors than the former..

> IIUC, a Cancel transaction can be generalized as a 1-in-1-out where the input is presigned with counterparties, SIGHASH_ANYONECANPAY. The fan-out UTXO pool approach is a clever solution. I also think this smells like a case where improving lower-level RBF rules is more appropriate than requiring applications to write workarounds and generate extra transactions. Seeing that the BIP125#2 (no new unconfirmed inputs) restriction really hurts in this case, if that rule were removed, would you be able to simply keep the 1 big UTXO per vault and cut out the exact nValue you need to fee-bump Cancel transactions? Would that feel less like "burning" for the sake of fee-bumping?

I am not sure. It's a question i raised when i was made aware of your finding of the "no unconfirmed" rule defect and your proposal to move to ancestor
score instead. Without further consideration i'd say yes, but this needs more research. I'm also biased as i really want to get rid of this coin pool for both
the complexity and the social cost..

>> First of all, when to fee-bump? At fixed time intervals? At each block connection? It sounds like,
>> given a large enough timelock, you could try to greed by "trying your luck" at a lower feerate and
>> only re-bumping every N blocks. You would then start aggressively bumping at every block after M
>> blocks have passed.
>
> I'm wondering if you also considered other questions like:
> - Should a fee-bumping strategy be dependent upon the rate of incoming transactions? To me, it seems like the two components are (1) what's in the mempool and (2) what's going to trickle into the mempool between now and the target block. The first component is best-effort keeping incentive-compatible mempool; historical data and crystal ball look like the only options for incorporating the 2nd component.
> - Should the fee-bumping strategy depend on how close you are to your timelock expiry? (though this seems like a potential privacy leak, and the game theory could get weird as you mentioned).
> - As long as you have a good fee estimator (i.e. given a current mempool, can get an accurate feerate given a % probability of getting into target block n), is there any reason to devise a fee-bumping strategy beyond picking a time interval?

I think (again, ideally) applications should take `estimatesmarfee` as a black box, and not look into the mempool by themselves. Now whether we should
take into account the mempool data for short target estimation, i don't know. The first issue that comes to mind is how to measure whether your mempool
is "up-to-date" (i mean if you have most of the current unconfirmed transactions). Weak blocks were mentionned elsewhere, and i think they can help for
this (you don't influence your estimate by the rate of new unconfirmed transactions you hear about, but what miners are currently working on). Now, sure,
the expected time before the next block would be 10min. But for a short target estimate it still seems better to base your estimate on the most up-to-date
data you can get? (maybe not? Can a statistician chime in?)

This section was about arguing that it doesn't make sense to start low and get to next-block feerate as you approach your timelock expiration. Is your
question about whether it makes sense to start, as you get closer to timelock maturation, feebumping not on the basis of what your fee estimator gives
you but blindly i don't believe it does. If all you have is a fee-bumping method, all confirmation problems look like a fee-paying one :p.
You are already assuming your fee estimator is working and isn't being manipulated. If it does, and you don't get confirmed after X blocks and as many
re-bumping attempts the problem is elsewhere imo.

> It would be interesting to see stats on the spread of feerates in blocks during periods of fee fluctuation.
>
>> > In the event that you notice a consequent portion of the block is filled with transactions paying
>> > less than your own, you might want to start panicking and bump your transaction fees by a certain
>> > percentage with no consideration for your fee estimator. You might skew miners incentives in doing
>> > so: if you increase the fees by a factor of N, any miner with a fraction larger than 1/N of the
>> > network hashrate now has an incentive to censor your transaction at first to get you to panic.
>
>> Yes I think miner-harvesting attacks should be weighed carefully in the design of offchain contracts fee-bumping strategies, at least in the future when the mining reward exhausts further.
>
> Miner-harvesting (such cool naming!) is interesting, but I want to clarify the value of N - I don't think it's the factor by which you increase the fees on just your transaction.
> To codify: your transaction pays a fee of `f1` right now and might pay a fee of `f2` in a later block that the miner expects to mine with 1/N probability. The economically rational miner isn't incentivized if simply `f2 = N * f1` unless their mempool is otherwise empty.
> By omitting your transaction in this block, the miner can include another transaction/package paying `g1` fees instead, so they lose `f1-g1` in fees right now. In the future block, they have the choice between collecting `f2` or `g2` (from another transaction/package) in fees, so their gain is `max(f2-g2, 0)`.
> So the equation is more like: a miner with 1/N of the hashrate, employing this censorship strategy, gains only if `max(f2-g2, 0) > N * (f1-g1)`. More broadly, the miner only profits if `f2` is significantly higher than `g2` and `f1` is about the same feerate as everything else in your mempool: it seems like they're betting on how much you _overshoot_, not how much you bump.

Right. I was talking in the worst case where they don't have a replacement package with a feerate of `g1`. They are even more incentivized to try that if they do.
Since `f1` is already expected to be the next block feerate, by how much you bump is technically by how much your overshoot. So much for dismissing your fee
estimator!

> Slightly related question: in contracts, generally, the timelock deadline is revealed in the script, so the miner knows how "desperate" we are right?

For P2WSH, yes.

> Is that a problem?

I don't think so. As long as we don't bump the feerate by the N factor mentioned above, they have an incentive to try to take the fees while they still can (or someone else will).

> For Revault, if your Cancel transaction is a keypath spend (I think I remember reading that somewhere?) and you don't reveal the script, they don't see your timelock deadline yes?

It *could* be once we move to Taproot (2weeks). Yep! They would only know about it on a successful spend which would reveal the branch with the timelock. It's
a good point in that the attack above could be made impractical through privacy. Although i don't think it's realistic: due to the necessary script-path spends it would
be trivial to cluster coins managed by a Revault setup and deduce whether a given transaction is a Cancel with very high accuracy.

> Again, thanks for the digging and sharing. :)

Thanks for the interest and getting me to re-think through this!

Best,
Antoine

> Best,
> Gloria
>
> On Tue, Nov 30, 2021 at 3:27 PM darosior via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> Hi Antoine,
>>
>> Thanks for your comment. I believe for Lightning it's simpler with regard to the management of the UTxO pool, but harder with regard to choosing
>> a threat model.
>> Responses inline.
>>
>>> For any opened channel, ensure the confirmation of a Commitment transaction and the children HTLC-Success/HTLC-Timeout transactions. Note, in the Lightning security game you have to consider (at least) 4 types of players moves and incentives : your node, your channel counterparties, the miners, the crowd of bitcoin users. The number of the last type of players is unknown from your node, however it should not be forgotten you're in competition for block space, therefore their block demands bids should be anticipated and reacted to in consequence. With that remark in mind, implications for your LN fee-bumping strategy will be raised afterwards.
>>>
>>> For a LN service provider, on-chain overpayments are bearing on your operational costs, thus downgrading your economic competitiveness. For the average LN user, overpayment might price out outside a LN non-custodial deployment, as you don't have the minimal security budget to be on your own.
>>
>> I think this problem statement can be easily generalised to any offchain contract. And your points stand for all of them.
>> "For any opened contract, ensure at any point the confirmation of a (set of) transaction(s) in a given number of blocks"
>>
>>> Same issue with Lightning, we can be pinned today on the basis of replace-by-fee rule 3. We can be also blinded by network mempool partitions, a pinning counterparty can segregate all the full-nodes in as many subsets by broadcasting a revoked Commitment transaction different for each. For Revault, I think you can also do unlimited partitions by mutating the ANYONECANPAY-input of the Cancel.
>>
>> Well you can already do unlimited partitions by adding different inputs to it. You could malleate the witness, but since we are using Miniscript i'm confident you would only be able in a marginal way.
>>
>>> That said, if you have a distributed towers deployment, spread across the p2p network topology, and they can't be clustered together through cross-layers or intra-layer heuristics, you should be able to reliably observe such partitions. I think such distributed monitors are deployed by few L1 merchants accepting 0-conf to detect naive double-spend.
>>
>> We should aim to more than 0-conf (in)security level..
>> It seems to me the only policy-level mitigation for RBF pinning around the "don't decrease the abolute fees of a less-than-a-block mempool" would be to drop the requirement on increasing absolute fees if the mempool is "full enough" (and the feerate increases exponentially, of course).
>> Another approach could be by introducing new consensus rules as proposed by Jeremy last year [0]. If we go in the realm of new consensus rules, then i think that simply committing to a maximum tx size would fix pinning by RBF rule 3. Could be in the annex, or in the unused sequence bits (although they currently are by Lightning, meh). You could also check in the output script that the input commits to this.
>>
>> [0] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-September/018168.html
>>
>>> Have we already discussed a fee-bumping "shared cache", a CPFP variation ? Strawman idea: Alice and Bob commit collateral inputs to a separate UTXO from the main "offchain contract" one. This UTXO is locked by a multi-sig. For any Commitment transaction pre-signed, also counter-sign a CPFP with top mempool feerate included, spending a Commitment anchor output and the shared-cache UTXO. If the fees spike, you can re-sign a high-feerate CPFP, assuming interactivity. As the CPFP is counter-signed by everyone, the outputs can be CSV-1 encumbered to prevent pinnings. If the share-cache is feeded at parity, there shouldn't be an incentive to waste or maliciously inflate the feerate. I think this solution can be easily generalized to more than 2 counterparties by using a multi-signature scheme. Big issue, if the feerate is short due to fee spikes and you need to re-sign a higher-feerate CPFP, you're trusting your counterparty to interact, though arguably not worse than the current update fee mechanism.
>>
>> It really looks just like `update_fee`. Except maybe with the property that you have the channel liquidity not depend on the onchain feerate.
>> In any case, for Lightning i think it's a bad idea to re-introduce trust on this side post anchor outputs. For Revault it's clearly out of the question to introduce trust in your counterparties (why would you bother having a fee-bumping mechanism in the first place then?). Probably the same holds for all offchain contracts.
>>
>>>> For Lightning, it'd mean keeping an equivalent amount of funds as the sum of all your
>>> channels balances sitting there unallocated "just in case". This is not reasonable.
>>>
>>> Agree, game-theory wise, you would like to keep a full fee-bumping reserve, ready to burn as much in fees as the contested HTLC value, as it's the maximum gain of your counterparty. Though perfect equilibrium is hard to achieve because your malicious counterparty might have an edge pushing you to broadcast your Commitment first by witholding HTLC resolution.
>>>
>>> Fractional fee-bumping reserves are much more realistic to expect in the LN network. Lower fee-bumping reserve, higher liquidity deployed, in theory higher routing fees. By observing historical feerates, average offchain balances at risk and routing fees expected gains, you should be able to discover an equilibrium where higher levels of reserve aren't worth the opportunity cost. I guess this equilibrium could be your LN fee-bumping reserve max feerate.
>>>
>>> Note, I think the LN approach is a bit different from what suits a custody protocol like Revault, as you compute a direct return of the frozen fee-bumping liquidity. With Revault, if you have numerous bitcoins protected, it's might be more interesting to adopt a "buy the mempool, stupid" strategy than risking fund safety for few percentages of interest returns.
>>
>> True for routing nodes. For wallets (if receiving funds), it's not about an investment: just users expectations to being able to transact without risking to lose their funds (ie being able to enforce their contract onchain). Although wallets they are much less at risk.
>>
>>> This is where the "anticipate the crowd of bitcoin users move" point can be laid out. As the crowd of bitcoin users' fee-bumping reserves are ultimately unknown from your node knowledge, you should be ready to be a bit more conservative than the vanilla fee-bumping strategies shipped by default. In case of massive mempool congestion, your additional conservatism might get your time-sensitive transactions and game on the crowd of bitcoin users. First Problem: if all offchain bitcoin software adopt that strategy we might inflate the worst-case feerate rate at the benefit of the miners, without holistically improving block throughput. Second problem : your class of offchain bitcoin softwares might have ridiculous fee-bumping reserve compared
>>> to other classes of offchain bitcoin softwares (Revault > Lightning) and just be priced out bydesign in case of mempool congestion. Third problem : as the number of offchain bitcoin applications should go up with time, your fee-bumping reserve levels based from historical data might be always late by one "bank-run" scenario.
>>
>> Black swan event 2.0? Just rule n?3 is inherent to any kind of fee estimation.
>>
>>> For Lightning, if you're short in fee-bumping reserves you might still do preemptive channel closures, either cooperatively or unilaterally and get back the off-chain liquidity to protect the more economically interesting channels. Though again, that kind of automatic behavior might be compelling at the individual node-level, but make the mempol congestion worse holistically.
>>
>> Yeah so we are back to the "fractional reserve" model: you can only enforce X% of the offchain contracts your participate in.. Actually it's even an added assumption: that you still have operating contracts, with honest counterparties.
>>
>>> In case of massive mempool congestion, you might try to front-run the crowd of bitcoin users relying on block connections for fee-bumping, and thus start your fee-bumping as soon as you observe feerate groups fluctuations in your local mempool(s).
>>
>> I don't think any kind of mempool-based estimate generalizes well, since at any point the expected time before the next block is 10 minutes (and a lot can happen in 10min).
>>
>>> Also you might proceed your fee-bumping ticks on a local clock instead of block connections in case of time-dilation or deeper eclipse attacks of your local node. Your view of the chain might be compromised but not your ability to broadcast transactions thanks to emergency channels (in the non-LN sense...though in fact quid of txn wrapped in onions ?) of communication.
>>
>> Oh, yeah, i didn't explicit "not getting eclipsed" (or more generally "data availability") as an assumption since it's generally one made by participants of any offchain contract. In this case you can't even have decent fee estimation, so you are screwed anyways.
>>
>>> Yes, stay open the question on how you enforce this block insurance market. Reputation, which might be to avoid due to the latent centralization effect, might be hard to stack and audit reliably for an emergency mechanism running, hopefully, once in a halvening period. Maybe maybe some cryptographic or economically based mechanism on slashing or swaps could be found...
>>
>> Unfortunately, given current mining centralisation, pools are in a very good position to offer pretty decent SLAs around that. With a block space insurance, you of course don't need all these convoluted fee-bumping hacks.
>> I'm very concerned that large stakeholders of the "offchain contracts ecosystem" would just go this (easier) way and further increase mining centralisation pressure.
>>
>> I agree that a cryptography-based scheme around this type of insurance services would be the best way out.
>>
>>> Antoine
>>>
>>> Le lun. 29 nov. 2021 ? 09:34, darosior via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> a ?crit :
>>>
>>>> Hi everyone,
>>>>
>>>> Fee-bumping is paramount to the security of many protocols building on Bitcoin, as they require the
>>>> confirmation of a transaction (which might be presigned) before the expiration of a timelock at any
>>>> point after the establishment of the contract.
>>>>
>>>> The part of Revault using presigned transactions (the delegation from a large to a smaller multisig)
>>>> is no exception. We have been working on how to approach this for a while now and i'd like to share
>>>> what we have in order to open a discussion on this problem so central to what seem to be The Right
>>>> Way [0] to build on Bitcoin but which has yet to be discussed in details (at least publicly).
>>>>
>>>> I'll discuss what we came up with for Revault (at least for what will be its first iteration) but my
>>>> intent with posting to the mailing list is more to frame the questions to this problem we are all
>>>> going to face rather than present the results of our study tailored to the Revault usecase.
>>>> The discussion is still pretty Revault-centric (as it's the case study) but hopefully this can help
>>>> future protocol designers and/or start a discussion around what everyone's doing for existing ones.
>>>>
>>>> ## 1. Reminder about Revault
>>>>
>>>> The part of Revault we are interested in for this study is the delegation process, and more
>>>> specifically the application of spending policies by network monitors (watchtowers).
>>>> Coins are received on a large multisig. Participants of this large multisig create 2 [1]
>>>> transactions. The Unvault, spending a deposit UTxO, creates an output paying either to the small
>>>> multisig after a timelock or to the large multisig immediately. The Cancel, spending the Unvault
>>>> output through the non-timelocked path, creates a new deposit UTxO.
>>>> Participants regularly exchange the Cancel transaction signatures for each deposit, sharing the
>>>> signatures with the watchtowers they operate. They then optionally [2] sign the Unvault transaction
>>>> and share the signatures with the small multisig participants who can in turn use them to proceed
>>>> with a spending. Watchtowers can enforce spending policies (say, can't Unvault outside of business
>>>> hours) by having the Cancel transaction be confirmed before the expiration of the timelock.
>>>>
>>>> ## 2. Problem statement
>>>>
>>>> For any delegated vault, ensure the confirmation of a Cancel transaction in a configured number of
>>>> blocks at any point. In so doing, minimize the overpayments and the UTxO set footprint. Overpayments
>>>> increase the burden on the watchtower operator by increasing the required frequency of refills of the
>>>> fee-bumping wallet, which is already the worst user experience. You are likely to manage a number of
>>>> UTxOs with your number of vaults, which comes at a cost for you as well as everyone running a full
>>>> node.
>>>>
>>>> Note that this assumes miners are economically rationale, are incentivized by *public* fees and that
>>>> you have a way to propagate your fee-bumped transaction to them. We also don't consider the block
>>>> space bounds.
>>>>
>>>> In the previous paragraph and the following text, "vault" can generally be replaced with "offchain
>>>> contract".
>>>>
>>>> ## 3. With presigned transactions
>>>>
>>>> As you all know, the first difficulty is to get to be able to unilaterally enforce your contract
>>>> onchain. That is, any participant must be able to unilaterally bump the fees of a transaction even
>>>> if it was co-signed by other participants.
>>>>
>>>> For Revault we can afford to introduce malleability in the Cancel transaction since there is no
>>>> second-stage transaction depending on its txid. Therefore it is pre-signed with ANYONECANPAY. We
>>>> can't use ANYONECANPAY|SINGLE since it would open a pinning vector [3]. Note how we can't leverage
>>>> the carve out rule, and neither can any other more-than-two-parties contract.
>>>> This has a significant implication for the rest, as we are entirely burning fee-bumping UTxOs.
>>>>
>>>> This opens up a pinning vector, or at least a significant nuisance: any other party can largely
>>>> increase the absolute fee without increasing the feerate, leveraging the RBF rules to prevent you
>>>> from replacing it without paying an insane fee. And you might not see it in your own mempool and
>>>> could only suppose it's happening by receiving non-full blocks or with transactions paying a lower
>>>> feerate.
>>>> Unfortunately i know of no other primitive that can be used by multi-party (i mean, >2) presigned
>>>> transactions protocols for fee-bumping that aren't (more) vulnerable to pinning.
>>>>
>>>> ## 4. We are still betting on future feerate
>>>>
>>>> The problem is still missing one more constraint. "Ensuring confirmation at any time" involves ensuring
>>>> confirmation at *any* feerate, which you *cannot* do. So what's the limit? In theory you should be ready
>>>> to burn as much in fees as the value of the funds you want to get out of the contract. So... For us
>>>> it'd mean keeping for each vault an equivalent amount of funds sitting there on the watchtower's hot
>>>> wallet. For Lightning, it'd mean keeping an equivalent amount of funds as the sum of all your
>>>> channels balances sitting there unallocated "just in case". This is not reasonable.
>>>>
>>>> So you need to keep a maximum feerate, above which you won't be able to ensure the enforcement of
>>>> all your contracts onchain at the same time. We call that the "reserve feerate" and you can have
>>>> different strategies for choosing it, for instance:
>>>> - The 85th percentile over the last year of transactions feerates
>>>> - The maximum historical feerate
>>>> - The maximum historical feerate adjusted in dollars (makes more sense but introduces a (set of?)
>>>> trusted oracle(s) in a security-critical component)
>>>> - Picking a random high feerate (why not? It's an arbitrary assumption anyways)
>>>>
>>>> Therefore, even if we don't have to bet on the broadcast-time feerate market at signing time anymore
>>>> (since we can unilaterally bump), we still need some kind of prediction in preparation of making
>>>> funds available to bump the fees at broadcast time.
>>>> Apart from judging that 500sat/vb is probably more reasonable than 10sat/vbyte, this unfortunately
>>>> sounds pretty much crystal-ball-driven.
>>>>
>>>> We currently use the maximum of the 95th percentiles over 90-days windows over historical block chain
>>>> feerates. [4]
>>>>
>>>> ## 5. How much funds does my watchtower need?
>>>>
>>>> That's what we call the "reserve". Depending on your reserve feerate strategy it might vary over
>>>> time. This is easier to reason about with a per-contract reserve. For Revault it's pretty
>>>> straightforward since the Cancel transaction size is static: `reserve_feerate * cancel_size`. For
>>>> other protocols with dynamic transaction sizes (or even packages of transactions) it's less so. For
>>>> your Lightning channel you would probably take the maximum size of your commitment transaction
>>>> according to your HTLC exposure settings + the size of as many `htlc_success` transaction?
>>>>
>>>> Then you either have your software or your user guesstimate how many offchain contracts the
>>>> watchtower will have to watch, time that by the per-contract reserve and refill this amount (plus
>>>> some slack in practice). Once again, a UX tradeoff (not even mentioning the guesstimation UX):
>>>> overestimating leads to too many unallocated funds sitting on a hot wallet, underestimating means
>>>> (at best) inability to participate in new contracts or being "at risk" (not being able to enforce
>>>> all your contracts onchain at your reserve feerate) before a new refill.
>>>>
>>>> For vaults you likely have large-value UTxOs and small transactions (the Cancel is one-in one-out in
>>>> Revault). For some other applications with large transactions and lower-value UTxOs on average it's
>>>> likely that only part of the offchain contracts might be enforceable at a reasonable feerate. Is it
>>>> reasonable?
>>>>
>>>> ## 6. UTxO pool layout
>>>>
>>>> Now that you somehow managed to settle on a refill amount, how are you going to use these funds?
>>>> Also, you'll need to manage your pool across time (consolidating small coins, and probably fanning
>>>> out large ones).
>>>>
>>>> You could keep a single large UTxO and peel it as you need to sponsor transactions. But this means
>>>> that you need to create a coin of a specific value according to your need at the current feerate
>>>> estimation, hope to have it confirmed in a few blocks (at least for now! [5]), and hope that the
>>>> value won't be obsolete by the time it confirmed. Also, you'd have to do that for any number of
>>>> Cancel, chaining feebump coin creation transactions off the change of the previous ones or replacing
>>>> them with more outputs. Both seem to become really un-manageable (and expensive) in many edge-cases,
>>>> shortening the time you have to confirm the actual Cancel transaction and creating uncertainty about
>>>> the reserve (how much is my just-in-time fanout going to cost me in fees that i need to refill in
>>>> advance on my watchtower wallet?).
>>>> This is less of a concern for protocols using CPFP to sponsor transactions, but they rely on a
>>>> policy rule specific to 2-parties contracts.
>>>>
>>>> Therefore for Revault we fan-out the coins per-vault in advance. We do so at refill time so the
>>>> refiller can give an excess to pay for the fees of the fanout transaction (which is reasonable since
>>>> it will occur just after the refilling transaction confirms). When the watchtower is asked to watch
>>>> for a new delegated vault it will allocate coins from the pool of fanned-out UTxOs to it (failing
>>>> that, it would refuse the delegation).
>>>> What is a good distribution of UTxOs amounts per vault? We want to minimize the number of coins,
>>>> still have coins small enough to not overpay (remember, we can't have change) and be able to bump a
>>>> Cancel up to the reserve feerate using these coins. The two latter constraints are directly in
>>>> contradiction as the minimal value of a coin usable at the reserve feerate (paying for its own input
>>>> fee + bumping the feerate by, say, 5sat/vb) is already pretty high. Therefore we decided to go with
>>>> two distributions per vault. The "reserve distribution" alone ensures that we can bump up to the
>>>> reserve feerate and is usable for high feerates. The "bonus distribution" is not, but contains
>>>> smaller coins useful to prevent overpayments during low and medium fee periods (which is most of the
>>>> time).
>>>> Both distributions are based on a basic geometric suite [6]. Each value is half the previous one.
>>>> This exponentially decreases the value, limiting the number of coins. But this also allows for
>>>> pretty small coins to exist and each coin's value is equal to the sum of the smaller coins,
>>>> or smaller by at most the value of the smallest coin. Therefore bounding the maximum overpayment to
>>>> the smallest coin's value [7].
>>>>
>>>> For the management of the UTxO pool across time we merged the consolidation with the fanout. When
>>>> fanning out a refilled UTxO, we scan the pool for coins that need to be consolidated according to a
>>>> heuristic. An instance of a heuristic is "the coin isn't allocated and would not have been able to
>>>> increase the fee at the median feerate over the past 90 days of blocks".
>>>> We had this assumption that feerate would tend to go up with time and therefore discarded having to
>>>> split some UTxOs from the pool. We however overlooked that a large increase in the exchange price of
>>>> BTC as we've seen during the past year could invalidate this assumption and that should arguably be
>>>> reconsidered.
>>>>
>>>> ## 7. Bumping and re-bumping
>>>>
>>>> First of all, when to fee-bump? At fixed time intervals? At each block connection? It sounds like,
>>>> given a large enough timelock, you could try to greed by "trying your luck" at a lower feerate and
>>>> only re-bumping every N blocks. You would then start aggressively bumping at every block after M
>>>> blocks have passed. But that's actually a bet (in disguised?) that the next block feerate in M blocks
>>>> will be lower than the current one. In the absence of any predictive model it is more reasonable to
>>>> just start being aggressive immediately.
>>>> You probably want to base your estimates on `estimatesmartfee` and as a consequence you would re-bump
>>>> (if needed )after each block connection, when your estimates get updated and you notice your
>>>> transaction was not included in the block.
>>>>
>>>> In the event that you notice a consequent portion of the block is filled with transactions paying
>>>> less than your own, you might want to start panicking and bump your transaction fees by a certain
>>>> percentage with no consideration for your fee estimator. You might skew miners incentives in doing
>>>> so: if you increase the fees by a factor of N, any miner with a fraction larger than 1/N of the
>>>> network hashrate now has an incentive to censor your transaction at first to get you to panic. Also
>>>> note this can happen if you want to pay the absolute fees for the 'pinning' attack mentioned in
>>>> section #2, and that might actually incentivize miners to perform it themselves..
>>>>
>>>> The gist is that the most effective way to bump and rebump (RBF the Cancel tx) seems to just be to
>>>> consider the `estimatesmartfee 2 CONSERVATIVE` feerate at every block your tx isn't included in, and
>>>> to RBF it if the feerate is higher.
>>>> In addition, we fallback to a block chain based estimation when estimates aren't available (eg if
>>>> the user stopped their WT for say a hour and we come back up): we use the 85th percentile over the
>>>> feerates in the last 6 blocks. Sure, miners can try to have an influence on that by stuffing their
>>>> blocks with large fee self-paying transactions, but they would need to:
>>>> 1. Be sure to catch a significant portion of the 6 blocks (at least 2, actually)
>>>> 2. Give up on 25% of the highest fee-paying transactions (assuming they got the 6 blocks, it's
>>>> proportionally larger and incertain as they get less of them)
>>>> 3. Hope that our estimator will fail and we need to fall back to the chain-based estimation
>>>>
>>>> ## 8. Our study
>>>>
>>>> We essentially replayed the historical data with different deployment configurations (number of
>>>> participants and timelock) and probability of an event occurring (event being say an Unvault, an
>>>> invalid Unvault, a new delegation, ..). We then observed different metrics such as the time at risk
>>>> (when we can't enforce all our contracts at the reserve feerate at the same time), or the
>>>> operational cost.
>>>> We got the historical fee estimates data from Statoshi [9], Txstats [10] and the historical chain
>>>> data from Riccardo Casatta's `blocks_iterator` [11]. Thanks!
>>>>
>>>> The (research-quality..) code can be found at https://github.com/revault/research under the section
>>>> "Fee bumping". Again it's very Revault specific, but at least the data can probably be reused for
>>>> studying other protocols.
>>>>
>>>> ## 9. Insurances
>>>>
>>>> Of course, given it's all hacks and workarounds and there is no good answer to "what is a reasonable
>>>> feerate up to which we need to make contracts enforceable onchain?", there is definitely room for an
>>>> insurance market. But this enters the realm of opinions. Although i do have some (having discussed
>>>> this topic for the past years with different people), i would like to keep this post focused on the
>>>> technical aspects of this problem.
>>>>
>>>> [0] As far as i can tell, having offchain contracts be enforceable onchain by confirming a
>>>> transaction before the expiration of a timelock is a widely agreed-upon approach. And i don't think
>>>> we can opt for any other fundamentally different one, as you want to know you can claim back your
>>>> coins from a contract after a deadline before taking part in it.
>>>>
>>>> [1] The Real Revault (tm) involves more transactions, but for the sake of conciseness i only
>>>> detailed a minimum instance of the problem.
>>>>
>>>> [2] Only presigning part of the Unvault transactions allows to only delegate part of the coins,
>>>> which can be abstracted as "delegate x% of your stash" in the user interface.
>>>>
>>>> [3] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-May/017835.html
>>>>
>>>> [4] https://github.com/revault/research/blob/1df953813708287c32a15e771ba74957ec44f354/feebumping/model/statemachine.py#L323-L329
>>>>
>>>> [5] https://github.com/bitcoin/bitcoin/pull/23121
>>>>
>>>> [6] https://github.com/revault/research/blob/1df953813708287c32a15e771ba74957ec44f354/feebumping/model/statemachine.py#L494-L507
>>>>
>>>> [7] Of course this assumes a combinatorial coin selection, but i believe it's ok given we limit the
>>>> number of coins beforehand.
>>>>
>>>> [8] Although there is the argument to outbid a censorship, anyone censoring you isn't necessarily a
>>>> miner.
>>>>
>>>> [9] https://www.statoshi.info/
>>>>
>>>> [10] https://www.statoshi.info/
>>>>
>>>> [11] https://github.com/RCasatta/blocks_iterator
>>>> _______________________________________________
>>>> bitcoin-dev mailing list
>>>> bitcoin-dev at lists.linuxfoundation.org
>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211208/06c111d0/attachment-0001.html>

From jlrubin at mit.edu  Wed Dec  8 17:18:49 2021
From: jlrubin at mit.edu (Jeremy)
Date: Wed, 8 Dec 2021 09:18:49 -0800
Subject: [bitcoin-dev] Take 2: Removing the Dust Limit
In-Reply-To: <CACdvm3Oynv4gWdaGXATxc3SoYDD8kuiPq-d9F2itsmayP0qeZQ@mail.gmail.com>
References: <CAD5xwhid2OH0GzXPvqWgsMag4J9zidsewEquT-JoOweVD5pxZg@mail.gmail.com>
 <CACdvm3Oynv4gWdaGXATxc3SoYDD8kuiPq-d9F2itsmayP0qeZQ@mail.gmail.com>
Message-ID: <CAD5xwhggU646dygtaPdtmHbKS6NzdnbVYyXPAUBZYdt+1iRbag@mail.gmail.com>

Bastien,

The issue is that with Decker Channels you either use SIGHASH_ALL / APO and
don't allow adding outs (this protects against certain RBF pinning on the
root with bloated wtxid data) and have anchor outputs or you do allow them
and then are RBF pinnable (but can have change).

Assuming you use anchor outs, then you really can't use dust-threshold
outputs as it either breaks the ratcheting update validity (if the specific
amount paid to output matters) OR it allows many non-latest updates to
fully drain the UTXO of any value.

You can get around the needing for N of them by having a congestion-control
tree setup in theory; then you only need log(n) data for one bumper, and
(say) 1.25x the data if all N want to bump. This can be a nice trade-off
between letting everyone bump and not. Since these could be chains of
IUTXO, they don't need to carry any weight directly.

The carve out would just be to ensure that CPFP 0 values are known how to
be spent.





--
@JeremyRubin <https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211208/8dfaefad/attachment.html>

From jlrubin at mit.edu  Wed Dec  8 17:41:34 2021
From: jlrubin at mit.edu (Jeremy)
Date: Wed, 8 Dec 2021 09:41:34 -0800
Subject: [bitcoin-dev] [Lightning-dev]  Take 2: Removing the Dust Limit
In-Reply-To: <CAPv7TjZBU2v2Nfw2_8Qz33rUWKJ=uJ7u+_5tFxjM94mk=RnmOA@mail.gmail.com>
References: <CAD5xwhid2OH0GzXPvqWgsMag4J9zidsewEquT-JoOweVD5pxZg@mail.gmail.com>
 <CACdvm3Oynv4gWdaGXATxc3SoYDD8kuiPq-d9F2itsmayP0qeZQ@mail.gmail.com>
 <CAPv7TjZBU2v2Nfw2_8Qz33rUWKJ=uJ7u+_5tFxjM94mk=RnmOA@mail.gmail.com>
Message-ID: <CAD5xwhiSEoBxw=NVUHnZ+s22nTZhMoWYoDrC=aQfPyvwgtLrTQ@mail.gmail.com>

IMO this is not a big problem. The problem is not if a 0 value ever enters
the mempool, it's if it is never spent. And even if C2/P1 goes in, C1 still
can be spent. In fact, it increases it's feerate with P1's confirmation so
it's somewhat likely it would go in. C2 further has to be pretty expensive
compared to C1 in order to be mined when C2 would not be, so the user
trying to do this has to pay for it.

If we're worried it might never be spent again since no incentive, it's
rational for miners *and users who care about bloat* to save to disk the
transaction spending it to resurrect it. The way this can be broken is if
the txn has two inputs and that input gets spent separately.

That said, I think if we can say that taking advantage of keeping the 0
value output will cost you more than if you just made it above dust
threshold, it shouldn't be economically rational to not just do a dust
threshold value output instead.

So I'm not sure the extent to which we should bend backwards to make 0
value outputs impossible v.s. making them inconvenient enough to not be
popular.



-------------------------------------
Consensus changes below:
-------------------------------------

Another possibility is to have a utxo with drop semantics; if UTXO X with
some flag on it is not spent in the block it is created, it expires and can
never be spent. This is essentially an inverse timelock, but severely
limited to one block and mempool evictions can be handled as if a conflict
were mined.

These types of 0 value outputs could be present just for attaching fee in
the mempool but be treated like an op_return otherwise. We could add two
cases for this: one bare segwit version (just the number, no data) and one
that's equivalent to taproot. This covers OP_TRUE anchors very efficiently
and ones that require a signature as well.

This is relatively similar to how Transaction Sponsors works, but without
full tx graph de-linkage... obviously I think if we'll entertain a
consensus change, sponsors makes more sense, but expiring utxos doesn't
change as many properties of the tx-graph validation so might be simpler.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211208/5bf1bd84/attachment.html>

From jlrubin at mit.edu  Wed Dec  8 19:20:54 2021
From: jlrubin at mit.edu (Jeremy)
Date: Wed, 8 Dec 2021 11:20:54 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Inheritance Schemes
Message-ID: <CAD5xwhjFxwgftr7ORa14MupWVaiqgoxHn16NR1vXH6K7ERghjA@mail.gmail.com>

Devs,

For today's post, something near and dear to our hearts: giving our sats to
our loved ones after we kick the bucket.

see: https://rubin.io/bitcoin/2021/12/08/advent-11/

Some interesting primitives, hopefully enough to spark a discussion around
different inheritance schemes that might be useful.

One note I think is particularly discussion worthy is how the UTXO model
makes inheritance backups sort of fundamentally difficult to do v.s. a
monolithic account model.

Cheers,

Jeremy

--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211208/f422a334/attachment.html>

From rsomsen at gmail.com  Wed Dec  8 22:51:50 2021
From: rsomsen at gmail.com (Ruben Somsen)
Date: Wed, 8 Dec 2021 23:51:50 +0100
Subject: [bitcoin-dev] [Lightning-dev]  Take 2: Removing the Dust Limit
In-Reply-To: <CAD5xwhiSEoBxw=NVUHnZ+s22nTZhMoWYoDrC=aQfPyvwgtLrTQ@mail.gmail.com>
References: <CAD5xwhid2OH0GzXPvqWgsMag4J9zidsewEquT-JoOweVD5pxZg@mail.gmail.com>
 <CACdvm3Oynv4gWdaGXATxc3SoYDD8kuiPq-d9F2itsmayP0qeZQ@mail.gmail.com>
 <CAPv7TjZBU2v2Nfw2_8Qz33rUWKJ=uJ7u+_5tFxjM94mk=RnmOA@mail.gmail.com>
 <CAD5xwhiSEoBxw=NVUHnZ+s22nTZhMoWYoDrC=aQfPyvwgtLrTQ@mail.gmail.com>
Message-ID: <CAPv7TjYTK=xrOxMbpD1JKQ1vTpiWWoOeGt86erFGBOP5grFYNA@mail.gmail.com>

Hi Jeremy,

Thanks for sharing your thoughts.

To summarize your arguments: the intentionally malicious path to getting
the 0 sat output confirmed without being spent is uneconomical compared to
simply creating dust outputs. And even if it does happen, the tx spending
from the 0 sat output may still be valid (as long as none of its inputs get
spent elsewhere) and could eventually get confirmed.

I think those are good points. I do still see a possibility where a user
non-maliciously happens to behave in a way that causes all of the above to
happen, but it does seem somewhat unlikely.

It could happen if all of the following occurs:
1. Another output happens to get spent at a higher feerate (e.g. because an
absolute timelock expires and the output gets used)
2. The tx spending the 0 sat output then happens to not make it into the
block due to the lower fees
3. The user then happens to invalidate the tx that was spending from the 0
sat output (seems rational at that point)

Assuming this is the only scenario (I am at least not currently aware of
others), the question then becomes whether the above is acceptable in order
to avoid a soft fork.

Cheers,
Ruben


On Wed, Dec 8, 2021 at 6:41 PM Jeremy <jlrubin at mit.edu> wrote:

> IMO this is not a big problem. The problem is not if a 0 value ever enters
> the mempool, it's if it is never spent. And even if C2/P1 goes in, C1 still
> can be spent. In fact, it increases it's feerate with P1's confirmation so
> it's somewhat likely it would go in. C2 further has to be pretty expensive
> compared to C1 in order to be mined when C2 would not be, so the user
> trying to do this has to pay for it.
>
> If we're worried it might never be spent again since no incentive, it's
> rational for miners *and users who care about bloat* to save to disk the
> transaction spending it to resurrect it. The way this can be broken is if
> the txn has two inputs and that input gets spent separately.
>
> That said, I think if we can say that taking advantage of keeping the 0
> value output will cost you more than if you just made it above dust
> threshold, it shouldn't be economically rational to not just do a dust
> threshold value output instead.
>
> So I'm not sure the extent to which we should bend backwards to make 0
> value outputs impossible v.s. making them inconvenient enough to not be
> popular.
>
>
>
> -------------------------------------
> Consensus changes below:
> -------------------------------------
>
> Another possibility is to have a utxo with drop semantics; if UTXO X with
> some flag on it is not spent in the block it is created, it expires and can
> never be spent. This is essentially an inverse timelock, but severely
> limited to one block and mempool evictions can be handled as if a conflict
> were mined.
>
> These types of 0 value outputs could be present just for attaching fee in
> the mempool but be treated like an op_return otherwise. We could add two
> cases for this: one bare segwit version (just the number, no data) and one
> that's equivalent to taproot. This covers OP_TRUE anchors very efficiently
> and ones that require a signature as well.
>
> This is relatively similar to how Transaction Sponsors works, but without
> full tx graph de-linkage... obviously I think if we'll entertain a
> consensus change, sponsors makes more sense, but expiring utxos doesn't
> change as many properties of the tx-graph validation so might be simpler.
>
>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211208/8f8b4e60/attachment-0001.html>

From ZmnSCPxj at protonmail.com  Wed Dec  8 23:23:46 2021
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Wed, 08 Dec 2021 23:23:46 +0000
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] What's Smart about
	Smart Contracts
In-Reply-To: <CAD5xwhgxaDzM3T7YiRjOzC7cjD65yyq2_Z=QZ0Ko-d3LGvB9jQ@mail.gmail.com>
References: <CAD5xwhjSZtm6X9J0w6uVg_ZDO7FuS=OCQ_kncURAcW_DuXq9HQ@mail.gmail.com>
 <4RdeDclGQpoDin2VLO5Ngmoghw03BZ_tvdO0vaIp_fNWWlKL9tHeIz1iQMpHxAww2pzjI4NXYtNFuND5Qkj7AmvLUajSp4AKxNg70VWr3Rw=@protonmail.com>
 <CAD5xwhgxaDzM3T7YiRjOzC7cjD65yyq2_Z=QZ0Ko-d3LGvB9jQ@mail.gmail.com>
Message-ID: <k50-_xW0s3WUdfQ2R8_189Q9omJjYglHAv9lXUpxlwabM_mHJmSa7mCzD8BOYnY3nOaYJGLhUwtbV6mJvEHzlk630u-ZfOuFCpUbEa9UY6M=@protonmail.com>

Good morning Jeremy,


> > ## Why would a "Smart" contract be "Smart"?
> >
> > A "smart" contract is simply one that somehow self-enforces rather than requires a third party to enforce it.
> > It is "smart" because its execution is done automatically.
>
> There are no automatic executing smart contracts on any platform I'm aware of. Bitcoin requires TX submission, same with Eth.
>
> Enforcement and execution are different subjects.

Nothing really prevents a cryptocurrency system from recording a "default branch" and enforcing that later.
In Bitcoin terms, nothing fundamentally prevents this redesign:

* A confirmed transaction can include one or more transactions (not part of its wtxid or txid) which spend an output of that confirmed transaction.
  * Like SegWit, they can be put in a new region that is not visible to pre-softfork nodes, but this new section is committed to in the coinbase.
* Those extra transactions must be `nLockTime`d to a future blockheight.
* When the future blockheight arrives, we add those transactions to the mempool.
  * If the TXO is already spent by then, then they are not put in the mempool.

That way, at least the timelocked branch can be automatically executed, because the tx can be submitted "early".
The only real limitation against the above is the amount of resources it would consume on typical nodes.

Even watchtower behavior can be programmed directly into the blockchain layer, i.e. we can put encrypted blobs into the same extra blockspace, with a partial txid key that triggers decryption and putting those transactions in the mempool, etc.
Thus, the line between execution and enforcement blurs.


But that is really beside the point.

The Real Point is that "smart"ness is not a Boolean flag, but a spectrum.
The above feature would allow for more "smart"ness in contracts, at the cost of increased resource utilization at each node.
In this point-of-view, even a paper contract is "smart", though less "smart" than a typical Bitcoin HTLC.

> > Consider the humble HTLC.
> > <snip>
> > This is why the reticence of Bitcoin node operators to change the programming model is a welcome feature of the network.
> > Any change to the programming model risks the introduction of bugs to the underlying virtual machine that the Bitcoin network presents to contract makers.
> > And without that strong reticence, we risk utterly demolishing the basis of the "smart"ness of "smart" contracts --- if a "smart" contract cannot reliably be executed, it cannot self-enforce, and if it cannot self-enforce, it is no longer particularly "smart".
>
> I don't think that anywhere in the post I advocated for playing fast and loose with the rules to introduce any sort of unreliability.

This is not a criticism of your post, merely an amusing article that fits the post title better.

> What I'm saying is more akin to we can actually improve the "hardware" that Bitcoin runs on to the extent that it actually does give us better ability to adjudicate the transfers of value, and we should absolutely and aggressively pursue that rather than keeping Bitcoin running on a set mechanisms that are insufficient to reach the scale, privacy, self custody, and decentralization goals we have.

Agreed.

> ?
>
> > ## The N-of-N Rule
> >
> > What is a "contract", anyway?
> >
> > A "contract" is an agreement between two or more parties.
> > You do not make a contract to yourself, since (we assume) you are completely a single unit (in practice, humans are internally divided into smaller compute modules with slightly different incentives (note: I did not get this information by *personally* dissecting the brains of any humans), hence the "we assume").
>
> ?
>
> > Thus, a contract must by necessity require N participants
>
> This is getting too pedantic about contracts. If you want to go there, you're also missing "consideration".
>
> Smart Contracts are really just programs. And you absolutely can enter smart contracts with yourself solely, for example, Vaults (as covered in day 10) are an example where you form a contract where you are intended to be the only party.

No, because a vault is a contract between your self-of-today and your self-of-tomorrow, with your self-of-today serving as an attorney-in-place of your self-of-tomorrow.
After all, at the next Planck Interval you will die and be replaced with a new entity that only *mostly* agrees with you.

> You could make the claim that a vault is just an open contract between you and some future would be hacker, but the intent is that the contract is there to just safeguard you and those terms should mostly never execute.?+ you usually want to define contract participants as not universally quantified...
>
> > This is of interest since in a reliability perspective, we often accept k-of-n.
> > <snip>
> > But with an N-of-N, *you* are a participant and your input is necessary for the execution of the smart contract, thus you can be *personally* assured that the smart contract *will* be executed faithfully.
>
> Yes I agree that N-N or K-N have uses -- Sapio is designed to work with arbitrary thresholds in lieu of CTV/other covenant proposals which can be used to emulate arbitrary business logic :)
>
> However, the benefit of the contracts without that is non-interactivity of sending. Having everyone online is a major obstacle for things like decentralized coordination free mining pools (kinda, the whole coordination free part). So if you just say "always do N-of-N" you basically lose the entire thread of"smart contract capabilities improving the four pillars (covered in earlier posts) which solidifies bitcoin's adjudication of transfers of value.

The point really is "buyer beware".
Any k-of-n where you do not puppet at least (n - k + 1) allows you to be evicted and your assets seized by somebody else puppeting k entities.
But if you trust that the other entities will not steal from you --- if you do not need the *definite* assurance that the smart contract *will* be executed faithfully --- then go ahead --- do k-of-n.


Regards,
ZmnSCPxj

From pete at petertodd.org  Thu Dec  9 09:12:59 2021
From: pete at petertodd.org (Peter Todd)
Date: Thu, 9 Dec 2021 04:12:59 -0500
Subject: [bitcoin-dev] [Lightning-dev] Sending OP_RETURN via Bitcoin
 Lightning
In-Reply-To: <CANQKmgLyaYAjL_=LziTCFT=Ahc2SXjJrWc+RO59pxd3mnJApfQ@mail.gmail.com>
References: <DD7D5A8B-F61F-4302-ACF4-CE731843D97D@gmail.com>
 <CALL-=e5mF9TqbbD=Cf-bawbw4dq2PGjC9W_nqAQeHsB829ZpNg@mail.gmail.com>
 <CALkkCJas_pf7Un45CJyFg8j9cBk8PtKN4iYAL81TtLSRNnKqeg@mail.gmail.com>
 <CANQKmgLyaYAjL_=LziTCFT=Ahc2SXjJrWc+RO59pxd3mnJApfQ@mail.gmail.com>
Message-ID: <YbHImwix1z8BgAc2@petertodd.org>

On Mon, Dec 06, 2021 at 04:35:19PM +0000, Christian Moss via bitcoin-dev wrote:
> As far as I understand it, RGB doesn't scale NFTs as each
> transaction to transfer ownership of an NFT would require an onchain
> transaction

RGB intends to scale NFTs and similar things in the future via scalable
single-use-seals: https://petertodd.org/2017/scalable-single-use-seal-asset-transfer

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211209/468e4620/attachment.sig>

From antoine.riard at gmail.com  Wed Dec  8 23:56:39 2021
From: antoine.riard at gmail.com (Antoine Riard)
Date: Wed, 8 Dec 2021 18:56:39 -0500
Subject: [bitcoin-dev] A fee-bumping model
In-Reply-To: <8wtAeG1p6qyiOWW0pIJP06_h-3ro7UTBsNO-0BMxLnSKUU6xFBMEvhyQGhjsh3gvQAjDpFajGEC0C6NSQ0Nfj8KtT1cGlaQMW_nnEkAuozM=@protonmail.com>
References: <hBx6OYA5Mv9C_anoMQ-s-9l_XNwNFPfDVmOND9pXBJEBi7qsULF3bgPGpagtqjOsKDTXu8iOTVzvOjflz-M6EfnfwVH81Cu-nnai0kakouo=@protonmail.com>
 <CALZpt+F6h8uLw48e4FRrkjPe2ci6Uqy-o9H=++hu5fx7+bxOZw@mail.gmail.com>
 <8wtAeG1p6qyiOWW0pIJP06_h-3ro7UTBsNO-0BMxLnSKUU6xFBMEvhyQGhjsh3gvQAjDpFajGEC0C6NSQ0Nfj8KtT1cGlaQMW_nnEkAuozM=@protonmail.com>
Message-ID: <CALZpt+Eb_aoD4Df9jA6g6+0RqbBMqijJrqRv3asH5vXZ5_uMcg@mail.gmail.com>

Hi Antoine,

> It seems to me the only policy-level mitigation for RBF pinning around
the "don't decrease the abolute fees of a less-than-a-block mempool" would
be to drop the requirement on increasing absolute fees if the mempool is
"full enough" (and the feerate increases exponentially, of course).

Yes, it's hard to say the "less-than-a-block-mempool" scenario is long-term
realistic. In the future, you can expect liquidity operations to be
triggered as soon as the network mempools start to be empty.  At a given
block space price, there is always room to improve your routing topology.

That said, you would like the default block construction strategy to be
"all-weather" economically aligned. To build such a more robust strategy, I
think a miner would have interest to level the  "full enough" bar.

I still think a policy-level mitigation is possible, where you have a
replace-by-fee rate above X MB of blocks and replace-by-fee under X.
Responsibility is on the L2 fee-bumper to guarantee the  honest bid is in
the X MB of blocks or the malicious pinning attacker has to overbid.

At first sight, yes committing the maximum tx size in the annex covered by
your counterparty signature should still allow you to add high-feerate
input. Though niice if we can save a consensus rule to fix pinnings.

> In any case, for Lightning i think it's a bad idea to re-introduce trust
on this side post anchor outputs. For Revault it's clearly out of the
question to introduce trust in your counterparties (why would you bother
having a fee-bumping mechanism in the >first place then?). Probably the
same holds for all offchain contracts.

Yeah it was a strawman exercise on the question "not knowledge of other
primitive that can be used by multi-party" :) I wouldn't recommend that
kind of fee-bumping "shared cache" scheme for a  trust-minimized setup.
Maybe interesting for watchtowers/LSP topologies.

> Black swan event 2.0? Just rule n?3 is inherent to any kind of fee
estimation.

It's just the old good massive mempool congestion systemic risk known since
the LN whitepaper. AFAIK, anchor output fee-bumping schemes have not really
started the work to be robust against that. What I'm aiming to point out is
that it might be even harder to build a fault-tolerant fee-bumping strategy
because of the "limited rationality" of your local node towards the
behaviors of the other bitcoin users in face of this phenomena. Would be
nice to have more research on that front.

> I don't think any kind of mempool-based estimate generalizes well, since
at any point the expected time before the next block is 10 minutes (and a
lot can happen in 10min).

Sure, you might be off-bid because of block variance, though if you're
ready to pay multiple RBF penalties which are linear, you might adjust your
shots in function of "real-time" mempool congestion.

> I'm very concerned that large stakeholders of the "offchain contracts
ecosystem" would just go this (easier) way and further increase mining
centralisation pressure.

*back on the whiteboard sweating on a consensus-enforced timestop primitive*

Cheers,
Antoine

Le mar. 30 nov. 2021 ? 10:19, darosior <darosior at protonmail.com> a ?crit :

> Hi Antoine,
>
> Thanks for your comment. I believe for Lightning it's simpler with regard
> to the management of the UTxO pool, but harder with regard to choosing
> a threat model.
> Responses inline.
>
>
> For any opened channel, ensure the confirmation of a Commitment
> transaction and the children HTLC-Success/HTLC-Timeout transactions. Note,
> in the Lightning security game you have to consider (at least) 4 types of
> players moves and incentives : your node, your channel counterparties, the
> miners, the crowd of bitcoin users. The number of the last type of players
> is unknown from your node, however it should not be forgotten you're in
> competition for block space, therefore their block demands bids should be
> anticipated and reacted to in consequence. With that remark in mind,
> implications for your LN fee-bumping strategy will be raised afterwards.
>
> For a LN service provider, on-chain overpayments are bearing on your
> operational costs, thus downgrading your economic competitiveness. For the
> average LN user, overpayment might price out outside a LN non-custodial
> deployment, as you don't have the minimal security budget to be on your own.
>
>
> I think this problem statement can be easily generalised to any offchain
> contract. And your points stand for all of them.
> "For any opened contract, ensure at any point the confirmation of a (set
> of) transaction(s) in a given number of blocks"
>
>
> Same issue with Lightning, we can be pinned today on the basis of
> replace-by-fee rule 3. We can be also blinded by network mempool
> partitions, a pinning counterparty can segregate all the full-nodes  in as
> many subsets by broadcasting a revoked Commitment transaction different for
> each. For Revault, I think you can also do unlimited partitions by mutating
> the ANYONECANPAY-input of the Cancel.
>
>
> Well you can already do unlimited partitions by adding different inputs to
> it. You could malleate the witness, but since we are using Miniscript i'm
> confident you would only be able in a marginal way.
>
>
> That said, if you have a distributed towers deployment, spread across the
> p2p network topology, and they can't be clustered together through
> cross-layers or intra-layer heuristics, you should be able to reliably
> observe such partitions. I think such distributed monitors are deployed by
> few L1 merchants accepting 0-conf to detect naive double-spend.
>
>
> We should aim to more than 0-conf (in)security level..
> It seems to me the only policy-level mitigation for RBF pinning around the
> "don't decrease the abolute fees of a less-than-a-block mempool" would be
> to drop the requirement on increasing absolute fees if the mempool is "full
> enough" (and the feerate increases exponentially, of course).
> Another approach could be by introducing new consensus rules as proposed
> by Jeremy last year [0]. If we go in the realm of new consensus rules, then
> i think that simply committing to a maximum tx size would fix pinning by
> RBF rule 3. Could be in the annex, or in the unused sequence bits (although
> they currently are by Lightning, meh). You could also check in the output
> script that the input commits to this.
>
> [0]
> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-September/018168.html
>
>
> Have we already discussed a fee-bumping "shared cache", a CPFP variation ?
> Strawman idea: Alice and Bob commit collateral inputs to a separate UTXO
> from the main "offchain contract" one. This UTXO is locked by a multi-sig.
> For any Commitment transaction pre-signed, also counter-sign a CPFP with
> top mempool feerate included, spending a Commitment anchor output and the
> shared-cache UTXO. If the fees spike,  you can re-sign a high-feerate CPFP,
> assuming interactivity. As the CPFP is counter-signed by everyone, the
> outputs can be CSV-1 encumbered to prevent pinnings. If the share-cache is
> feeded at parity, there shouldn't be an incentive to waste or maliciously
> inflate the feerate. I think this solution can be easily generalized to
> more than 2 counterparties by using a multi-signature scheme. Big issue, if
> the feerate is short due to fee spikes and you need to re-sign a
> higher-feerate CPFP, you're trusting your counterparty to interact, though
> arguably not worse than the current update fee mechanism.
>
>
> It really looks just like `update_fee`. Except maybe with the property
> that you have the channel liquidity not depend on the onchain feerate.
> In any case, for Lightning i think it's a bad idea to re-introduce trust
> on this side post anchor outputs. For Revault it's clearly out of the
> question to introduce trust in your counterparties (why would you bother
> having a fee-bumping mechanism in the first place then?). Probably the same
> holds for all offchain contracts.
>
>
> > For Lightning, it'd mean keeping an equivalent amount of funds as the
> sum of all your
> channels balances sitting there unallocated "just in case". This is not
> reasonable.
>
> Agree, game-theory wise, you would like to keep a full fee-bumping
> reserve, ready to burn as much in fees as the contested HTLC value, as it's
> the maximum gain of your counterparty. Though perfect equilibrium is hard
> to achieve because your malicious counterparty might have an edge pushing
> you to broadcast your Commitment first by witholding HTLC resolution.
>
> Fractional fee-bumping reserves are much more realistic to expect in the
> LN network. Lower fee-bumping reserve, higher liquidity deployed, in theory
> higher routing fees. By observing historical feerates, average offchain
> balances at risk and routing fees expected gains, you should be able to
> discover an equilibrium where higher levels of reserve aren't worth the
> opportunity cost. I guess this  equilibrium could be your LN fee-bumping
> reserve max feerate.
>
> Note, I think the LN approach is a bit different from what suits a custody
> protocol like Revault,  as you compute a direct return of the frozen
> fee-bumping liquidity. With Revault, if you have numerous bitcoins
> protected, it's might be more interesting to adopt a "buy the mempool,
> stupid" strategy than risking fund safety for few percentages of interest
> returns.
>
>
> True for routing nodes. For wallets (if receiving funds), it's not about
> an investment: just users expectations to being able to transact without
> risking to lose their funds (ie being able to enforce their contract
> onchain). Although wallets they are much less at risk.
>
>
> This is where the "anticipate the crowd of bitcoin users move" point can
> be laid out. As the crowd of bitcoin users' fee-bumping reserves are
> ultimately unknown from your node knowledge, you should be ready to be a
> bit more conservative than the vanilla fee-bumping strategies shipped by
> default. In case of massive mempool congestion, your additional
> conservatism might get your time-sensitive transactions and game on the
> crowd of bitcoin users. First Problem: if all offchain bitcoin software
> adopt that strategy we might inflate the worst-case feerate rate at the
> benefit of the miners, without holistically improving block throughput.
> Second problem : your class of offchain bitcoin softwares might have
> ridiculous fee-bumping reserve compared
> to other classes of offchain bitcoin softwares (Revault > Lightning) and
> just be priced out bydesign in case of mempool congestion. Third problem :
> as the number of offchain bitcoin applications should go up with time, your
> fee-bumping reserve levels based from historical data might be always late
> by one "bank-run" scenario.
>
>
> Black swan event 2.0? Just rule n?3 is inherent to any kind of fee
> estimation.
>
> For Lightning, if you're short in fee-bumping reserves you might still do
> preemptive channel closures, either cooperatively or unilaterally and get
> back the off-chain liquidity to protect the more economically interesting
> channels. Though again, that kind of automatic behavior might be compelling
> at the individual node-level, but make the mempol congestion worse
> holistically.
>
>
> Yeah so we are back to the "fractional reserve" model: you can only
> enforce X% of the offchain contracts your participate in.. Actually it's
> even an added assumption: that you still have operating contracts, with
> honest counterparties.
>
>
> In case of massive mempool congestion, you might try to front-run the
> crowd of bitcoin users relying on block connections for fee-bumping, and
> thus start your fee-bumping as soon as you observe feerate groups
> fluctuations in your local mempool(s).
>
>
> I don't think any kind of mempool-based estimate generalizes well, since
> at any point the expected time before the next block is 10 minutes (and a
> lot can happen in 10min).
>
> Also you might proceed your fee-bumping ticks on a local clock instead of
> block connections in case of time-dilation or deeper eclipse attacks of
> your local node. Your view of the chain might be compromised but not your
> ability to broadcast transactions thanks to emergency channels (in the
> non-LN sense...though in fact quid of txn wrapped in onions ?) of
> communication.
>
>
> Oh, yeah, i didn't explicit "not getting eclipsed" (or more generally
> "data availability") as an assumption since it's generally one made by
> participants of any offchain contract. In this case you can't even have
> decent fee estimation, so you are screwed anyways.
>
>
> Yes, stay open the question on how you enforce this block insurance
> market. Reputation, which might be to avoid due to the latent
> centralization effect, might be hard to stack and audit reliably for an
> emergency mechanism running, hopefully, once in a halvening period. Maybe
> maybe some cryptographic or economically based mechanism on slashing or
> swaps could be found...
>
>
> Unfortunately, given current mining centralisation, pools are in a very
> good position to offer pretty decent SLAs around that. With a block space
> insurance, you of course don't need all these convoluted fee-bumping hacks.
> I'm very concerned that large stakeholders of the "offchain contracts
> ecosystem" would just go this (easier) way and further increase mining
> centralisation pressure.
>
> I agree that a cryptography-based scheme around this type of insurance
> services would be the best way out.
>
>
> Antoine
>
> Le lun. 29 nov. 2021 ? 09:34, darosior via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> a ?crit :
>
>> Hi everyone,
>>
>> Fee-bumping is paramount to the security of many protocols building on
>> Bitcoin, as they require the
>> confirmation of a transaction (which might be presigned) before the
>> expiration of a timelock at any
>> point after the establishment of the contract.
>>
>> The part of Revault using presigned transactions (the delegation from a
>> large to a smaller multisig)
>> is no exception. We have been working on how to approach this for a while
>> now and i'd like to share
>> what we have in order to open a discussion on this problem so central to
>> what seem to be The Right
>> Way [0] to build on Bitcoin but which has yet to be discussed in details
>> (at least publicly).
>>
>> I'll discuss what we came up with for Revault (at least for what will be
>> its first iteration) but my
>> intent with posting to the mailing list is more to frame the questions to
>> this problem we are all
>> going to face rather than present the results of our study tailored to
>> the Revault usecase.
>> The discussion is still pretty Revault-centric (as it's the case study)
>> but hopefully this can help
>> future protocol designers and/or start a discussion around what
>> everyone's doing for existing ones.
>>
>>
>> ## 1. Reminder about Revault
>>
>> The part of Revault we are interested in for this study is the delegation
>> process, and more
>> specifically the application of spending policies by network monitors
>> (watchtowers).
>> Coins are received on a large multisig. Participants of this large
>> multisig create 2 [1]
>> transactions. The Unvault, spending a deposit UTxO, creates an output
>> paying either to the small
>> multisig after a timelock or to the large multisig immediately. The
>> Cancel, spending the Unvault
>> output through the non-timelocked path, creates a new deposit UTxO.
>> Participants regularly exchange the Cancel transaction signatures for
>> each deposit, sharing the
>> signatures with the watchtowers they operate. They then optionally [2]
>> sign the Unvault transaction
>> and share the signatures with the small multisig participants who can in
>> turn use them to proceed
>> with a spending. Watchtowers can enforce spending policies (say, can't
>> Unvault outside of business
>> hours) by having the Cancel transaction be confirmed before the
>> expiration of the timelock.
>>
>>
>> ## 2. Problem statement
>>
>> For any delegated vault, ensure the confirmation of a Cancel transaction
>> in a configured number of
>> blocks at any point. In so doing, minimize the overpayments and the UTxO
>> set footprint. Overpayments
>> increase the burden on the watchtower operator by increasing the required
>> frequency of refills of the
>> fee-bumping wallet, which is already the worst user experience. You are
>> likely to manage a number of
>> UTxOs with your number of vaults, which comes at a cost for you as well
>> as everyone running a full
>> node.
>>
>> Note that this assumes miners are economically rationale, are
>> incentivized by *public* fees and that
>> you have a way to propagate your fee-bumped transaction to them. We also
>> don't consider the block
>> space bounds.
>>
>> In the previous paragraph and the following text, "vault" can generally
>> be replaced with "offchain
>> contract".
>>
>>
>> ## 3. With presigned transactions
>>
>> As you all know, the first difficulty is to get to be able to
>> unilaterally enforce your contract
>> onchain. That is, any participant must be able to unilaterally bump the
>> fees of a transaction even
>> if it was co-signed by other participants.
>>
>> For Revault we can afford to introduce malleability in the Cancel
>> transaction since there is no
>> second-stage transaction depending on its txid. Therefore it is
>> pre-signed with ANYONECANPAY. We
>> can't use ANYONECANPAY|SINGLE since it would open a pinning vector [3].
>> Note how we can't leverage
>> the carve out rule, and neither can any other more-than-two-parties
>> contract.
>> This has a significant implication for the rest, as we are entirely
>> burning fee-bumping UTxOs.
>>
>> This opens up a pinning vector, or at least a significant nuisance: any
>> other party can largely
>> increase the absolute fee without increasing the feerate, leveraging the
>> RBF rules to prevent you
>> from replacing it without paying an insane fee. And you might not see it
>> in your own mempool and
>> could only suppose it's happening by receiving non-full blocks or with
>> transactions paying a lower
>> feerate.
>> Unfortunately i know of no other primitive that can be used by
>> multi-party (i mean, >2) presigned
>> transactions protocols for fee-bumping that aren't (more) vulnerable to
>> pinning.
>>
>>
>> ## 4. We are still betting on future feerate
>>
>> The problem is still missing one more constraint. "Ensuring confirmation
>> at any time" involves ensuring
>> confirmation at *any* feerate, which you *cannot* do. So what's the
>> limit? In theory you should be ready
>> to burn as much in fees as the value of the funds you want to get out of
>> the contract. So... For us
>> it'd mean keeping for each vault an equivalent amount of funds sitting
>> there on the watchtower's hot
>> wallet. For Lightning, it'd mean keeping an equivalent amount of funds as
>> the sum of all your
>> channels balances sitting there unallocated "just in case". This is not
>> reasonable.
>>
>> So you need to keep a maximum feerate, above which you won't be able to
>> ensure the enforcement of
>> all your contracts onchain at the same time. We call that the "reserve
>> feerate" and you can have
>> different strategies for choosing it, for instance:
>> - The 85th percentile over the last year of transactions feerates
>> - The maximum historical feerate
>> - The maximum historical feerate adjusted in dollars (makes more sense
>> but introduces a (set of?)
>>   trusted oracle(s) in a security-critical component)
>> - Picking a random high feerate (why not? It's an arbitrary assumption
>> anyways)
>>
>> Therefore, even if we don't have to bet on the broadcast-time feerate
>> market at signing time anymore
>> (since we can unilaterally bump), we still need some kind of prediction
>> in preparation of making
>> funds available to bump the fees at broadcast time.
>> Apart from judging that 500sat/vb is probably more reasonable than
>> 10sat/vbyte, this unfortunately
>> sounds pretty much crystal-ball-driven.
>>
>> We currently use the maximum of the 95th percentiles over 90-days windows
>> over historical block chain
>> feerates. [4]
>>
>>
>> ## 5. How much funds does my watchtower need?
>>
>> That's what we call the "reserve". Depending on your reserve feerate
>> strategy it might vary over
>> time. This is easier to reason about with a per-contract reserve. For
>> Revault it's pretty
>> straightforward since the Cancel transaction size is static:
>> `reserve_feerate * cancel_size`. For
>> other protocols with dynamic transaction sizes (or even packages of
>> transactions) it's less so. For
>> your Lightning channel you would probably take the maximum size of your
>> commitment transaction
>> according to your HTLC exposure settings + the size of as many
>> `htlc_success` transaction?
>>
>> Then you either have your software or your user guesstimate how many
>> offchain contracts the
>> watchtower will have to watch, time that by the per-contract reserve and
>> refill this amount (plus
>> some slack in practice). Once again, a UX tradeoff (not even mentioning
>> the guesstimation UX):
>> overestimating leads to too many unallocated funds sitting on a hot
>> wallet, underestimating means
>> (at best) inability to participate in new contracts or being "at risk"
>> (not being able to enforce
>> all your contracts onchain at your reserve feerate) before a new refill.
>>
>> For vaults you likely have large-value UTxOs and small transactions (the
>> Cancel is one-in one-out in
>> Revault). For some other applications with large transactions and
>> lower-value UTxOs on average it's
>> likely that only part of the offchain contracts might be enforceable at a
>> reasonable feerate. Is it
>> reasonable?
>>
>>
>> ## 6. UTxO pool layout
>>
>> Now that you somehow managed to settle on a refill amount, how are you
>> going to use these funds?
>> Also, you'll need to manage your pool across time (consolidating small
>> coins, and probably fanning
>> out large ones).
>>
>> You could keep a single large UTxO and peel it as you need to sponsor
>> transactions. But this means
>> that you need to create a coin of a specific value according to your need
>> at the current feerate
>> estimation, hope to have it confirmed in a few blocks (at least for now!
>> [5]), and hope that the
>> value won't be obsolete by the time it confirmed. Also, you'd have to do
>> that for any number of
>> Cancel, chaining feebump coin creation transactions off the change of the
>> previous ones or replacing
>> them with more outputs. Both seem to become really un-manageable (and
>> expensive) in many edge-cases,
>> shortening the time you have to confirm the actual Cancel transaction and
>> creating uncertainty about
>> the reserve (how much is my just-in-time fanout going to cost me in fees
>> that i need to refill in
>> advance on my watchtower wallet?).
>> This is less of a concern for protocols using CPFP to sponsor
>> transactions, but they rely on a
>> policy rule specific to 2-parties contracts.
>>
>> Therefore for Revault we fan-out the coins per-vault in advance. We do so
>> at refill time so the
>> refiller can give an excess to pay for the fees of the fanout transaction
>> (which is reasonable since
>> it will occur just after the refilling transaction confirms). When the
>> watchtower is asked to watch
>> for a new delegated vault it will allocate coins from the pool of
>> fanned-out UTxOs to it (failing
>> that, it would refuse the delegation).
>> What is a good distribution of UTxOs amounts per vault? We want to
>> minimize the number of coins,
>> still have coins small enough to not overpay (remember, we can't have
>> change) and be able to bump a
>> Cancel up to the reserve feerate using these coins. The two latter
>> constraints are directly in
>> contradiction as the minimal value of a coin usable at the reserve
>> feerate (paying for its own input
>> fee + bumping the feerate by, say, 5sat/vb) is already pretty high.
>> Therefore we decided to go with
>> two distributions per vault. The "reserve distribution" alone ensures
>> that we can bump up to the
>> reserve feerate and is usable for high feerates. The "bonus distribution"
>> is not, but contains
>> smaller coins useful to prevent overpayments during low and medium fee
>> periods (which is most of the
>> time).
>> Both distributions are based on a basic geometric suite [6]. Each value
>> is half the previous one.
>> This exponentially decreases the value, limiting the number of coins. But
>> this also allows for
>> pretty small coins to exist and each coin's value is equal to the sum of
>> the smaller coins,
>> or smaller by at most the value of the smallest coin. Therefore bounding
>> the maximum overpayment to
>> the smallest coin's value [7].
>>
>> For the management of the UTxO pool across time we merged the
>> consolidation with the fanout. When
>> fanning out a refilled UTxO, we scan the pool for coins that need to be
>> consolidated according to a
>> heuristic. An instance of a heuristic is "the coin isn't allocated and
>> would not have been able to
>> increase the fee at the median feerate over the past 90 days of blocks".
>> We had this assumption that feerate would tend to go up with time and
>> therefore discarded having to
>> split some UTxOs from the pool. We however overlooked that a large
>> increase in the exchange price of
>> BTC as we've seen during the past year could invalidate this assumption
>> and that should arguably be
>> reconsidered.
>>
>>
>> ## 7. Bumping and re-bumping
>>
>> First of all, when to fee-bump? At fixed time intervals? At each block
>> connection? It sounds like,
>> given a large enough timelock, you could try to greed by "trying your
>> luck" at a lower feerate and
>> only re-bumping every N blocks. You would then start aggressively bumping
>> at every block after M
>> blocks have passed. But that's actually a bet (in disguised?) that the
>> next block feerate in M blocks
>> will be lower than the current one. In the absence of any predictive
>> model it is more reasonable to
>> just start being aggressive immediately.
>> You probably want to base your estimates on `estimatesmartfee` and as a
>> consequence you would re-bump
>> (if needed )after each block connection, when your estimates get updated
>> and you notice your
>> transaction was not included in the block.
>>
>> In the event that you notice a consequent portion of the block is filled
>> with transactions paying
>> less than your own, you might want to start panicking and bump your
>> transaction fees by a certain
>> percentage with no consideration for your fee estimator. You might skew
>> miners incentives in doing
>> so: if you increase the fees by a factor of N, any miner with a fraction
>> larger than 1/N of the
>> network hashrate now has an incentive to censor your transaction at first
>> to get you to panic. Also
>> note this can happen if you want to pay the absolute fees for the
>> 'pinning' attack mentioned in
>> section #2, and that might actually incentivize miners to perform it
>> themselves..
>>
>> The gist is that the most effective way to bump and rebump (RBF the
>> Cancel tx) seems to just be to
>> consider the `estimatesmartfee 2 CONSERVATIVE` feerate at every block
>> your tx isn't included in, and
>> to RBF it if the feerate is higher.
>> In addition, we fallback to a block chain based estimation when estimates
>> aren't available (eg if
>> the user stopped their WT for say a hour and we come back up): we use the
>> 85th percentile over the
>> feerates in the last 6 blocks. Sure, miners can try to have an influence
>> on that by stuffing their
>> blocks with large fee self-paying transactions, but they would need to:
>> 1. Be sure to catch a significant portion of the 6 blocks (at least 2,
>> actually)
>> 2. Give up on 25% of the highest fee-paying transactions (assuming they
>> got the 6 blocks, it's
>>    proportionally larger and incertain as they get less of them)
>> 3. Hope that our estimator will fail and we need to fall back to the
>> chain-based estimation
>>
>>
>> ## 8. Our study
>>
>> We essentially replayed the historical data with different deployment
>> configurations (number of
>> participants and timelock) and probability of an event occurring (event
>> being say an Unvault, an
>> invalid Unvault, a new delegation, ..). We then observed different
>> metrics such as the time at risk
>> (when we can't enforce all our contracts at the reserve feerate at the
>> same time), or the
>> operational cost.
>> We got the historical fee estimates data from Statoshi [9], Txstats [10]
>> and the historical chain
>> data from Riccardo Casatta's `blocks_iterator` [11]. Thanks!
>>
>> The (research-quality..) code can be found at
>> https://github.com/revault/research under the section
>> "Fee bumping". Again it's very Revault specific, but at least the data
>> can probably be reused for
>> studying other protocols.
>>
>>
>> ## 9. Insurances
>>
>> Of course, given it's all hacks and workarounds and there is no good
>> answer to "what is a reasonable
>> feerate up to which we need to make contracts enforceable onchain?",
>> there is definitely room for an
>> insurance market. But this enters the realm of opinions. Although i do
>> have some (having discussed
>> this topic for the past years with different people), i would like to
>> keep this post focused on the
>> technical aspects of this problem.
>>
>>
>>
>> [0] As far as i can tell, having offchain contracts be enforceable
>> onchain by confirming a
>> transaction before the expiration of a timelock is a widely agreed-upon
>> approach. And i don't think
>> we can opt for any other fundamentally different one, as you want to know
>> you can claim back your
>> coins from a contract after a deadline before taking part in it.
>>
>> [1] The Real Revault (tm) involves more transactions, but for the sake of
>> conciseness i only
>> detailed a minimum instance of the problem.
>>
>> [2] Only presigning part of the Unvault transactions allows to only
>> delegate part of the coins,
>> which can be abstracted as "delegate x% of your stash" in the user
>> interface.
>>
>> [3]
>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-May/017835.html
>>
>> [4]
>> https://github.com/revault/research/blob/1df953813708287c32a15e771ba74957ec44f354/feebumping/model/statemachine.py#L323-L329
>>
>> [5] https://github.com/bitcoin/bitcoin/pull/23121
>>
>> [6]
>> https://github.com/revault/research/blob/1df953813708287c32a15e771ba74957ec44f354/feebumping/model/statemachine.py#L494-L507
>>
>> [7] Of course this assumes a combinatorial coin selection, but i believe
>> it's ok given we limit the
>> number of coins beforehand.
>>
>> [8] Although there is the argument to outbid a censorship, anyone
>> censoring you isn't necessarily a
>> miner.
>>
>> [9] https://www.statoshi.info/
>>
>> [10] https://www.statoshi.info/
>>
>> [11] https://github.com/RCasatta/blocks_iterator
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211208/99481646/attachment-0001.html>

From antoine.riard at gmail.com  Thu Dec  9 00:55:23 2021
From: antoine.riard at gmail.com (Antoine Riard)
Date: Wed, 8 Dec 2021 19:55:23 -0500
Subject: [bitcoin-dev] A fee-bumping model
In-Reply-To: <CAFXO6=KESTUAoHt0ZeizctEwPqFPaQs_e-NCG+i6-3RDbRPz+A@mail.gmail.com>
References: <hBx6OYA5Mv9C_anoMQ-s-9l_XNwNFPfDVmOND9pXBJEBi7qsULF3bgPGpagtqjOsKDTXu8iOTVzvOjflz-M6EfnfwVH81Cu-nnai0kakouo=@protonmail.com>
 <CALZpt+F6h8uLw48e4FRrkjPe2ci6Uqy-o9H=++hu5fx7+bxOZw@mail.gmail.com>
 <8wtAeG1p6qyiOWW0pIJP06_h-3ro7UTBsNO-0BMxLnSKUU6xFBMEvhyQGhjsh3gvQAjDpFajGEC0C6NSQ0Nfj8KtT1cGlaQMW_nnEkAuozM=@protonmail.com>
 <CAFXO6=KESTUAoHt0ZeizctEwPqFPaQs_e-NCG+i6-3RDbRPz+A@mail.gmail.com>
Message-ID: <CALZpt+ErZpcepN-p9zgidNpK0x-_uA0gTZLENaD_1c5cC6G24A@mail.gmail.com>

Hi Gloria,

For LN, I think 3 tower rewards models have been discussed : per-penalty
on-chain bounty/per-job micropayment/customer subscription. If curious, see
the wip specification :
https://github.com/sr-gi/bolt13/blob/master/13-watchtowers.md

> - Do we expect watchtowers tracking multiple vaults to be batching
multiple
> Cancel transaction fee-bumps?

For LN, I can definitely see LSP to batch closure of their spokes, with one
CPFP spending multiple anchors outputs of commitment transactions, and
RBF'ing when needed.

> - Do we expect vault users to be using multiple watchtowers for a better
> trust model? If so, and we're expecting batched fee-bumps, won't those
> conflict?

Even worse, a malicious counterparty could force an unilateral closure by
the honest participant and observe the fee-bumping transaction propagation
by the towers to discover their full-nodes topologies. Might be good to
have an ordering algo among your towers to select who is fee-bumping first,
and broadcast all when you're reaching near timelock expiration.

> Well stated about CPFP carve out. I suppose the generalization is that
> allowing n extra ancestorcount=2 descendants to a transaction means it can
> help contracts with <=n+1 parties (more accurately, outputs)? I wonder if
> it's possible to devise a different approach for limiting
> ancestors/descendants, e.g. by height/width/branching factor of the family
> instead of count... :shrug:

I think CPFP carve out can be deprecated once package relay and a
pinning-hardened RBF is deployed ?  Like if your counterparty is abusing
the ancestors/descendants limits, your RBF'ed package should evict the
malicious pinning starting by the root commitment transaction (I think).
And I believe it can be generalized to n-parties contracts, if your
transaction includes one "any-contract-can-spend" anchor ouput.

> - Should the fee-bumping strategy depend on how close you are to your
> timelock expiry? (though this seems like a potential privacy leak, and the
> game theory could get weird as you mentioned).

Yes, at first it's hard to predict how tight it is going to be and it's
nice to save on fees. At some point, you might fall-back off this
fee-bumping warm up-phase to accelerate the rate and start to be more
aggressive. In that direction, see DLC spec fee-bumping recommendation :
https://github.com/discreetlogcontracts/dlcspecs/blob/master/Non-Interactive-Protocol.md

Note, at least for LN, the transaction weight isn't proportional with the
value at stake, and there  is a focal point where it's more interesting to
save fee reserves rather than keep bumping.

> - As long as you have a good fee estimator (i.e. given a current mempool,
can get an accurate feerate given a % probability of getting into target
block n), is there any reason to devise a fee-bumping strategy beyond
picking a time interval?

You might be a LSP, you observe rapid changes in the global network HTLC
traffic and would like to react in consequence. You accelerate the
fee-bumping to free/reallocate your liquidity elsewhere.

> So the equation is more like: a miner with 1/N of the hashrate, employing
this censorship strategy, gains only if `max(f2-g2, 0) > N * (f1-g1)`. More
broadly, the miner only profits if `f2` is significantly higher than `g2

This is where it becomes hard. From your "limited rationality" of a
fee-bumping node `g2` is unknown, And you might be incentivized to
overshoot to front-run `g2` issuer (?)

> In general, I agree it would really suck to inadvertently create a game
where miners can drive feerates up by triggering desperation-driven
fee-bumping procedures. I guess this is a reason to avoid
increasingly-aggressive feebumping, or strategies where we predictably
overshoot.

Good topic of research! Few other vectors of analysis :
https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-February/002569.html

Cheers,
Antoine

Le mar. 7 d?c. 2021 ? 12:24, Gloria Zhao <gloriajzhao at gmail.com> a ?crit :

> Hi Darosior and Ariard,
>
> Thank you for your work looking into fee-bumping so thoroughly, and for
> sharing your results. I agree about fee-bumping's importance in contract
> security and feel that it's often under-prioritized. In general, what
> you've described in this post, to me, is strong motivation for some of the
> proposed changes to RBF we've been discussing. Mostly, I have some
> questions.
>
> > The part of Revault we are interested in for this study is the
> delegation process, and more
> > specifically the application of spending policies by network monitors
> (watchtowers).
>
> I'd like to better understand how fee-bumping would be used, i.e. how the
> watchtower model works:
> - Do all of the vault parties both deposit to the vault and a refill/fee
> to the watchtower, is there a reward the watchtower collects for a
> successful Cancel, or something else? (Apologies if there's a thorough
> explanation somewhere that I haven't already seen).
> - Do we expect watchtowers tracking multiple vaults to be batching
> multiple Cancel transaction fee-bumps?
> - Do we expect vault users to be using multiple watchtowers for a better
> trust model? If so, and we're expecting batched fee-bumps, won't those
> conflict?
>
> > For Revault we can afford to introduce malleability in the Cancel
> transaction since there is no
> > second-stage transaction depending on its txid. Therefore it is
> pre-signed with ANYONECANPAY. We
> > can't use ANYONECANPAY|SINGLE since it would open a pinning vector [3].
> Note how we can't leverage
> > the carve out rule, and neither can any other more-than-two-parties
> contract.
>
> We've already talked about this offline, but I'd like to point out here
> that even transactions signed with ANYONECANPAY|ALL can be pinned by RBF
> unless we add an ancestor score rule. [0], [1] (numbers are inaccurate,
> Cancel Tx feerates wouldn't be that low, but just to illustrate what the
> attack would look like)
>
> [0]:
> https://user-images.githubusercontent.com/25183001/135104603-9e775062-5c8d-4d55-9bc9-6e9db92cfe6d.png
> [1]:
> https://user-images.githubusercontent.com/25183001/145044333-2f85da4a-af71-44a1-bc21-30c388713a0d.png
>
> > can't use ANYONECANPAY|SINGLE since it would open a pinning vector [3].
> Note how we can't leverage
> > the carve out rule, and neither can any other more-than-two-parties
> contract.
>
> Well stated about CPFP carve out. I suppose the generalization is that
> allowing n extra ancestorcount=2 descendants to a transaction means it can
> help contracts with <=n+1 parties (more accurately, outputs)? I wonder if
> it's possible to devise a different approach for limiting
> ancestors/descendants, e.g. by height/width/branching factor of the family
> instead of count... :shrug:
>
> > You could keep a single large UTxO and peel it as you need to sponsor
> transactions. But this means
> > that you need to create a coin of a specific value according to your
> need at the current feerate
> > estimation, hope to have it confirmed in a few blocks (at least for now!
> [5]), and hope that the
> > value won't be obsolete by the time it confirmed.
>
> IIUC, a Cancel transaction can be generalized as a 1-in-1-out where the
> input is presigned with counterparties, SIGHASH_ANYONECANPAY. The fan-out
> UTXO pool approach is a clever solution. I also think this smells like a
> case where improving lower-level RBF rules is more appropriate than
> requiring applications to write workarounds and generate extra
> transactions. Seeing that the BIP125#2 (no new unconfirmed inputs)
> restriction really hurts in this case, if that rule were removed, would you
> be able to simply keep the 1 big UTXO per vault and cut out the exact
> nValue you need to fee-bump Cancel transactions? Would that feel less like
> "burning" for the sake of fee-bumping?
>
> > First of all, when to fee-bump? At fixed time intervals? At each block
> connection? It sounds like,
> > given a large enough timelock, you could try to greed by "trying your
> luck" at a lower feerate and
> > only re-bumping every N blocks. You would then start aggressively
> bumping at every block after M
> > blocks have passed.
>
> I'm wondering if you also considered other questions like:
> - Should a fee-bumping strategy be dependent upon the rate of incoming
> transactions? To me, it seems like the two components are (1) what's in the
> mempool and (2) what's going to trickle into the mempool between now and
> the target block. The first component is best-effort keeping
> incentive-compatible mempool; historical data and crystal ball look like
> the only options for incorporating the 2nd component.
> - Should the fee-bumping strategy depend on how close you are to your
> timelock expiry? (though this seems like a potential privacy leak, and the
> game theory could get weird as you mentioned).
> - As long as you have a good fee estimator (i.e. given a current mempool,
> can get an accurate feerate given a % probability of getting into target
> block n), is there any reason to devise a fee-bumping strategy beyond
> picking a time interval?
>
> It would be interesting to see stats on the spread of feerates in blocks
> during periods of fee fluctuation.
>
> > > In the event that you notice a consequent portion of the block is
> filled with transactions paying
> > > less than your own, you might want to start panicking and bump your
> transaction fees by a certain
> > > percentage with no consideration for your fee estimator. You might
> skew miners incentives in doing
> > > so: if you increase the fees by a factor of N, any miner with a
> fraction larger than 1/N of the
> > > network hashrate now has an incentive to censor your transaction at
> first to get you to panic.
>
> > Yes I think miner-harvesting attacks should be weighed carefully in the
> design of offchain contracts fee-bumping strategies, at least in the future
> when the mining reward exhausts further.
>
> Miner-harvesting (such cool naming!) is interesting, but I want to clarify
> the value of N - I don't think it's the factor by which you increase the
> fees on just your transaction.
>
> To codify: your transaction pays a fee of `f1` right now and might pay a
> fee of `f2` in a later block that the miner expects to mine with 1/N
> probability. The economically rational miner isn't incentivized if simply
> `f2 = N * f1` unless their mempool is otherwise empty.
> By omitting your transaction in this block, the miner can include another
> transaction/package paying `g1` fees instead, so they lose `f1-g1` in fees
> right now. In the future block, they have the choice between collecting
> `f2` or `g2` (from another transaction/package) in fees, so their gain is
> `max(f2-g2, 0)`.
> So the equation is more like: a miner with 1/N of the hashrate, employing
> this censorship strategy, gains only if `max(f2-g2, 0) > N * (f1-g1)`. More
> broadly, the miner only profits if `f2` is significantly higher than `g2`
> and `f1` is about the same feerate as everything else in your mempool: it
> seems like they're betting on how much you _overshoot_, not how much you
> bump.
>
> In general, I agree it would really suck to inadvertently create a game
> where miners can drive feerates up by triggering desperation-driven
> fee-bumping procedures. I guess this is a reason to avoid
> increasingly-aggressive feebumping, or strategies where we predictably
> overshoot.
>
> Slightly related question: in contracts, generally, the timelock deadline
> is revealed in the script, so the miner knows how "desperate" we are right?
> Is that a problem? For Revault, if your Cancel transaction is a keypath
> spend (I think I remember reading that somewhere?) and you don't reveal the
> script, they don't see your timelock deadline yes?
>
> Again, thanks for the digging and sharing. :)
>
> Best,
> Gloria
>
> On Tue, Nov 30, 2021 at 3:27 PM darosior via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> Hi Antoine,
>>
>> Thanks for your comment. I believe for Lightning it's simpler with regard
>> to the management of the UTxO pool, but harder with regard to choosing
>> a threat model.
>> Responses inline.
>>
>>
>> For any opened channel, ensure the confirmation of a Commitment
>> transaction and the children HTLC-Success/HTLC-Timeout transactions. Note,
>> in the Lightning security game you have to consider (at least) 4 types of
>> players moves and incentives : your node, your channel counterparties, the
>> miners, the crowd of bitcoin users. The number of the last type of players
>> is unknown from your node, however it should not be forgotten you're in
>> competition for block space, therefore their block demands bids should be
>> anticipated and reacted to in consequence. With that remark in mind,
>> implications for your LN fee-bumping strategy will be raised afterwards.
>>
>> For a LN service provider, on-chain overpayments are bearing on your
>> operational costs, thus downgrading your economic competitiveness. For the
>> average LN user, overpayment might price out outside a LN non-custodial
>> deployment, as you don't have the minimal security budget to be on your own.
>>
>>
>> I think this problem statement can be easily generalised to any offchain
>> contract. And your points stand for all of them.
>> "For any opened contract, ensure at any point the confirmation of a (set
>> of) transaction(s) in a given number of blocks"
>>
>>
>> Same issue with Lightning, we can be pinned today on the basis of
>> replace-by-fee rule 3. We can be also blinded by network mempool
>> partitions, a pinning counterparty can segregate all the full-nodes  in as
>> many subsets by broadcasting a revoked Commitment transaction different for
>> each. For Revault, I think you can also do unlimited partitions by mutating
>> the ANYONECANPAY-input of the Cancel.
>>
>>
>> Well you can already do unlimited partitions by adding different inputs
>> to it. You could malleate the witness, but since we are using Miniscript
>> i'm confident you would only be able in a marginal way.
>>
>>
>> That said, if you have a distributed towers deployment, spread across the
>> p2p network topology, and they can't be clustered together through
>> cross-layers or intra-layer heuristics, you should be able to reliably
>> observe such partitions. I think such distributed monitors are deployed by
>> few L1 merchants accepting 0-conf to detect naive double-spend.
>>
>>
>> We should aim to more than 0-conf (in)security level..
>> It seems to me the only policy-level mitigation for RBF pinning around
>> the "don't decrease the abolute fees of a less-than-a-block mempool" would
>> be to drop the requirement on increasing absolute fees if the mempool is
>> "full enough" (and the feerate increases exponentially, of course).
>> Another approach could be by introducing new consensus rules as proposed
>> by Jeremy last year [0]. If we go in the realm of new consensus rules, then
>> i think that simply committing to a maximum tx size would fix pinning by
>> RBF rule 3. Could be in the annex, or in the unused sequence bits (although
>> they currently are by Lightning, meh). You could also check in the output
>> script that the input commits to this.
>>
>> [0]
>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-September/018168.html
>>
>>
>> Have we already discussed a fee-bumping "shared cache", a CPFP variation
>> ? Strawman idea: Alice and Bob commit collateral inputs to a separate UTXO
>> from the main "offchain contract" one. This UTXO is locked by a multi-sig.
>> For any Commitment transaction pre-signed, also counter-sign a CPFP with
>> top mempool feerate included, spending a Commitment anchor output and the
>> shared-cache UTXO. If the fees spike,  you can re-sign a high-feerate CPFP,
>> assuming interactivity. As the CPFP is counter-signed by everyone, the
>> outputs can be CSV-1 encumbered to prevent pinnings. If the share-cache is
>> feeded at parity, there shouldn't be an incentive to waste or maliciously
>> inflate the feerate. I think this solution can be easily generalized to
>> more than 2 counterparties by using a multi-signature scheme. Big issue, if
>> the feerate is short due to fee spikes and you need to re-sign a
>> higher-feerate CPFP, you're trusting your counterparty to interact, though
>> arguably not worse than the current update fee mechanism.
>>
>>
>> It really looks just like `update_fee`. Except maybe with the property
>> that you have the channel liquidity not depend on the onchain feerate.
>> In any case, for Lightning i think it's a bad idea to re-introduce trust
>> on this side post anchor outputs. For Revault it's clearly out of the
>> question to introduce trust in your counterparties (why would you bother
>> having a fee-bumping mechanism in the first place then?). Probably the same
>> holds for all offchain contracts.
>>
>>
>> > For Lightning, it'd mean keeping an equivalent amount of funds as the
>> sum of all your
>> channels balances sitting there unallocated "just in case". This is not
>> reasonable.
>>
>> Agree, game-theory wise, you would like to keep a full fee-bumping
>> reserve, ready to burn as much in fees as the contested HTLC value, as it's
>> the maximum gain of your counterparty. Though perfect equilibrium is hard
>> to achieve because your malicious counterparty might have an edge pushing
>> you to broadcast your Commitment first by witholding HTLC resolution.
>>
>> Fractional fee-bumping reserves are much more realistic to expect in the
>> LN network. Lower fee-bumping reserve, higher liquidity deployed, in theory
>> higher routing fees. By observing historical feerates, average offchain
>> balances at risk and routing fees expected gains, you should be able to
>> discover an equilibrium where higher levels of reserve aren't worth the
>> opportunity cost. I guess this  equilibrium could be your LN fee-bumping
>> reserve max feerate.
>>
>> Note, I think the LN approach is a bit different from what suits a
>> custody protocol like Revault,  as you compute a direct return of the
>> frozen fee-bumping liquidity. With Revault, if you have numerous bitcoins
>> protected, it's might be more interesting to adopt a "buy the mempool,
>> stupid" strategy than risking fund safety for few percentages of interest
>> returns.
>>
>>
>> True for routing nodes. For wallets (if receiving funds), it's not about
>> an investment: just users expectations to being able to transact without
>> risking to lose their funds (ie being able to enforce their contract
>> onchain). Although wallets they are much less at risk.
>>
>>
>> This is where the "anticipate the crowd of bitcoin users move" point can
>> be laid out. As the crowd of bitcoin users' fee-bumping reserves are
>> ultimately unknown from your node knowledge, you should be ready to be a
>> bit more conservative than the vanilla fee-bumping strategies shipped by
>> default. In case of massive mempool congestion, your additional
>> conservatism might get your time-sensitive transactions and game on the
>> crowd of bitcoin users. First Problem: if all offchain bitcoin software
>> adopt that strategy we might inflate the worst-case feerate rate at the
>> benefit of the miners, without holistically improving block throughput.
>> Second problem : your class of offchain bitcoin softwares might have
>> ridiculous fee-bumping reserve compared
>> to other classes of offchain bitcoin softwares (Revault > Lightning) and
>> just be priced out bydesign in case of mempool congestion. Third problem :
>> as the number of offchain bitcoin applications should go up with time, your
>> fee-bumping reserve levels based from historical data might be always late
>> by one "bank-run" scenario.
>>
>>
>> Black swan event 2.0? Just rule n?3 is inherent to any kind of fee
>> estimation.
>>
>> For Lightning, if you're short in fee-bumping reserves you might still do
>> preemptive channel closures, either cooperatively or unilaterally and get
>> back the off-chain liquidity to protect the more economically interesting
>> channels. Though again, that kind of automatic behavior might be compelling
>> at the individual node-level, but make the mempol congestion worse
>> holistically.
>>
>>
>> Yeah so we are back to the "fractional reserve" model: you can only
>> enforce X% of the offchain contracts your participate in.. Actually it's
>> even an added assumption: that you still have operating contracts, with
>> honest counterparties.
>>
>>
>> In case of massive mempool congestion, you might try to front-run the
>> crowd of bitcoin users relying on block connections for fee-bumping, and
>> thus start your fee-bumping as soon as you observe feerate groups
>> fluctuations in your local mempool(s).
>>
>>
>> I don't think any kind of mempool-based estimate generalizes well, since
>> at any point the expected time before the next block is 10 minutes (and a
>> lot can happen in 10min).
>>
>> Also you might proceed your fee-bumping ticks on a local clock instead of
>> block connections in case of time-dilation or deeper eclipse attacks of
>> your local node. Your view of the chain might be compromised but not your
>> ability to broadcast transactions thanks to emergency channels (in the
>> non-LN sense...though in fact quid of txn wrapped in onions ?) of
>> communication.
>>
>>
>> Oh, yeah, i didn't explicit "not getting eclipsed" (or more generally
>> "data availability") as an assumption since it's generally one made by
>> participants of any offchain contract. In this case you can't even have
>> decent fee estimation, so you are screwed anyways.
>>
>>
>> Yes, stay open the question on how you enforce this block insurance
>> market. Reputation, which might be to avoid due to the latent
>> centralization effect, might be hard to stack and audit reliably for an
>> emergency mechanism running, hopefully, once in a halvening period. Maybe
>> maybe some cryptographic or economically based mechanism on slashing or
>> swaps could be found...
>>
>>
>> Unfortunately, given current mining centralisation, pools are in a very
>> good position to offer pretty decent SLAs around that. With a block space
>> insurance, you of course don't need all these convoluted fee-bumping hacks.
>> I'm very concerned that large stakeholders of the "offchain contracts
>> ecosystem" would just go this (easier) way and further increase mining
>> centralisation pressure.
>>
>> I agree that a cryptography-based scheme around this type of insurance
>> services would be the best way out.
>>
>>
>> Antoine
>>
>> Le lun. 29 nov. 2021 ? 09:34, darosior via bitcoin-dev <
>> bitcoin-dev at lists.linuxfoundation.org> a ?crit :
>>
>>> Hi everyone,
>>>
>>> Fee-bumping is paramount to the security of many protocols building on
>>> Bitcoin, as they require the
>>> confirmation of a transaction (which might be presigned) before the
>>> expiration of a timelock at any
>>> point after the establishment of the contract.
>>>
>>> The part of Revault using presigned transactions (the delegation from a
>>> large to a smaller multisig)
>>> is no exception. We have been working on how to approach this for a
>>> while now and i'd like to share
>>> what we have in order to open a discussion on this problem so central to
>>> what seem to be The Right
>>> Way [0] to build on Bitcoin but which has yet to be discussed in details
>>> (at least publicly).
>>>
>>> I'll discuss what we came up with for Revault (at least for what will be
>>> its first iteration) but my
>>> intent with posting to the mailing list is more to frame the questions
>>> to this problem we are all
>>> going to face rather than present the results of our study tailored to
>>> the Revault usecase.
>>> The discussion is still pretty Revault-centric (as it's the case study)
>>> but hopefully this can help
>>> future protocol designers and/or start a discussion around what
>>> everyone's doing for existing ones.
>>>
>>>
>>> ## 1. Reminder about Revault
>>>
>>> The part of Revault we are interested in for this study is the
>>> delegation process, and more
>>> specifically the application of spending policies by network monitors
>>> (watchtowers).
>>> Coins are received on a large multisig. Participants of this large
>>> multisig create 2 [1]
>>> transactions. The Unvault, spending a deposit UTxO, creates an output
>>> paying either to the small
>>> multisig after a timelock or to the large multisig immediately. The
>>> Cancel, spending the Unvault
>>> output through the non-timelocked path, creates a new deposit UTxO.
>>> Participants regularly exchange the Cancel transaction signatures for
>>> each deposit, sharing the
>>> signatures with the watchtowers they operate. They then optionally [2]
>>> sign the Unvault transaction
>>> and share the signatures with the small multisig participants who can in
>>> turn use them to proceed
>>> with a spending. Watchtowers can enforce spending policies (say, can't
>>> Unvault outside of business
>>> hours) by having the Cancel transaction be confirmed before the
>>> expiration of the timelock.
>>>
>>>
>>> ## 2. Problem statement
>>>
>>> For any delegated vault, ensure the confirmation of a Cancel transaction
>>> in a configured number of
>>> blocks at any point. In so doing, minimize the overpayments and the UTxO
>>> set footprint. Overpayments
>>> increase the burden on the watchtower operator by increasing the
>>> required frequency of refills of the
>>> fee-bumping wallet, which is already the worst user experience. You are
>>> likely to manage a number of
>>> UTxOs with your number of vaults, which comes at a cost for you as well
>>> as everyone running a full
>>> node.
>>>
>>> Note that this assumes miners are economically rationale, are
>>> incentivized by *public* fees and that
>>> you have a way to propagate your fee-bumped transaction to them. We also
>>> don't consider the block
>>> space bounds.
>>>
>>> In the previous paragraph and the following text, "vault" can generally
>>> be replaced with "offchain
>>> contract".
>>>
>>>
>>> ## 3. With presigned transactions
>>>
>>> As you all know, the first difficulty is to get to be able to
>>> unilaterally enforce your contract
>>> onchain. That is, any participant must be able to unilaterally bump the
>>> fees of a transaction even
>>> if it was co-signed by other participants.
>>>
>>> For Revault we can afford to introduce malleability in the Cancel
>>> transaction since there is no
>>> second-stage transaction depending on its txid. Therefore it is
>>> pre-signed with ANYONECANPAY. We
>>> can't use ANYONECANPAY|SINGLE since it would open a pinning vector [3].
>>> Note how we can't leverage
>>> the carve out rule, and neither can any other more-than-two-parties
>>> contract.
>>> This has a significant implication for the rest, as we are entirely
>>> burning fee-bumping UTxOs.
>>>
>>> This opens up a pinning vector, or at least a significant nuisance: any
>>> other party can largely
>>> increase the absolute fee without increasing the feerate, leveraging the
>>> RBF rules to prevent you
>>> from replacing it without paying an insane fee. And you might not see it
>>> in your own mempool and
>>> could only suppose it's happening by receiving non-full blocks or with
>>> transactions paying a lower
>>> feerate.
>>> Unfortunately i know of no other primitive that can be used by
>>> multi-party (i mean, >2) presigned
>>> transactions protocols for fee-bumping that aren't (more) vulnerable to
>>> pinning.
>>>
>>>
>>> ## 4. We are still betting on future feerate
>>>
>>> The problem is still missing one more constraint. "Ensuring confirmation
>>> at any time" involves ensuring
>>> confirmation at *any* feerate, which you *cannot* do. So what's the
>>> limit? In theory you should be ready
>>> to burn as much in fees as the value of the funds you want to get out of
>>> the contract. So... For us
>>> it'd mean keeping for each vault an equivalent amount of funds sitting
>>> there on the watchtower's hot
>>> wallet. For Lightning, it'd mean keeping an equivalent amount of funds
>>> as the sum of all your
>>> channels balances sitting there unallocated "just in case". This is not
>>> reasonable.
>>>
>>> So you need to keep a maximum feerate, above which you won't be able to
>>> ensure the enforcement of
>>> all your contracts onchain at the same time. We call that the "reserve
>>> feerate" and you can have
>>> different strategies for choosing it, for instance:
>>> - The 85th percentile over the last year of transactions feerates
>>> - The maximum historical feerate
>>> - The maximum historical feerate adjusted in dollars (makes more sense
>>> but introduces a (set of?)
>>>   trusted oracle(s) in a security-critical component)
>>> - Picking a random high feerate (why not? It's an arbitrary assumption
>>> anyways)
>>>
>>> Therefore, even if we don't have to bet on the broadcast-time feerate
>>> market at signing time anymore
>>> (since we can unilaterally bump), we still need some kind of prediction
>>> in preparation of making
>>> funds available to bump the fees at broadcast time.
>>> Apart from judging that 500sat/vb is probably more reasonable than
>>> 10sat/vbyte, this unfortunately
>>> sounds pretty much crystal-ball-driven.
>>>
>>> We currently use the maximum of the 95th percentiles over 90-days
>>> windows over historical block chain
>>> feerates. [4]
>>>
>>>
>>> ## 5. How much funds does my watchtower need?
>>>
>>> That's what we call the "reserve". Depending on your reserve feerate
>>> strategy it might vary over
>>> time. This is easier to reason about with a per-contract reserve. For
>>> Revault it's pretty
>>> straightforward since the Cancel transaction size is static:
>>> `reserve_feerate * cancel_size`. For
>>> other protocols with dynamic transaction sizes (or even packages of
>>> transactions) it's less so. For
>>> your Lightning channel you would probably take the maximum size of your
>>> commitment transaction
>>> according to your HTLC exposure settings + the size of as many
>>> `htlc_success` transaction?
>>>
>>> Then you either have your software or your user guesstimate how many
>>> offchain contracts the
>>> watchtower will have to watch, time that by the per-contract reserve and
>>> refill this amount (plus
>>> some slack in practice). Once again, a UX tradeoff (not even mentioning
>>> the guesstimation UX):
>>> overestimating leads to too many unallocated funds sitting on a hot
>>> wallet, underestimating means
>>> (at best) inability to participate in new contracts or being "at risk"
>>> (not being able to enforce
>>> all your contracts onchain at your reserve feerate) before a new refill.
>>>
>>> For vaults you likely have large-value UTxOs and small transactions (the
>>> Cancel is one-in one-out in
>>> Revault). For some other applications with large transactions and
>>> lower-value UTxOs on average it's
>>> likely that only part of the offchain contracts might be enforceable at
>>> a reasonable feerate. Is it
>>> reasonable?
>>>
>>>
>>> ## 6. UTxO pool layout
>>>
>>> Now that you somehow managed to settle on a refill amount, how are you
>>> going to use these funds?
>>> Also, you'll need to manage your pool across time (consolidating small
>>> coins, and probably fanning
>>> out large ones).
>>>
>>> You could keep a single large UTxO and peel it as you need to sponsor
>>> transactions. But this means
>>> that you need to create a coin of a specific value according to your
>>> need at the current feerate
>>> estimation, hope to have it confirmed in a few blocks (at least for now!
>>> [5]), and hope that the
>>> value won't be obsolete by the time it confirmed. Also, you'd have to do
>>> that for any number of
>>> Cancel, chaining feebump coin creation transactions off the change of
>>> the previous ones or replacing
>>> them with more outputs. Both seem to become really un-manageable (and
>>> expensive) in many edge-cases,
>>> shortening the time you have to confirm the actual Cancel transaction
>>> and creating uncertainty about
>>> the reserve (how much is my just-in-time fanout going to cost me in fees
>>> that i need to refill in
>>> advance on my watchtower wallet?).
>>> This is less of a concern for protocols using CPFP to sponsor
>>> transactions, but they rely on a
>>> policy rule specific to 2-parties contracts.
>>>
>>> Therefore for Revault we fan-out the coins per-vault in advance. We do
>>> so at refill time so the
>>> refiller can give an excess to pay for the fees of the fanout
>>> transaction (which is reasonable since
>>> it will occur just after the refilling transaction confirms). When the
>>> watchtower is asked to watch
>>> for a new delegated vault it will allocate coins from the pool of
>>> fanned-out UTxOs to it (failing
>>> that, it would refuse the delegation).
>>> What is a good distribution of UTxOs amounts per vault? We want to
>>> minimize the number of coins,
>>> still have coins small enough to not overpay (remember, we can't have
>>> change) and be able to bump a
>>> Cancel up to the reserve feerate using these coins. The two latter
>>> constraints are directly in
>>> contradiction as the minimal value of a coin usable at the reserve
>>> feerate (paying for its own input
>>> fee + bumping the feerate by, say, 5sat/vb) is already pretty high.
>>> Therefore we decided to go with
>>> two distributions per vault. The "reserve distribution" alone ensures
>>> that we can bump up to the
>>> reserve feerate and is usable for high feerates. The "bonus
>>> distribution" is not, but contains
>>> smaller coins useful to prevent overpayments during low and medium fee
>>> periods (which is most of the
>>> time).
>>> Both distributions are based on a basic geometric suite [6]. Each value
>>> is half the previous one.
>>> This exponentially decreases the value, limiting the number of coins.
>>> But this also allows for
>>> pretty small coins to exist and each coin's value is equal to the sum of
>>> the smaller coins,
>>> or smaller by at most the value of the smallest coin. Therefore bounding
>>> the maximum overpayment to
>>> the smallest coin's value [7].
>>>
>>> For the management of the UTxO pool across time we merged the
>>> consolidation with the fanout. When
>>> fanning out a refilled UTxO, we scan the pool for coins that need to be
>>> consolidated according to a
>>> heuristic. An instance of a heuristic is "the coin isn't allocated and
>>> would not have been able to
>>> increase the fee at the median feerate over the past 90 days of blocks".
>>> We had this assumption that feerate would tend to go up with time and
>>> therefore discarded having to
>>> split some UTxOs from the pool. We however overlooked that a large
>>> increase in the exchange price of
>>> BTC as we've seen during the past year could invalidate this assumption
>>> and that should arguably be
>>> reconsidered.
>>>
>>>
>>> ## 7. Bumping and re-bumping
>>>
>>> First of all, when to fee-bump? At fixed time intervals? At each block
>>> connection? It sounds like,
>>> given a large enough timelock, you could try to greed by "trying your
>>> luck" at a lower feerate and
>>> only re-bumping every N blocks. You would then start aggressively
>>> bumping at every block after M
>>> blocks have passed. But that's actually a bet (in disguised?) that the
>>> next block feerate in M blocks
>>> will be lower than the current one. In the absence of any predictive
>>> model it is more reasonable to
>>> just start being aggressive immediately.
>>> You probably want to base your estimates on `estimatesmartfee` and as a
>>> consequence you would re-bump
>>> (if needed )after each block connection, when your estimates get updated
>>> and you notice your
>>> transaction was not included in the block.
>>>
>>> In the event that you notice a consequent portion of the block is filled
>>> with transactions paying
>>> less than your own, you might want to start panicking and bump your
>>> transaction fees by a certain
>>> percentage with no consideration for your fee estimator. You might skew
>>> miners incentives in doing
>>> so: if you increase the fees by a factor of N, any miner with a fraction
>>> larger than 1/N of the
>>> network hashrate now has an incentive to censor your transaction at
>>> first to get you to panic. Also
>>> note this can happen if you want to pay the absolute fees for the
>>> 'pinning' attack mentioned in
>>> section #2, and that might actually incentivize miners to perform it
>>> themselves..
>>>
>>> The gist is that the most effective way to bump and rebump (RBF the
>>> Cancel tx) seems to just be to
>>> consider the `estimatesmartfee 2 CONSERVATIVE` feerate at every block
>>> your tx isn't included in, and
>>> to RBF it if the feerate is higher.
>>> In addition, we fallback to a block chain based estimation when
>>> estimates aren't available (eg if
>>> the user stopped their WT for say a hour and we come back up): we use
>>> the 85th percentile over the
>>> feerates in the last 6 blocks. Sure, miners can try to have an influence
>>> on that by stuffing their
>>> blocks with large fee self-paying transactions, but they would need to:
>>> 1. Be sure to catch a significant portion of the 6 blocks (at least 2,
>>> actually)
>>> 2. Give up on 25% of the highest fee-paying transactions (assuming they
>>> got the 6 blocks, it's
>>>    proportionally larger and incertain as they get less of them)
>>> 3. Hope that our estimator will fail and we need to fall back to the
>>> chain-based estimation
>>>
>>>
>>> ## 8. Our study
>>>
>>> We essentially replayed the historical data with different deployment
>>> configurations (number of
>>> participants and timelock) and probability of an event occurring (event
>>> being say an Unvault, an
>>> invalid Unvault, a new delegation, ..). We then observed different
>>> metrics such as the time at risk
>>> (when we can't enforce all our contracts at the reserve feerate at the
>>> same time), or the
>>> operational cost.
>>> We got the historical fee estimates data from Statoshi [9], Txstats [10]
>>> and the historical chain
>>> data from Riccardo Casatta's `blocks_iterator` [11]. Thanks!
>>>
>>> The (research-quality..) code can be found at
>>> https://github.com/revault/research under the section
>>> "Fee bumping". Again it's very Revault specific, but at least the data
>>> can probably be reused for
>>> studying other protocols.
>>>
>>>
>>> ## 9. Insurances
>>>
>>> Of course, given it's all hacks and workarounds and there is no good
>>> answer to "what is a reasonable
>>> feerate up to which we need to make contracts enforceable onchain?",
>>> there is definitely room for an
>>> insurance market. But this enters the realm of opinions. Although i do
>>> have some (having discussed
>>> this topic for the past years with different people), i would like to
>>> keep this post focused on the
>>> technical aspects of this problem.
>>>
>>>
>>>
>>> [0] As far as i can tell, having offchain contracts be enforceable
>>> onchain by confirming a
>>> transaction before the expiration of a timelock is a widely agreed-upon
>>> approach. And i don't think
>>> we can opt for any other fundamentally different one, as you want to
>>> know you can claim back your
>>> coins from a contract after a deadline before taking part in it.
>>>
>>> [1] The Real Revault (tm) involves more transactions, but for the sake
>>> of conciseness i only
>>> detailed a minimum instance of the problem.
>>>
>>> [2] Only presigning part of the Unvault transactions allows to only
>>> delegate part of the coins,
>>> which can be abstracted as "delegate x% of your stash" in the user
>>> interface.
>>>
>>> [3]
>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-May/017835.html
>>>
>>> [4]
>>> https://github.com/revault/research/blob/1df953813708287c32a15e771ba74957ec44f354/feebumping/model/statemachine.py#L323-L329
>>>
>>> [5] https://github.com/bitcoin/bitcoin/pull/23121
>>>
>>> [6]
>>> https://github.com/revault/research/blob/1df953813708287c32a15e771ba74957ec44f354/feebumping/model/statemachine.py#L494-L507
>>>
>>> [7] Of course this assumes a combinatorial coin selection, but i believe
>>> it's ok given we limit the
>>> number of coins beforehand.
>>>
>>> [8] Although there is the argument to outbid a censorship, anyone
>>> censoring you isn't necessarily a
>>> miner.
>>>
>>> [9] https://www.statoshi.info/
>>>
>>> [10] https://www.statoshi.info/
>>>
>>> [11] https://github.com/RCasatta/blocks_iterator
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev at lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211208/fabce717/attachment-0001.html>

From damian at willtech.com.au  Thu Dec  9 06:27:04 2021
From: damian at willtech.com.au (damian at willtech.com.au)
Date: Wed, 08 Dec 2021 22:27:04 -0800
Subject: [bitcoin-dev] [Lightning-dev]  Take 2: Removing the Dust Limit
In-Reply-To: <CAPv7TjYTK=xrOxMbpD1JKQ1vTpiWWoOeGt86erFGBOP5grFYNA@mail.gmail.com>
References: <CAD5xwhid2OH0GzXPvqWgsMag4J9zidsewEquT-JoOweVD5pxZg@mail.gmail.com>
 <CACdvm3Oynv4gWdaGXATxc3SoYDD8kuiPq-d9F2itsmayP0qeZQ@mail.gmail.com>
 <CAPv7TjZBU2v2Nfw2_8Qz33rUWKJ=uJ7u+_5tFxjM94mk=RnmOA@mail.gmail.com>
 <CAD5xwhiSEoBxw=NVUHnZ+s22nTZhMoWYoDrC=aQfPyvwgtLrTQ@mail.gmail.com>
 <CAPv7TjYTK=xrOxMbpD1JKQ1vTpiWWoOeGt86erFGBOP5grFYNA@mail.gmail.com>
Message-ID: <32e6a20882fa13da03aa6238f5dfff69@willtech.com.au>

Good Afternoon,

'Avoiding a soft-fork' is a political concession. Consensus is none of 
that.

KING JAMES HRMH
Great British Empire

Regards,
The Australian
LORD HIS EXCELLENCY JAMES HRMH (& HMRH)
of Hougun Manor & Glencoe & British Empire
MR. Damian A. James Williamson
Wills

et al.


Willtech
www.willtech.com.au
www.go-overt.com
and other projects

earn.com/willtech
linkedin.com/in/damianwilliamson


m. 0487135719
f. +61261470192


This email does not constitute a general advice. Please disregard this 
email if misdelivered.
On 2021-12-08 14:51, Ruben Somsen via bitcoin-dev wrote:
> Hi Jeremy,
> 
> Thanks for sharing your thoughts.
> 
> To summarize your arguments: the intentionally malicious path to
> getting the 0 sat output confirmed without being spent is uneconomical
> compared to simply creating dust outputs. And even if it does happen,
> the tx spending from the 0 sat output may still be valid (as long as
> none of its inputs get spent elsewhere) and could eventually get
> confirmed.
> 
> I think those are good points. I do still see a possibility where a
> user non-maliciously happens to behave in a way that causes all of the
> above to happen, but it does seem somewhat unlikely.
> 
> It could happen if all of the following occurs:
> 1. Another output happens to get spent at a higher feerate (e.g.
> because an absolute timelock expires and the output gets used)
> 2. The tx spending the 0 sat output then happens to not make it into
> the block due to the lower fees
> 3. The user then happens to invalidate the tx that was spending from
> the 0 sat output (seems rational at that point)
> 
> Assuming this is the only scenario (I am at least not currently aware
> of others), the question then becomes whether the above is acceptable
> in order to avoid a soft fork.
> 
> Cheers,
> Ruben
> 
> On Wed, Dec 8, 2021 at 6:41 PM Jeremy <jlrubin at mit.edu> wrote:
> 
>> IMO this is not a big problem. The problem is not if a 0 value ever
>> enters the mempool, it's if it is never spent. And even if C2/P1
>> goes in, C1 still can be spent. In fact, it increases it's feerate
>> with P1's confirmation so it's somewhat likely it would go in. C2
>> further has to be pretty expensive compared to C1 in order to be
>> mined when C2 would not be, so the user trying to do this has to pay
>> for it.
>> 
>> If we're worried it might never be spent again since no incentive,
>> it's rational for miners *and users who care about bloat* to save to
>> disk the transaction spending it to resurrect it. The way this can
>> be broken is if the txn has two inputs and that input gets spent
>> separately.
>> 
>> That said, I think if we can say that taking advantage of keeping
>> the 0 value output will cost you more than if you just made it above
>> dust threshold, it shouldn't be economically rational to not just do
>> a dust threshold value output instead.
>> 
>> So I'm not sure the extent to which we should bend backwards to make
>> 0 value outputs impossible v.s. making them inconvenient enough to
>> not be popular.
>> 
>> -------------------------------------
>> Consensus changes below:
>> -------------------------------------
>> 
>> Another possibility is to have a utxo with drop semantics; if UTXO X
>> with some flag on it is not spent in the block it is created, it
>> expires and can never be spent. This is essentially an inverse
>> timelock, but severely limited to one block and mempool evictions
>> can be handled as if a conflict were mined.
>> 
>> These types of 0 value outputs could be present just for attaching
>> fee in the mempool but be treated like an op_return otherwise. We
>> could add two cases for this: one bare segwit version (just the
>> number, no data) and one that's equivalent to taproot. This covers
>> OP_TRUE anchors very efficiently and ones that require a signature
>> as well.
>> 
>> This is relatively similar to how Transaction Sponsors works, but
>> without full tx graph de-linkage... obviously I think if we'll
>> entertain a consensus change, sponsors makes more sense, but
>> expiring utxos doesn't change as many properties of the tx-graph
>> validation so might be simpler.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From chrismoss411 at gmail.com  Thu Dec  9 09:49:11 2021
From: chrismoss411 at gmail.com (Christian Moss)
Date: Thu, 9 Dec 2021 09:49:11 +0000
Subject: [bitcoin-dev] [Lightning-dev] Sending OP_RETURN via Bitcoin
	Lightning
In-Reply-To: <YbHImwix1z8BgAc2@petertodd.org>
References: <DD7D5A8B-F61F-4302-ACF4-CE731843D97D@gmail.com>
 <CALL-=e5mF9TqbbD=Cf-bawbw4dq2PGjC9W_nqAQeHsB829ZpNg@mail.gmail.com>
 <CALkkCJas_pf7Un45CJyFg8j9cBk8PtKN4iYAL81TtLSRNnKqeg@mail.gmail.com>
 <CANQKmgLyaYAjL_=LziTCFT=Ahc2SXjJrWc+RO59pxd3mnJApfQ@mail.gmail.com>
 <YbHImwix1z8BgAc2@petertodd.org>
Message-ID: <CANQKmg+D22GrLFDBAawop1uha3GG5F4TmixMajcTPQUdTF4-PQ@mail.gmail.com>

pete at petertodd.org, so single use seals require an onchain transaction to
post the proof of publication to the ledger (assuming bitcoin is used as
the ledger) when an asset is transferred, but it can scale because you can
batch many proofs (transfer of ownerships) into a merkle tree and just add
the merkle root into the single tx going into the ledger?

On Thu, Dec 9, 2021 at 9:13 AM Peter Todd <pete at petertodd.org> wrote:

> On Mon, Dec 06, 2021 at 04:35:19PM +0000, Christian Moss via bitcoin-dev
> wrote:
> > As far as I understand it, RGB doesn't scale NFTs as each
> > transaction to transfer ownership of an NFT would require an onchain
> > transaction
>
> RGB intends to scale NFTs and similar things in the future via scalable
> single-use-seals:
> https://petertodd.org/2017/scalable-single-use-seal-asset-transfer
>
> --
> https://petertodd.org 'peter'[:-1]@petertodd.org
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211209/9c5f1126/attachment.html>

From pete at petertodd.org  Thu Dec  9 10:07:37 2021
From: pete at petertodd.org (Peter Todd)
Date: Thu, 9 Dec 2021 05:07:37 -0500
Subject: [bitcoin-dev] [Lightning-dev] Sending OP_RETURN via Bitcoin
 Lightning
In-Reply-To: <CANQKmg+D22GrLFDBAawop1uha3GG5F4TmixMajcTPQUdTF4-PQ@mail.gmail.com>
References: <DD7D5A8B-F61F-4302-ACF4-CE731843D97D@gmail.com>
 <CALL-=e5mF9TqbbD=Cf-bawbw4dq2PGjC9W_nqAQeHsB829ZpNg@mail.gmail.com>
 <CALkkCJas_pf7Un45CJyFg8j9cBk8PtKN4iYAL81TtLSRNnKqeg@mail.gmail.com>
 <CANQKmgLyaYAjL_=LziTCFT=Ahc2SXjJrWc+RO59pxd3mnJApfQ@mail.gmail.com>
 <YbHImwix1z8BgAc2@petertodd.org>
 <CANQKmg+D22GrLFDBAawop1uha3GG5F4TmixMajcTPQUdTF4-PQ@mail.gmail.com>
Message-ID: <YbHVaU19CFvOQ3MB@petertodd.org>

On Thu, Dec 09, 2021 at 09:49:11AM +0000, Christian Moss wrote:
> pete at petertodd.org, so single use seals require an onchain transaction to
> post the proof of publication to the ledger (assuming bitcoin is used as
> the ledger) when an asset is transferred, but it can scale because you can
> batch many proofs (transfer of ownerships) into a merkle tree and just add
> the merkle root into the single tx going into the ledger?

Exactly. And since the aggregation is trustless with respect to validity, users
can choose what kind of censorship risk they're willing to take (as well as
mitigate it with "multisig" schemes that use multiple aggregators in parallel).

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211209/1f650ac2/attachment.sig>

From alex.schoof at gmail.com  Thu Dec  9 12:12:45 2021
From: alex.schoof at gmail.com (Alex Schoof)
Date: Thu, 9 Dec 2021 07:12:45 -0500
Subject: [bitcoin-dev] [Lightning-dev] Sending OP_RETURN via Bitcoin
	Lightning
In-Reply-To: <YbHVaU19CFvOQ3MB@petertodd.org>
References: <DD7D5A8B-F61F-4302-ACF4-CE731843D97D@gmail.com>
 <CALL-=e5mF9TqbbD=Cf-bawbw4dq2PGjC9W_nqAQeHsB829ZpNg@mail.gmail.com>
 <CALkkCJas_pf7Un45CJyFg8j9cBk8PtKN4iYAL81TtLSRNnKqeg@mail.gmail.com>
 <CANQKmgLyaYAjL_=LziTCFT=Ahc2SXjJrWc+RO59pxd3mnJApfQ@mail.gmail.com>
 <YbHImwix1z8BgAc2@petertodd.org>
 <CANQKmg+D22GrLFDBAawop1uha3GG5F4TmixMajcTPQUdTF4-PQ@mail.gmail.com>
 <YbHVaU19CFvOQ3MB@petertodd.org>
Message-ID: <CA+2b5C1bfHnNtz6jWRmzr-2Dz3DpyvE1Z_LFCsJgBpbP-bULYQ@mail.gmail.com>

The multisig scheme is interesting. From my understanding of Single Use
Seals, since seal n commits to seal n+1, for the on-chain aggregation seals
you would want to pick some common aggregation service provider ahead of
time and if that provider disappears, you?re stuck and cant close the next
seal. If instead you say ?this seal commits to three of the five of these
next seals? then you mitigate both availability and censorship risk. Am I
getting that right?

Alex

On Thu, Dec 9, 2021 at 5:23 AM Peter Todd <pete at petertodd.org> wrote:

> On Thu, Dec 09, 2021 at 09:49:11AM +0000, Christian Moss wrote:
> > pete at petertodd.org, so single use seals require an onchain transaction
> to
> > post the proof of publication to the ledger (assuming bitcoin is used as
> > the ledger) when an asset is transferred, but it can scale because you
> can
> > batch many proofs (transfer of ownerships) into a merkle tree and just
> add
> > the merkle root into the single tx going into the ledger?
>
> Exactly. And since the aggregation is trustless with respect to validity,
> users
> can choose what kind of censorship risk they're willing to take (as well as
> mitigate it with "multisig" schemes that use multiple aggregators in
> parallel).
>
> --
> https://petertodd.org 'peter'[:-1]@petertodd.org
> _______________________________________________
> Lightning-dev mailing list
> Lightning-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev
>
-- 


Alex Schoof
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211209/0a9cde55/attachment.html>

From pete at petertodd.org  Thu Dec  9 12:56:19 2021
From: pete at petertodd.org (Peter Todd)
Date: Thu, 9 Dec 2021 07:56:19 -0500
Subject: [bitcoin-dev] [Lightning-dev] Sending OP_RETURN via Bitcoin
 Lightning
In-Reply-To: <CA+2b5C1bfHnNtz6jWRmzr-2Dz3DpyvE1Z_LFCsJgBpbP-bULYQ@mail.gmail.com>
References: <DD7D5A8B-F61F-4302-ACF4-CE731843D97D@gmail.com>
 <CALL-=e5mF9TqbbD=Cf-bawbw4dq2PGjC9W_nqAQeHsB829ZpNg@mail.gmail.com>
 <CALkkCJas_pf7Un45CJyFg8j9cBk8PtKN4iYAL81TtLSRNnKqeg@mail.gmail.com>
 <CANQKmgLyaYAjL_=LziTCFT=Ahc2SXjJrWc+RO59pxd3mnJApfQ@mail.gmail.com>
 <YbHImwix1z8BgAc2@petertodd.org>
 <CANQKmg+D22GrLFDBAawop1uha3GG5F4TmixMajcTPQUdTF4-PQ@mail.gmail.com>
 <YbHVaU19CFvOQ3MB@petertodd.org>
 <CA+2b5C1bfHnNtz6jWRmzr-2Dz3DpyvE1Z_LFCsJgBpbP-bULYQ@mail.gmail.com>
Message-ID: <YbH884j/+rZNcMjG@petertodd.org>

On Thu, Dec 09, 2021 at 07:12:45AM -0500, Alex Schoof wrote:
> The multisig scheme is interesting. From my understanding of Single Use
> Seals, since seal n commits to seal n+1, for the on-chain aggregation seals
> you would want to pick some common aggregation service provider ahead of
> time and if that provider disappears, you?re stuck and cant close the next
> seal. If instead you say ?this seal commits to three of the five of these
> next seals? then you mitigate both availability and censorship risk. Am I
> getting that right?

Re: "some common aggregation service provider", you might be misunderstanding
the protocol: since seals are trustless with regard to validity, I can validate
your seal, regardless of which aggregation service you use.

But other than that, I think we're on the same page!

A concrete example would be an exchange: they do a lot of transactions, so they
could choose to be their own aggregator, and wouldn't need any multisig at all
because they can trust themselves not to censor themselves. :) Meanwhile, one
of their customers might use 3-of-5 as you suggest, as they only do a few
transactions a month.

Interestingly, in some scenarios it might be worthwhile to both run your own
aggregator, and use multisig. Eg Alice could use a 2-of-3 with two third-party
aggregators, and her own aggregation chain. If both third-parties are up, she
does no on-chain transactions at all; if one third-party is down, she can use
her own, and the remaining third-party. Thus she would only do an on-chain
transaction to defeat censorship/failure.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211209/6edd19ec/attachment.sig>

From pete at petertodd.org  Thu Dec  9 13:50:33 2021
From: pete at petertodd.org (Peter Todd)
Date: Thu, 9 Dec 2021 08:50:33 -0500
Subject: [bitcoin-dev] A fee-bumping model
In-Reply-To: <hBx6OYA5Mv9C_anoMQ-s-9l_XNwNFPfDVmOND9pXBJEBi7qsULF3bgPGpagtqjOsKDTXu8iOTVzvOjflz-M6EfnfwVH81Cu-nnai0kakouo=@protonmail.com>
References: <hBx6OYA5Mv9C_anoMQ-s-9l_XNwNFPfDVmOND9pXBJEBi7qsULF3bgPGpagtqjOsKDTXu8iOTVzvOjflz-M6EfnfwVH81Cu-nnai0kakouo=@protonmail.com>
Message-ID: <YbIJqX+oK6hio4C3@petertodd.org>

On Mon, Nov 29, 2021 at 02:27:23PM +0000, darosior via bitcoin-dev wrote:
> ## 2. Problem statement
> 
> For any delegated vault, ensure the confirmation of a Cancel transaction in a configured number of
> blocks at any point. In so doing, minimize the overpayments and the UTxO set footprint. Overpayments
> increase the burden on the watchtower operator by increasing the required frequency of refills of the
> fee-bumping wallet, which is already the worst user experience. You are likely to manage a number of
> UTxOs with your number of vaults, which comes at a cost for you as well as everyone running a full
> node.
> 
> Note that this assumes miners are economically rationale, are incentivized by *public* fees and that
> you have a way to propagate your fee-bumped transaction to them. We also don't consider the block
> space bounds.
> 
> In the previous paragraph and the following text, "vault" can generally be replaced with "offchain
> contract".

For this section I think it'd help if you re-wrote it mathematically in terms
of probabilities, variance, and costs. It's impossible to ensure confirmation
with 100% probability, so obviously we can start by asking what is the cost of
failing to get a confirmation by the deadline?

Now suppose that cost is X. Note how the lowest _variance_ approach would be to
pay a fee of X immediately: that would get us the highest possible probability
of securing a confirmation prior to the deadline, without paying more than that
confirmation is worth.

Of course, this is silly! So the next step is to trade off some variance -
higher probability of failure - for lower expected cost. A trivial way to do
that could be to just bump the fee linearly between now and that deadline. That
lowers expected cost. But does increase the probability of failure. We can of
course account for this in an expected cost.


One final nuance here is if this whole process is visible in the UI we might
want to take into account user discomfort: if I know the process could fail,
the user will probably be happier if it succeeds quickly, even if the
probability of success in the future is still very high.


FWIW the approach taken by the OpenTimestamps calendars is a trivial linear
increase. While they don't have deadlines in the same sense as your
application, there is a trade-off between cost and confirmation time. So our
strategy is to spend money at a constant rate by simply bumping the fee by the
same amount at every new block. I could improve this by using knowledge of the
mempool. But so far I haven't bothered.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211209/7d38040e/attachment.sig>

From jlrubin at mit.edu  Thu Dec  9 18:24:44 2021
From: jlrubin at mit.edu (Jeremy)
Date: Thu, 9 Dec 2021 10:24:44 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar]: Congestion Control
Message-ID: <CAD5xwhh7azm8jydaTwwyXT1w_VwgLyXf_KAgyBM2P8XHkUqfLg@mail.gmail.com>

Today's post is a follow up to some older content about congestion control
& CTV.

It's written (as with the rest of the series) to be a bit more approachable
than technical, but there are code samples in Sapio of constructing a
payout tree.

today's post:
https://rubin.io/bitcoin/2021/12/09/advent-12/

older posts:
- https://utxos.org/analysis/bip_simulation/
- https://utxos.org/analysis/batching_sim/

Generally, I think the importance and potential of congestion control is
currently understated. The next couple posts will build on this with Coin
Pools, Mining Pools, and Lighting which also leverage congestion control
structures with multi-party opt-outs for added punch. But even in the base
case, these congestion control primitives can be really important for large
volume large value businesses to close out liabilities reliably without
being impacted too much by transient chain weather. Those types of demand
(high volume, high value) aren't served well by the lightning network
(ever) since the large values of flows would be difficult to route and
might prefer being deposited directly into cold storage given the amounts
at stake.

best,

Jeremy

--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211209/f7e175bc/attachment-0001.html>

From prayank at tutanota.de  Fri Dec 10 15:13:02 2021
From: prayank at tutanota.de (Prayank)
Date: Fri, 10 Dec 2021 16:13:02 +0100 (CET)
Subject: [bitcoin-dev] Rebroadcast mechanism in Bitcoin P2P network
Message-ID: <MqZttWy--3-2@tutanota.de>

Hello World,

I had started working on this blog dedicated to Hal Finney in August: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-August/019367.html

I have been able to track more than 10 Issues and Pull Requests from different Bitcoin projects that are focused on privacy. Wrote 3 blog posts and will write more often as I learn new things. There is a section called 'Hall of Fame' and 7 developers are listed in hof who worked on one or more pull requests that helped improve privacy in Bitcoin projects: Andrew Chow, chimp1984, jmacxx, Luke Dashjr, Samuel Dobson, Vasil Dimov and wpaulino.

Last post is about 'Rebroadcast mechanism' used in Bitcoin full node implementations: https://prayank23.github.io/camouflage//blog/rebroadcast/

Problem: Rebroadcast mechanism used in Bitcoin Core and Knots, rebroadcasts only our transactions. This helps spy nodes to link bitcoin addresses with IP addresses and also know that wallets are enabled for a node.

Solution by Amiti Uttarwar: New rebroadcast mechanism in which transactions are re-broadcasted based on fee rate and mempool age.

I have shared other details, my opinion and links to comments by Suhas Daftuar in the blog post since related pull request has been in draft mode for some time now.

-- 
Prayank

A3B1 E430 2298 178F
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211210/994e89f4/attachment.html>

From jlrubin at mit.edu  Fri Dec 10 23:01:34 2021
From: jlrubin at mit.edu (Jeremy)
Date: Fri, 10 Dec 2021 15:01:34 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Payment Pools/ Coin Pools
Message-ID: <CAD5xwhj2ncK70xjbXKeF=7oubHJs29D6Hwkt-XQxN1CG8-7Fbg@mail.gmail.com>

This post showcases building payment pools / coin pools* in Sapio!

https://rubin.io/bitcoin/2021/12/10/advent-13/

There will be many more posts in the series that will take this concept a
lot further and showcase some more advanced things that can be built.

I think that payment pools are incredibly exciting -- we know that it's
going to be tough to give every human a UTXO, even with Lightning. Payment
Pools promise to help compress that chain load into single utxos so that
users can be perfectly secure with a proof root and just need to do some
transactions to recover their coins. While channels could live inside of
payment pools, scaling via payment pools without nested channels can be
nice because there is no degradation of assumptions for the coins inside
being able to broadcast transactions quickly.

Payment pools in Sapio also provide a natural evolution path for things
like Rollups (they're essentially federated rollups with unilateral exits),
where state transitions in pools could one day be enforced by either
covenants or some sort of ZK system in place of N-of-N signatures.

Hopefully this stimulates some folks to muck around with Sapio and
experiment creating their own custom Payment Pools! I'd love to see someone
hack some kind of EVM into the state transition function of a payment pool
;)

Cheers,

Jeremy

* we should probably nail down some terminology -- I think Payment Pools /
Coin Pools are kinda "generic" names for the technique, but we should give
specific protocols more specific names like payment channels : lightning
network.

--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211210/4358fb3d/attachment.html>

From damian at willtech.com.au  Sat Dec 11 03:49:55 2021
From: damian at willtech.com.au (damian at willtech.com.au)
Date: Fri, 10 Dec 2021 19:49:55 -0800
Subject: [bitcoin-dev] Rebroadcast mechanism in Bitcoin P2P network
In-Reply-To: <MqZttWy--3-2@tutanota.de>
References: <MqZttWy--3-2@tutanota.de>
Message-ID: <1fbf0ef8b1b42979361b5df0b09c2dcd@willtech.com.au>

Good Afternoon,

Thank-you for your good work cataloguing and writing about the 
contributions to Bitcoin.

It is that the solution to privacy is to use privacy-enhancing network 
communications, such as TOR. I am not against a mechanism to rebroadcast 
transactions more robustly if the mempool of adjoining nodes has 
forgotten about them, but the truth is, all transactions originate from 
some node, and there are methods that allow an individual node to be 
identified as the likely source of a transaction unless privacy-enabled 
networks are utilised. Having a different method to cause rebroadcast 
does not obfuscate the origin.

KING JAMES HRMH
Great British Empire

Regards,
The Australian
LORD HIS EXCELLENCY JAMES HRMH (& HMRH)
of Hougun Manor & Glencoe & British Empire
MR. Damian A. James Williamson
Wills

et al.


Willtech
www.willtech.com.au
www.go-overt.com
duigco.org DUIGCO API
and other projects


m. 0487135719
f. +61261470192


This email does not constitute a general advice. Please disregard this 
email if misdelivered.
On 2021-12-10 07:13, Prayank via bitcoin-dev wrote:
> Hello World,
> 
> I had started working on this blog dedicated to Hal Finney in August:
> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-August/019367.html
> 
> I have been able to track more than 10 Issues and Pull Requests from
> different Bitcoin projects that are focused on privacy. Wrote 3 blog
> posts and will write more often as I learn new things. There is a
> section called 'Hall of Fame' and 7 developers are listed in hof who
> worked on one or more pull requests that helped improve privacy in
> Bitcoin projects: Andrew Chow, chimp1984, jmacxx, Luke Dashjr, Samuel
> Dobson, Vasil Dimov and wpaulino.
> 
> Last post is about 'Rebroadcast mechanism' used in Bitcoin full node
> implementations:
> https://prayank23.github.io/camouflage//blog/rebroadcast/
> 
> Problem: Rebroadcast mechanism used in Bitcoin Core and Knots,
> rebroadcasts only our transactions. This helps spy nodes to link
> bitcoin addresses with IP addresses and also know that wallets are
> enabled for a node.
> 
> Solution by Amiti Uttarwar: New rebroadcast mechanism in which
> transactions are re-broadcasted based on fee rate and mempool age.
> 
> I have shared other details, my opinion and links to comments by Suhas
> Daftuar in the blog post since related pull request has been in draft
> mode for some time now.
> 
> --
> 
> Prayank
> 
> A3B1 E430 2298 178F
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From bitcoin-dev at wuille.net  Sat Dec 11 16:21:21 2021
From: bitcoin-dev at wuille.net (Pieter Wuille)
Date: Sat, 11 Dec 2021 16:21:21 +0000
Subject: [bitcoin-dev] Rebroadcast mechanism in Bitcoin P2P network
In-Reply-To: <1fbf0ef8b1b42979361b5df0b09c2dcd@willtech.com.au>
References: <MqZttWy--3-2@tutanota.de>
 <1fbf0ef8b1b42979361b5df0b09c2dcd@willtech.com.au>
Message-ID: <dSBrti8VFr0lMNgTHw8pD-mAPLm0E4auJa234o0FNY67EUuEy1Dfb93INNIzoUb3j2dWSJjZtG7qncci1LhKAHOXzAzbEWOtXjnggr19J6w=@wuille.net>

> It is that the solution to privacy is to use privacy-enhancing network
> communications, such as TOR. I am not against a mechanism to rebroadcast
> transactions more robustly if the mempool of adjoining nodes has
> forgotten about them, but the truth is, all transactions originate from
> some node, and there are methods that allow an individual node to be
> identified as the likely source of a transaction unless privacy-enabled
> networks are utilised. Having a different method to cause rebroadcast
> does not obfuscate the origin.

You're talking about distinct aspects of transaction privacy.

The rebroadcasting approach as it exists on the network, where wallets are responsible for their own rebroadcasting, directly reveals to your peers a relation between nodes and transactions: whenever any node relays the same transaction twice, it almost certainly implies they are the origin.

This is just a node-transaction relation, and not necessarily IP-transaction relation. The latter can indeed be avoided by only connecting over Tor, or using other privacy networks, but just hiding the relation with IP addresses isn't sufficient (and has its own downsides; e.g. Tor-only connectivity is far more susceptible to partition/Eclipse/DoS attacks). For example seeing the same node (even without knowing its IP) rebroadcast two transaction lets an observe infer a relation between those transactions, and that too is a privacy leak.

I believe moving to a model where mempools/nodes themselves are responsible for rebroadcasting is a great solution to improving this specific problem, simply because if everyone rebroadcasts, the original author doing it too does not stand out anymore. It isn't "fixing privacy", it's fixing a specific leak, one of many, but this isn't a black and white property.

Cheers,

--
Pieter


From jlrubin at mit.edu  Sat Dec 11 18:01:31 2021
From: jlrubin at mit.edu (Jeremy)
Date: Sat, 11 Dec 2021 10:01:31 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Payment Channels in a
	CTV+Sapio World
Message-ID: <CAD5xwhj5JOW+-m67SeTC3_K3cX12P1h2AOxn_FP2gCXiSZQ-KQ@mail.gmail.com>

hola devs,

This post details more formally a basic version of payment channels built
on top of CTV/Sapio and the implications of having non-interactive channel
creation.

https://rubin.io/bitcoin/2021/12/11/advent-14/

I'm personally incredibly bullish on where this concept can go since it
would make channel opening much more efficient, especially when paired with
the payment pool concept shared the other day.

Best,

Jeremy

--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211211/1e9e50c8/attachment.html>

From aymeric at peersm.com  Sun Dec 12 11:48:18 2021
From: aymeric at peersm.com (Aymeric Vitte)
Date: Sun, 12 Dec 2021 12:48:18 +0100
Subject: [bitcoin-dev] Rebroadcast mechanism in Bitcoin P2P network
In-Reply-To: <dSBrti8VFr0lMNgTHw8pD-mAPLm0E4auJa234o0FNY67EUuEy1Dfb93INNIzoUb3j2dWSJjZtG7qncci1LhKAHOXzAzbEWOtXjnggr19J6w=@wuille.net>
References: <MqZttWy--3-2@tutanota.de>
 <1fbf0ef8b1b42979361b5df0b09c2dcd@willtech.com.au>
 <dSBrti8VFr0lMNgTHw8pD-mAPLm0E4auJa234o0FNY67EUuEy1Dfb93INNIzoUb3j2dWSJjZtG7qncci1LhKAHOXzAzbEWOtXjnggr19J6w=@wuille.net>
Message-ID: <86d49c80-2f8f-245c-5fdb-17c6ca6b5f2b@peersm.com>

Indeed, I reiterate that using the Tor network for Bitcoin or whatever
protocol not related to the Tor Browser (ie browsing and HS) does not
make sense, for plenty of reasons

But using the Tor protocol outside of the Tor network (and inside
browsers for wallets for example) does:
https://github.com/Ayms/node-Tor#presentation and
https://github.com/Ayms/node-Tor#phase-4-and-phase-5, anonymizing nodes
can just be already existing bitcoin nodes, example:
https://github.com/bitcoin/bitcoin/pull/18988#issuecomment-646564853


Le 11/12/2021 ? 17:21, Pieter Wuille via bitcoin-dev a ?crit :
>> It is that the solution to privacy is to use privacy-enhancing network
>> communications, such as TOR. I am not against a mechanism to rebroadcast
>> transactions more robustly if the mempool of adjoining nodes has
>> forgotten about them, but the truth is, all transactions originate from
>> some node, and there are methods that allow an individual node to be
>> identified as the likely source of a transaction unless privacy-enabled
>> networks are utilised. Having a different method to cause rebroadcast
>> does not obfuscate the origin.
> You're talking about distinct aspects of transaction privacy.
>
> The rebroadcasting approach as it exists on the network, where wallets are responsible for their own rebroadcasting, directly reveals to your peers a relation between nodes and transactions: whenever any node relays the same transaction twice, it almost certainly implies they are the origin.
>
> This is just a node-transaction relation, and not necessarily IP-transaction relation. The latter can indeed be avoided by only connecting over Tor, or using other privacy networks, but just hiding the relation with IP addresses isn't sufficient (and has its own downsides; e.g. Tor-only connectivity is far more susceptible to partition/Eclipse/DoS attacks). For example seeing the same node (even without knowing its IP) rebroadcast two transaction lets an observe infer a relation between those transactions, and that too is a privacy leak.
>
> I believe moving to a model where mempools/nodes themselves are responsible for rebroadcasting is a great solution to improving this specific problem, simply because if everyone rebroadcasts, the original author doing it too does not stand out anymore. It isn't "fixing privacy", it's fixing a specific leak, one of many, but this isn't a black and white property.
>
> Cheers,
>
> --
> Pieter
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211212/21aee049/attachment.html>

From gmkarl at gmail.com  Sun Dec 12 13:38:18 2021
From: gmkarl at gmail.com (Karl)
Date: Sun, 12 Dec 2021 08:38:18 -0500
Subject: [bitcoin-dev] Rebroadcast mechanism in Bitcoin P2P network
In-Reply-To: <86d49c80-2f8f-245c-5fdb-17c6ca6b5f2b@peersm.com>
References: <MqZttWy--3-2@tutanota.de>
 <1fbf0ef8b1b42979361b5df0b09c2dcd@willtech.com.au>
 <dSBrti8VFr0lMNgTHw8pD-mAPLm0E4auJa234o0FNY67EUuEy1Dfb93INNIzoUb3j2dWSJjZtG7qncci1LhKAHOXzAzbEWOtXjnggr19J6w=@wuille.net>
 <86d49c80-2f8f-245c-5fdb-17c6ca6b5f2b@peersm.com>
Message-ID: <CALL-=e5mv5cr=Oqip0ob8P2XCbWPgOc67nnRjn7_OuDXPYUpVg@mail.gmail.com>

On Sun, Dec 12, 2021, 7:42 AM Aymeric Vitte via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Indeed, I reiterate that using the Tor network for Bitcoin or whatever
> protocol not related to the Tor Browser (ie browsing and HS) does not make
> sense, for plenty of reasons
>

Please cite this.  It is very hard to believe.

Personally, I have encountered network blocking of bitcoin peers, and Tor
is one way to reconnect with the network when this happens.


Regardless, reasonable rebroadcasting of nonlocal transactions is a
hands-down good thing.  This does not make them anonymous, but it does make
it a little harder to track their origin, and additionally it makes their
transmission more robust.

Every extra measure is a good thing, as everything eventually fails.

>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211212/b747ded8/attachment.html>

From aymeric at peersm.com  Sun Dec 12 14:23:44 2021
From: aymeric at peersm.com (Aymeric Vitte)
Date: Sun, 12 Dec 2021 15:23:44 +0100
Subject: [bitcoin-dev] Rebroadcast mechanism in Bitcoin P2P network
In-Reply-To: <CALL-=e5mv5cr=Oqip0ob8P2XCbWPgOc67nnRjn7_OuDXPYUpVg@mail.gmail.com>
References: <MqZttWy--3-2@tutanota.de>
 <1fbf0ef8b1b42979361b5df0b09c2dcd@willtech.com.au>
 <dSBrti8VFr0lMNgTHw8pD-mAPLm0E4auJa234o0FNY67EUuEy1Dfb93INNIzoUb3j2dWSJjZtG7qncci1LhKAHOXzAzbEWOtXjnggr19J6w=@wuille.net>
 <86d49c80-2f8f-245c-5fdb-17c6ca6b5f2b@peersm.com>
 <CALL-=e5mv5cr=Oqip0ob8P2XCbWPgOc67nnRjn7_OuDXPYUpVg@mail.gmail.com>
Message-ID: <10dcd9a9-0c39-7f4f-acde-04912a32e103@peersm.com>

Using the Tor network to bypass censorship for bitcoin can work but is a
very poor solution, the Tor network is very centralized, very small,
watched and controlled, with plenty of features that do not apply to
other protocols than those made to be used with the Tor Browser, Pieter
gave a simple example, that you can solve easily changing the circuits,
the problem remains that you really need to be a super expert to escape
all the dangers of the Tor network, not even sure it's possible unless
you use something else than the Tor project code

Believe it or not, node-Tor is a more than ten years old project (and
not a duplicate of the Tor network), so I know what I am talking about,
different studies of mine show also that the more you try to hide the
more you can get caught, even on really decentralized networks like
bittorrent, unlike another common belief that in such big networks it's
difficult to track/deanonymize peers, it is not

Extra measures like rebroadcasting can maybe add something, but back to
the previous sentence, extra measures can also help to catch/track you
if not well designed/thought

What I am proposing since years, not only to bitcoin, is to use the Tor
protocol independently of the Tor network, and from the browsers also
acting as nodes (not to be misunderstood with the Tor Browser, this has
nothing to do) probably someone one day will understand it

Le 12/12/2021 ? 14:38, Karl a ?crit :
>
>
> On Sun, Dec 12, 2021, 7:42 AM Aymeric Vitte via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org
> <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:
>
>     Indeed, I reiterate that using the Tor network for Bitcoin or
>     whatever protocol not related to the Tor Browser (ie browsing and
>     HS) does not make sense, for plenty of reasons
>
>
> Please cite this.  It is very hard to believe.
>
> Personally, I have encountered network blocking of bitcoin peers, and
> Tor is one way to reconnect with the network when this happens.
>
>
> Regardless, reasonable rebroadcasting of nonlocal transactions is a
> hands-down good thing.  This does not make them anonymous, but it does
> make it a little harder to track their origin, and additionally it
> makes their transmission more robust.
>
> Every extra measure is a good thing, as everything eventually fails.
>

-- 
Sophia-Antipolis, France
LinkedIn: https://fr.linkedin.com/in/aymeric-vitte-05855b26
GitHub : https://www.github.com/Ayms
Move your coins by yourself (browser version): https://peersm.com/wallet
Bitcoin transactions made simple: https://github.com/Ayms/bitcoin-transactions
torrent-live: https://github.com/Ayms/torrent-live
node-Tor : https://www.github.com/Ayms/node-Tor
Zcash wallets made simple: https://github.com/Ayms/zcash-wallets
Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets
Anti-spies and private torrents, dynamic blocklist: http://torrent-live.peersm.com
Peersm : http://www.peersm.com

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211212/4e661983/attachment-0001.html>

From jlrubin at mit.edu  Sun Dec 12 16:43:12 2021
From: jlrubin at mit.edu (Jeremy)
Date: Sun, 12 Dec 2021 08:43:12 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Decentralized Coordination
	Free Mining Pools
Message-ID: <CAD5xwhgOK6p7fqZPha1jvDgo=4Syti9K46a2A48Eas44dn9v6Q@mail.gmail.com>

Howdy, welcome to day 15!

Today's post covers a form of a mining pool that can be operated as sort of
a map-reduce over blocks without any "infrastructure".

https://rubin.io/bitcoin/2021/12/12/advent-15/

There's still some really open-ended questions (perhaps for y'all to
consider) around how to select an analyze the choice of window and payout
functions, but something like this could alleviate a lot of the
centralization pressures typically faced by pools.

Notably, compared to previous attempts, combining the payment pool payout
with this concept means that there is practically very little on-chain
overhead from this approach as the chain-load
for including payouts in every block is deferred for future cooperation
among miners. Although that can be considered cooperation itself, if you
think of it like a pipeline, the cooperation happens out of band from
mining and block production so it really is coordination free to mine.


Cheers,

Jeremy

--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211212/ac3c5c3d/attachment.html>

From bitcoin-dev at wuille.net  Sun Dec 12 15:15:16 2021
From: bitcoin-dev at wuille.net (Pieter Wuille)
Date: Sun, 12 Dec 2021 15:15:16 +0000
Subject: [bitcoin-dev] Rebroadcast mechanism in Bitcoin P2P network
In-Reply-To: <10dcd9a9-0c39-7f4f-acde-04912a32e103@peersm.com>
References: <MqZttWy--3-2@tutanota.de>
 <1fbf0ef8b1b42979361b5df0b09c2dcd@willtech.com.au>
 <dSBrti8VFr0lMNgTHw8pD-mAPLm0E4auJa234o0FNY67EUuEy1Dfb93INNIzoUb3j2dWSJjZtG7qncci1LhKAHOXzAzbEWOtXjnggr19J6w=@wuille.net>
 <86d49c80-2f8f-245c-5fdb-17c6ca6b5f2b@peersm.com>
 <CALL-=e5mv5cr=Oqip0ob8P2XCbWPgOc67nnRjn7_OuDXPYUpVg@mail.gmail.com>
 <10dcd9a9-0c39-7f4f-acde-04912a32e103@peersm.com>
Message-ID: <cd6J1hP7ryGspj5LgGk8uwSsh5FGoDwW9UCp4qHhbSn3MXgzMw1slbo8xwWv9kZxn95CcPPE5elPiOktz4-1drpiOgfi-TtGzr66cdS6AuI=@wuille.net>

On Sunday, December 12th, 2021 at 9:23 AM, Aymeric Vitte via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:

> Using the Tor network to bypass censorship for bitcoin can work but is a very poor solution, the Tor network is very centralized, very small, watched and controlled, with plenty of features that do not apply to other protocols than those made to be used with the Tor Browser, Pieter gave a simple example, that you can solve easily changing the circuits, the problem remains that you really need to be a super expert to escape all the dangers of the Tor network, not even sure it's possible unless you use something else than the Tor project code

FWIW, I wasn't talking about anything related to Tor's protocol or organization at all. What I meant is that because creating a hidden service has ~0 cost, it is trivial for anyone to spin up an arbitrary number of Bitcoin hidden services. Thus, if one runs a node that only connects to hidden services, it is fairly easily eclipsable.

It's just one example of a downside of (a particular way of) using Tor. That doesn't mean I recommend against using Tor for Bitcoin traffic at all; my point was simply that there are trade-offs, and aspects of privacy of the P2P protocol that Tor does not address, and thus one shouldn't assume that all problems are solved by "just use Tor".

Cheers,

--
Pieter
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211212/f92b2ade/attachment.html>

From prayank at tutanota.de  Sun Dec 12 18:49:37 2021
From: prayank at tutanota.de (Prayank)
Date: Sun, 12 Dec 2021 19:49:37 +0100 (CET)
Subject: [bitcoin-dev] Rebroadcast mechanism in Bitcoin P2P network
Message-ID: <MqjydfG--7-2@tutanota.de>

Hi Aymeric,
> What I am proposing since years, not only to bitcoin, is to use the Tor
protocol independently of the Tor network, and from the browsers alsoacting as nodes (not to be misunderstood with the Tor Browser, this hasnothing to do) probably someone one day will understand it
I understand the concept and like it. However had some issues with use of JavaScript and WebRTC which were partially answered in https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-August/019373.html

I like that you don't give up, passionate about privacy, nodes and contributing to Bitcoin. Not sure if you have commits in Bitcoin Core repository which is one of the weird requirements to get free tickets for open source stage of https://b.tc/conference/

I think presenting your idea with some demo, talking to other developers in community IRL would help your project. If you agree and interested to participate, please apply here: https://b.tc/conference/open-source

I have already requested few people and recommend you to share things about your project in conference. Let me know if you need sponsors for flight tickets as well.

Happy Weekend!
-- 
Prayank

A3B1 E430 2298 178F
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211212/fd6607c2/attachment-0001.html>

From damian at willtech.com.au  Sun Dec 12 22:32:21 2021
From: damian at willtech.com.au (damian at willtech.com.au)
Date: Sun, 12 Dec 2021 14:32:21 -0800
Subject: [bitcoin-dev] Rebroadcast mechanism in Bitcoin P2P network
In-Reply-To: <dSBrti8VFr0lMNgTHw8pD-mAPLm0E4auJa234o0FNY67EUuEy1Dfb93INNIzoUb3j2dWSJjZtG7qncci1LhKAHOXzAzbEWOtXjnggr19J6w=@wuille.net>
References: <MqZttWy--3-2@tutanota.de>
 <1fbf0ef8b1b42979361b5df0b09c2dcd@willtech.com.au>
 <dSBrti8VFr0lMNgTHw8pD-mAPLm0E4auJa234o0FNY67EUuEy1Dfb93INNIzoUb3j2dWSJjZtG7qncci1LhKAHOXzAzbEWOtXjnggr19J6w=@wuille.net>
Message-ID: <a249de6ad8bbd739612e4b177459c626@willtech.com.au>

Good Afternoon,

You are right, of course, I did nothing to differentiate between the 
privacy of the connection of the node, the identification of the public 
IP of the node, and the suspected original of a transaction.

If I understand, the reason for only the originating node to rebroadcast 
was because only that node can be authoritative,  but that logic is 
fallible once the transaction is signed - none of the nodes apart from 
the origin know about the transaction but they always manage to gossip.

Anyway, it is concept ACK from me and I know it has been a concern that 
I have raised previously, I presume some pseudo-random and lengthening 
per attempt length of time between receiving gossip about a transaction 
and rebroadcasting attempts. I have always worked with 
`mempoolexpiry=2160` and `maxmempool=900` and so far as I can presume 
mempool has never been full.

Regards,
The Australian
LORD HIS EXCELLENCY JAMES HRMH (& HMRH)
of Hougun Manor & Glencoe & British Empire
MR. Damian A. James Williamson
Wills

et al.


Willtech
www.willtech.com.au
www.go-overt.com
duigco.org DUIGCO API
and other projects


m. 0487135719
f. +61261470192


This email does not constitute a general advice. Please disregard this 
email if misdelivered.
On 2021-12-11 08:21, Pieter Wuille via bitcoin-dev wrote:
>> It is that the solution to privacy is to use privacy-enhancing network
>> communications, such as TOR. I am not against a mechanism to 
>> rebroadcast
>> transactions more robustly if the mempool of adjoining nodes has
>> forgotten about them, but the truth is, all transactions originate 
>> from
>> some node, and there are methods that allow an individual node to be
>> identified as the likely source of a transaction unless 
>> privacy-enabled
>> networks are utilised. Having a different method to cause rebroadcast
>> does not obfuscate the origin.
> 
> You're talking about distinct aspects of transaction privacy.
> 
> The rebroadcasting approach as it exists on the network, where wallets
> are responsible for their own rebroadcasting, directly reveals to your
> peers a relation between nodes and transactions: whenever any node
> relays the same transaction twice, it almost certainly implies they
> are the origin.
> 
> This is just a node-transaction relation, and not necessarily
> IP-transaction relation. The latter can indeed be avoided by only
> connecting over Tor, or using other privacy networks, but just hiding
> the relation with IP addresses isn't sufficient (and has its own
> downsides; e.g. Tor-only connectivity is far more susceptible to
> partition/Eclipse/DoS attacks). For example seeing the same node (even
> without knowing its IP) rebroadcast two transaction lets an observe
> infer a relation between those transactions, and that too is a privacy
> leak.
> 
> I believe moving to a model where mempools/nodes themselves are
> responsible for rebroadcasting is a great solution to improving this
> specific problem, simply because if everyone rebroadcasts, the
> original author doing it too does not stand out anymore. It isn't
> "fixing privacy", it's fixing a specific leak, one of many, but this
> isn't a black and white property.
> 
> Cheers,
> 
> --
> Pieter
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From vjudeu at gazeta.pl  Sun Dec 12 23:14:45 2021
From: vjudeu at gazeta.pl (vjudeu at gazeta.pl)
Date: Mon, 13 Dec 2021 00:14:45 +0100
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Decentralized
 Coordination Free Mining Pools
In-Reply-To: <CAD5xwhgOK6p7fqZPha1jvDgo=4Syti9K46a2A48Eas44dn9v6Q@mail.gmail.com>
Message-ID: <57315643-a4c33a2ff7196be1070725cf1903435e@pmq6v.m5r2.onet>

> how to select an analyze the choice of window
Currently, we need 100 blocks to spend the coinbase transaction and I think that should be our "window".
> and payout functions
Something like "miner-based difficulty" should do the trick. So, each miner is trying to produce its own block, with its own transactions, and its own coinbase reward (based on those transactions, if we want to think ahead and do it right from the start, we should be ready for situation where the basic block reward is zero and the whole coinbase is based only on transaction fees). So, each miner can mine a block with its own coinbase amount (based on transaction fees). Then, that miner should multiply the target by the number of satoshis collected in the coinbase transaction to get "target per satoshi". Then, by dividing this target by its block hash, it would produce the number of satoshis that miner should receive.
Some example:
difficulty: 170ba21f
target: 0000000000000000000ba21f0000000000000000000000000000000000000000
coinbase: 6.27930034 BTC (627930034 satoshis = 0x256d73b2 satoshis)
targetPerSatoshi: 0000000000000000000ba21f0000000000000000000000000000000000000000*0x256d73b2
targetPerSatoshi: 000000000001b367c41da68e0000000000000000000000000000000000000000
sampleShare: 0000000000000000b613738816247a7f4d357cae555996519cf5b543e9b3554b
minerReward: targetPerSatoshi/sampleShare=0x2642e (156718 satoshis = 0.00156718 BTC for this share)
Because we assume that the basic reward will be zero, we assume that all miners will include their own set of transactions. That means, to check if the miner really should receive that reward, checking all transactions is required. Assuming that most of the miners will have similar transactions in their mempools, for each share there is a need to only check transactions that were unknown by that miner. For all other previously validated transactions, miners can store a table like: "<txid> <fee>" and then quickly validate if the amount specified in the coinbase transaction is correct.
To avoid "share spam", we can use something like "miner-based difficulty" mentioned above. Everyone knows the network difficulty, but not all miners are directly connected. So, for each connection with each miner in our decentralized pool, we can define a difficulty for each connection. In this way, each node can specify the absolute minimum difficulty, where paying any reward is above the dust limit, and where including that miner makes sense. Then, each miner can produce shares and adjust miner-based difficulty, just to produce for example one share per 10 minutes (or per 30 seconds if we have enough resources to fully validate each share from each miner we are connected with in that time).
If we want to include really small miners (like CPU miners), then we need a way to allow sub-satoshi payments. That means, each small miner should mine to a single N-of-N taproot-based multisig, where the whole pot is then splitted between N miners in LN. That means, for example one output of 1000 satoshis can be shared between one million small CPU miners. Then, our target from example above is denominated in millisatoshis.
targetPerSatoshi: 000000000001b367c41da68e0000000000000000000000000000000000000000*0x3e8 (1000 in decimal)
targetPerMillisatoshi: 0000000006a4cd5613d29ab00000000000000000000000000000000000000000
On 2021-12-12 17:43:39 user Jeremy via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
Howdy, welcome to day 15!
?
Today's post covers a form of a mining pool that can be operated as sort of a map-reduce over blocks without any "infrastructure".
?
https://rubin.io/bitcoin/2021/12/12/advent-15/
?
There's still some really open-ended questions (perhaps for y'all to consider) around how to select an analyze the choice of window and payout functions, but something like this could alleviate a lot of the centralization pressures typically faced by pools.
?
Notably, compared to previous attempts, combining the payment pool payout with this concept means that there is practically very little on-chain overhead from this approach as the chain-load
for including payouts in every block is deferred for future cooperation among miners. Although that can be considered cooperation itself, if you think of it like a pipeline, the cooperation happens out of band from mining and block production so it really is coordination free to mine.
?
Cheers,
?
Jeremy
--
@JeremyRubin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211213/a6680906/attachment.html>

From jlrubin at mit.edu  Mon Dec 13 01:31:42 2021
From: jlrubin at mit.edu (Jeremy)
Date: Sun, 12 Dec 2021 17:31:42 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Decentralized
 Coordination Free Mining Pools
In-Reply-To: <57315643-a4c33a2ff7196be1070725cf1903435e@pmq6v.m5r2.onet>
References: <CAD5xwhgOK6p7fqZPha1jvDgo=4Syti9K46a2A48Eas44dn9v6Q@mail.gmail.com>
 <57315643-a4c33a2ff7196be1070725cf1903435e@pmq6v.m5r2.onet>
Message-ID: <CAD5xwhgPFxm9_K_z4gomDa9AV1FAdhCq+-YYnreJAS+jcW+4Sw@mail.gmail.com>

Hey there!

Thanks for your response!

One of the reasons to pick a longer window of, say, a couple difficulty
periods would be that you can make participation in the pool hedge you
against hashrate changes.

You're absolutely spot on to think about the impact of pooling w.r.t.
variance when fees > subsidy. That's not really in the analysis I had in
the (old) post, but when the block revenues swing, dcfmp over longer
periods can really smooth out the revenues for miners in a great way. This can
also help with the "mind the gap" problem when there isn't a backlog of
transactions, since producing an empty block still has some value (in order
to incentivize mining transaction at all and not cheating, we need to
reward txn inclusion as I think you're trying to point out.

Sadly, I've read the rest of your email a couple times and I don't really
get what you're proposing at all. It jumps right into "things you could
compute". Can you maybe try stating the goals of your payout function, and
then demonstrate how what you're proposing meets that? E.g., we want to pay
more to miners that do x?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211212/092788f5/attachment.html>

From aymeric at peersm.com  Mon Dec 13 12:40:51 2021
From: aymeric at peersm.com (Aymeric Vitte)
Date: Mon, 13 Dec 2021 13:40:51 +0100
Subject: [bitcoin-dev] Rebroadcast mechanism in Bitcoin P2P network
In-Reply-To: <cd6J1hP7ryGspj5LgGk8uwSsh5FGoDwW9UCp4qHhbSn3MXgzMw1slbo8xwWv9kZxn95CcPPE5elPiOktz4-1drpiOgfi-TtGzr66cdS6AuI=@wuille.net>
References: <MqZttWy--3-2@tutanota.de>
 <1fbf0ef8b1b42979361b5df0b09c2dcd@willtech.com.au>
 <dSBrti8VFr0lMNgTHw8pD-mAPLm0E4auJa234o0FNY67EUuEy1Dfb93INNIzoUb3j2dWSJjZtG7qncci1LhKAHOXzAzbEWOtXjnggr19J6w=@wuille.net>
 <86d49c80-2f8f-245c-5fdb-17c6ca6b5f2b@peersm.com>
 <CALL-=e5mv5cr=Oqip0ob8P2XCbWPgOc67nnRjn7_OuDXPYUpVg@mail.gmail.com>
 <10dcd9a9-0c39-7f4f-acde-04912a32e103@peersm.com>
 <cd6J1hP7ryGspj5LgGk8uwSsh5FGoDwW9UCp4qHhbSn3MXgzMw1slbo8xwWv9kZxn95CcPPE5elPiOktz4-1drpiOgfi-TtGzr66cdS6AuI=@wuille.net>
Message-ID: <6341eec0-e6d8-fe15-4fde-b98d87b00c92@peersm.com>


> It's just one example of a downside of (a particular way of) using
> Tor. That doesn't mean I recommend against using Tor for Bitcoin
> traffic at all; my point was simply that there are trade-offs, and
> aspects of privacy of the P2P protocol that Tor does not address, and
> thus one shouldn't assume that all problems are solved by "just use Tor".
There are many downsides since the default behavior of the Tor network
does not apply to p2p networks, another example is a bitcoin node
exiting transactions (I thought you were referring to this), since the
same Tor circuit is used during some time most likely the transactions
are related to the same node even if we don't know its IP

According to the bitcoin github example discussion link I gave, I am not
saying that Tor network nodes should not be used, I am saying that they
should be used ? la node-Tor, or more precisely like the github example
and http://www.peersm.com/Convergence-2020.pdf, one of the main
differences are how behave the first node (ie the originating bitcoin
node), HS/RDV, nb of hops, hybrid nodes

Another drawback is that bitcoin community lets bitcoin nodes operators
play the way they like with torrc

@Prayank, regarding js/webrtc my previous answer was not partial, please
email in private if you need more, it's just a part of the project (but
important since disruptive), which is already advertised widely
(bitcoin, ipfs, covid apps, videoconf, etc, there are plenty of links on
github, lists, specs discussion, probably I should reference them), the
answer is always the same: "very interesting, go ahead", but no, it is
designed to be integrated by the projects, not by myself, and the only
thing missing to get rid of myself is to release phase4



From vjudeu at gazeta.pl  Mon Dec 13 14:10:49 2021
From: vjudeu at gazeta.pl (vjudeu at gazeta.pl)
Date: Mon, 13 Dec 2021 15:10:49 +0100
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Decentralized
 Coordination Free Mining Pools
Message-ID: <150216403-2fc1a5b1e9f48639bccf4fa9a5ebd62e@pmq4v.m5r2.onet>

> Can you maybe try stating the goals of your payout function, and then demonstrate how what you're proposing meets that?
?
The goals are quite simple: if you are a solo miner, you are trying to mine a block that meets the network difficulty. If you are using some kind of pool, then you are trying to mine N times easier blocks and receive N times lower reward for doing that. If many miners work on similar transactions, then each miner can validate each transaction once and assign transaction fee to transaction id, in this way the coinbase reward can be quickly checked, because you have to check only those transactions, which were unknown to you and for example included only by this miner and not broadcasted. Assuming that most of the transactions will be the same and included by most of the miners, that verification would be quick and can be simplified only to checking "what is different from what I am mining".
Also, to determine the proper amount of shares received, you can assign a difficulty for each miner. So, if you are connected to eight mining nodes, you can assign a difficulty to each of them, just to limit how much work for each share they can produce to have it accepted and included for payments. It is needed to avoid spamming by producing a lot of shares at difficulty one by bigger miners, they should find it more profitable to create bigger shares, because by accumulating them, it is cheaper to receive one bigger payment than a lot of smaller payments.
On 2021-12-13 14:59:58 user Jeremy <jlrubin at mit.edu> wrote:
Hey there!
?
Thanks for your response!
?
One of the reasons to pick a longer window of, say, a couple difficulty periods would be that you can make participation in the pool hedge you against hashrate changes.
?
You're absolutely spot on to think about the impact of pooling w.r.t. variance when fees > subsidy. That's not really in the analysis I had in the (old) post, but when the block revenues swing, dcfmp over longer periods can really smooth out the revenues for miners in a great way. This?can also help with the "mind the gap" problem when there isn't a backlog of transactions, since producing an empty block still has some value (in order to incentivize mining transaction at all and not cheating, we need to reward txn inclusion as I think you're trying to point out.
?
Sadly, I've read the rest of your email a couple times and I don't really get what you're proposing at all. It jumps right into "things you could compute". Can you maybe try stating the goals of your payout function, and then demonstrate how what you're proposing meets that? E.g., we want to pay more to miners that do x?
?
?
?
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211213/ac9d832f/attachment.html>

From jlrubin at mit.edu  Mon Dec 13 18:25:33 2021
From: jlrubin at mit.edu (Jeremy)
Date: Mon, 13 Dec 2021 10:25:33 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Composability in Sapio
	Contracts
Message-ID: <CAD5xwhhj+tF-oqOw6v4apPdP2OVB1Rb5+kiYpSmR0rc8cRQvcg@mail.gmail.com>

Devs,

Here's today's post: https://rubin.io/bitcoin/2021/12/13/advent-16/

It covers how you can use Sapio modules composably. This is an active area
of research for the Sapio platform, so definitely welcome and appreciate
ideas and feedback.

One area I'm particularly happy with but also unhappy with is the
"JSONSchema Type System". It is remarkably flexible, which is useful, but a
better type system would be able to enforce guarantees more strongly. Of
course, comparing to things like ERC-20, Eth interfaces aren't particularly
binding (functions could do anything) so maybe it's OK. If you have
thoughts on better ways to accomplish this, would love to think it through
more deeply. I'm particularly excited about ways to introduce more formal
correctness.

Cheers,

Jeremy

p.s. -- feel free to send me any general feedback on the series out
of band. There's a couple posts in the pipeline that are a bit less
development focused like the earlier posts I excluded, and I could filter
them if folks are feeling like it's too much information, but I'd bias
towards posting the remaining pieces as they come for continuity. Let me
know if you feel strongly about a couple posts that might be a topical
reach for this list.

--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211213/f9d8d86d/attachment.html>

From jlrubin at mit.edu  Tue Dec 14 18:37:59 2021
From: jlrubin at mit.edu (Jeremy)
Date: Tue, 14 Dec 2021 10:37:59 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] A Defense of Having Fun
	(and maybe staying poor)
Message-ID: <CAD5xwhj9SYwBK9J-K8W0wAnSQuT9e165W=k+=+ik8_ddrPPPFA@mail.gmail.com>

Hi Devs,

Today's post is a little more philosophical and less technical. Based on
the private feedback I received (from >1 persons, perhaps surprisingly)
I'll continue to syndicate the remaining posts to this list.

Here it is: https://rubin.io/bitcoin/2021/12/14/advent-17/

To having a little fun every now and again, as a treat,

Jeremy

--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211214/d26aab52/attachment.html>

From jlrubin at mit.edu  Tue Dec 14 19:39:06 2021
From: jlrubin at mit.edu (Jeremy)
Date: Tue, 14 Dec 2021 11:39:06 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Decentralized
 Coordination Free Mining Pools
In-Reply-To: <20211214190524.GA30559@mcelrath.org>
References: <CAD5xwhgOK6p7fqZPha1jvDgo=4Syti9K46a2A48Eas44dn9v6Q@mail.gmail.com>
 <20211214190524.GA30559@mcelrath.org>
Message-ID: <CAD5xwhiLBSCpErJTRbh05v+_i09daJTQQAtzYd-JcWXQojzT2A@mail.gmail.com>

Bitcoin didn't invent the concept of pooling:
https://en.wikipedia.org/wiki/Pooling_(resource_management). This is a
Bitcoin Mining Pool, although it may not be your favorite kind, which is
fixated on specific properties of computing contributions before finding a
block. Pooling is just a general technique for aggregating resources to
accomplish something. If you have another name like pooling that is in
common use for this type of activity I would be more than happy to adopt it.

This sort of pool can hedge not only against fee rates but also against
increases in hashrate since your historical rate 'carries' into the future
as a function of the window. Further, windows and reward functions can be
defined in a myriad of ways that could, e.g., pay less to blocks found in
more rapid succession, contributing to the smoothing functionality.

With respect to sub-block pooling, as described in the article, this sort
of design also helps with micro-pools being able to split resources
non-custodially in every block as a part of the higher order DCFMP. The
point is not, as noted, to enable solo mining an S9, but to decrease the
size of the minimum viable pool. It's also possible to add, without much
validation or data, some 'uncle block' type mechanism in an incentive
compatible way (e.g., add 10 pow-heavy headers on the last block for cost
48 bytes header + 32 bytes payout key) such that there's an incentive to
include the heaviest ones you've seen, not just your own, that are worth
further study and consideration (particularly because it's non-consensus,
only for opt-in participation in the pool).

With respect to space usage, it seems you wholly reject the viability of a
payment pool mechanism to cut-through chain space. Is this a critique that
holds for all Payment Pools, or just in the context of mining? Is there a
particular reason why you think it infeasible that "strongly online"
counterparties would be able to coordinate more efficiently? Is it
preferable for miners, the nexus of decentralization for Bitcoin, to prefer
to use custodial services for pooling (which may require KYC/AM) over
bearing a cost of some extra potential chainload?

Lastly, with respect to complexity, the proposal is actually incredibly
simple when you take it in a broader context. Non Interactive Channels and
Payment Pools are useful by themselves, so are the operations to merge them
and swap balance across them. Therefore most of the complexity in this
proposal is relying on tools we'll likely see in everyday use in any case,
DCFMP or no.

Jeremy
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211214/8d63da6e/attachment.html>

From jlrubin at mit.edu  Tue Dec 14 19:50:33 2021
From: jlrubin at mit.edu (Jeremy)
Date: Tue, 14 Dec 2021 11:50:33 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Decentralized
 Coordination Free Mining Pools
In-Reply-To: <CAD5xwhiLBSCpErJTRbh05v+_i09daJTQQAtzYd-JcWXQojzT2A@mail.gmail.com>
References: <CAD5xwhgOK6p7fqZPha1jvDgo=4Syti9K46a2A48Eas44dn9v6Q@mail.gmail.com>
 <20211214190524.GA30559@mcelrath.org>
 <CAD5xwhiLBSCpErJTRbh05v+_i09daJTQQAtzYd-JcWXQojzT2A@mail.gmail.com>
Message-ID: <CAD5xwhhx3JzUE0ggFo4Hmj4FTmWHBzeY7JHTrD__gFVFYz=H-g@mail.gmail.com>

I've received some confused messages that whatever I was replying to didn't
come through, I've reproduced Bob's e-mail below that I was responding to
for context:

























*This, quite simply, is not a "pool". A pool is by definition a tool to
reduceprofit variance by miners by collecting "weak blocks" that do not
meet thedifficulty target, so as to get a better statistical measure of
each miner'shashrate, which is used to subdivide profits. These are called
"shares" and areentirely absent here.The only available information here to
decide payouts is the blocks themselves,I do not have any higher statistics
measurement to subdivide payments. If Iexpect to earn 3 blocks within the
window, sometimes I will earn 2 and sometimesI will earn 4. Whether I keep
the entire coinbase in those 2-4 blocks, or I have100 other miners paying
me 1/100 as much 100 times, my payment is the same andmust be proportional
to the number of blocks I mine in the window.  My varianceis not
reduced.Further, by making miners pay other miners within the window N,
this results inN^2 payments to miners which otherwise would have had N
coinbase payments. So,this is extremely block-space inefficient for no good
reason. P2Pool had thesame problem and generated giant coinbases which
competed with fee revenue."Congestion control" makes this somewhat worse
since is it is an absoluteincrease in the block space consumed for these
N^2 payments.The only thing this proposal does do is smooth out fee
revenue. While hedging onfee revenue is valuable, this is an extremely
complicated and expensive way togo about it, that simultaneously *reduces*
fee revenue due to all the extrablock space used for miner payouts.*

>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211214/9be6323e/attachment.html>

From bob at mcelrath.org  Tue Dec 14 23:33:05 2021
From: bob at mcelrath.org (Bob McElrath)
Date: Tue, 14 Dec 2021 23:33:05 +0000
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Decentralized
 Coordination Free Mining Pools
In-Reply-To: <CAD5xwhgOK6p7fqZPha1jvDgo=4Syti9K46a2A48Eas44dn9v6Q@mail.gmail.com>
References: <CAD5xwhgOK6p7fqZPha1jvDgo=4Syti9K46a2A48Eas44dn9v6Q@mail.gmail.com>
Message-ID: <20211214233305.GB30559@mcelrath.org>

This, quite simply, is not a "pool". A pool is by definition a tool to reduce
profit variance by miners by collecting "weak blocks" that do not meet the
difficulty target, so as to get a better statistical measure of each miner's
hashrate, which is used to subdivide profits. These are called "shares" and are
entirely absent here.

The only available information here to decide payouts is the blocks themselves,
I do not have any higher statistics measurement to subdivide payments. If I
expect to earn 3 blocks within the window, sometimes I will earn 2 and sometimes
I will earn 4. Whether I keep the entire coinbase in those 2-4 blocks, or I have
100 other miners paying me 1/100 as much 100 times, my payment is the same and
must be proportional to the number of blocks I mine in the window.  My variance
is not reduced.

Further, by making miners pay other miners within the window N, this results in
N^2 payments to miners which otherwise would have had N coinbase payments. So,
this is extremely block-space inefficient for no good reason. P2Pool had the
same problem and generated giant coinbases which competed with fee revenue.
"Congestion control" makes this somewhat worse since is it is an absolute
increase in the block space consumed for these N^2 payments.

The only thing this proposal does do is smooth out fee revenue. While hedging on
fee revenue is valuable, this is an extremely complicated and expensive way to
go about it, that simultaneously *reduces* fee revenue due to all the extra
block space used for miner payouts.

Jeremy via bitcoin-dev [bitcoin-dev at lists.linuxfoundation.org] wrote:
> Howdy, welcome to day 15!
> 
> Today's post covers a form of a mining pool that can be operated as sort of a
> map-reduce over blocks without any "infrastructure".
> 
> https://rubin.io/bitcoin/2021/12/12/advent-15/
> 
> There's still some really open-ended questions (perhaps for y'all to consider)
> around how to select an analyze the choice of window and payout functions, but
> something like this could alleviate a lot of the centralization pressures
> typically faced by pools.
> 
> Notably, compared to previous attempts, combining the payment pool payout with
> this concept means that there is practically very little on-chain overhead from
> this approach as the chain-load
> for including payouts in every block is deferred for future cooperation among
> miners. Although that can be considered cooperation itself, if you think of it
> like a pipeline, the cooperation happens out of band from mining and block
> production so it really is coordination free to mine.
> 
> 
> Cheers,
> 
> Jeremy
> 
> --
> @JeremyRubin 
> !DSPAM:61b626be345321821816715!

> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 
> 
> !DSPAM:61b626be345321821816715!

--
Cheers, Bob McElrath

"For every complex problem, there is a solution that is simple, neat, and wrong."
    -- H. L. Mencken 


From bob at mcelrath.org  Wed Dec 15 00:12:00 2021
From: bob at mcelrath.org (Bob McElrath)
Date: Wed, 15 Dec 2021 00:12:00 +0000
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Decentralized
 Coordination Free Mining Pools
In-Reply-To: <CAD5xwhiLBSCpErJTRbh05v+_i09daJTQQAtzYd-JcWXQojzT2A@mail.gmail.com>
References: <CAD5xwhgOK6p7fqZPha1jvDgo=4Syti9K46a2A48Eas44dn9v6Q@mail.gmail.com>
 <20211214190524.GA30559@mcelrath.org>
 <CAD5xwhiLBSCpErJTRbh05v+_i09daJTQQAtzYd-JcWXQojzT2A@mail.gmail.com>
Message-ID: <20211215001200.GA35108@mcelrath.org>

You are hand waving. Attempting to redefine terms to justify your argument is
intellectually dishonest. Bitcoin pools have *always* been about variance
reduction. Your window function fundamentally CANNOT be used to hedge hashrate.
Various suggestions below introduce dangerous new games that might be played by
miners.

The fact is that the half-baked design you posted is less than useless, and
doesn't do anything that anyone wants.

You are trying to justify CTV by making it be all things to all people. "When
all you have is a hammer, every problem looks like a nail".  Instead I humbly
suggest that you pick ONE problem for which CTV is demonstrably the right and
best solution, instead of snowing us with a ton of half-baked things that
*could* be done, and often don't even require CTV, and some (like this one)
fundamentally don't work. I do like some of your ideas, but if you had to pick
just one "use case", which would it be?

Jeremy [jlrubin at mit.edu] wrote:
> Bitcoin didn't invent the concept of pooling: https://en.wikipedia.org/wiki/
> Pooling_(resource_management). This is a Bitcoin Mining Pool, although it may
> not be your favorite kind, which is fixated on specific properties of computing
> contributions before finding a block. Pooling is just a general technique for
> aggregating resources to accomplish something. If you have another name like
> pooling that is in common use for this type of activity I would be more than
> happy to adopt it.
> 
> This sort of pool can hedge not only against fee rates but also against
> increases in hashrate since your historical rate 'carries' into the future as a
> function of the window. Further, windows and reward functions can be defined in
> a myriad of ways that could, e.g., pay less to blocks found in more rapid
> succession, contributing to the smoothing functionality.
> 
> With respect to sub-block pooling, as described in the article, this sort of
> design also helps with micro-pools being able to split resources
> non-custodially in every block as a part of the higher order DCFMP. The point
> is not, as noted, to enable solo mining an S9, but to decrease the size of the
> minimum viable pool. It's also possible to add, without much validation or
> data, some 'uncle block' type mechanism in an incentive compatible way (e.g.,
> add 10 pow-heavy headers on the last block for cost 48 bytes header + 32 bytes
> payout key) such that there's an incentive to include the heaviest ones you've
> seen, not just your own, that are worth further study and consideration
> (particularly because it's non-consensus, only for opt-in participation in the
> pool).
> 
> With respect to space usage, it seems you wholly reject the viability of a
> payment pool mechanism to cut-through chain space. Is this a critique that
> holds for all Payment Pools, or just in the context of mining? Is there a
> particular reason why you think it infeasible that "strongly online"
> counterparties would be able to coordinate more efficiently? Is it preferable
> for miners, the nexus of decentralization for Bitcoin, to prefer to use
> custodial services for pooling (which may require KYC/AM) over bearing a cost
> of some extra potential chainload?
> 
> Lastly, with respect to complexity, the proposal is actually incredibly simple
> when you take it in a broader context. Non Interactive Channels and Payment
> Pools are useful?by themselves, so are the operations to merge them and swap
> balance across them. Therefore most of the complexity in this proposal is
> relying on tools we'll likely see in everyday use in any case, DCFMP or no.
> 
> Jeremy
> !DSPAM:61b8f2f5321461582627336!
--
Cheers, Bob McElrath

"For every complex problem, there is a solution that is simple, neat, and wrong."
    -- H. L. Mencken 


From jamtlu at gmail.com  Wed Dec 15 08:25:08 2021
From: jamtlu at gmail.com (James Lu)
Date: Wed, 15 Dec 2021 03:25:08 -0500
Subject: [bitcoin-dev] Bitcoin is a protocol
Message-ID: <CANQHGB1nBkAvb-71NLBGCSdAYBcpEmw83fvF7sdMQ+EAL7j9EQ@mail.gmail.com>

Bitcoin is a protocol. Protocols should be:

Secure;
Backwards compatible;
Forward compatible;
and agreed by consensus

For Bitcoin, these properties are particularly important.

The fourth one is important not just because Bitcoin is a payment network,
but because more eyes on code creates security. More eyes on code may help
protocol design.

Taproot is good.
Sapio proposal is good.

Let?s work together for constructive, positive, secure, and forward
compatible upgrades to Bitcoin.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211215/baabcba3/attachment.html>

From billy.tetrud at gmail.com  Wed Dec 15 17:25:15 2021
From: billy.tetrud at gmail.com (Billy Tetrud)
Date: Wed, 15 Dec 2021 09:25:15 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Decentralized
 Coordination Free Mining Pools
In-Reply-To: <20211215001200.GA35108@mcelrath.org>
References: <CAD5xwhgOK6p7fqZPha1jvDgo=4Syti9K46a2A48Eas44dn9v6Q@mail.gmail.com>
 <20211214190524.GA30559@mcelrath.org>
 <CAD5xwhiLBSCpErJTRbh05v+_i09daJTQQAtzYd-JcWXQojzT2A@mail.gmail.com>
 <20211215001200.GA35108@mcelrath.org>
Message-ID: <CAGpPWDYWnKNFGpxqY0WGq2cMf-rzEbu0paBa-3kL48FKtkQ-Cw@mail.gmail.com>

Looks like an interesting proposal, but it doesn't seem to quite match the
goals you mentioned. As you do mention, this mining pool coordination
doesn't get rid of the need for mining pools in the first place. So it
doesn't satisfy item 1 on your goal list afaict.

The primary benefits over what we have today that I can see are:
1. increased payout regularity, which lowers the viable size of mining
pools, and
2. Lower on chain footprint through combining pay outs from multiple pools.

Am I missing some?

These are interesting benefits, but it would be nice if your post was
clearer on that, since the goals list is not the same as the list of
potential benefits of this kind of design.

As far as enabling solo mining, what if this concept were used off chain?
Have a public network of solo miners who publish "weak blocks" to that
network, and the next 100 (or 1000 etc) nice miners pay you out as long as
you're also being nice by following the protocol? All the nice
optimizations you mentioned about eg combined taproot payouts would apply i
think. The only goals this wouldn't satisfy are 3 and 5 since an extra
network is needed, but to be fair, your proposal requires pools which all
need their own extra network anyways.

The missing piece here would be an ordering of weak blocks to make the
window possible. Or at least a way to determine what blocks should
definitely be part of a particular block's pay out. I could see this being
done by a separate ephemeral blockchain (which starts fresh after each
Bitcoin block) that keeps track of which weak blocks have been submitted,
potentially using the pow already in each block to secure it. Granted that
piece is a bit half baked, but it seems quite solvable. Wdyt?

One thing that jumped out at me as not safe is throwing block rewards into
a channel and being able to spend them immediately. There's a reason block
rewards aren't spendable for a while, and channels don't solve that
problem, do they? Why not simply reduce the on chain wait time for spending
block rewards at that point? Seems like the consequences would be the same.

On Tue, Dec 14, 2021, 16:12 Bob McElrath via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> You are hand waving. Attempting to redefine terms to justify your argument
> is
> intellectually dishonest. Bitcoin pools have *always* been about variance
> reduction. Your window function fundamentally CANNOT be used to hedge
> hashrate.
> Various suggestions below introduce dangerous new games that might be
> played by
> miners.
>
> The fact is that the half-baked design you posted is less than useless, and
> doesn't do anything that anyone wants.
>
> You are trying to justify CTV by making it be all things to all people.
> "When
> all you have is a hammer, every problem looks like a nail".  Instead I
> humbly
> suggest that you pick ONE problem for which CTV is demonstrably the right
> and
> best solution, instead of snowing us with a ton of half-baked things that
> *could* be done, and often don't even require CTV, and some (like this one)
> fundamentally don't work. I do like some of your ideas, but if you had to
> pick
> just one "use case", which would it be?
>
> Jeremy [jlrubin at mit.edu] wrote:
> > Bitcoin didn't invent the concept of pooling:
> https://en.wikipedia.org/wiki/
> > Pooling_(resource_management). This is a Bitcoin Mining Pool, although
> it may
> > not be your favorite kind, which is fixated on specific properties of
> computing
> > contributions before finding a block. Pooling is just a general
> technique for
> > aggregating resources to accomplish something. If you have another name
> like
> > pooling that is in common use for this type of activity I would be more
> than
> > happy to adopt it.
> >
> > This sort of pool can hedge not only against fee rates but also against
> > increases in hashrate since your historical rate 'carries' into the
> future as a
> > function of the window. Further, windows and reward functions can be
> defined in
> > a myriad of ways that could, e.g., pay less to blocks found in more rapid
> > succession, contributing to the smoothing functionality.
> >
> > With respect to sub-block pooling, as described in the article, this
> sort of
> > design also helps with micro-pools being able to split resources
> > non-custodially in every block as a part of the higher order DCFMP. The
> point
> > is not, as noted, to enable solo mining an S9, but to decrease the size
> of the
> > minimum viable pool. It's also possible to add, without much validation
> or
> > data, some 'uncle block' type mechanism in an incentive compatible way
> (e.g.,
> > add 10 pow-heavy headers on the last block for cost 48 bytes header + 32
> bytes
> > payout key) such that there's an incentive to include the heaviest ones
> you've
> > seen, not just your own, that are worth further study and consideration
> > (particularly because it's non-consensus, only for opt-in participation
> in the
> > pool).
> >
> > With respect to space usage, it seems you wholly reject the viability of
> a
> > payment pool mechanism to cut-through chain space. Is this a critique
> that
> > holds for all Payment Pools, or just in the context of mining? Is there a
> > particular reason why you think it infeasible that "strongly online"
> > counterparties would be able to coordinate more efficiently? Is it
> preferable
> > for miners, the nexus of decentralization for Bitcoin, to prefer to use
> > custodial services for pooling (which may require KYC/AM) over bearing a
> cost
> > of some extra potential chainload?
> >
> > Lastly, with respect to complexity, the proposal is actually incredibly
> simple
> > when you take it in a broader context. Non Interactive Channels and
> Payment
> > Pools are useful by themselves, so are the operations to merge them and
> swap
> > balance across them. Therefore most of the complexity in this proposal is
> > relying on tools we'll likely see in everyday use in any case, DCFMP or
> no.
> >
> > Jeremy
> > !DSPAM:61b8f2f5321461582627336!
> --
> Cheers, Bob McElrath
>
> "For every complex problem, there is a solution that is simple, neat, and
> wrong."
>     -- H. L. Mencken
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211215/9210a8de/attachment.html>

From jlrubin at mit.edu  Wed Dec 15 18:02:23 2021
From: jlrubin at mit.edu (Jeremy)
Date: Wed, 15 Dec 2021 10:02:23 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Sapio Studio Payment Pool
	Walthrough
Message-ID: <CAD5xwhg_Xedjsd68LgBJKH1y9cTp0yLX+Gees3kAZrC3gJie0g@mail.gmail.com>

Hi Devs,

Today's post is showing off how the Sapio Studio, the GUI smart contract
composer for Sapio, functions.
https://rubin.io/bitcoin/2021/12/15/advent-18/

In contrast to other posts this is mostly pictures.

This is a part of the project that could definitely use some development
assistance if anyone is interested in pushing the frontier of bitcoin
wallet functionality :)

Best,

Jeremy

--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211215/ba365020/attachment.html>

From jlrubin at mit.edu  Wed Dec 15 18:39:28 2021
From: jlrubin at mit.edu (Jeremy)
Date: Wed, 15 Dec 2021 10:39:28 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Decentralized
 Coordination Free Mining Pools
In-Reply-To: <CAGpPWDYWnKNFGpxqY0WGq2cMf-rzEbu0paBa-3kL48FKtkQ-Cw@mail.gmail.com>
References: <CAD5xwhgOK6p7fqZPha1jvDgo=4Syti9K46a2A48Eas44dn9v6Q@mail.gmail.com>
 <20211214190524.GA30559@mcelrath.org>
 <CAD5xwhiLBSCpErJTRbh05v+_i09daJTQQAtzYd-JcWXQojzT2A@mail.gmail.com>
 <20211215001200.GA35108@mcelrath.org>
 <CAGpPWDYWnKNFGpxqY0WGq2cMf-rzEbu0paBa-3kL48FKtkQ-Cw@mail.gmail.com>
Message-ID: <CAD5xwhhgk_2Es-YvRPnwjjpOChPmQwHeFaM9LQ1T8L+hYdz45Q@mail.gmail.com>

Hi Billy!

Thanks for your response. Some replies inline:


On Wed, Dec 15, 2021 at 10:01 AM Billy Tetrud via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Looks like an interesting proposal, but it doesn't seem to quite match the
> goals you mentioned. As you do mention, this mining pool coordination
> doesn't get rid of the need for mining pools in the first place. So it
> doesn't satisfy item 1 on your goal list afaict.
>

It does, actually :) Point 1 was

   1. Funds should not be centrally custodied, ever, if at all

And for top-level pool participants there is never any central custody.
What the windows are there (100 blocks, 2016, 4032, etc) is up to the
specific implementation which sets limits on how small you can be to
participate.

Further, for the entities that are too small:

from the article:
*> **The blocks that they mine should use a taproot address/key which is a
multisig of some portion of the workshares, that gets included in the
top-level pool as a part of Payment Pool.*

The micro-pools embed a multisig of top-contributors, 'reputable' members,
or on a rotating basis, as a leaf node to the parent. They then opt-out of
having their leaf channel-ized, as noted.

This would be fully non-custodial if we always included all miners. The
issue is that opens up DoS if one miner goes away, so you do want to anchor
around a few.

In this mode, you can set the protocol up such that immediately after
getting a reward in a block, you should see the chosen nodes for multi-sigs
distribute the spoils according to the schedule that is agreed on in the
block causing the share to be granted.

the main issue is data availability, without extra in-band storage local
mining pools have to track the work shares (which can be committed to in a
block) locally for auditing.

This is not fully non-custodial, but it doesn't have to be centrally
custodied by one party. We can multisig immediately after every block (and
nodes should quit their pool if they don't get sigs quickly perhaps).
Further, nodes can hash into multiple pools dividing their risk (modulo
sybil attack) across many pools.

If we had stronger covenants (CAT, AMOUNT, DIVIDE/MUL), we could make every
leaf node commit to payment pools that operate on percents instead of fixed
amounts and we'd be able to handle this in a manner that the payment pools
work no matter what amount is assigned to them.



The primary benefits over what we have today that I can see are:
> 1. increased payout regularity, which lowers the viable size of mining
> pools, and
> 2. Lower on chain footprint through combining pay outs from multiple pools.
>
> Am I missing some?
>
> These are interesting benefits, but it would be nice if your post was
> clearer on that, since the goals list is not the same as the list of
> potential benefits of this kind of design.
>

I think I hit all the benefits mentioned:

1. Funds should not be centrally custodied, ever, if at all.
see above -- we can do better for smaller miners, but we hit this for
miners above the threshold.

2. No KYC/AML.
see above, payouts are done 'decentralized' by every miner mining to the
payout

3. No ?Extra network? software required.
you need the WASM, but do not need any networked software to participate,
so there are no DoS concerns from participating.

You do need extra software to e.g. use channels or cut-through multiple
pools, but only after the fact of minding.

4. No blockchain bloat.

Very little, if cut-through + LN works.


5. No extra infrastructure.

Not much needed, if anything. I don't really know what 'infrastructure'
means, but I kind of imagined it to mean 'big expensive things' that would
make it hard to partake.


6. The size of a viable pool should be smaller. Remember our singer ? if
you just pool with one other songwriter it doesn?t make your expected time
till payout in your lifetime. So bigger the pools, more regular the
payouts. We want the smallest possible ?units of control? with the most
regular payouts possible.

I think this works, roughly?


> As far as enabling solo mining, what if this concept were used off chain?
> Have a public network of solo miners who publish "weak blocks" to that
> network, and the next 100 (or 1000 etc) nice miners pay you out as long as
> you're also being nice by following the protocol? All the nice
> optimizations you mentioned about eg combined taproot payouts would apply i
> think. The only goals this wouldn't satisfy are 3 and 5 since an extra
> network is needed, but to be fair, your proposal requires pools which all
> need their own extra network anyways.
>
> The missing piece here would be an ordering of weak blocks to make the
> window possible. Or at least a way to determine what blocks should
> definitely be part of a particular block's pay out. I could see this being
> done by a separate ephemeral blockchain (which starts fresh after each
> Bitcoin block) that keeps track of which weak blocks have been submitted,
> potentially using the pow already in each block to secure it. Granted that
> piece is a bit half baked, but it seems quite solvable. Wdyt?
>
>
Yeah, it's worth thinking more about 100%. This post wasn't a deployable
thing, more an exposition of a technique. I'd love to see a weak-block
based pool, the main issue as noted is the extra software component + data
availability, but perhaps that's solvable!



> One thing that jumped out at me as not safe is throwing block rewards into
> a channel and being able to spend them immediately. There's a reason block
> rewards aren't spendable for a while, and channels don't solve that
> problem, do they? Why not simply reduce the on chain wait time for spending
> block rewards at that point? Seems like the consequences would be the same.
>

Miners could already do this if they mine to e.g. a multisig (trustlessly
if they form blocks with their counterparty and pre-sign before hashing).
Also in lightning we don't generally have to check that our routes channels
exist, we don't care as long as they are happy. Thus it doesn't "hurt"
anyone except for the miners who are taking the not fully locked in funds
risk, a risk they already take. But that risk can't infect the rest of
Bitcoin's users.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211215/ebd6b6af/attachment-0001.html>

From bob at mcelrath.org  Wed Dec 15 18:51:41 2021
From: bob at mcelrath.org (Bob McElrath)
Date: Wed, 15 Dec 2021 18:51:41 +0000
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Decentralized
 Coordination Free Mining Pools
In-Reply-To: <CAGpPWDYWnKNFGpxqY0WGq2cMf-rzEbu0paBa-3kL48FKtkQ-Cw@mail.gmail.com>
References: <CAD5xwhgOK6p7fqZPha1jvDgo=4Syti9K46a2A48Eas44dn9v6Q@mail.gmail.com>
 <20211214190524.GA30559@mcelrath.org>
 <CAD5xwhiLBSCpErJTRbh05v+_i09daJTQQAtzYd-JcWXQojzT2A@mail.gmail.com>
 <20211215001200.GA35108@mcelrath.org>
 <CAGpPWDYWnKNFGpxqY0WGq2cMf-rzEbu0paBa-3kL48FKtkQ-Cw@mail.gmail.com>
Message-ID: <20211215185140.GB35108@mcelrath.org>

You basically described Braidpool:
    https://github.com/pool2win/braidpool
    https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-August/019371.html

We're working on this actively and will have some updates soon. Additional
contributors are most welcome.

To your points below:
1. Increased payout regularity does not lower the viable size of mining pools,
    because smaller mining pools using this mechanism still have higher variance.
2. The on-chain footprint is *higher* due to the increased payout regularity.

For a talk a while back I computed that if you want to have a 1% annual variance
on your profits, you need a pool that is about 20% of the network. The only way
to get this is with smaller, faster blocks or "shares". 
    https://www.youtube.com/watch?v=91WKy7RYHD4

There is a discussion forum at:
    https://matrix.to/#/#braidpool:matrix.org
and a mailing list:
    https://sourceforge.net/p/braidpool/mailman/

All of the existing discussion has been happening privately unfortunately but
I'll try to start using Matrix. ;-)

We've been discussing alternatives for both fee-rate and hashrate derivatives
lately. I'm not opposed to using CTV for some of the things in braidpool, if it
makes sense. Payment pools and unilateral channel openings may be interesting in
this context.

P.S. if anyone wants me to write up a blurb of exactly *why* a construction
without shares cannot be used for hashrate derivatives I can do that, just ask.
It comes down to maximum likelihood estimators for the Poisson distribution...

Billy Tetrud [billy.tetrud at gmail.com] wrote:
> Looks like an interesting proposal, but it doesn't seem to quite match the
> goals you mentioned. As you do mention, this mining pool coordination doesn't
> get rid of the need for mining pools in the first place. So it doesn't satisfy
> item 1 on your goal list afaict.?
> 
> The primary benefits over what we have today that I can see are:
> 1. increased payout regularity, which lowers the viable size of mining pools,
> and
> 2. Lower on chain footprint through combining pay outs from multiple pools.
> 
> Am I missing some?
> 
> These are interesting benefits, but it would be nice if your post was clearer
> on that, since the goals list is not the same as the list of potential benefits
> of this kind of design.
> 
> As far as enabling solo mining, what if this concept were used off chain? Have
> a public network of solo miners who publish "weak blocks" to that network, and
> the next 100 (or 1000 etc) nice miners pay you out as long as you're also being
> nice by following the protocol? All the nice optimizations you mentioned about
> eg combined taproot payouts would apply i think. The only goals this wouldn't
> satisfy are 3 and 5 since an extra network is needed, but to be fair, your
> proposal requires pools which all need their own extra network anyways.?
> 
> The missing piece here would be an ordering of weak blocks to make the window
> possible. Or at least a way to determine what blocks should definitely be part
> of a particular block's pay out. I could see this being done by a separate
> ephemeral blockchain (which starts fresh after each Bitcoin block) that keeps
> track of which weak blocks have been submitted, potentially using the pow
> already in each block to secure it. Granted that piece is a bit half baked, but
> it seems quite solvable. Wdyt?
> 
> One thing that jumped out at me as not safe is throwing block rewards into a
> channel and being able to spend them immediately. There's a reason block
> rewards aren't spendable for a while, and channels don't solve that problem, do
> they? Why not simply reduce the on chain wait time for spending block rewards
> at that point? Seems like the consequences would be the same.
> 
> On Tue, Dec 14, 2021, 16:12 Bob McElrath via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
> 
>     You are hand waving. Attempting to redefine terms to justify your argument
>     is
>     intellectually dishonest. Bitcoin pools have *always* been about variance
>     reduction. Your window function fundamentally CANNOT be used to hedge
>     hashrate.
>     Various suggestions below introduce dangerous new games that might be
>     played by
>     miners.
> 
>     The fact is that the half-baked design you posted is less than useless, and
>     doesn't do anything that anyone wants.
> 
>     You are trying to justify CTV by making it be all things to all people.
>     "When
>     all you have is a hammer, every problem looks like a nail".? Instead I
>     humbly
>     suggest that you pick ONE problem for which CTV is demonstrably the right
>     and
>     best solution, instead of snowing us with a ton of half-baked things that
>     *could* be done, and often don't even require CTV, and some (like this one)
>     fundamentally don't work. I do like some of your ideas, but if you had to
>     pick
>     just one "use case", which would it be?
> 
>     Jeremy [jlrubin at mit.edu] wrote:
>     > Bitcoin didn't invent the concept of pooling: https://en.wikipedia.org/
>     wiki/
>     > Pooling_(resource_management). This is a Bitcoin Mining Pool, although it
>     may
>     > not be your favorite kind, which is fixated on specific properties of
>     computing
>     > contributions before finding a block. Pooling is just a general technique
>     for
>     > aggregating resources to accomplish something. If you have another name
>     like
>     > pooling that is in common use for this type of activity I would be more
>     than
>     > happy to adopt it.
>     >
>     > This sort of pool can hedge not only against fee rates but also against
>     > increases in hashrate since your historical rate 'carries' into the
>     future as a
>     > function of the window. Further, windows and reward functions can be
>     defined in
>     > a myriad of ways that could, e.g., pay less to blocks found in more rapid
>     > succession, contributing to the smoothing functionality.
>     >
>     > With respect to sub-block pooling, as described in the article, this sort
>     of
>     > design also helps with micro-pools being able to split resources
>     > non-custodially in every block as a part of the higher order DCFMP. The
>     point
>     > is not, as noted, to enable solo mining an S9, but to decrease the size
>     of the
>     > minimum viable pool. It's also possible to add, without much validation
>     or
>     > data, some 'uncle block' type mechanism in an incentive compatible way
>     (e.g.,
>     > add 10 pow-heavy headers on the last block for cost 48 bytes header + 32
>     bytes
>     > payout key) such that there's an incentive to include the heaviest ones
>     you've
>     > seen, not just your own, that are worth further study and consideration
>     > (particularly because it's non-consensus, only for opt-in participation
>     in the
>     > pool).
>     >
>     > With respect to space usage, it seems you wholly reject the viability of
>     a
>     > payment pool mechanism to cut-through chain space. Is this a critique
>     that
>     > holds for all Payment Pools, or just in the context of mining? Is there a
>     > particular reason why you think it infeasible that "strongly online"
>     > counterparties would be able to coordinate more efficiently? Is it
>     preferable
>     > for miners, the nexus of decentralization for Bitcoin, to prefer to use
>     > custodial services for pooling (which may require KYC/AM) over bearing a
>     cost
>     > of some extra potential chainload?
>     >
>     > Lastly, with respect to complexity, the proposal is actually incredibly
>     simple
>     > when you take it in a broader context. Non Interactive Channels and
>     Payment
>     > Pools are useful?by themselves, so are the operations to merge them and
>     swap
>     > balance across them. Therefore most of the complexity in this proposal is
>     > relying on tools we'll likely see in everyday use in any case, DCFMP or
>     no.
>     >
>     > Jeremy
>     >
>     --
>     Cheers, Bob McElrath
> 
>     "For every complex problem, there is a solution that is simple, neat, and
>     wrong."
>     ? ? -- H. L. Mencken
> 
>     _______________________________________________
>     bitcoin-dev mailing list
>     bitcoin-dev at lists.linuxfoundation.org
>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 
> !DSPAM:61ba2512470948607217095!
--
Cheers, Bob McElrath

"For every complex problem, there is a solution that is simple, neat, and wrong."
    -- H. L. Mencken 


From yanmaani at cock.li  Wed Dec 15 21:10:51 2021
From: yanmaani at cock.li (yanmaani at cock.li)
Date: Wed, 15 Dec 2021 21:10:51 +0000
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Decentralized
 Coordination Free Mining Pools
In-Reply-To: <CAD5xwhgOK6p7fqZPha1jvDgo=4Syti9K46a2A48Eas44dn9v6Q@mail.gmail.com>
References: <CAD5xwhgOK6p7fqZPha1jvDgo=4Syti9K46a2A48Eas44dn9v6Q@mail.gmail.com>
Message-ID: <5b87372e0a6a825e2927656a7a5d9cc9@cock.li>

How does this differ from p2pool?

If you've just re-invented p2pool, shouldn't you credit their prior art?

Monero is doing their implementation of p2pool. They have viable solo 
mining, as far as I understand. The basic idea is you have several 
P2pools. If you have a block time of 10 minutes, p2pool has 20% of 
hashrate, and there's 100 p2pool chains, each chain gets 0.2% of net 
hash. If you're OK with 20s block times (orphans aren't really a big 
problem), you need (20/600) * (0.02/100) = 0.00067% of network hash to 
get a payout every 10m.

From jlrubin at mit.edu  Wed Dec 15 21:53:50 2021
From: jlrubin at mit.edu (Jeremy)
Date: Wed, 15 Dec 2021 13:53:50 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Decentralized
 Coordination Free Mining Pools
In-Reply-To: <5b87372e0a6a825e2927656a7a5d9cc9@cock.li>
References: <CAD5xwhgOK6p7fqZPha1jvDgo=4Syti9K46a2A48Eas44dn9v6Q@mail.gmail.com>
 <5b87372e0a6a825e2927656a7a5d9cc9@cock.li>
Message-ID: <CAD5xwhhSVL9XvzrvdJAs1aPYyn8Qq0EZh+3KVhSo10Q+n9GG+w@mail.gmail.com>

I could add a comparison to p2pool if you want, but bear in mind this is a
blog post designed to introduce a complex topic to a wide audience, not a
literature review of all possible designs and prior art.

In particular, while P2Pool and DCFMP share a goal (decentralize mining),
the approaches to them bear very little similarity as DCFMP is focused on
making the pooling a pure client side validatable function of the existing
chain, and not create a major risk to mining centralization with a reliance
on a new network running on top of Bitcoin. DCFMP also lacks the core value
prop of P2Pool which is higher resolution on share assignment.

Further, DCFMP's core innovations are Payment Pool and non interactive
channel based, something the P2Pool does not have, but could adopt, in
theory, to solve their payout problems[^note]. I still believe that making
a unified layer of networked software all miners are running on top of
Bitcoin in the loop of mining is a major risk and architecturally bad idea,
hence my advocacy for doing such designs as micro pools inside a DCFMP; It
would be possible to make the "micropools" run on a P2Pool like software,
the DCFMP allows for smaller P2Pools to aggregate their hashrate
trustlessly with the main DCFMP shares.



[^note]: for what it's worth, I was not familiar with p2pool very much
before I came up with DCFMP. The lineage of my conceptual work was
determinism, payment pools, and then realizing they could do something for
mining.
--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>


On Wed, Dec 15, 2021 at 1:11 PM <yanmaani at cock.li> wrote:

> How does this differ from p2pool?
>
> If you've just re-invented p2pool, shouldn't you credit their prior art?
>
> Monero is doing their implementation of p2pool. They have viable solo
> mining, as far as I understand. The basic idea is you have several
> P2pools. If you have a block time of 10 minutes, p2pool has 20% of
> hashrate, and there's 100 p2pool chains, each chain gets 0.2% of net
> hash. If you're OK with 20s block times (orphans aren't really a big
> problem), you need (20/600) * (0.02/100) = 0.00067% of network hash to
> get a payout every 10m.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211215/3e190ead/attachment.html>

From vjudeu at gazeta.pl  Thu Dec 16 09:35:04 2021
From: vjudeu at gazeta.pl (vjudeu at gazeta.pl)
Date: Thu, 16 Dec 2021 10:35:04 +0100
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Decentralized
 Coordination Free Mining Pools
In-Reply-To: <CAGpPWDYWnKNFGpxqY0WGq2cMf-rzEbu0paBa-3kL48FKtkQ-Cw@mail.gmail.com>
Message-ID: <125410522-883ad4a6e0feb9e4c1436bf1d9a3d2d9@pmq8v.m5r2.onet>

> The missing piece here would be an ordering of weak blocks to make the window possible. Or at least a way to determine what blocks should definitely be part of a particular block's pay out. I could see this being done by a separate ephemeral blockchain (which starts fresh after each Bitcoin block) that keeps track of which weak blocks have been submitted, potentially using the pow already in each block to secure it. Granted that piece is a bit half baked, but it seems quite solvable. Wdyt?
?
I thought about something like that, but there is one problem: how many block headers should be stored per one "superblock"? Currently, we have single block header, where the whole coinbase transaction is taken by some mining pool or solo miner. But instead, each miner could submit its own block header. Then, we can collect all headers with the same previous block hash, and distribute block reward between all coinbase transactions in those headers. One "superblock" then would be created in a similar way as existing blocks, we would just have block headers instead of transactions. If most transactions inside those blocks will be the same, then each block could be expressed just as a set of transaction hashes, only coinbase transactions or custom, non-broadcasted transactions included by miners will be revealed, everything else will be known.
> One thing that jumped out at me as not safe is throwing block rewards into a channel and being able to spend them immediately. There's a reason block rewards aren't spendable for a while, and channels don't solve that problem, do they? Why not simply reduce the on chain wait time for spending block rewards at that point? Seems like the consequences would be the same.
All coinbase rewards are unspendable for 100 blocks, it is enforced by consensus. It does not matter if there are outputs owned directly by miners, or if there is one huge N-of-N taproot multisig for the whole pool, where every miner signed the closing transaction. The only option to take coins faster I can see is swapping the coins by some LN transaction. But then, the other party can check if some deposit to the LN channel is a part of the coinbase transaction or not, and then decide if it is acceptable to do the swap.
On 2021-12-15 19:00:44 user Billy Tetrud via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
Looks like an interesting proposal, but it doesn't seem to quite match the goals you mentioned. As you do mention, this mining pool coordination doesn't get rid of the need for mining pools in the first place. So it doesn't satisfy item 1 on your goal list afaict.? ?
The primary benefits over what we have today that I can see are:
1. increased payout regularity, which lowers the viable size of mining pools, and
2. Lower on chain footprint through combining pay outs from multiple pools.
?
Am I missing some?
?
These are interesting benefits, but it would be nice if your post was clearer on that, since the goals list is not the same as the list of potential benefits of this kind of design.
?
As far as enabling solo mining, what if this concept were used off chain? Have a public network of solo miners who publish "weak blocks" to that network, and the next 100 (or 1000 etc) nice miners pay you out as long as you're also being nice by following the protocol? All the nice optimizations you mentioned about eg combined taproot payouts would apply i think. The only goals this wouldn't satisfy are 3 and 5 since an extra network is needed, but to be fair, your proposal requires pools which all need their own extra network anyways.?
?
The missing piece here would be an ordering of weak blocks to make the window possible. Or at least a way to determine what blocks should definitely be part of a particular block's pay out. I could see this being done by a separate ephemeral blockchain (which starts fresh after each Bitcoin block) that keeps track of which weak blocks have been submitted, potentially using the pow already in each block to secure it. Granted that piece is a bit half baked, but it seems quite solvable. Wdyt?
?
One thing that jumped out at me as not safe is throwing block rewards into a channel and being able to spend them immediately. There's a reason block rewards aren't spendable for a while, and channels don't solve that problem, do they? Why not simply reduce the on chain wait time for spending block rewards at that point? Seems like the consequences would be the same.
On Tue, Dec 14, 2021, 16:12 Bob McElrath via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
You are hand waving. Attempting to redefine terms to justify your argument is
intellectually dishonest. Bitcoin pools have *always* been about variance
reduction. Your window function fundamentally CANNOT be used to hedge hashrate.
Various suggestions below introduce dangerous new games that might be played by
miners.
The fact is that the half-baked design you posted is less than useless, and
doesn't do anything that anyone wants.
You are trying to justify CTV by making it be all things to all people. "When
all you have is a hammer, every problem looks like a nail".? Instead I humbly
suggest that you pick ONE problem for which CTV is demonstrably the right and
best solution, instead of snowing us with a ton of half-baked things that
*could* be done, and often don't even require CTV, and some (like this one)
fundamentally don't work. I do like some of your ideas, but if you had to pick
just one "use case", which would it be?
Jeremy [jlrubin at mit.edu] wrote:
> Bitcoin didn't invent the concept of pooling: https://en.wikipedia.org/wiki/
> Pooling_(resource_management). This is a Bitcoin Mining Pool, although it may
> not be your favorite kind, which is fixated on specific properties of computing
> contributions before finding a block. Pooling is just a general technique for
> aggregating resources to accomplish something. If you have another name like
> pooling that is in common use for this type of activity I would be more than
> happy to adopt it.
>
> This sort of pool can hedge not only against fee rates but also against
> increases in hashrate since your historical rate 'carries' into the future as a
> function of the window. Further, windows and reward functions can be defined in
> a myriad of ways that could, e.g., pay less to blocks found in more rapid
> succession, contributing to the smoothing functionality.
>
> With respect to sub-block pooling, as described in the article, this sort of
> design also helps with micro-pools being able to split resources
> non-custodially in every block as a part of the higher order DCFMP. The point
> is not, as noted, to enable solo mining an S9, but to decrease the size of the
> minimum viable pool. It's also possible to add, without much validation or
> data, some 'uncle block' type mechanism in an incentive compatible way (e.g.,
> add 10 pow-heavy headers on the last block for cost 48 bytes header + 32 bytes
> payout key) such that there's an incentive to include the heaviest ones you've
> seen, not just your own, that are worth further study and consideration
> (particularly because it's non-consensus, only for opt-in participation in the
> pool).
>
> With respect to space usage, it seems you wholly reject the viability of a
> payment pool mechanism to cut-through chain space. Is this a critique that
> holds for all Payment Pools, or just in the context of mining? Is there a
> particular reason why you think it infeasible that "strongly online"
> counterparties would be able to coordinate more efficiently? Is it preferable
> for miners, the nexus of decentralization for Bitcoin, to prefer to use
> custodial services for pooling (which may require KYC/AM) over bearing a cost
> of some extra potential chainload?
>
> Lastly, with respect to complexity, the proposal is actually incredibly simple
> when you take it in a broader context. Non Interactive Channels and Payment
> Pools are useful?by themselves, so are the operations to merge them and swap
> balance across them. Therefore most of the complexity in this proposal is
> relying on tools we'll likely see in everyday use in any case, DCFMP or no.
>
> Jeremy
> !DSPAM:61b8f2f5321461582627336!
--
Cheers, Bob McElrath
"For every complex problem, there is a solution that is simple, neat, and wrong."
? ? -- H. L. Mencken
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211216/672b3d09/attachment-0001.html>

From billy.tetrud at gmail.com  Thu Dec 16 16:57:03 2021
From: billy.tetrud at gmail.com (Billy Tetrud)
Date: Thu, 16 Dec 2021 08:57:03 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Decentralized
 Coordination Free Mining Pools
In-Reply-To: <125410522-883ad4a6e0feb9e4c1436bf1d9a3d2d9@pmq8v.m5r2.onet>
References: <CAGpPWDYWnKNFGpxqY0WGq2cMf-rzEbu0paBa-3kL48FKtkQ-Cw@mail.gmail.com>
 <125410522-883ad4a6e0feb9e4c1436bf1d9a3d2d9@pmq8v.m5r2.onet>
Message-ID: <CAGpPWDbph1VPa6Kqy1HsB0XbZ=Warn+qN7m=yNdJfYwQ3G-nSw@mail.gmail.com>

@Jeremy
>   for top-level pool participants there is never any central custody.

I definitely see that. That was actually what I meant when I said the goals
aren't the same as benefits. While your idea definitely satisfies all your
goals in a modular way, the fact that it relies on pools means that unless
the pools can also satisfy the goals, the total system also doesn't satisfy
the goals (even tho the piece of that system you designed does).

> Thus it doesn't "hurt" anyone except for the miners who are taking the
not fully locked in funds risk

True, it only potentially hurts whoever the channel partner is accepting
the unspendable coins. And no one can really stop anyone from taking that
risk if they really want to. But in that case, its not exactly a fully
functional channel, since recourse mechanisms couldn't be performed.
Wouldn't that open such a channel up to a pretty bad theft possibility?

@Bob
> Increased payout regularity does not lower the viable size of mining
pools, because smaller mining pools using this mechanism still have higher
variance.

Yes, smaller mining pools will always have higher variance. However, lower
variance has diminishing benefits. Below a certain amount of variance, less
variance isn't very valuable. So increased payout regularity does indeed
lower the viable size of mining pools because a given low-enough level of
variance can be achieved with less pool hashpower.

> The on-chain footprint is *higher* due to the increased payout regularity.

That's a reasonable point. However, I think there is a difference here
between the regularity of rewards vs payouts. Rewards for each miner can be
more regular without necessarily increasing the number of on-chain payouts.
In fact, theoretically, an individual miner could let their rewards
accumulate in a pool over many rewards and only redeem when they need the
coins for something. The incentive is there for each miner to be judicious
on how much onchain space they take up.

@vjudeu

> how many block headers should be stored per one "superblock"?

I was thinking that this would be a separate blockchain with separate
headers that progress linearly like a normal blockchain. A block creator
would collect together as many blocks that haven't been collected yet into
the next superblock (and maybe receive a reward proportional to how many /
how much weight they include). This could be done using merge mining, or it
could be done using a signing scheme (eg where the block creator signs to
say "I created this superblock" and have mechanisms to punish those who
sign multiple superblocks at the same height. For merge mining, I could
even imagine the data necessary to validate that it has been merge mined
could be put into a taproot script branch (creating an invalid script, but
a valid hash of the superblock).

> we can collect all headers with the same previous block hash, and
distribute block reward between all coinbase transactions in those headers

Exactly.

> we would just have block headers instead of transactions

Yeah, I think that would be the way to go. Really, you could even just use
hashes of the block headers. But the size doesn't matter much because it
would be both a small blockchain and an ephemeral one (which can be fully
discarded after all parties have been paid out, or at least their payout
has been committed to on the bitcoin blockchain).

On Thu, Dec 16, 2021 at 1:35 AM <vjudeu at gazeta.pl> wrote:

> > The missing piece here would be an ordering of weak blocks to make the
> window possible. Or at least a way to determine what blocks should
> definitely be part of a particular block's pay out. I could see this being
> done by a separate ephemeral blockchain (which starts fresh after each
> Bitcoin block) that keeps track of which weak blocks have been submitted,
> potentially using the pow already in each block to secure it. Granted that
> piece is a bit half baked, but it seems quite solvable. Wdyt?
>
> I thought about something like that, but there is one problem: how many
> block headers should be stored per one "superblock"? Currently, we have
> single block header, where the whole coinbase transaction is taken by some
> mining pool or solo miner. But instead, each miner could submit its own
> block header. Then, we can collect all headers with the same previous block
> hash, and distribute block reward between all coinbase transactions in
> those headers. One "superblock" then would be created in a similar way as
> existing blocks, we would just have block headers instead of transactions.
> If most transactions inside those blocks will be the same, then each block
> could be expressed just as a set of transaction hashes, only coinbase
> transactions or custom, non-broadcasted transactions included by miners
> will be revealed, everything else will be known.
>
> > One thing that jumped out at me as not safe is throwing block rewards
> into a channel and being able to spend them immediately. There's a reason
> block rewards aren't spendable for a while, and channels don't solve that
> problem, do they? Why not simply reduce the on chain wait time for spending
> block rewards at that point? Seems like the consequences would be the same.
>
> All coinbase rewards are unspendable for 100 blocks, it is enforced by
> consensus. It does not matter if there are outputs owned directly by
> miners, or if there is one huge N-of-N taproot multisig for the whole pool,
> where every miner signed the closing transaction. The only option to take
> coins faster I can see is swapping the coins by some LN transaction. But
> then, the other party can check if some deposit to the LN channel is a part
> of the coinbase transaction or not, and then decide if it is acceptable to
> do the swap.
>
> On 2021-12-15 19:00:44 user Billy Tetrud via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
> Looks like an interesting proposal, but it doesn't seem to quite match the
> goals you mentioned. As you do mention, this mining pool coordination
> doesn't get rid of the need for mining pools in the first place. So it
> doesn't satisfy item 1 on your goal list afaict.
>
> The primary benefits over what we have today that I can see are:
> 1. increased payout regularity, which lowers the viable size of mining
> pools, and
> 2. Lower on chain footprint through combining pay outs from multiple pools.
>
> Am I missing some?
>
> These are interesting benefits, but it would be nice if your post was
> clearer on that, since the goals list is not the same as the list of
> potential benefits of this kind of design.
>
> As far as enabling solo mining, what if this concept were used off chain?
> Have a public network of solo miners who publish "weak blocks" to that
> network, and the next 100 (or 1000 etc) nice miners pay you out as long as
> you're also being nice by following the protocol? All the nice
> optimizations you mentioned about eg combined taproot payouts would apply i
> think. The only goals this wouldn't satisfy are 3 and 5 since an extra
> network is needed, but to be fair, your proposal requires pools which all
> need their own extra network anyways.
>
> The missing piece here would be an ordering of weak blocks to make the
> window possible. Or at least a way to determine what blocks should
> definitely be part of a particular block's pay out. I could see this being
> done by a separate ephemeral blockchain (which starts fresh after each
> Bitcoin block) that keeps track of which weak blocks have been submitted,
> potentially using the pow already in each block to secure it. Granted that
> piece is a bit half baked, but it seems quite solvable. Wdyt?
>
> One thing that jumped out at me as not safe is throwing block rewards into
> a channel and being able to spend them immediately. There's a reason block
> rewards aren't spendable for a while, and channels don't solve that
> problem, do they? Why not simply reduce the on chain wait time for spending
> block rewards at that point? Seems like the consequences would be the same.
>
> On Tue, Dec 14, 2021, 16:12 Bob McElrath via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org
> <http://../NowaWiadomosc/Do/QlIkBFQ6QUFhIVRZX192dnQBeCtCchE6GhA5LFpLCUc7EVZQVl9dQRIXXR8NCBMbCwIGChJXQFxcXEgcFh8UVVVDEyBdVkE9JVRdEwFhYXVlblhVIkosEAszLR5BQVV7U0MID0BAQUgIGh0RHgAMGAMXBQJfW1sdXRQUQUoDQlAiBFY8>>
> wrote:
>
>> You are hand waving. Attempting to redefine terms to justify your
>> argument is
>> intellectually dishonest. Bitcoin pools have *always* been about variance
>> reduction. Your window function fundamentally CANNOT be used to hedge
>> hashrate.
>> Various suggestions below introduce dangerous new games that might be
>> played by
>> miners.
>>
>> The fact is that the half-baked design you posted is less than useless,
>> and
>> doesn't do anything that anyone wants.
>>
>> You are trying to justify CTV by making it be all things to all people.
>> "When
>> all you have is a hammer, every problem looks like a nail".  Instead I
>> humbly
>> suggest that you pick ONE problem for which CTV is demonstrably the right
>> and
>> best solution, instead of snowing us with a ton of half-baked things that
>> *could* be done, and often don't even require CTV, and some (like this
>> one)
>> fundamentally don't work. I do like some of your ideas, but if you had to
>> pick
>> just one "use case", which would it be?
>>
>> Jeremy [jlrubin at mit.edu
>> <http://../NowaWiadomosc/Do/QlIkBFQ6QUFhIVRZX192dnQBeCtCchEyHxYvIVpLARduChoQSFZQR0NWQVZWJUNRXwMSCRMTBgcWASdWVkpbCxUTQwoWQUdjKVBMGFY3MWMWeU9QBAZtNw%3D%3D>]
>> wrote:
>> > Bitcoin didn't invent the concept of pooling:
>> https://en.wikipedia.org/wiki/
>> > Pooling_(resource_management). This is a Bitcoin Mining Pool, although
>> it may
>> > not be your favorite kind, which is fixated on specific properties of
>> computing
>> > contributions before finding a block. Pooling is just a general
>> technique for
>> > aggregating resources to accomplish something. If you have another name
>> like
>> > pooling that is in common use for this type of activity I would be more
>> than
>> > happy to adopt it.
>> >
>> > This sort of pool can hedge not only against fee rates but also against
>> > increases in hashrate since your historical rate 'carries' into the
>> future as a
>> > function of the window. Further, windows and reward functions can be
>> defined in
>> > a myriad of ways that could, e.g., pay less to blocks found in more
>> rapid
>> > succession, contributing to the smoothing functionality.
>> >
>> > With respect to sub-block pooling, as described in the article, this
>> sort of
>> > design also helps with micro-pools being able to split resources
>> > non-custodially in every block as a part of the higher order DCFMP. The
>> point
>> > is not, as noted, to enable solo mining an S9, but to decrease the size
>> of the
>> > minimum viable pool. It's also possible to add, without much validation
>> or
>> > data, some 'uncle block' type mechanism in an incentive compatible way
>> (e.g.,
>> > add 10 pow-heavy headers on the last block for cost 48 bytes header +
>> 32 bytes
>> > payout key) such that there's an incentive to include the heaviest ones
>> you've
>> > seen, not just your own, that are worth further study and consideration
>> > (particularly because it's non-consensus, only for opt-in participation
>> in the
>> > pool).
>> >
>> > With respect to space usage, it seems you wholly reject the viability
>> of a
>> > payment pool mechanism to cut-through chain space. Is this a critique
>> that
>> > holds for all Payment Pools, or just in the context of mining? Is there
>> a
>> > particular reason why you think it infeasible that "strongly online"
>> > counterparties would be able to coordinate more efficiently? Is it
>> preferable
>> > for miners, the nexus of decentralization for Bitcoin, to prefer to use
>> > custodial services for pooling (which may require KYC/AM) over bearing
>> a cost
>> > of some extra potential chainload?
>> >
>> > Lastly, with respect to complexity, the proposal is actually incredibly
>> simple
>> > when you take it in a broader context. Non Interactive Channels and
>> Payment
>> > Pools are useful by themselves, so are the operations to merge them and
>> swap
>> > balance across them. Therefore most of the complexity in this proposal
>> is
>> > relying on tools we'll likely see in everyday use in any case, DCFMP or
>> no.
>> >
>> > Jeremy
>> > !DSPAM:61b8f2f5321461582627336!
>> --
>> Cheers, Bob McElrath
>>
>> "For every complex problem, there is a solution that is simple, neat, and
>> wrong."
>>     -- H. L. Mencken
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> <http://../NowaWiadomosc/Do/QlIkBFQ6QUFhIVRZX192dnQBeCtCchE6GhA5LFpLCUc7EVZQVl9dQRIXXR8NCBMbCwIGChJXQFxcXEgcFh8UVVVDEyBdVkE9JVRdEwFhYXVlblhVIkosEAszLR5BQVV7U0MID0BAQUgIGh0RHgAMGAMXBQJfW1sdXRQUQUoDQlAiBFY8>
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211216/d18efb77/attachment-0001.html>

From jlrubin at mit.edu  Fri Dec 17 00:37:09 2021
From: jlrubin at mit.edu (Jeremy)
Date: Thu, 16 Dec 2021 16:37:09 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Decentralized
 Coordination Free Mining Pools
In-Reply-To: <CAGpPWDbph1VPa6Kqy1HsB0XbZ=Warn+qN7m=yNdJfYwQ3G-nSw@mail.gmail.com>
References: <CAGpPWDYWnKNFGpxqY0WGq2cMf-rzEbu0paBa-3kL48FKtkQ-Cw@mail.gmail.com>
 <125410522-883ad4a6e0feb9e4c1436bf1d9a3d2d9@pmq8v.m5r2.onet>
 <CAGpPWDbph1VPa6Kqy1HsB0XbZ=Warn+qN7m=yNdJfYwQ3G-nSw@mail.gmail.com>
Message-ID: <CAD5xwhiF2hOkV-8jL3NLJAL1Z=7ThzBp+3z=Ji1LCeRZGbHqBw@mail.gmail.com>

high level response:

including a small number of block headers (10?) directly as op_return
metadata (or something) doesn't have that high overhead necessarily, but
could be super effective at helping miners participate with lower hashrate.
the reason to include this as on-chain data is so that the mining pool
doesn't require any external network software.

this would balance out the issues if the data is somewhat bounded (e.g., 10
headers). what's nice is this data has no consensus meaning as it's client
side validated by the DCFMP block filter.

interestingly, the participating pools could 'vote' on how difficult shares
should be as a metaparameter to the pool over blocks... but analysis gets
more complex with that.

cheers,

jeremy
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211216/b0d4f253/attachment.html>

From jlrubin at mit.edu  Fri Dec 17 00:49:30 2021
From: jlrubin at mit.edu (Jeremy)
Date: Thu, 16 Dec 2021 16:49:30 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Part One: Implementing NFTs
	in Sapio
Message-ID: <CAD5xwhiG1z_0=cgdWfQSq3P0aR_iKyfbnk9hCWaNxYdKD-VYWg@mail.gmail.com>

I know NFTs are controversial, but here's my take on them in Sapio:

https://rubin.io/bitcoin/2021/12/16/advent-19/

If you don't like NFTs, don't worry: the results and techniques are
entirely generalizable here and can apply to many other types of things
that aren't stupid JPGs.

E.g.,

- If you squint, Lightning Channels are NFTs: I have a channel with someone
and I can't transfer it to a third party fungibly because both the
remaining side and entering side want to know about the counterparty
reputation.
- DLCs are NFTs because I want to know not just counterparties, but also
which oracles.
- Colored Coins/Tokens, definitionally, are not NFTs, but fractional shares
of an NFT are Colored Coins, so NFT research might yield new results for
Colored Coins.

Advancing the state of the art for NFTs advances the state of the art for
all sorts of other purposes, while letting us have a little fun. This is a
strong callback to https://rubin.io/bitcoin/2021/12/14/advent-17/ and
https://rubin.io/bitcoin/2021/12/03/advent-6/ if you want to read more on
why things like NFTs are cool even if JPGs are lame.

Cheers,

Jeremy



--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211216/2634db55/attachment.html>

From luke at dashjr.org  Fri Dec 17 06:21:06 2021
From: luke at dashjr.org (Luke Dashjr)
Date: Fri, 17 Dec 2021 06:21:06 +0000
Subject: [bitcoin-dev] Bitcoin Knots "Steel Rope" LTS 21.2.knots20210629
	released
Message-ID: <202112170621.12821.luke@dashjr.org>

Bitcoin Knots version 21.2.knots20210629 is now available from:

  https://bitcoinknots.org/files/21.x/21.2.knots20210629/

This Long Term Support (LTS) "Steel Rope" release is based on the unchanged
Bitcoin Knots feature set from 2021 June 29th, with only bug fixes and updated
translations.

Please report bugs using the issue tracker at GitHub:

  https://github.com/bitcoinknots/bitcoin/issues

To receive security and update notifications, please subscribe to:

  https://bitcoinknots.org/list/announcements/join/

For the full release notes and change log, see:

https://github.com/bitcoinknots/bitcoin/blob/v21.2.knots20210629/doc/release-notes.md
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 1528 bytes
Desc: This is a digitally signed message part.
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211217/40240a62/attachment.sig>

From vjudeu at gazeta.pl  Fri Dec 17 06:37:17 2021
From: vjudeu at gazeta.pl (vjudeu at gazeta.pl)
Date: Fri, 17 Dec 2021 07:37:17 +0100
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Decentralized
 Coordination Free Mining Pools
In-Reply-To: <CAGpPWDbph1VPa6Kqy1HsB0XbZ=Warn+qN7m=yNdJfYwQ3G-nSw@mail.gmail.com>
Message-ID: <150492262-e89b67dc2c010e65008ad976e2647ec1@pmq4v.m5r2.onet>

> I was thinking that this would be a separate blockchain with separate headers that progress linearly like a normal blockchain.
Exactly, that's what I called "superblocks", where you have a separate chain, just to keep block headers instead of transactions.
> A block creator would collect together as many blocks that haven't been collected yet into the next superblock (and maybe receive a reward proportional to how many / how much weight they include).
You cannot "catch them all". If you do, you can end up with a lot of block headers, where each of them has difficulty equal to one. You need some limit, you can limit amount of blocks, you can assign some minimal difficulty, it does not matter that much, but some limit is needed, also because mining on top of the latest superblock should be more profitable than replacing someone else's reward in the previous superblock by your own reward and getting a bigger share in the previous superblock.
> This could be done using merge mining, or it could be done using a signing scheme (eg where the block creator signs to say "I created this superblock" and have mechanisms to punish those who sign multiple superblocks at the same height.
I would pick merge mining, because it is more compatible with existing mining scheme. Signing sounds more like Proof of Stake and I am trying to avoid that solution. Also, there is no need to sign anything, because you are solo mining where you have your own coinbase transaction or you are mining in a pool, where you have some shared address, and then you cannot produce any incompatible superblock, because the protocol can tell you, which address you should use (and if it is N-of-N taproot multisig and you have some closing transaction, then you can safely mine it).
> Really, you could even just use hashes of the block headers.
Replacing transactions with block headers will do the same trick. Each transaction is first hashed with double SHA-256, in exactly the same way as block headers are. If you replace transactions with block headers, you would get a superblock header, then varint saying how many block headers are there, and then you can place all block headers. During superblock merkle tree construction, you will hash all block headers (so you will get block hashes as leaves), and then you will combine block hashes in the same way as transaction hashes are combined.
>From the Script point of view, you can always use "OP_SIZE 80 OP_EQUALVERIFY OP_HASH256 <hash> OP_EQUAL". Then, you can just change the size, just to show which object is hashed. Value 80 will work for block headers, small values below 520 will work for small transactions, value 64 will work for any merkle tree proof, no matter if it is for superblock or normal block. Also, by using block headers instead of hashes, you can prove that at least a proper amount of work was done to produce it, because if you use just hashes, then they could be random.
On 2021-12-16 17:57:23 user Billy Tetrud <billy.tetrud at gmail.com> wrote:
@Jeremy
>? ?for top-level pool participants there is never any central custody.
?
I definitely see that. That was actually what I meant when I said the goals aren't the same as benefits. While your idea definitely satisfies all your goals in a modular way, the fact that it relies on pools means that unless the pools can also satisfy the goals, the total system also doesn't satisfy the goals (even tho the piece of that system you designed does).?
?
>?Thus it doesn't "hurt" anyone except for the miners who are taking the not fully locked in funds risk
?
True, it only potentially hurts whoever the channel partner is accepting the unspendable coins. And no one can really stop anyone from taking that risk if they really want to. But in that case, its not exactly a fully functional channel, since recourse mechanisms couldn't be performed. Wouldn't that open such a channel up to a pretty bad theft possibility?
?
@Bob
>?Increased payout regularity does not lower the viable size of mining pools, because smaller mining pools using this mechanism still have higher variance.
?
Yes, smaller mining pools will always have higher variance. However, lower variance has diminishing benefits. Below a certain amount of variance, less variance isn't very valuable. So increased payout regularity does indeed lower the viable size of mining pools because a given low-enough level of variance can be achieved with less pool hashpower.
?
> The on-chain footprint is *higher* due to the increased payout regularity.
?
That's a reasonable point. However, I think there is a difference here between the regularity of rewards vs payouts. Rewards for each miner can be more regular without necessarily increasing the number of on-chain payouts. In fact, theoretically, an individual miner could let their rewards accumulate in a pool over many rewards and only redeem when they need the coins for something. The incentive is there for each miner to be judicious on how much onchain space they take up.
?
@vjudeu
?
> how many block headers should be stored per one "superblock"?
?
I was thinking that this would be a separate blockchain with separate headers that progress linearly like a normal blockchain. A block creator would collect together as many blocks that haven't been collected yet into the next superblock (and maybe receive a reward proportional to how many / how much weight they include). This could be done using merge mining, or it could be done using a signing scheme (eg where the block creator signs to say "I created this superblock" and have mechanisms to punish those who sign multiple superblocks at the same height. For merge mining, I could even imagine the data necessary to validate that it has been merge mined could be put into a taproot script branch (creating an invalid script, but a valid hash of the superblock).?
?
> we can collect all headers with the same previous block hash, and distribute block reward between all coinbase transactions in those headers
?
Exactly.
?
> we would just have block headers instead of transactions
?
Yeah, I think that would be the way to go. Really, you could even just use hashes of the block headers. But the size doesn't matter much because it would be both a small blockchain and an ephemeral one (which can be fully discarded after all parties have been paid out, or at least their payout has been committed to on the bitcoin blockchain).?
On Thu, Dec 16, 2021 at 1:35 AM <vjudeu at gazeta.pl> wrote:
> The missing piece here would be an ordering of weak blocks to make the window possible. Or at least a way to determine what blocks should definitely be part of a particular block's pay out. I could see this being done by a separate ephemeral blockchain (which starts fresh after each Bitcoin block) that keeps track of which weak blocks have been submitted, potentially using the pow already in each block to secure it. Granted that piece is a bit half baked, but it seems quite solvable. Wdyt?
?
I thought about something like that, but there is one problem: how many block headers should be stored per one "superblock"? Currently, we have single block header, where the whole coinbase transaction is taken by some mining pool or solo miner. But instead, each miner could submit its own block header. Then, we can collect all headers with the same previous block hash, and distribute block reward between all coinbase transactions in those headers. One "superblock" then would be created in a similar way as existing blocks, we would just have block headers instead of transactions. If most transactions inside those blocks will be the same, then each block could be expressed just as a set of transaction hashes, only coinbase transactions or custom, non-broadcasted transactions included by miners will be revealed, everything else will be known.
> One thing that jumped out at me as not safe is throwing block rewards into a channel and being able to spend them immediately. There's a reason block rewards aren't spendable for a while, and channels don't solve that problem, do they? Why not simply reduce the on chain wait time for spending block rewards at that point? Seems like the consequences would be the same.
All coinbase rewards are unspendable for 100 blocks, it is enforced by consensus. It does not matter if there are outputs owned directly by miners, or if there is one huge N-of-N taproot multisig for the whole pool, where every miner signed the closing transaction. The only option to take coins faster I can see is swapping the coins by some LN transaction. But then, the other party can check if some deposit to the LN channel is a part of the coinbase transaction or not, and then decide if it is acceptable to do the swap.
On 2021-12-15 19:00:44 user Billy Tetrud via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
Looks like an interesting proposal, but it doesn't seem to quite match the goals you mentioned. As you do mention, this mining pool coordination doesn't get rid of the need for mining pools in the first place. So it doesn't satisfy item 1 on your goal list afaict.? ?
The primary benefits over what we have today that I can see are:
1. increased payout regularity, which lowers the viable size of mining pools, and
2. Lower on chain footprint through combining pay outs from multiple pools.
?
Am I missing some?
?
These are interesting benefits, but it would be nice if your post was clearer on that, since the goals list is not the same as the list of potential benefits of this kind of design.
?
As far as enabling solo mining, what if this concept were used off chain? Have a public network of solo miners who publish "weak blocks" to that network, and the next 100 (or 1000 etc) nice miners pay you out as long as you're also being nice by following the protocol? All the nice optimizations you mentioned about eg combined taproot payouts would apply i think. The only goals this wouldn't satisfy are 3 and 5 since an extra network is needed, but to be fair, your proposal requires pools which all need their own extra network anyways.?
?
The missing piece here would be an ordering of weak blocks to make the window possible. Or at least a way to determine what blocks should definitely be part of a particular block's pay out. I could see this being done by a separate ephemeral blockchain (which starts fresh after each Bitcoin block) that keeps track of which weak blocks have been submitted, potentially using the pow already in each block to secure it. Granted that piece is a bit half baked, but it seems quite solvable. Wdyt?
?
One thing that jumped out at me as not safe is throwing block rewards into a channel and being able to spend them immediately. There's a reason block rewards aren't spendable for a while, and channels don't solve that problem, do they? Why not simply reduce the on chain wait time for spending block rewards at that point? Seems like the consequences would be the same.
On Tue, Dec 14, 2021, 16:12 Bob McElrath via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
You are hand waving. Attempting to redefine terms to justify your argument is
intellectually dishonest. Bitcoin pools have *always* been about variance
reduction. Your window function fundamentally CANNOT be used to hedge hashrate.
Various suggestions below introduce dangerous new games that might be played by
miners.
The fact is that the half-baked design you posted is less than useless, and
doesn't do anything that anyone wants.
You are trying to justify CTV by making it be all things to all people. "When
all you have is a hammer, every problem looks like a nail".? Instead I humbly
suggest that you pick ONE problem for which CTV is demonstrably the right and
best solution, instead of snowing us with a ton of half-baked things that
*could* be done, and often don't even require CTV, and some (like this one)
fundamentally don't work. I do like some of your ideas, but if you had to pick
just one "use case", which would it be?
Jeremy [jlrubin at mit.edu] wrote:
> Bitcoin didn't invent the concept of pooling: https://en.wikipedia.org/wiki/
> Pooling_(resource_management). This is a Bitcoin Mining Pool, although it may
> not be your favorite kind, which is fixated on specific properties of computing
> contributions before finding a block. Pooling is just a general technique for
> aggregating resources to accomplish something. If you have another name like
> pooling that is in common use for this type of activity I would be more than
> happy to adopt it.
>
> This sort of pool can hedge not only against fee rates but also against
> increases in hashrate since your historical rate 'carries' into the future as a
> function of the window. Further, windows and reward functions can be defined in
> a myriad of ways that could, e.g., pay less to blocks found in more rapid
> succession, contributing to the smoothing functionality.
>
> With respect to sub-block pooling, as described in the article, this sort of
> design also helps with micro-pools being able to split resources
> non-custodially in every block as a part of the higher order DCFMP. The point
> is not, as noted, to enable solo mining an S9, but to decrease the size of the
> minimum viable pool. It's also possible to add, without much validation or
> data, some 'uncle block' type mechanism in an incentive compatible way (e.g.,
> add 10 pow-heavy headers on the last block for cost 48 bytes header + 32 bytes
> payout key) such that there's an incentive to include the heaviest ones you've
> seen, not just your own, that are worth further study and consideration
> (particularly because it's non-consensus, only for opt-in participation in the
> pool).
>
> With respect to space usage, it seems you wholly reject the viability of a
> payment pool mechanism to cut-through chain space. Is this a critique that
> holds for all Payment Pools, or just in the context of mining? Is there a
> particular reason why you think it infeasible that "strongly online"
> counterparties would be able to coordinate more efficiently? Is it preferable
> for miners, the nexus of decentralization for Bitcoin, to prefer to use
> custodial services for pooling (which may require KYC/AM) over bearing a cost
> of some extra potential chainload?
>
> Lastly, with respect to complexity, the proposal is actually incredibly simple
> when you take it in a broader context. Non Interactive Channels and Payment
> Pools are useful?by themselves, so are the operations to merge them and swap
> balance across them. Therefore most of the complexity in this proposal is
> relying on tools we'll likely see in everyday use in any case, DCFMP or no.
>
> Jeremy
> !DSPAM:61b8f2f5321461582627336!
--
Cheers, Bob McElrath
"For every complex problem, there is a solution that is simple, neat, and wrong."
? ? -- H. L. Mencken
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211217/84b4855e/attachment-0001.html>

From jlrubin at mit.edu  Fri Dec 17 18:24:01 2021
From: jlrubin at mit.edu (Jeremy)
Date: Fri, 17 Dec 2021 10:24:01 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Oracles, Bonds,
	and Attestation Chains
Message-ID: <CAD5xwhjDBD38Xt=p=AW7XU3OCh0_9nb=-7neXuLcd-VAXJBE-Q@mail.gmail.com>

Today's post is pretty cool: it details how covenants like CTV can be used
to improve on-chain bitcoin signing oracles by solving the timeout/rollover
issue and solving the miner/oracle collusion issue on punishment. This
issue is similar to the Blockstream Liquid Custody Federation rollover bug
from a while back (which this type of design also helps to fix).

https://rubin.io/bitcoin/2021/12/17/advent-20/

It also describes:
- how a protocol on top can make 'branch free' attestation chains where if
you equivocate your funds get burned.
- lightly, various uses for these chained attestations

In addition, Robin Linus has a great whitepaper he put out getting much
more in the weeds on the concepts described in the post, it's linked in the
first bit of the post.

cheers,

Jeremy

--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211217/aafa4fd6/attachment.html>

From jlrubin at mit.edu  Fri Dec 17 18:53:55 2021
From: jlrubin at mit.edu (Jeremy)
Date: Fri, 17 Dec 2021 10:53:55 -0800
Subject: [bitcoin-dev] Globally Broadcasting Workshares to Improve Finality
	Heuristics
Message-ID: <CAD5xwhjqKC3qUFyOA6Jf=tgQz5KFr97CVVVzemDsX=jLoq2GSg@mail.gmail.com>

An interesting concept occurred to me today while chatting with Nic Carter.

If we set Bitcoin Core up to gossip headers for work shares (e.g., expected
500 headers per block would have 20kb overhead, assuming we don't need to
send the prev hash) we'd be able to have more accurate finality estimates
and warnings if we see hashrate abandoning our chain tip. This is
observable regardless of if dishonest miners choose not to publish their
work on non tip shares, since you can notice the missing work.

In the GUI, we could give users an additional warning if they are
accepting a payment during a sudden hashrate decrease that they might wait
longer.

Has this been discussed before?

Cheers,

Jeremy

--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211217/fbca153e/attachment.html>

From ZmnSCPxj at protonmail.com  Sat Dec 18 01:00:14 2021
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Sat, 18 Dec 2021 01:00:14 +0000
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Oracles, Bonds,
	and Attestation Chains
In-Reply-To: <CAD5xwhjDBD38Xt=p=AW7XU3OCh0_9nb=-7neXuLcd-VAXJBE-Q@mail.gmail.com>
References: <CAD5xwhjDBD38Xt=p=AW7XU3OCh0_9nb=-7neXuLcd-VAXJBE-Q@mail.gmail.com>
Message-ID: <BPaAFvgTqtCsxcAr1W_jlvWkHOaRi41GCfdHsblFGmqBVKR20qC18xk89Wh28Cmaf2YGUlN_N9_g9S_ij3bZbhsD_0qaDMcvqV1GdScVoTs=@protonmail.com>

Good morning Jeremy,

> Today's post is pretty cool: it details how covenants like CTV can be used to improve on-chain bitcoin signing oracles by solving the timeout/rollover issue and solving the miner/oracle collusion issue on punishment. This issue is similar to the Blockstream Liquid Custody Federation rollover bug from a while back (which this type of design also helps to fix).
>
> https://rubin.io/bitcoin/2021/12/17/advent-20/
>
> It also describes:
> - how a protocol on top can make 'branch free' attestation chains where if you equivocate your funds get burned.
> - lightly, various uses for these chained attestations
>
> In addition, Robin Linus has a great whitepaper he put out getting much more in the weeds on the concepts described in the post, it's linked in the first bit of the post.

Nice, bonds are significantly better if you can ensure that the bonder cannot recover their funds.
Without a covenant the best you could do would be to have the bonder risk loss of funds on equivocation, not have the bonder actually definitely lose funds.

We should note that "equivocate" is not "lie".
An oracle can still lie, it just needs to consistently lie (i.e. not equivocate).

As an example, if the oracle is a signer for a federated sidechain, it could still sign an invalid sidechain block that inflates the sidecoin supply.
It is simply prevented from later denying this by signing an alternative valid sidechain block and acting as if it never signed the invalid sidechain block.
But if it sticks to its guns, then the sidechain simply stops operation with everyone owning sidecoins losing their funds (and if the oracle already exited the sidechain, its bond remains safe, as it did not equivocate, it only lied).

Regards,
ZmnSCPxj

From jlrubin at mit.edu  Sat Dec 18 02:00:32 2021
From: jlrubin at mit.edu (Jeremy)
Date: Fri, 17 Dec 2021 18:00:32 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Oracles, Bonds,
 and Attestation Chains
In-Reply-To: <BPaAFvgTqtCsxcAr1W_jlvWkHOaRi41GCfdHsblFGmqBVKR20qC18xk89Wh28Cmaf2YGUlN_N9_g9S_ij3bZbhsD_0qaDMcvqV1GdScVoTs=@protonmail.com>
References: <CAD5xwhjDBD38Xt=p=AW7XU3OCh0_9nb=-7neXuLcd-VAXJBE-Q@mail.gmail.com>
 <BPaAFvgTqtCsxcAr1W_jlvWkHOaRi41GCfdHsblFGmqBVKR20qC18xk89Wh28Cmaf2YGUlN_N9_g9S_ij3bZbhsD_0qaDMcvqV1GdScVoTs=@protonmail.com>
Message-ID: <CAD5xwhjDBaQvP89eh4LhYZ3E+8RQycafr9mxeOrFbivd3e=LOw@mail.gmail.com>

Yep, these are great points. There is no way to punish signing the wrong
thing directly, just not changing your answers without risk to funds.

One of the interesting things is that upon a single equivocation you get
unbounded equivocation by 3rd parties, e.g., you can completely rewrite the
entire signature chain!

Another interesting point: if you use a musig key for your staking key that
is musig(a,b,c) you can sign with a until you equivocate once, then switch
to b, then c. Three strikes and you're out! IDK what that could be used for.

Lastly, while you can't punish lying, you could say "only the stakers who
sign with the majority get allocated reward tokens for that slot". So you
could equivocate to switch and get tokens, but you'd burn your collateral
for them. But this does make an incentive for the stakers to try to sign
the "correct" statement in line with peers.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211217/24fdc3d2/attachment.html>

From ZmnSCPxj at protonmail.com  Sat Dec 18 03:49:15 2021
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Sat, 18 Dec 2021 03:49:15 +0000
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Oracles, Bonds,
	and Attestation Chains
In-Reply-To: <CAD5xwhjDBaQvP89eh4LhYZ3E+8RQycafr9mxeOrFbivd3e=LOw@mail.gmail.com>
References: <CAD5xwhjDBD38Xt=p=AW7XU3OCh0_9nb=-7neXuLcd-VAXJBE-Q@mail.gmail.com>
 <BPaAFvgTqtCsxcAr1W_jlvWkHOaRi41GCfdHsblFGmqBVKR20qC18xk89Wh28Cmaf2YGUlN_N9_g9S_ij3bZbhsD_0qaDMcvqV1GdScVoTs=@protonmail.com>
 <CAD5xwhjDBaQvP89eh4LhYZ3E+8RQycafr9mxeOrFbivd3e=LOw@mail.gmail.com>
Message-ID: <z3doYwv_p3_hnl2UZW9hC5_RYclAp4bu44ZdkNSZsaP4JtdvIMz2zOqrdoDiKKADe0zV1wHZ-1ZfEQ8kyLnACkejJplTrWRCh_kEy11VtN4=@protonmail.com>

Good morning Jeremy,


> Another interesting point: if you use a musig key for your staking key that is musig(a,b,c) you can sign with a until you equivocate once, then switch to b, then c. Three strikes and you're out! IDK what that could be used for.

You could say "oops, I made a mistake, can I correct it by equivocating just this time?".
Three strikes and you are out.

> Lastly, while you can't punish lying, you could say "only the stakers who sign with the majority get allocated reward tokens for that slot". So you could equivocate to switch and get tokens, but you'd burn your collateral for them. But this does make an incentive for the stakers to try to sign the "correct" statement in line with peers.

Note the quote marks around "correct" --- the majority of peers could be conspiring to lie, too.
Conspiracy theory time.....

Regards,
ZmnSCPxj

From newsletters at briancloutier.com  Sat Dec 18 08:36:07 2021
From: newsletters at briancloutier.com (Newsletter Catcher)
Date: Sat, 18 Dec 2021 00:36:07 -0800
Subject: [bitcoin-dev] Globally Broadcasting Workshares to Improve
 Finality Heuristics
In-Reply-To: <CAD5xwhjqKC3qUFyOA6Jf=tgQz5KFr97CVVVzemDsX=jLoq2GSg@mail.gmail.com>
References: <CAD5xwhjqKC3qUFyOA6Jf=tgQz5KFr97CVVVzemDsX=jLoq2GSg@mail.gmail.com>
Message-ID: <CAGBDHp0WkX4S6Ctgq=7YD75s_PSy==wchEtKPR3YYw0ZVxOtiA@mail.gmail.com>

It's not exactly what you're looking for but this is very similar to the
premise of Bobtail, which was presented at Scaling Bitcoin a few years ago:
https://arxiv.org/abs/1709.08750

On Fri, Dec 17, 2021, 10:54 Jeremy via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> An interesting concept occurred to me today while chatting with Nic Carter.
>
> If we set Bitcoin Core up to gossip headers for work shares (e.g.,
> expected 500 headers per block would have 20kb overhead, assuming we don't
> need to send the prev hash) we'd be able to have more accurate finality
> estimates and warnings if we see hashrate abandoning our chain tip. This is
> observable regardless of if dishonest miners choose not to publish their
> work on non tip shares, since you can notice the missing work.
>
> In the GUI, we could give users an additional warning if they are
> accepting a payment during a sudden hashrate decrease that they might wait
> longer.
>
> Has this been discussed before?
>
> Cheers,
>
> Jeremy
>
> --
> @JeremyRubin <https://twitter.com/JeremyRubin>
> <https://twitter.com/JeremyRubin>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211218/422c0b92/attachment.html>

From runesvend at gmail.com  Sat Dec 18 13:25:08 2021
From: runesvend at gmail.com (Rune K. Svendsen)
Date: Sat, 18 Dec 2021 14:25:08 +0100
Subject: [bitcoin-dev] Globally Broadcasting Workshares to Improve
 Finality Heuristics
Message-ID: <CE7BA764-95EC-406F-843B-246922D06B52@gmail.com>

Hi Jeremy,

If I understand you correctly, then I believe I touch upon this concept here: https://bitcointalk.org/index.php?topic=97153.msg1309930#msg1309930


/Rune


From jlrubin at mit.edu  Sat Dec 18 16:51:46 2021
From: jlrubin at mit.edu (Jeremy)
Date: Sat, 18 Dec 2021 08:51:46 -0800
Subject: [bitcoin-dev] Proposal: Full-RBF in Bitcoin Core 24.0
In-Reply-To: <CALZpt+F2b3tdu1+kLZiBPCH2O-pDzZytoRFtX6X0a8UX4OBrDQ@mail.gmail.com>
References: <CALZpt+F2b3tdu1+kLZiBPCH2O-pDzZytoRFtX6X0a8UX4OBrDQ@mail.gmail.com>
Message-ID: <CAD5xwhjVkxgu2+M+Ft576GYM6Tv=ZEwtV82v1cLeYaoU5mSRnA@mail.gmail.com>

Small idea:

ease into getting rid of full-rbf by keeping the flag working, but make
enforcement of non-replaceability something that happens n seconds after
first seen.

this reduces the ability to partition the mempools by broadcasting
irreplaceable conflicts all at once, and slowly eases clients off of
relying on non-RBF.

we might start with 60 seconds, and then double every release till we get
to 600 at which point we disable it.
--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>


On Tue, Jun 15, 2021 at 10:00 AM Antoine Riard via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Hi,
>
> I'm writing to propose deprecation of opt-in RBF in favor of full-RBF as
> the Bitcoin Core's default replacement policy in version 24.0. As a
> reminder, the next release is 22.0, aimed for August 1st, assuming
> agreement is reached, this policy change would enter into deployment phase
> a year from now.
>
> Even if this replacement policy has been deemed as highly controversial a
> few years ago, ongoing and anticipated changes in the Bitcoin ecosystem are
> motivating this proposal.
>
> # RBF opt-out as a DoS Vector against Multi-Party Funded Transactions
>
> As explained in "On Mempool Funny Games against Multi-Party Funded
> Transactions'', 2nd issue [0], an attacker can easily DoS a multi-party
> funded transactions by propagating an RBF opt-out double-spend of its
> contributed input before the honest transaction is broadcasted by the
> protocol orchester. DoSes are qualified in the sense of either an attacker
> wasting timevalue of victim's inputs or forcing exhaustion of the
> fee-bumping  reserve.
>
> This affects a series of Bitcoin protocols such as Coinjoin, onchain DLCs
> and dual-funded LN channels. As those protocols are still in the early
> phase of deployment, it doesn't seem to have been executed in the wild for
> now.  That said, considering that dual-funded are more efficient from a
> liquidity standpoint, we can expect them to be widely relied on, once
> Lightning enters in a more mature phase. At that point, it should become
> economically rational for liquidity service providers to launch those DoS
> attacks against their competitors to hijack user traffic.
>
> Beyond that, presence of those DoSes will complicate the design and
> deployment of multi-party Bitcoin protocols such as payment
> pools/multi-party channels. Note, Lightning Pool isn't affected as there is
> a preliminary stage where batch participants are locked-in their funds
> within an account witnessScript shared with the orchestrer.
>
> Of course, even assuming full-rbf, propagation of the multi-party funded
> transactions can still be interfered with by an attacker, simply
> broadcasting a double-spend with a feerate equivalent to the honest
> transaction. However, it tightens the attack scenario to a scorched earth
> approach, where the attacker has to commit equivalent fee-bumping reserve
> to maintain the pinning and might lose the "competing" fees to miners.
>
> # RBF opt-out as a Mempools Partitions Vector
>
> A longer-term issue is the risk of mempools malicious partitions, where an
> attacker exploits network topology or divergence in mempools policies to
> partition network mempools in different subsets. From then a wide range of
> attacks can be envisioned such as package pinning [1], artificial
> congestion to provoke LN channels closure or manipulation of
> fee-estimator's feerate (the Core's one wouldn't be affected as it relies
> on block confirmation, though other fee estimators designs deployed across
> the ecosystem are likely going to be affected).
>
> Traditionally, mempools partitions have been gauged as a spontaneous
> outcome of a distributed systems like Bitcoin p2p network and I'm not aware
> it has been studied in-depth for adversarial purposes. Though, deployment
> of second-layer
> protocols, heavily relying on sanity of a local mempool for fee-estimation
> and robust propagation of their time-sensitive transactions might lead to
> reconsider this position. Acknowledging this, RBF opt-out is a low-cost
> partitioning tool, of which the existence nullifies most of potential
> progresses to mitigate malicious partitioning.
>
>
> To resume, opt-in RBF doesn't suit well deployment of robust second-layers
> protocol, even if those issues are still early and deserve more research.
> At the same time, I believe a meaningful subset of the ecosystem  are still
> relying
> on 0-confs transactions, even if their security is relying on far weaker
> assumptions (opt-in RBF rule is a policy rule, not a consensus one) [2] A
> rapid change of Core's mempool rules would be harming their quality of
> services and should be
> weighed carefully. On the other hand, it would be great to nudge them
> towards more secure handling of their 0-confs flows [3]
>
> Let's examine what could be deployed ecosystem-wise as enhancements to the
> 0-confs security model.
>
> # Proactive security models : Double-spend Monitoring/Receiver-side
> Fee-Topping with Package Relay
>
> From an attacker viewpoint, opt-in RBF isn't a big blocker to successful
> double-spends. Any motivated attacker can modify Core to mass-connect to a
> wide portion of the network, announce txA to this subset, announce txA' to
> the
> merchant. TxA' propagation will be encumbered by the privacy-preserving
> inventory timers (`OUTBOUND_INVENTORY_BROADCAST_INTERVAL`), of which an
> attacker has no care to respect.
>
> To detect a successful double-spend attempt, a Bitcoin service should run
> few full-nodes with well-spread connection graphs and unlinkable between
> them, to avoid being identified then maliciously partitioned from the rest
> of the network.
>
> I believe this tactic is already deployed by few Bitcoin services, and
> even one can throw flame at it because it over consumes network resources
> (bandwidth, connection slots, ...), it does procure a security advantage to
> the ones doing it.
>
> One further improvement on top of this protection could be to react after
> the double-spend detection by attaching a CPFP to the merchant transaction,
> with a higher package feerate than the double-spend. Expected deployment of
> package-relay as a p2p mechanism/mempool policy in Bitcoin Core should
> enable it to do so.
>
> # Reactive security models : EconomicReputation-based Compensations
>
> Another approach could be to react after the fact if a double-spend has
> been qualified. If the sender is already known to the service provider, the
> service account can be slashed.  If the sender is a low-trusted
> counterparty to the merchant, "side-trust" models could be relied on. For
> e.g a LN pubkey with a stacked reputation from your autopilot, LSATs, stake
> certificates, a HTLC-as-a-fidelity-bond, ... The space is quite wide there
> but I foresee those trust-minimized, decentralized solutions being adopted
> by the LN ecosystem to patch the risks when you enter in a channel/HTLC
> operation with an anonymous counterparty.
>
> What other cool new tools could be considered to enhance 0-confs security ?
>
> To conclude, let's avoid replaying the contentious threads of a few years
> ago. What this new thread highlights is the fact that a transaction
> relay/mempool acceptance policy might be beneficial to some class of
> already-deployed
> Bitcoin applications while being detrimental to newer ones. How do we
> preserve the current interests of 0-confs users while enabling upcoming
> interests of fancy L2s to flourish is a good conversation to have. I think.
>
> If there is ecosystem agreement on switching to full-RBF, but 0.24 sounds
> too early, let's defer it to 0.25 or 0.26. I don't think Core has a
> consistent deprecation process w.r.t to policy rules heavily relied-on by
> Bitcoin users, if we do so let sets a precedent satisfying as many folks as
> we can.
>
> Cheers,
> Antoine
>
> [0]
> https://lists.linuxfoundation.org/pipermail/lightning-dev/2021-May/003033.html
>
> [1] See scenario 3 :
> https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-June/002758.html
>
> [2] https://github.com/bitcoin/bitcoin/pull/10823#issuecomment-466485121
>
> [3] And the LN ecosystem does have an interest to fix zero-confs security,
> if "turbo-channels"-like become normalized for mobile nodes
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211218/d033f8d9/attachment.html>

From pete at petertodd.org  Sat Dec 18 17:52:07 2021
From: pete at petertodd.org (Peter Todd)
Date: Sat, 18 Dec 2021 12:52:07 -0500
Subject: [bitcoin-dev] Proposal: Full-RBF in Bitcoin Core 24.0
In-Reply-To: <CAD5xwhjVkxgu2+M+Ft576GYM6Tv=ZEwtV82v1cLeYaoU5mSRnA@mail.gmail.com>
References: <CALZpt+F2b3tdu1+kLZiBPCH2O-pDzZytoRFtX6X0a8UX4OBrDQ@mail.gmail.com>
 <CAD5xwhjVkxgu2+M+Ft576GYM6Tv=ZEwtV82v1cLeYaoU5mSRnA@mail.gmail.com>
Message-ID: <Yb4fx0ny0tZYKmrD@petertodd.org>

On Sat, Dec 18, 2021 at 08:51:46AM -0800, Jeremy via bitcoin-dev wrote:
> Small idea:
> 
> ease into getting rid of full-rbf by keeping the flag working, but make
> enforcement of non-replaceability something that happens n seconds after
> first seen.
> 
> this reduces the ability to partition the mempools by broadcasting
> irreplaceable conflicts all at once, and slowly eases clients off of
> relying on non-RBF.
> 
> we might start with 60 seconds, and then double every release till we get
> to 600 at which point we disable it.

Making replacability turn on _after_ an expiry time is reached has been
suggested before, IIRC by Matt Corallo. However I believe the approach of
enabling full-rbf _until_ a time is reached is clever and novel.

I'd suggest doing both at once. Long-running txs are certainly useful. But if a
tx hasn't been mined in a few blocks, it certainly can't be relied on for
zeroconf.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211218/4f6f02f0/attachment-0001.sig>

From jlrubin at mit.edu  Sat Dec 18 21:14:16 2021
From: jlrubin at mit.edu (Jeremy)
Date: Sat, 18 Dec 2021 13:14:16 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Packaging Sapio Applications
Message-ID: <CAD5xwhjquyfecaoL_K723EQ=7Gvp9UCs382mH8f01e4KuewnKg@mail.gmail.com>

hi devs,

today's topic is packaging Sapio applications. maybe a bit more annoying
than usual, but important.

https://rubin.io/bitcoin/2021/12/18/advent-21/


I think WASM is really really cool! It's definitely been very helpful for
Sapio. It'd be kinda neat if at some point software like Bitcoin Core could
run Sapio modules natively and offer users extended functionality based on
that. For now I'm building out the wallet as Sapio Studio, but a boy can
dream. I know there are some bitcoiners (in particular, the rust-bitcoiners
& rust-lightning) who like WASM for shipping stuff to browsers!

WASM is also something I've been thinking about w.r.t. how we ship
consensus upgrades. It would be kinda groovy if we could implement the
semantics of pieces of bitcoin code as WASM modules... e.g., imagine pieces
of consensus being able to be compiled to and run through a WASM system, it
would help guarantee that those pieces of the code are entirely
deterministic. Maybe something for Simplicity to consider WASM being the
host language for JET extensions!


Cheers,

Jeremy

--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211218/0deffeca/attachment.html>

From antoine.riard at gmail.com  Sun Dec 19 18:55:01 2021
From: antoine.riard at gmail.com (Antoine Riard)
Date: Sun, 19 Dec 2021 13:55:01 -0500
Subject: [bitcoin-dev] Proposal: Full-RBF in Bitcoin Core 24.0
In-Reply-To: <CAD5xwhjVkxgu2+M+Ft576GYM6Tv=ZEwtV82v1cLeYaoU5mSRnA@mail.gmail.com>
References: <CALZpt+F2b3tdu1+kLZiBPCH2O-pDzZytoRFtX6X0a8UX4OBrDQ@mail.gmail.com>
 <CAD5xwhjVkxgu2+M+Ft576GYM6Tv=ZEwtV82v1cLeYaoU5mSRnA@mail.gmail.com>
Message-ID: <CALZpt+F9zgBNPzcmtN5HzkgUDkbXdkxhN2ksfo4uExB8jYEzjw@mail.gmail.com>

> we might start with 60 seconds, and then double every release till we get
to 600 at which point we disable it.

This is clearly new. However, I'm not sure if it's solving multi-party
funding transaction DoS, which was of the motivation to propose to
deprecate opt-in RBF. The malicious counterparty can broadcast its
low-feerate, opt-out spending of its own collateral input far before to
engage in the cooperative funding.

When the funding transaction starts to propagate, the opt-out has been
"first seen" for a while, the replaceability is turned off, the honest
funding is bounced off ?


Taking opportunity to laid out another proposal which has whispered to me
offline :

"(what) if the nversion of outputs (which is set by their creating
transaction) were inspected and
triggered any spend of the output to be required to be flagged to be
replaceable-- as a standardness rule?"

While working to solve the DoS, I believe this approach is introducing an
overhead cost in the funding of multi-party transactions, as from now on,
you have to sanitize your collateral inputs by sending them first to a
replaceable nVersion outputs ? (iirc, this is done by Lightning Pool, where
you have a first step where the inputs are locked in a 2-of-2 with the
orchester before to engage in the batch execution tx).

Current state of the discussion is to introduce a `fullrbf` config-knob
turned to false, see more context here :
https://gnusha.org/bitcoin-core-dev/2021-10-21.log. Proposing an
implementation soon.

Antoine

Le sam. 18 d?c. 2021 ? 11:51, Jeremy <jlrubin at mit.edu> a ?crit :

> Small idea:
>
> ease into getting rid of full-rbf by keeping the flag working, but make
> enforcement of non-replaceability something that happens n seconds after
> first seen.
>
> this reduces the ability to partition the mempools by broadcasting
> irreplaceable conflicts all at once, and slowly eases clients off of
> relying on non-RBF.
>
> we might start with 60 seconds, and then double every release till we get
> to 600 at which point we disable it.
> --
> @JeremyRubin <https://twitter.com/JeremyRubin>
> <https://twitter.com/JeremyRubin>
>
>
> On Tue, Jun 15, 2021 at 10:00 AM Antoine Riard via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> Hi,
>>
>> I'm writing to propose deprecation of opt-in RBF in favor of full-RBF as
>> the Bitcoin Core's default replacement policy in version 24.0. As a
>> reminder, the next release is 22.0, aimed for August 1st, assuming
>> agreement is reached, this policy change would enter into deployment phase
>> a year from now.
>>
>> Even if this replacement policy has been deemed as highly controversial a
>> few years ago, ongoing and anticipated changes in the Bitcoin ecosystem are
>> motivating this proposal.
>>
>> # RBF opt-out as a DoS Vector against Multi-Party Funded Transactions
>>
>> As explained in "On Mempool Funny Games against Multi-Party Funded
>> Transactions'', 2nd issue [0], an attacker can easily DoS a multi-party
>> funded transactions by propagating an RBF opt-out double-spend of its
>> contributed input before the honest transaction is broadcasted by the
>> protocol orchester. DoSes are qualified in the sense of either an attacker
>> wasting timevalue of victim's inputs or forcing exhaustion of the
>> fee-bumping  reserve.
>>
>> This affects a series of Bitcoin protocols such as Coinjoin, onchain DLCs
>> and dual-funded LN channels. As those protocols are still in the early
>> phase of deployment, it doesn't seem to have been executed in the wild for
>> now.  That said, considering that dual-funded are more efficient from a
>> liquidity standpoint, we can expect them to be widely relied on, once
>> Lightning enters in a more mature phase. At that point, it should become
>> economically rational for liquidity service providers to launch those DoS
>> attacks against their competitors to hijack user traffic.
>>
>> Beyond that, presence of those DoSes will complicate the design and
>> deployment of multi-party Bitcoin protocols such as payment
>> pools/multi-party channels. Note, Lightning Pool isn't affected as there is
>> a preliminary stage where batch participants are locked-in their funds
>> within an account witnessScript shared with the orchestrer.
>>
>> Of course, even assuming full-rbf, propagation of the multi-party funded
>> transactions can still be interfered with by an attacker, simply
>> broadcasting a double-spend with a feerate equivalent to the honest
>> transaction. However, it tightens the attack scenario to a scorched earth
>> approach, where the attacker has to commit equivalent fee-bumping reserve
>> to maintain the pinning and might lose the "competing" fees to miners.
>>
>> # RBF opt-out as a Mempools Partitions Vector
>>
>> A longer-term issue is the risk of mempools malicious partitions, where
>> an attacker exploits network topology or divergence in mempools policies to
>> partition network mempools in different subsets. From then a wide range of
>> attacks can be envisioned such as package pinning [1], artificial
>> congestion to provoke LN channels closure or manipulation of
>> fee-estimator's feerate (the Core's one wouldn't be affected as it relies
>> on block confirmation, though other fee estimators designs deployed across
>> the ecosystem are likely going to be affected).
>>
>> Traditionally, mempools partitions have been gauged as a spontaneous
>> outcome of a distributed systems like Bitcoin p2p network and I'm not aware
>> it has been studied in-depth for adversarial purposes. Though, deployment
>> of second-layer
>> protocols, heavily relying on sanity of a local mempool for
>> fee-estimation and robust propagation of their time-sensitive transactions
>> might lead to reconsider this position. Acknowledging this, RBF opt-out is
>> a low-cost partitioning tool, of which the existence nullifies most of
>> potential progresses to mitigate malicious partitioning.
>>
>>
>> To resume, opt-in RBF doesn't suit well deployment of robust
>> second-layers protocol, even if those issues are still early and deserve
>> more research. At the same time, I believe a meaningful subset of the
>> ecosystem  are still relying
>> on 0-confs transactions, even if their security is relying on far weaker
>> assumptions (opt-in RBF rule is a policy rule, not a consensus one) [2] A
>> rapid change of Core's mempool rules would be harming their quality of
>> services and should be
>> weighed carefully. On the other hand, it would be great to nudge them
>> towards more secure handling of their 0-confs flows [3]
>>
>> Let's examine what could be deployed ecosystem-wise as enhancements to
>> the 0-confs security model.
>>
>> # Proactive security models : Double-spend Monitoring/Receiver-side
>> Fee-Topping with Package Relay
>>
>> From an attacker viewpoint, opt-in RBF isn't a big blocker to successful
>> double-spends. Any motivated attacker can modify Core to mass-connect to a
>> wide portion of the network, announce txA to this subset, announce txA' to
>> the
>> merchant. TxA' propagation will be encumbered by the privacy-preserving
>> inventory timers (`OUTBOUND_INVENTORY_BROADCAST_INTERVAL`), of which an
>> attacker has no care to respect.
>>
>> To detect a successful double-spend attempt, a Bitcoin service should run
>> few full-nodes with well-spread connection graphs and unlinkable between
>> them, to avoid being identified then maliciously partitioned from the rest
>> of the network.
>>
>> I believe this tactic is already deployed by few Bitcoin services, and
>> even one can throw flame at it because it over consumes network resources
>> (bandwidth, connection slots, ...), it does procure a security advantage to
>> the ones doing it.
>>
>> One further improvement on top of this protection could be to react after
>> the double-spend detection by attaching a CPFP to the merchant transaction,
>> with a higher package feerate than the double-spend. Expected deployment of
>> package-relay as a p2p mechanism/mempool policy in Bitcoin Core should
>> enable it to do so.
>>
>> # Reactive security models : EconomicReputation-based Compensations
>>
>> Another approach could be to react after the fact if a double-spend has
>> been qualified. If the sender is already known to the service provider, the
>> service account can be slashed.  If the sender is a low-trusted
>> counterparty to the merchant, "side-trust" models could be relied on. For
>> e.g a LN pubkey with a stacked reputation from your autopilot, LSATs, stake
>> certificates, a HTLC-as-a-fidelity-bond, ... The space is quite wide there
>> but I foresee those trust-minimized, decentralized solutions being adopted
>> by the LN ecosystem to patch the risks when you enter in a channel/HTLC
>> operation with an anonymous counterparty.
>>
>> What other cool new tools could be considered to enhance 0-confs security
>> ?
>>
>> To conclude, let's avoid replaying the contentious threads of a few years
>> ago. What this new thread highlights is the fact that a transaction
>> relay/mempool acceptance policy might be beneficial to some class of
>> already-deployed
>> Bitcoin applications while being detrimental to newer ones. How do we
>> preserve the current interests of 0-confs users while enabling upcoming
>> interests of fancy L2s to flourish is a good conversation to have. I think.
>>
>> If there is ecosystem agreement on switching to full-RBF, but 0.24 sounds
>> too early, let's defer it to 0.25 or 0.26. I don't think Core has a
>> consistent deprecation process w.r.t to policy rules heavily relied-on by
>> Bitcoin users, if we do so let sets a precedent satisfying as many folks as
>> we can.
>>
>> Cheers,
>> Antoine
>>
>> [0]
>> https://lists.linuxfoundation.org/pipermail/lightning-dev/2021-May/003033.html
>>
>> [1] See scenario 3 :
>> https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-June/002758.html
>>
>> [2] https://github.com/bitcoin/bitcoin/pull/10823#issuecomment-466485121
>>
>> [3] And the LN ecosystem does have an interest to fix zero-confs
>> security, if "turbo-channels"-like become normalized for mobile nodes
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211219/55df5c1e/attachment.html>

From jlrubin at mit.edu  Mon Dec 20 02:37:39 2021
From: jlrubin at mit.edu (Jeremy)
Date: Sun, 19 Dec 2021 18:37:39 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] NFTs Part Two: Auctions,
 Royalties, Mints, Generative, Game Items
Message-ID: <CAD5xwhiRieD6HVAANqjS3Dt6h6aScOip+4X3mvA7veYiMD=RoA@mail.gmail.com>

Hi Devs!

More on NFTs today! Code demos of dutch auctions of NFTs + royalties, and
then discussion of a few other concepts I'm excited about.

https://rubin.io/bitcoin/2021/12/19/advent-22/

Particularly novel is the combination of attestation chains, lightning
invoices, and NFTs to create off-chain updatable and on-chain sellable
in-game items.

Till tomorrow!

Cheers,

Jeremy

--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211219/54ba2655/attachment-0001.html>

From damian at willtech.com.au  Mon Dec 20 02:30:57 2021
From: damian at willtech.com.au (damian at willtech.com.au)
Date: Sun, 19 Dec 2021 18:30:57 -0800
Subject: [bitcoin-dev] Proposal: Full-RBF in Bitcoin Core 24.0
In-Reply-To: <Yb4fx0ny0tZYKmrD@petertodd.org>
References: <CALZpt+F2b3tdu1+kLZiBPCH2O-pDzZytoRFtX6X0a8UX4OBrDQ@mail.gmail.com>
 <CAD5xwhjVkxgu2+M+Ft576GYM6Tv=ZEwtV82v1cLeYaoU5mSRnA@mail.gmail.com>
 <Yb4fx0ny0tZYKmrD@petertodd.org>
Message-ID: <a156e0fb6fc2a62813a4ff1aecfe5ae7@willtech.com.au>

Good Afternoon,

There is no such thing in Bitcoin as zeroconf but any individual may use 
gossip from mempool if they choose it to prefer it could be possible a 
transaction could exist in the future. You are talking about the 
mempool. The mempool exists on gossip. There are no transactions until 
they are mined and included in a block and information can disappear 
from the mempool. This is Bitcoin where we scientifically make a 
consensus to assure fungibility.

KING JAMES HRMH
Great British Empire

Regards,
The Australian
LORD HIS EXCELLENCY JAMES HRMH (& HMRH)
of Hougun Manor & Glencoe & British Empire
MR. Damian A. James Williamson
Wills

et al.


Willtech
www.willtech.com.au
www.go-overt.com
duigco.org DUIGCO API
and other projects


m. 0487135719
f. +61261470192


This email does not constitute a general advice. Please disregard this 
email if misdelivered.
On 2021-12-18 09:52, Peter Todd via bitcoin-dev wrote:
> On Sat, Dec 18, 2021 at 08:51:46AM -0800, Jeremy via bitcoin-dev wrote:
>> Small idea:
>> 
>> ease into getting rid of full-rbf by keeping the flag working, but 
>> make
>> enforcement of non-replaceability something that happens n seconds 
>> after
>> first seen.
>> 
>> this reduces the ability to partition the mempools by broadcasting
>> irreplaceable conflicts all at once, and slowly eases clients off of
>> relying on non-RBF.
>> 
>> we might start with 60 seconds, and then double every release till we 
>> get
>> to 600 at which point we disable it.
> 
> Making replacability turn on _after_ an expiry time is reached has been
> suggested before, IIRC by Matt Corallo. However I believe the approach 
> of
> enabling full-rbf _until_ a time is reached is clever and novel.
> 
> I'd suggest doing both at once. Long-running txs are certainly useful. 
> But if a
> tx hasn't been mined in a few blocks, it certainly can't be relied on 
> for
> zeroconf.
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From billy.tetrud at gmail.com  Mon Dec 20 17:18:44 2021
From: billy.tetrud at gmail.com (Billy Tetrud)
Date: Mon, 20 Dec 2021 09:18:44 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Decentralized
 Coordination Free Mining Pools
In-Reply-To: <150492262-e89b67dc2c010e65008ad976e2647ec1@pmq4v.m5r2.onet>
References: <CAGpPWDbph1VPa6Kqy1HsB0XbZ=Warn+qN7m=yNdJfYwQ3G-nSw@mail.gmail.com>
 <150492262-e89b67dc2c010e65008ad976e2647ec1@pmq4v.m5r2.onet>
Message-ID: <CAGpPWDbGTET27Hq8kKrQQoOavtOUEGzYkohVurSqZJ5r+o4gpQ@mail.gmail.com>

> you can assign some minimal difficulty,

I was assuming that would be part of the plan.

> Signing sounds more like Proof of Stake

Associating signing with proof of stake and thereby concluding that signing
is something to avoid sounds like superstitious thinking. A signing scheme
with proof of work would clearly not be proof of stake. I would suggest not
dismissing a design out of hand like that. The benefit of that over merge
mining would be no extra on chain foot print. What do you think the
downsides might be?

> if you use just hashes, then they could be random.

You're right. Nodes would of course need to validate the Bitcoin block
headers being included, so i concede hashing them doesn't gain you anything.


On Thu, Dec 16, 2021, 22:37 <vjudeu at gazeta.pl> wrote:

> > I was thinking that this would be a separate blockchain with separate
> headers that progress linearly like a normal blockchain.
>
> Exactly, that's what I called "superblocks", where you have a separate
> chain, just to keep block headers instead of transactions.
>
> > A block creator would collect together as many blocks that haven't been
> collected yet into the next superblock (and maybe receive a reward
> proportional to how many / how much weight they include).
>
> You cannot "catch them all". If you do, you can end up with a lot of block
> headers, where each of them has difficulty equal to one. You need some
> limit, you can limit amount of blocks, you can assign some minimal
> difficulty, it does not matter that much, but some limit is needed, also
> because mining on top of the latest superblock should be more profitable
> than replacing someone else's reward in the previous superblock by your own
> reward and getting a bigger share in the previous superblock.
>
> > This could be done using merge mining, or it could be done using a
> signing scheme (eg where the block creator signs to say "I created this
> superblock" and have mechanisms to punish those who sign multiple
> superblocks at the same height.
>
> I would pick merge mining, because it is more compatible with existing
> mining scheme. Signing sounds more like Proof of Stake and I am trying to
> avoid that solution. Also, there is no need to sign anything, because you
> are solo mining where you have your own coinbase transaction or you are
> mining in a pool, where you have some shared address, and then you cannot
> produce any incompatible superblock, because the protocol can tell you,
> which address you should use (and if it is N-of-N taproot multisig and you
> have some closing transaction, then you can safely mine it).
>
> > Really, you could even just use hashes of the block headers.
>
> Replacing transactions with block headers will do the same trick. Each
> transaction is first hashed with double SHA-256, in exactly the same way as
> block headers are. If you replace transactions with block headers, you
> would get a superblock header, then varint saying how many block headers
> are there, and then you can place all block headers. During superblock
> merkle tree construction, you will hash all block headers (so you will get
> block hashes as leaves), and then you will combine block hashes in the same
> way as transaction hashes are combined.
>
> From the Script point of view, you can always use "OP_SIZE 80
> OP_EQUALVERIFY OP_HASH256 <hash> OP_EQUAL". Then, you can just change the
> size, just to show which object is hashed. Value 80 will work for block
> headers, small values below 520 will work for small transactions, value 64
> will work for any merkle tree proof, no matter if it is for superblock or
> normal block. Also, by using block headers instead of hashes, you can prove
> that at least a proper amount of work was done to produce it, because if
> you use just hashes, then they could be random.
>
> On 2021-12-16 17:57:23 user Billy Tetrud <billy.tetrud at gmail.com> wrote:
>
> @Jeremy
> >   for top-level pool participants there is never any central custody.
>
> I definitely see that. That was actually what I meant when I said the
> goals aren't the same as benefits. While your idea definitely satisfies all
> your goals in a modular way, the fact that it relies on pools means that
> unless the pools can also satisfy the goals, the total system also doesn't
> satisfy the goals (even tho the piece of that system you designed does).
>
> > Thus it doesn't "hurt" anyone except for the miners who are taking the
> not fully locked in funds risk
>
> True, it only potentially hurts whoever the channel partner is accepting
> the unspendable coins. And no one can really stop anyone from taking that
> risk if they really want to. But in that case, its not exactly a fully
> functional channel, since recourse mechanisms couldn't be performed.
> Wouldn't that open such a channel up to a pretty bad theft possibility?
>
> @Bob
> > Increased payout regularity does not lower the viable size of mining
> pools, because smaller mining pools using this mechanism still have higher
> variance.
>
> Yes, smaller mining pools will always have higher variance. However, lower
> variance has diminishing benefits. Below a certain amount of variance, less
> variance isn't very valuable. So increased payout regularity does indeed
> lower the viable size of mining pools because a given low-enough level of
> variance can be achieved with less pool hashpower.
>
> > The on-chain footprint is *higher* due to the increased payout
> regularity.
>
> That's a reasonable point. However, I think there is a difference here
> between the regularity of rewards vs payouts. Rewards for each miner can be
> more regular without necessarily increasing the number of on-chain payouts.
> In fact, theoretically, an individual miner could let their rewards
> accumulate in a pool over many rewards and only redeem when they need the
> coins for something. The incentive is there for each miner to be judicious
> on how much onchain space they take up.
>
> @vjudeu
>
> > how many block headers should be stored per one "superblock"?
>
> I was thinking that this would be a separate blockchain with separate
> headers that progress linearly like a normal blockchain. A block creator
> would collect together as many blocks that haven't been collected yet into
> the next superblock (and maybe receive a reward proportional to how many /
> how much weight they include). This could be done using merge mining, or it
> could be done using a signing scheme (eg where the block creator signs to
> say "I created this superblock" and have mechanisms to punish those who
> sign multiple superblocks at the same height. For merge mining, I could
> even imagine the data necessary to validate that it has been merge mined
> could be put into a taproot script branch (creating an invalid script, but
> a valid hash of the superblock).
>
> > we can collect all headers with the same previous block hash, and
> distribute block reward between all coinbase transactions in those headers
>
> Exactly.
>
> > we would just have block headers instead of transactions
>
> Yeah, I think that would be the way to go. Really, you could even just use
> hashes of the block headers. But the size doesn't matter much because it
> would be both a small blockchain and an ephemeral one (which can be fully
> discarded after all parties have been paid out, or at least their payout
> has been committed to on the bitcoin blockchain).
>
> On Thu, Dec 16, 2021 at 1:35 AM <vjudeu at gazeta.pl
> <http://../NowaWiadomosc/Do/QlIkBFQ6QUFhIVRZX192dnQBeCtCchEuGRE%2BJkYAEBM5BgkBElIaQgpBQUFBVCVGX18dBRtTEQcBF1UyQUoDEQ0TRQYNQUdjI1hCU0cyajZIblhVZRQcVlEe>>
> wrote:
>
>> > The missing piece here would be an ordering of weak blocks to make the
>> window possible. Or at least a way to determine what blocks should
>> definitely be part of a particular block's pay out. I could see this being
>> done by a separate ephemeral blockchain (which starts fresh after each
>> Bitcoin block) that keeps track of which weak blocks have been submitted,
>> potentially using the pow already in each block to secure it. Granted that
>> piece is a bit half baked, but it seems quite solvable. Wdyt?
>>
>> I thought about something like that, but there is one problem: how many
>> block headers should be stored per one "superblock"? Currently, we have
>> single block header, where the whole coinbase transaction is taken by some
>> mining pool or solo miner. But instead, each miner could submit its own
>> block header. Then, we can collect all headers with the same previous block
>> hash, and distribute block reward between all coinbase transactions in
>> those headers. One "superblock" then would be created in a similar way as
>> existing blocks, we would just have block headers instead of transactions.
>> If most transactions inside those blocks will be the same, then each block
>> could be expressed just as a set of transaction hashes, only coinbase
>> transactions or custom, non-broadcasted transactions included by miners
>> will be revealed, everything else will be known.
>>
>> > One thing that jumped out at me as not safe is throwing block rewards
>> into a channel and being able to spend them immediately. There's a reason
>> block rewards aren't spendable for a while, and channels don't solve that
>> problem, do they? Why not simply reduce the on chain wait time for spending
>> block rewards at that point? Seems like the consequences would be the same.
>>
>> All coinbase rewards are unspendable for 100 blocks, it is enforced by
>> consensus. It does not matter if there are outputs owned directly by
>> miners, or if there is one huge N-of-N taproot multisig for the whole pool,
>> where every miner signed the closing transaction. The only option to take
>> coins faster I can see is swapping the coins by some LN transaction. But
>> then, the other party can check if some deposit to the LN channel is a part
>> of the coinbase transaction or not, and then decide if it is acceptable to
>> do the swap.
>>
>> On 2021-12-15 19:00:44 user Billy Tetrud via bitcoin-dev <
>> bitcoin-dev at lists.linuxfoundation.org
>> <http://../NowaWiadomosc/Do/QlIkBFQ6QUFhIVRZX192dnQBeCtCchE6GhA5LFpLCUc7EVZQVl9dQRIXXR8NCBMbCwIGChJXQFxcXEgcFh8UVVVDEyBdVkE9JVRdEwFhYXVlblhVIkosEAszLR5BQVV7U0MID0BAQUgIGh0RHgAMGAMXBQJfW1sdXRQUQUoDQlAiBFY8>>
>> wrote:
>>
>> Looks like an interesting proposal, but it doesn't seem to quite match
>> the goals you mentioned. As you do mention, this mining pool coordination
>> doesn't get rid of the need for mining pools in the first place. So it
>> doesn't satisfy item 1 on your goal list afaict.
>>
>> The primary benefits over what we have today that I can see are:
>> 1. increased payout regularity, which lowers the viable size of mining
>> pools, and
>> 2. Lower on chain footprint through combining pay outs from multiple
>> pools.
>>
>> Am I missing some?
>>
>> These are interesting benefits, but it would be nice if your post was
>> clearer on that, since the goals list is not the same as the list of
>> potential benefits of this kind of design.
>>
>> As far as enabling solo mining, what if this concept were used off chain?
>> Have a public network of solo miners who publish "weak blocks" to that
>> network, and the next 100 (or 1000 etc) nice miners pay you out as long as
>> you're also being nice by following the protocol? All the nice
>> optimizations you mentioned about eg combined taproot payouts would apply i
>> think. The only goals this wouldn't satisfy are 3 and 5 since an extra
>> network is needed, but to be fair, your proposal requires pools which all
>> need their own extra network anyways.
>>
>> The missing piece here would be an ordering of weak blocks to make the
>> window possible. Or at least a way to determine what blocks should
>> definitely be part of a particular block's pay out. I could see this being
>> done by a separate ephemeral blockchain (which starts fresh after each
>> Bitcoin block) that keeps track of which weak blocks have been submitted,
>> potentially using the pow already in each block to secure it. Granted that
>> piece is a bit half baked, but it seems quite solvable. Wdyt?
>>
>> One thing that jumped out at me as not safe is throwing block rewards
>> into a channel and being able to spend them immediately. There's a reason
>> block rewards aren't spendable for a while, and channels don't solve that
>> problem, do they? Why not simply reduce the on chain wait time for spending
>> block rewards at that point? Seems like the consequences would be the same.
>>
>> On Tue, Dec 14, 2021, 16:12 Bob McElrath via bitcoin-dev <
>> bitcoin-dev at lists.linuxfoundation.org
>> <http://../NowaWiadomosc/Do/QlIkBFQ6QUFhIVRZX192dnQBeCtCchE6GhA5LFpLCUc7EVZQVl9dQRIXXR8NCBMbCwIGChJXQFxcXEgcFh8UVVVDEyBdVkE9JVRdEwFhYXVlblhVIkosEAszLR5BQVV7U0MID0BAQUgIGh0RHgAMGAMXBQJfW1sdXRQUQUoDQlAiBFY8>>
>> wrote:
>>
>>> You are hand waving. Attempting to redefine terms to justify your
>>> argument is
>>> intellectually dishonest. Bitcoin pools have *always* been about variance
>>> reduction. Your window function fundamentally CANNOT be used to hedge
>>> hashrate.
>>> Various suggestions below introduce dangerous new games that might be
>>> played by
>>> miners.
>>>
>>> The fact is that the half-baked design you posted is less than useless,
>>> and
>>> doesn't do anything that anyone wants.
>>>
>>> You are trying to justify CTV by making it be all things to all people.
>>> "When
>>> all you have is a hammer, every problem looks like a nail".  Instead I
>>> humbly
>>> suggest that you pick ONE problem for which CTV is demonstrably the
>>> right and
>>> best solution, instead of snowing us with a ton of half-baked things that
>>> *could* be done, and often don't even require CTV, and some (like this
>>> one)
>>> fundamentally don't work. I do like some of your ideas, but if you had
>>> to pick
>>> just one "use case", which would it be?
>>>
>>> Jeremy [jlrubin at mit.edu
>>> <http://../NowaWiadomosc/Do/QlIkBFQ6QUFhIVRZX192dnQBeCtCchEyHxYvIVpLARduChoQSFZQR0NWQVZWJUNRXwMSCRMTBgcWASdWVkpbCxUTQwoWQUdjKVBMGFY3MWMWeU9QBAZtNw%3D%3D>]
>>> wrote:
>>> > Bitcoin didn't invent the concept of pooling:
>>> https://en.wikipedia.org/wiki/
>>> > Pooling_(resource_management). This is a Bitcoin Mining Pool, although
>>> it may
>>> > not be your favorite kind, which is fixated on specific properties of
>>> computing
>>> > contributions before finding a block. Pooling is just a general
>>> technique for
>>> > aggregating resources to accomplish something. If you have another
>>> name like
>>> > pooling that is in common use for this type of activity I would be
>>> more than
>>> > happy to adopt it.
>>> >
>>> > This sort of pool can hedge not only against fee rates but also against
>>> > increases in hashrate since your historical rate 'carries' into the
>>> future as a
>>> > function of the window. Further, windows and reward functions can be
>>> defined in
>>> > a myriad of ways that could, e.g., pay less to blocks found in more
>>> rapid
>>> > succession, contributing to the smoothing functionality.
>>> >
>>> > With respect to sub-block pooling, as described in the article, this
>>> sort of
>>> > design also helps with micro-pools being able to split resources
>>> > non-custodially in every block as a part of the higher order DCFMP.
>>> The point
>>> > is not, as noted, to enable solo mining an S9, but to decrease the
>>> size of the
>>> > minimum viable pool. It's also possible to add, without much
>>> validation or
>>> > data, some 'uncle block' type mechanism in an incentive compatible way
>>> (e.g.,
>>> > add 10 pow-heavy headers on the last block for cost 48 bytes header +
>>> 32 bytes
>>> > payout key) such that there's an incentive to include the heaviest
>>> ones you've
>>> > seen, not just your own, that are worth further study and consideration
>>> > (particularly because it's non-consensus, only for opt-in
>>> participation in the
>>> > pool).
>>> >
>>> > With respect to space usage, it seems you wholly reject the viability
>>> of a
>>> > payment pool mechanism to cut-through chain space. Is this a critique
>>> that
>>> > holds for all Payment Pools, or just in the context of mining? Is
>>> there a
>>> > particular reason why you think it infeasible that "strongly online"
>>> > counterparties would be able to coordinate more efficiently? Is it
>>> preferable
>>> > for miners, the nexus of decentralization for Bitcoin, to prefer to use
>>> > custodial services for pooling (which may require KYC/AM) over bearing
>>> a cost
>>> > of some extra potential chainload?
>>> >
>>> > Lastly, with respect to complexity, the proposal is actually
>>> incredibly simple
>>> > when you take it in a broader context. Non Interactive Channels and
>>> Payment
>>> > Pools are useful by themselves, so are the operations to merge them
>>> and swap
>>> > balance across them. Therefore most of the complexity in this proposal
>>> is
>>> > relying on tools we'll likely see in everyday use in any case, DCFMP
>>> or no.
>>> >
>>> > Jeremy
>>> > !DSPAM:61b8f2f5321461582627336!
>>> --
>>> Cheers, Bob McElrath
>>>
>>> "For every complex problem, there is a solution that is simple, neat,
>>> and wrong."
>>>     -- H. L. Mencken
>>>
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev at lists.linuxfoundation.org
>>> <http://../NowaWiadomosc/Do/QlIkBFQ6QUFhIVRZX192dnQBeCtCchE6GhA5LFpLCUc7EVZQVl9dQRIXXR8NCBMbCwIGChJXQFxcXEgcFh8UVVVDEyBdVkE9JVRdEwFhYXVlblhVIkosEAszLR5BQVV7U0MID0BAQUgIGh0RHgAMGAMXBQJfW1sdXRQUQUoDQlAiBFY8>
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211220/c859e9f2/attachment-0001.html>

From jlrubin at mit.edu  Tue Dec 21 01:17:34 2021
From: jlrubin at mit.edu (Jeremy)
Date: Mon, 20 Dec 2021 17:17:34 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Derivatives and Options
Message-ID: <CAD5xwhiQakWUg8ub40vepnfftmmvAtv5neuuR_ooxDw-ithYQA@mail.gmail.com>

Hi Devs,

Today's post is on building options/derivatives in Sapio!

https://rubin.io/bitcoin/2021/12/20/advent-23

Enjoy!

Cheers,

Jeremy



--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211220/11983870/attachment.html>

From jlrubin at mit.edu  Wed Dec 22 00:25:04 2021
From: jlrubin at mit.edu (Jeremy)
Date: Tue, 21 Dec 2021 16:25:04 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] POWSWAP: Oracle Free
	Bitcoin Hashrate Derivatives
Message-ID: <CAD5xwhj6TT11Mg91wF_e5+CGa0ERFKOxYe08+PpNaaGkws04zQ@mail.gmail.com>

Hi devs,

Today's post details how to make fully trustless hashrate derivative
contracts that can be embedded on-chain, inside of channels, options, or
inside of DCFMPs. These contracts can be used today without CTV, but they
obviously get better with CTV :)

enjoy: https://rubin.io/bitcoin/2021/12/21/advent-24/

I have not done any work to analyze the profitability of these contracts or
how you might price and risk them, or if a two sided market among miners
actually exists. That's not really my expertise.

But maybe someone can figure that out and let us all know :)

cheers,

Jeremy

--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211221/ec303376/attachment.html>

From jlrubin at mit.edu  Thu Dec 23 03:47:06 2021
From: jlrubin at mit.edu (Jeremy)
Date: Wed, 22 Dec 2021 19:47:06 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Decentralized Autonomous
 Organizations (DAOs) Will Save Bitcoin
Message-ID: <CAD5xwhh7J5H_L5UDG3NtuULgnuZP_PY8qqq1QajqXSiXsKgSTQ@mail.gmail.com>

Hi Devs,

Enjoy! https://rubin.io/bitcoin/2021/12/22/advent-25/

I'm really excited about opportunities for capital formation to happen
natively in Bitcoin. This is actually a really big deal and something (I
think) to pay close attention to. This is basically like running a little
company with shareholders inside of Bitcoin, which to me really helps us
inhabit the "be your own bank" part of Bitcoin. None of this particularly
requires CTV, but it does require the type of composable and flexible
software that I aspire to deliver with Sapio.

business matter:

There are two more posts, and they will both be focused on getting this
stuff out into the wild more. If you particularly have thoughts on BIP-119
activation I would love to hear them publicly, or at your preference,
privately.

If you like or dislike BIP-119 and wish to "soft-signal" yes or no
publicly, you may do so on https://utxos.org/signals by editing the
appropriate file(s) and making a PR. Alternatively, comment somewhere
publicly I can link to, send it to me, and I will make the edits.

edit links:
- for individuals/devs:
https://github.com/JeremyRubin/utxos.org/edit/master/data/devs.yaml
- organizations:
https://github.com/JeremyRubin/utxos.org/edit/master/data/bizs.yaml
- miners/pools:
https://github.com/JeremyRubin/utxos.org/edit/master/data/hashratesnapshot.json

Best,

Jeremy

--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211222/1ec13f4d/attachment.html>

From prayank at tutanota.de  Thu Dec 23 09:42:02 2021
From: prayank at tutanota.de (Prayank)
Date: Thu, 23 Dec 2021 10:42:02 +0100 (CET)
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] What's Smart about
 Smart Contracts
Message-ID: <MraenVQ--B-2@tutanota.de>

Hi Jeremy,

> Here's the day 6 post: https://rubin.io/bitcoin/2021/12/03/advent-6/, the
topic is why smart contracts (in extended form) may be a critical precursor
to securing Bitcoin's future rather than something we should do after
making the base layer more robust.

There are few comparisons in this post and links that I consider misleading or incomplete. I had already tweeted this but such discussions are better archived here:

Difference between Bitcoin and Ethereum that is not mentioned on the website which should be considered while looking at fees: 1. Size of blocks added to chain everyday (600 MB) 2. Block limit (500 MB per 10 mins) 3. UTXO vs Account model 4. Failed transactions that pay fees (50k per day) 5. Will these fancy smart contracts work without nodes? No. Where are these nodes running? AWS and Infura has nice articles to highlight their importance 6. Who is paying the fees? Stablecoins, DEX, NFT platforms, CEX and VCs

There can be lot of other differences that affect the fee market including lot of users in Bitcoin obsessed?with supply and hodling. Things that have changed in last few years: 1. Darknet markets using Monero 2. Stablecoins stopped using Omni and lot of alternatives exist right now 3. Most of the transactions are related to exchanges. They have started using their own tokens, less exchanges support layer 2 for Bitcoin and users are forced to withdraw some altcoin even if they need bitcoin. 4. Newbies are reading influencers like Elon Musk and happy with their doggy coins to get rich quick/rekt. 5. Bitcoin users or influencers declared DeFi a scam and even sidechains like Liquid, Rootstock do not qualify their purity tests. Projects like DLCs are still not used in any projects with good volume.


-- 
Prayank

A3B1 E430 2298 178F
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211223/115e25b3/attachment.html>

From prayank at tutanota.de  Thu Dec 23 11:55:31 2021
From: prayank at tutanota.de (Prayank)
Date: Thu, 23 Dec 2021 12:55:31 +0100 (CET)
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Review of Smart
 Contract Concepts
Message-ID: <Mrak4sc--7-2@tutanota.de>

Hi Jeremy,

> This post covers some high-level smart contract concepts that different
opcodes or proposals could have (or not).
https://rubin.io/bitcoin/2021/12/04/advent-7/

Interesting post. I love the concept of recursion in programming. There is one Indian movie called 'Karthik calling Karthik' which is one of the ways I remember this concept. 

> Recursive is pretty much just a fancy way of saying ?loops?. This is sometimesalso called ?Turing Complete?.

Recently asked one dumb question on Stakexchange after reading a comment on reddit, maybe you can add anything new in this:

https://bitcoin.stackexchange.com/questions/111337/loops-in-bitcoin-scripting

> Here, the contract terminates after one canceled request by moving the coinelsewhere.  It?s possible to emulate recursive behavior a limited amount by?unrolling? a loop.

I think this is what I did in the above link where for loop was replaced with if-else statements.

> However, unrolling has it?s limits. When choices(action A or B) are introduced, unrolling can be less effective since you haveand exponential blowup (that means unrolling even like 32 steps might be toomany). However, there are some tricks that can be employed by a clever andcareful programmer to reduce this complexity through, for example, memoization.

Agree with limits and possibility of optimization.

> The key difference being that in the fully enumerated case we must know the exact specifics of the contract and how it will execute, and in the open ended contract case there are bits and pieces we can dynamically specify. If Alice is paid 1 BTC by December 25th, 2021 Midnight, then transfer 100 tokensto one of Bob?s Address B1, B2, or B3 at Bob?s discretion.

Interesting

> Signing the transaction fee rate as a function of locktime

TIL


-- 
Prayank

A3B1 E430 2298 178F
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211223/d53b094f/attachment-0001.html>

From vjudeu at gazeta.pl  Thu Dec 23 11:56:18 2021
From: vjudeu at gazeta.pl (vjudeu at gazeta.pl)
Date: Thu, 23 Dec 2021 12:56:18 +0100
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Decentralized
 Coordination Free Mining Pools
In-Reply-To: <CAGpPWDbGTET27Hq8kKrQQoOavtOUEGzYkohVurSqZJ5r+o4gpQ@mail.gmail.com>
Message-ID: <151161119-8858833d76ec6beed19cf87cc542dc62@pmq3v.m5r2.onet>

> Associating signing with proof of stake and thereby concluding that signing is something to avoid sounds like superstitious thinking.
If you introduce signing into mining, then you will have cases, where someone is powerful enough to produce blocks, but cannot, because signing is needed. Then, your consensus is no longer "the heaviest chain", but "the heaviest signed chain". That means, your computing power is no longer enough by itself (as today), because to make a block, you also need some kind of "permission to mine", because first you sign things (like in signet) and then you mine them. That kind of being "reliably unreliable" may be ok for testing, but not for the main network.
> A signing scheme with proof of work would clearly not be proof of stake.
It is not that far from Proof of Stake as you may think. In Proof of Stake, you sign your whale balance with thousands of BTC, and then you can mine a new block, just by putting your coins at stake. In Proof of Work with signing, you also make a signature, and even if there is no amount in satoshis for your signing key, it is still somehow checked, who can sign a block and how many signatures are needed. So, is your private key that allows you to extend the chain is really worth zero satoshis, because there is no balance connected with your address/public key? Not really, it is worth a lot of coins, because it can be used to control the chain.
> The benefit of that over merge mining would be no extra on chain foot print.
Your signatures would be that "extra on chain foot print". In signet, by default you have 1-of-2 multisig, so you have one signature for every block. In the main chain, there is no need for any foot print, because you can reveal taproot public key and hide other things in tapscript, so your foot print is present only when people will not cooperate.
On 2021-12-20 18:18:58 user Billy Tetrud <billy.tetrud at gmail.com> wrote:
> you can assign some minimal difficulty,
?
I was assuming that would be part of the plan.
?
>?Signing sounds more like Proof of Stake
?
Associating signing with proof of stake and thereby concluding that signing is something to avoid sounds like superstitious thinking. A signing scheme with proof of work would clearly not be proof of stake. I would suggest not dismissing a design out of hand like that. The benefit of that over merge mining would be no extra on chain foot print. What do you think the downsides might be?
?
> if you use just hashes, then they could be random.
?
You're right. Nodes would of course need to validate the Bitcoin block headers being included, so i concede hashing them doesn't gain you anything.
On Thu, Dec 16, 2021, 22:37 <vjudeu at gazeta.pl> wrote:
> I was thinking that this would be a separate blockchain with separate headers that progress linearly like a normal blockchain.
Exactly, that's what I called "superblocks", where you have a separate chain, just to keep block headers instead of transactions.
> A block creator would collect together as many blocks that haven't been collected yet into the next superblock (and maybe receive a reward proportional to how many / how much weight they include).
You cannot "catch them all". If you do, you can end up with a lot of block headers, where each of them has difficulty equal to one. You need some limit, you can limit amount of blocks, you can assign some minimal difficulty, it does not matter that much, but some limit is needed, also because mining on top of the latest superblock should be more profitable than replacing someone else's reward in the previous superblock by your own reward and getting a bigger share in the previous superblock.
> This could be done using merge mining, or it could be done using a signing scheme (eg where the block creator signs to say "I created this superblock" and have mechanisms to punish those who sign multiple superblocks at the same height.
I would pick merge mining, because it is more compatible with existing mining scheme. Signing sounds more like Proof of Stake and I am trying to avoid that solution. Also, there is no need to sign anything, because you are solo mining where you have your own coinbase transaction or you are mining in a pool, where you have some shared address, and then you cannot produce any incompatible superblock, because the protocol can tell you, which address you should use (and if it is N-of-N taproot multisig and you have some closing transaction, then you can safely mine it).
> Really, you could even just use hashes of the block headers.
Replacing transactions with block headers will do the same trick. Each transaction is first hashed with double SHA-256, in exactly the same way as block headers are. If you replace transactions with block headers, you would get a superblock header, then varint saying how many block headers are there, and then you can place all block headers. During superblock merkle tree construction, you will hash all block headers (so you will get block hashes as leaves), and then you will combine block hashes in the same way as transaction hashes are combined.
>From the Script point of view, you can always use "OP_SIZE 80 OP_EQUALVERIFY OP_HASH256 <hash> OP_EQUAL". Then, you can just change the size, just to show which object is hashed. Value 80 will work for block headers, small values below 520 will work for small transactions, value 64 will work for any merkle tree proof, no matter if it is for superblock or normal block. Also, by using block headers instead of hashes, you can prove that at least a proper amount of work was done to produce it, because if you use just hashes, then they could be random.
On 2021-12-16 17:57:23 user Billy Tetrud <billy.tetrud at gmail.com> wrote:
@Jeremy
>? ?for top-level pool participants there is never any central custody.
?
I definitely see that. That was actually what I meant when I said the goals aren't the same as benefits. While your idea definitely satisfies all your goals in a modular way, the fact that it relies on pools means that unless the pools can also satisfy the goals, the total system also doesn't satisfy the goals (even tho the piece of that system you designed does).?
?
>?Thus it doesn't "hurt" anyone except for the miners who are taking the not fully locked in funds risk
?
True, it only potentially hurts whoever the channel partner is accepting the unspendable coins. And no one can really stop anyone from taking that risk if they really want to. But in that case, its not exactly a fully functional channel, since recourse mechanisms couldn't be performed. Wouldn't that open such a channel up to a pretty bad theft possibility?
?
@Bob
>?Increased payout regularity does not lower the viable size of mining pools, because smaller mining pools using this mechanism still have higher variance.
?
Yes, smaller mining pools will always have higher variance. However, lower variance has diminishing benefits. Below a certain amount of variance, less variance isn't very valuable. So increased payout regularity does indeed lower the viable size of mining pools because a given low-enough level of variance can be achieved with less pool hashpower.
?
> The on-chain footprint is *higher* due to the increased payout regularity.
?
That's a reasonable point. However, I think there is a difference here between the regularity of rewards vs payouts. Rewards for each miner can be more regular without necessarily increasing the number of on-chain payouts. In fact, theoretically, an individual miner could let their rewards accumulate in a pool over many rewards and only redeem when they need the coins for something. The incentive is there for each miner to be judicious on how much onchain space they take up.
?
@vjudeu
?
> how many block headers should be stored per one "superblock"?
?
I was thinking that this would be a separate blockchain with separate headers that progress linearly like a normal blockchain. A block creator would collect together as many blocks that haven't been collected yet into the next superblock (and maybe receive a reward proportional to how many / how much weight they include). This could be done using merge mining, or it could be done using a signing scheme (eg where the block creator signs to say "I created this superblock" and have mechanisms to punish those who sign multiple superblocks at the same height. For merge mining, I could even imagine the data necessary to validate that it has been merge mined could be put into a taproot script branch (creating an invalid script, but a valid hash of the superblock).?
?
> we can collect all headers with the same previous block hash, and distribute block reward between all coinbase transactions in those headers
?
Exactly.
?
> we would just have block headers instead of transactions
?
Yeah, I think that would be the way to go. Really, you could even just use hashes of the block headers. But the size doesn't matter much because it would be both a small blockchain and an ephemeral one (which can be fully discarded after all parties have been paid out, or at least their payout has been committed to on the bitcoin blockchain).?
On Thu, Dec 16, 2021 at 1:35 AM <vjudeu at gazeta.pl> wrote:
> The missing piece here would be an ordering of weak blocks to make the window possible. Or at least a way to determine what blocks should definitely be part of a particular block's pay out. I could see this being done by a separate ephemeral blockchain (which starts fresh after each Bitcoin block) that keeps track of which weak blocks have been submitted, potentially using the pow already in each block to secure it. Granted that piece is a bit half baked, but it seems quite solvable. Wdyt?
?
I thought about something like that, but there is one problem: how many block headers should be stored per one "superblock"? Currently, we have single block header, where the whole coinbase transaction is taken by some mining pool or solo miner. But instead, each miner could submit its own block header. Then, we can collect all headers with the same previous block hash, and distribute block reward between all coinbase transactions in those headers. One "superblock" then would be created in a similar way as existing blocks, we would just have block headers instead of transactions. If most transactions inside those blocks will be the same, then each block could be expressed just as a set of transaction hashes, only coinbase transactions or custom, non-broadcasted transactions included by miners will be revealed, everything else will be known.
> One thing that jumped out at me as not safe is throwing block rewards into a channel and being able to spend them immediately. There's a reason block rewards aren't spendable for a while, and channels don't solve that problem, do they? Why not simply reduce the on chain wait time for spending block rewards at that point? Seems like the consequences would be the same.
All coinbase rewards are unspendable for 100 blocks, it is enforced by consensus. It does not matter if there are outputs owned directly by miners, or if there is one huge N-of-N taproot multisig for the whole pool, where every miner signed the closing transaction. The only option to take coins faster I can see is swapping the coins by some LN transaction. But then, the other party can check if some deposit to the LN channel is a part of the coinbase transaction or not, and then decide if it is acceptable to do the swap.
On 2021-12-15 19:00:44 user Billy Tetrud via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
Looks like an interesting proposal, but it doesn't seem to quite match the goals you mentioned. As you do mention, this mining pool coordination doesn't get rid of the need for mining pools in the first place. So it doesn't satisfy item 1 on your goal list afaict.? ?
The primary benefits over what we have today that I can see are:
1. increased payout regularity, which lowers the viable size of mining pools, and
2. Lower on chain footprint through combining pay outs from multiple pools.
?
Am I missing some?
?
These are interesting benefits, but it would be nice if your post was clearer on that, since the goals list is not the same as the list of potential benefits of this kind of design.
?
As far as enabling solo mining, what if this concept were used off chain? Have a public network of solo miners who publish "weak blocks" to that network, and the next 100 (or 1000 etc) nice miners pay you out as long as you're also being nice by following the protocol? All the nice optimizations you mentioned about eg combined taproot payouts would apply i think. The only goals this wouldn't satisfy are 3 and 5 since an extra network is needed, but to be fair, your proposal requires pools which all need their own extra network anyways.?
?
The missing piece here would be an ordering of weak blocks to make the window possible. Or at least a way to determine what blocks should definitely be part of a particular block's pay out. I could see this being done by a separate ephemeral blockchain (which starts fresh after each Bitcoin block) that keeps track of which weak blocks have been submitted, potentially using the pow already in each block to secure it. Granted that piece is a bit half baked, but it seems quite solvable. Wdyt?
?
One thing that jumped out at me as not safe is throwing block rewards into a channel and being able to spend them immediately. There's a reason block rewards aren't spendable for a while, and channels don't solve that problem, do they? Why not simply reduce the on chain wait time for spending block rewards at that point? Seems like the consequences would be the same.
On Tue, Dec 14, 2021, 16:12 Bob McElrath via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
You are hand waving. Attempting to redefine terms to justify your argument is
intellectually dishonest. Bitcoin pools have *always* been about variance
reduction. Your window function fundamentally CANNOT be used to hedge hashrate.
Various suggestions below introduce dangerous new games that might be played by
miners.
The fact is that the half-baked design you posted is less than useless, and
doesn't do anything that anyone wants.
You are trying to justify CTV by making it be all things to all people. "When
all you have is a hammer, every problem looks like a nail".? Instead I humbly
suggest that you pick ONE problem for which CTV is demonstrably the right and
best solution, instead of snowing us with a ton of half-baked things that
*could* be done, and often don't even require CTV, and some (like this one)
fundamentally don't work. I do like some of your ideas, but if you had to pick
just one "use case", which would it be?
Jeremy [jlrubin at mit.edu] wrote:
> Bitcoin didn't invent the concept of pooling: https://en.wikipedia.org/wiki/
> Pooling_(resource_management). This is a Bitcoin Mining Pool, although it may
> not be your favorite kind, which is fixated on specific properties of computing
> contributions before finding a block. Pooling is just a general technique for
> aggregating resources to accomplish something. If you have another name like
> pooling that is in common use for this type of activity I would be more than
> happy to adopt it.
>
> This sort of pool can hedge not only against fee rates but also against
> increases in hashrate since your historical rate 'carries' into the future as a
> function of the window. Further, windows and reward functions can be defined in
> a myriad of ways that could, e.g., pay less to blocks found in more rapid
> succession, contributing to the smoothing functionality.
>
> With respect to sub-block pooling, as described in the article, this sort of
> design also helps with micro-pools being able to split resources
> non-custodially in every block as a part of the higher order DCFMP. The point
> is not, as noted, to enable solo mining an S9, but to decrease the size of the
> minimum viable pool. It's also possible to add, without much validation or
> data, some 'uncle block' type mechanism in an incentive compatible way (e.g.,
> add 10 pow-heavy headers on the last block for cost 48 bytes header + 32 bytes
> payout key) such that there's an incentive to include the heaviest ones you've
> seen, not just your own, that are worth further study and consideration
> (particularly because it's non-consensus, only for opt-in participation in the
> pool).
>
> With respect to space usage, it seems you wholly reject the viability of a
> payment pool mechanism to cut-through chain space. Is this a critique that
> holds for all Payment Pools, or just in the context of mining? Is there a
> particular reason why you think it infeasible that "strongly online"
> counterparties would be able to coordinate more efficiently? Is it preferable
> for miners, the nexus of decentralization for Bitcoin, to prefer to use
> custodial services for pooling (which may require KYC/AM) over bearing a cost
> of some extra potential chainload?
>
> Lastly, with respect to complexity, the proposal is actually incredibly simple
> when you take it in a broader context. Non Interactive Channels and Payment
> Pools are useful?by themselves, so are the operations to merge them and swap
> balance across them. Therefore most of the complexity in this proposal is
> relying on tools we'll likely see in everyday use in any case, DCFMP or no.
>
> Jeremy
> !DSPAM:61b8f2f5321461582627336!
--
Cheers, Bob McElrath
"For every complex problem, there is a solution that is simple, neat, and wrong."
? ? -- H. L. Mencken
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211223/2b2e0e8c/attachment-0001.html>

From prayank at tutanota.de  Thu Dec 23 14:09:13 2021
From: prayank at tutanota.de (Prayank)
Date: Thu, 23 Dec 2021 15:09:13 +0100 (CET)
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] A Defense of Having Fun
 (and maybe staying poor)
Message-ID: <MrbMgLt--3-2@tutanota.de>

Hi Jeremy,

> Eugene just dropped a project he?s been working on, and it?s really freakin? cool. He basically implemented a human v. chess engine in Solidity that mints beautiful interactive NFTs of representations of the contract?s internal states.

Not sure why NFT is involved here but experiment looks interesting. Maybe Eugene should try few things on Lightning as well: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-December/019619.html

> Why isn?t Eugene working on Bitcoin?

There can be many reasons however Eugene can answer this question better.

> Working on Bitcoin is can be fun. But mostly it?s not. My post yesterday? Theone describing new techniques to make Bitcoin more decentralized? I had a lotof fun writing it. And then someone claimed that my work is ?very dangerous? toBitcoin.

I don't support what someone said about you in IRC however people do say crazy things online which has nothing to do with Bitcoin. Bitcoin can be different things for everyone.

> Bitcoin development has a bit of a burnout problem, with multiple contributors stepping down their engagement recently. A likely cause is the struggle it takes to ship even the smallest features, not to mention the monumental effort it takes to ship a single large project.

I am not sure if this is the reason for people who will be less active in Bitcoin development now. And its a part of life, people will come and go. Show must go on. There will always be another developer who is more passionate and got more ideas to contribute. Funding in open source projects is an issue which exists outside Bitcoin as well and it is being addresses by several individuals and organizations.
> It?s hard to tell people, especially younger folk just entering the space, to work on Bitcoin full-time. What I say is as follows:

It is hard but not impossible. I tried recently in a meetup in India. Ethereum is not the answer to all the problems in the worlds. If someone has issues with Bitcoin development, they can be solved.

> I don?t think I?m going to convince you here to care about NFTs. But I am ?hopefully ? going to convince you to care about NFTs the phenomenon.

To be honest, its the trend right now and people care about it to get rich. There is nothing new or innovative about it. 

> For example, scaling challenge in Ethereum have led to the development of Zero Knowledge Roll-Ups, privacy issues things like Tornado Cash, and more.

Rollups are memes and there are several articles, threads, research etc. to read about this. Most of their implementations are not even decentralized. Tornado Cash is not good privacy and even became worse after their governance token.


-- 
Prayank

A3B1 E430 2298 178F
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211223/51e3702f/attachment.html>

From jlrubin at mit.edu  Thu Dec 23 19:00:48 2021
From: jlrubin at mit.edu (Jeremy)
Date: Thu, 23 Dec 2021 11:00:48 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] History and Future of Sapio
Message-ID: <CAD5xwhgr6q=-tzTH3kW+jf5MaTy4T=9NEQumrfw6hcMQi=+PUw@mail.gmail.com>

Hi devs,

This post details a little on the origins of Sapio as well as features that
are in development this year (other than bugfixes).

https://rubin.io/bitcoin/2021/12/23/advent-26/

cheers,

Jeremy

--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211223/0350e0c4/attachment.html>

From jlrubin at mit.edu  Thu Dec 23 19:05:22 2021
From: jlrubin at mit.edu (Jeremy)
Date: Thu, 23 Dec 2021 11:05:22 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Decentralized
 Coordination Free Mining Pools
In-Reply-To: <151161119-8858833d76ec6beed19cf87cc542dc62@pmq3v.m5r2.onet>
References: <CAGpPWDbGTET27Hq8kKrQQoOavtOUEGzYkohVurSqZJ5r+o4gpQ@mail.gmail.com>
 <151161119-8858833d76ec6beed19cf87cc542dc62@pmq3v.m5r2.onet>
Message-ID: <CAD5xwhhNgVp1wb3+CEAnGmCoYHFKQPPqRg-WvkCuaD+8WAAG_Q@mail.gmail.com>

> If you introduce signing into mining, then you will have cases, where
> someone is powerful enough to produce blocks, but cannot, because signing
> is needed. Then, your consensus is no longer "the heaviest chain", but "the
> heaviest signed chain". That means, your computing power is no longer
> enough by itself (as today), because to make a block, you also need some
> kind of "permission to mine", because first you sign things (like in
> signet) and then you mine them. That kind of being "reliably unreliable"
> may be ok for testing, but not for the main network.


this is a really great point worth underscoring. this is the 'key
ingredient' for DCFMP, which is that there is no signing or other network
system that is 'in the way' of normal bitcoin mining, just an opt-in set of
rules for sharing the bounties of your block in exchange for future shares.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211223/9dd4e268/attachment.html>

From otech47 at gmail.com  Thu Dec 23 18:10:48 2021
From: otech47 at gmail.com (Oscar Lafarga)
Date: Thu, 23 Dec 2021 13:10:48 -0500
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Decentralized
 Autonomous Organizations (DAOs) Will Save Bitcoin
In-Reply-To: <CAD5xwhh7J5H_L5UDG3NtuULgnuZP_PY8qqq1QajqXSiXsKgSTQ@mail.gmail.com>
References: <CAD5xwhh7J5H_L5UDG3NtuULgnuZP_PY8qqq1QajqXSiXsKgSTQ@mail.gmail.com>
Message-ID: <CAO7Y_eVnxWLCcXmgq7NSS82Z8B2uuZM-N_sXZA5Yvw8sZ2KBrg@mail.gmail.com>

>
> None of this particularly requires CTV, but it does require the type of
> composable and flexible software that I aspire to deliver with Sapio.


 Does this imply that there is some kind of Sapio client to be run
alongside a Bitcoin full node similar to how a Lightning node would
operate? If so, are the computation, bandwidth, and liveness requirements
for someone running Sapio contracts more or less comparable to Lightning?

The implementation approach seems pretty interesting overall and reminds me
of concepts like bitcoin bug bounties (https://bitcoinacks.com/ for
example) but perhaps with the potential for more sophisticated
functionality.

Will try to take a closer look at the https://github.com/sapio-lang/sapio
if that seems like a recommended starting point.

Thanks!

On Wed, Dec 22, 2021 at 10:47 PM Jeremy via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Hi Devs,
>
> Enjoy! https://rubin.io/bitcoin/2021/12/22/advent-25/
>
> I'm really excited about opportunities for capital formation to happen
> natively in Bitcoin. This is actually a really big deal and something (I
> think) to pay close attention to. This is basically like running a little
> company with shareholders inside of Bitcoin, which to me really helps us
> inhabit the "be your own bank" part of Bitcoin. None of this particularly
> requires CTV, but it does require the type of composable and flexible
> software that I aspire to deliver with Sapio.
>
> business matter:
>
> There are two more posts, and they will both be focused on getting this
> stuff out into the wild more. If you particularly have thoughts on BIP-119
> activation I would love to hear them publicly, or at your preference,
> privately.
>
> If you like or dislike BIP-119 and wish to "soft-signal" yes or no
> publicly, you may do so on https://utxos.org/signals by editing the
> appropriate file(s) and making a PR. Alternatively, comment somewhere
> publicly I can link to, send it to me, and I will make the edits.
>
> edit links:
> - for individuals/devs:
> https://github.com/JeremyRubin/utxos.org/edit/master/data/devs.yaml
> - organizations:
> https://github.com/JeremyRubin/utxos.org/edit/master/data/bizs.yaml
> - miners/pools:
> https://github.com/JeremyRubin/utxos.org/edit/master/data/hashratesnapshot.json
>
> Best,
>
> Jeremy
>
> --
> @JeremyRubin <https://twitter.com/JeremyRubin>
> <https://twitter.com/JeremyRubin>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>


-- 
Oscar Lafarga
https://www.setlife.network
<https://www.setdev.io/>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211223/1e853899/attachment-0001.html>

From jlrubin at mit.edu  Thu Dec 23 19:30:50 2021
From: jlrubin at mit.edu (Jeremy)
Date: Thu, 23 Dec 2021 11:30:50 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Decentralized
 Autonomous Organizations (DAOs) Will Save Bitcoin
In-Reply-To: <CAO7Y_eVnxWLCcXmgq7NSS82Z8B2uuZM-N_sXZA5Yvw8sZ2KBrg@mail.gmail.com>
References: <CAD5xwhh7J5H_L5UDG3NtuULgnuZP_PY8qqq1QajqXSiXsKgSTQ@mail.gmail.com>
 <CAO7Y_eVnxWLCcXmgq7NSS82Z8B2uuZM-N_sXZA5Yvw8sZ2KBrg@mail.gmail.com>
Message-ID: <CAD5xwhjnEB2aMeOCeNg_B04ZDqzWmViDiMTP6UPvM1zi+pVebw@mail.gmail.com>

Oscar,

Sapio is essentially a 'Compiler toolchain' you run it once and then send
money to the contract. This is like Solidity in Ethereum.

Sapio Studio is a GUI for interacting with the outputs of a Sapio contract.
This is like Metamask/web3.js in Ethereum.

It's really not comparable to Lightning.

recommend starting with learn.sapio-lang.org :)

Cheers,

Jeremy
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211223/d74044ef/attachment.html>

From jlrubin at mit.edu  Fri Dec 24 17:17:16 2021
From: jlrubin at mit.edu (Jeremy)
Date: Fri, 24 Dec 2021 09:17:16 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Derivatives and Options
In-Reply-To: <MrhJf_p--3-2@tutanota.de>
References: <MrhJf_p--3-2@tutanota.de>
Message-ID: <CAD5xwhja5+vAjsMpDoJRGpoJXwTVde+QmYtTofUgWO_+=Y1PPw@mail.gmail.com>

On Fri, Dec 24, 2021, 8:42 AM Prayank <prayank at tutanota.de> wrote:

> Hi Jeremy,
>
> > Wheres the info come from? Well, multiple places. We could get it from a
> third party (maybe using an attestation chain of some sort?), or there are
> certain ways it could be self-referential (like for powswap
> <https://powswap.com>).
>
> > Now let?s define a threshold oracle ? we wouldn?t want to trust just one
> lousy oracle, so let?s trust M out of N of them!
>
> Similar approach is used in discreet log contracts for multi oracles.
> There is even a project for P2P derivatives but it was not used for any
> real trades on mainnet or further developed. What difference would OP_CTV
> make in this project if its implemented in Bitcoin?
>
> https://github.com/p2pderivatives/p2pderivatives-client
>
> https://github.com/p2pderivatives/p2pderivatives-server
>
> https://github.com/p2pderivatives/p2pderivatives-oracle
>

Discussed a bit here
https://twitter.com/JeremyRubin/status/1473175356366458883?t=7U4vI4CYIM82vNc8T8n6_g&s=19


A core benefit is unilateral opens. I.e. you can pay someone into a
derivative without them being online.


For example, you want to receive your payment in a Bitcoin backed Magnesium
risk reversal in exchange for some phys magnesium. I can create the
contract with your signing keys offline.

>
>
> > Does this NEED CTV?
> No, not in particular. Most of this stuff could be done with online signer
> server federation between you and counterparty. CTV makes some stuff nicer
> though, and opens up new possibilities for opening these contracts
> unilaterally.
>
> Nicer? How would unilateral derivatives work because my understanding was
> that you always need a peer to take the other side of the trade. I wish we
> could discuss this topic in a trading community with some Bitcoiners that
> even had some programming knowledge.
>
> Derivatives are interesting and less explored or used in Bitcoin projects.
> They could be useful in solving lot of problems.
>
>
I have a decent understanding of a bit of the trading world and can answer
most questions you have, or point you to someone else who would.


The way a unilateral option would work is that I can create a payment to
you paying you into an Option expiring next week that gives you the right
to purchase from me a magnesium risk reversal contract that settles next
month.



An example where this type of pattern must be used is in conjunction with
DCFMP and PowSwap where miners could commit to, instead of just keys,
'trade specs' and an Automatic market maker inside the DCFMP could attempt
to match that miner to a counterparty who wants the opposite hashrate
hedge. The need to exchange signatures would make this unviable otherwise.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211224/c594082e/attachment.html>

From prayank at tutanota.de  Fri Dec 24 16:42:42 2021
From: prayank at tutanota.de (Prayank)
Date: Fri, 24 Dec 2021 17:42:42 +0100 (CET)
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Derivatives and Options
Message-ID: <MrhJf_p--3-2@tutanota.de>

Hi Jeremy,

> Wheres the info come from? Well, multiple places. We could get it from a third party (maybe using anattestation chain of some sort?), or there are certain ways it could beself-referential (like for powswap <https://powswap.com>).

> Now let?s define a threshold oracle ? we wouldn?t want to trust just onelousy oracle, so let?s trust M out of N of them!

Similar approach is used in discreet log contracts for multi oracles. There is even a project for P2P derivatives but it was not used for any real trades on mainnet or further developed. What difference would OP_CTV make in this project if its implemented in Bitcoin?
https://github.com/p2pderivatives/p2pderivatives-client

https://github.com/p2pderivatives/p2pderivatives-server

https://github.com/p2pderivatives/p2pderivatives-oracle

> Does this NEED CTV?
No, not in particular. Most of this stuff could be done with online signer server federation between you and counterparty. CTV makes some stuff nicer though, and opens up new possibilities for opening these contracts unilaterally.

Nicer? How would unilateral derivatives work because my understanding was that you always need a peer to take the other side of the trade. I wish we could discuss this topic in a trading community with some Bitcoiners that even had some programming knowledge.

Derivatives are interesting and less explored or used in Bitcoin projects. They could be useful in solving lot of problems.


-- 
Prayank

A3B1 E430 2298 178F
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211224/97c13df5/attachment.html>

From jlrubin at mit.edu  Fri Dec 24 21:49:19 2021
From: jlrubin at mit.edu (Jeremy)
Date: Fri, 24 Dec 2021 13:49:19 -0800
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] RoadMap or Load o' Crap?
Message-ID: <CAD5xwhhsEvThk1kGxXd+RhpQgFbkwAUCgDze0AL4rOCD7Qf=CQ@mail.gmail.com>

Devs,

For the final post of the advent calendar, a discussion around feasible
schedules for deployment of upgrades to Bitcoin and the relevant window of
opportunity for BIP-119 in 2022.

Hopefully this serves as a good launch point for discussion.

https://rubin.io/bitcoin/2021/12/24/advent-27/

Merry Christmas to those who celebrate, thanks for coming along for the
advent calendar ride!

Jeremy

--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211224/d9d22d22/attachment.html>

From email at yancy.lol  Sun Dec 26 20:49:30 2021
From: email at yancy.lol (email at yancy.lol)
Date: Sun, 26 Dec 2021 21:49:30 +0100
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Derivatives and Options
In-Reply-To: <MrhJf_p--3-2@tutanota.de>
References: <MrhJf_p--3-2@tutanota.de>
Message-ID: <86b6f138a9dbd2cd1dc4c9166199b319@yancy.lol>

Prayank,

I believe the p2pderivatives DLC application is still under active 
development here (single oracle):
https://github.com/p2pderivatives/rust-dlc

I was once involved in the project in a galaxy far far away but haven't 
kept up with the project.  Also, I'm a few days behind in the Bitcoin 
Advent Calendar :)

Cheers,
-Yancy


On 2021-12-24 17:42, Prayank via bitcoin-dev wrote:
> Hi Jeremy,
> 
>> Wheres the info come from? Well, multiple places. We could get it
> from a third party (maybe using an attestation chain of some sort?),
> or there are certain ways it could be self-referential (like for
> powswap [1]).
> 
>> Now let?s define a threshold oracle ? we wouldn?t want to
> trust just one lousy oracle, so let?s trust M out of N of them!
> 
> Similar approach is used in discreet log contracts for multi oracles.
> There is even a project for P2P derivatives but it was not used for
> any real trades on mainnet or further developed. What difference would
> OP_CTV make in this project if its implemented in Bitcoin?
> 
> https://github.com/p2pderivatives/p2pderivatives-client
> 
> https://github.com/p2pderivatives/p2pderivatives-server
> 
> https://github.com/p2pderivatives/p2pderivatives-oracle
> 
>> Does this NEED CTV?
> 
> No, not in particular. Most of this stuff could be done with online
> signer server federation between you and counterparty. CTV makes some
> stuff nicer though, and opens up new possibilities for opening these
> contracts unilaterally.
> 
> Nicer? How would unilateral derivatives work because my understanding
> was that you always need a peer to take the other side of the trade. I
> wish we could discuss this topic in a trading community with some
> Bitcoiners that even had some programming knowledge.
> 
> Derivatives are interesting and less explored or used in Bitcoin
> projects. They could be useful in solving lot of problems.
> 
> --
> 
> Prayank
> 
> A3B1 E430 2298 178F
> 
> 
> Links:
> ------
> [1] https://powswap.com
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From thibaut at cryptogarage.co.jp  Mon Dec 27 12:05:36 2021
From: thibaut at cryptogarage.co.jp (Thibaut Le Guilly)
Date: Mon, 27 Dec 2021 21:05:36 +0900
Subject: [bitcoin-dev] [Bitcoin Advent Calendar] Derivatives and Options
In-Reply-To: <86b6f138a9dbd2cd1dc4c9166199b319@yancy.lol>
References: <MrhJf_p--3-2@tutanota.de>
 <86b6f138a9dbd2cd1dc4c9166199b319@yancy.lol>
Message-ID: <CABPZDUxkjSRzCSh1ueq_ONAyc010QsxAshei4gQNpHO10Y31Lg@mail.gmail.com>

Hi all,

Did someone say rust-dlc? Just kidding, but wanted to mention that indeed
it's under active development, supports multi oracle contracts and many
other cool things (pretty much everything you can find in the dlc specs)!

Otherwise nice article Jeremy. Maybe you should drop by our monthly DLC
spec meeting one of these days. I'm sure everybody would be happy to hear
how we could improve the Bitcoin derivatives ecosystem with CTV and what
infrastructures or code could be reused from DLCs.

Cheers,

Thibaut

On Mon, Dec 27, 2021 at 7:39 AM yancy via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Prayank,
>
> I believe the p2pderivatives DLC application is still under active
> development here (single oracle):
> https://github.com/p2pderivatives/rust-dlc
>
> I was once involved in the project in a galaxy far far away but haven't
> kept up with the project.  Also, I'm a few days behind in the Bitcoin
> Advent Calendar :)
>
> Cheers,
> -Yancy
>
>
> On 2021-12-24 17:42, Prayank via bitcoin-dev wrote:
> > Hi Jeremy,
> >
> >> Wheres the info come from? Well, multiple places. We could get it
> > from a third party (maybe using an attestation chain of some sort?),
> > or there are certain ways it could be self-referential (like for
> > powswap [1]).
> >
> >> Now let?s define a threshold oracle ? we wouldn?t want to
> > trust just one lousy oracle, so let?s trust M out of N of them!
> >
> > Similar approach is used in discreet log contracts for multi oracles.
> > There is even a project for P2P derivatives but it was not used for
> > any real trades on mainnet or further developed. What difference would
> > OP_CTV make in this project if its implemented in Bitcoin?
> >
> > https://github.com/p2pderivatives/p2pderivatives-client
> >
> > https://github.com/p2pderivatives/p2pderivatives-server
> >
> > https://github.com/p2pderivatives/p2pderivatives-oracle
> >
> >> Does this NEED CTV?
> >
> > No, not in particular. Most of this stuff could be done with online
> > signer server federation between you and counterparty. CTV makes some
> > stuff nicer though, and opens up new possibilities for opening these
> > contracts unilaterally.
> >
> > Nicer? How would unilateral derivatives work because my understanding
> > was that you always need a peer to take the other side of the trade. I
> > wish we could discuss this topic in a trading community with some
> > Bitcoiners that even had some programming knowledge.
> >
> > Derivatives are interesting and less explored or used in Bitcoin
> > projects. They could be useful in solving lot of problems.
> >
> > --
> >
> > Prayank
> >
> > A3B1 E430 2298 178F
> >
> >
> > Links:
> > ------
> > [1] https://powswap.com
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev at lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211227/a1d82a30/attachment.html>

From jlrubin at mit.edu  Thu Dec 30 20:28:48 2021
From: jlrubin at mit.edu (Jeremy)
Date: Thu, 30 Dec 2021 12:28:48 -0800
Subject: [bitcoin-dev] BIP-119 Deployment and Review Workshops
Message-ID: <CAD5xwhjdpmOSdsGZALBq5UZKdG+cu7+-e+4UB8+NpMenrxcR8Q@mail.gmail.com>

Dear Bitcoin Developers,

I've selected a time (every 2 weeks on Tuesday 12:00 PM PT starting January
11th in Liber ##ctv-bip-review) to host a recurring meeting to discuss
BIP-119 review and deployment topics.

Before the meeting, please send me any topics you would like to cover & I
will circulate a preliminary agenda incorporating feedback in advance of
the meeting.

The rough plan will be to coordinate review, testing, and iron out any
wrinkles around release timelines and procedures based on my request for
comment here https://rubin.io/bitcoin/2021/12/24/advent-27/. Meetings will
be moderated tightly to remain on topic, but all relevant topics will be
welcome (similar to how I moderated the meetings I hosted for Taproot).

Participation in these meetings does not signify any sort of endorsement of
BIP-119's inclusion, nor a specific roadmap, but rather to carry on the
'gradient descent process' of consensus in an open, interactive, and
productive format. Logs will be available via gnusha.

Happy New Year,

Jeremy

p.s. If you plan to attend feel free to drop me a private note / reply
here, otherwise I may ping you to ask if you can attend.

--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211230/db7bf110/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: text/calendar
Size: 999 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211230/db7bf110/attachment.ics>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: BIP-119 Events_9k5gabum1lca4vs00rsk9bcrhk at group.calendar.google.com.ics
Type: application/ics
Size: 1041 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211230/db7bf110/attachment.bin>

From keagan.mcclelland at gmail.com  Fri Dec 31 03:10:48 2021
From: keagan.mcclelland at gmail.com (Keagan McClelland)
Date: Thu, 30 Dec 2021 20:10:48 -0700
Subject: [bitcoin-dev] On the regularity of soft forks
In-Reply-To: <1HjQQw-RXvEW5i73Hjx_QqDms44sQMnNWWl9oQ_SwIoYGpog6LzGK4M_omAEMXxgXIID37V7sdyG_AW8WkaNByppB2EJ7wlzOZgrDloMv2c=@protonmail.com>
References: <LmX3Gnfkf1T0Eb_wUXxPe8c0Tf2DNipfIqufkRS6oOPhttr4iZIOWtjUL_7QkcWEHr8eFvehHooaM140ZBKLwi98F5NwyQKSyEhAPZDK1YQ=@protonmail.com>
 <CAD5xwhj3JCxH1=5Tj+hgiSxLWchLgT584X0YutKVeuibnpwmtA@mail.gmail.com>
 <20211014235207.GB6451@erisian.com.au>
 <CAHvMVPQ8jtfdbLg8NJv7bNM3a_nhF_aUfD2gwSdxpfgXQomn3A@mail.gmail.com>
 <1HjQQw-RXvEW5i73Hjx_QqDms44sQMnNWWl9oQ_SwIoYGpog6LzGK4M_omAEMXxgXIID37V7sdyG_AW8WkaNByppB2EJ7wlzOZgrDloMv2c=@protonmail.com>
Message-ID: <CALeFGL3EGJ-kHs2C5qfTVcfZ0QNAHECOgMevoFEJJjTEcBLeEw@mail.gmail.com>

>  But whether or not it is a basic principle of general software
engineering kind of misses the point. Security critical software clearly
isn't engineered in the same way as a new social media app. Bugs are easily
reverted in a new social media app.On top of that we aren't just dealing
with security critical software. One of the most important objectives is to
keep all the nodes on the network in consensus. Introducing a consensus
change before we are comfortable there is community consensus for it is a
massive effective bug in itself. The network can split in multiple ways
e.g. part of the network disagrees on whether to activate the consensus
change, part of the network disagrees on how to resist that consensus
change, part of the network disagrees on how to activate that consensus
change etc

>  A consensus change is extremely hard to revert and probably requires a
hard fork, a level of central coordination we generally attempt to avoid
and a speed of deployment that we also attempt to avoid.

This seems to assert the idea that soft forks are all the same: they are
not. For instance a soft fork, lowering the block subsidy is completely
different than changing the semantics of an OP_NOP to have semantics that
may reject a subset of the witnesses that attest to the transactions
permissibility. As a result, reversion means two entirely different things
in these contexts. While a strict reversion of both soft forks is by
definition a hard fork, the requirement of reversion as a result of
undesired behavior is not the same. In the case of opcodes, there is almost
never a requirement to revert it. If you don't like the way the opcodes
behave, then you just don't use them. If you don't like the reduction of
the block subsidy, well that's a much bigger problem.

I make this point to elucidate the idea that we cannot treat SoftForks? as
a single monolithic idea. Perhaps we need to come up with better
terminology to be specific about what each fork actually is. The soft vs.
hard distinction is a critical one but it is not enough and treating soft
forks that are noninvasive such as OP_NOP tightenings. This has been
proposed before [1], and while I do not necessarily think the terms cited
are necessarily complete, they admit the low resolution of our current
terminology.

> Soft fork features can (and should) obviously be tested thoroughly on
testnet, signet, custom signets, sidechains etc on a standalone basis and a
bundled basis.

I vehemently disagree that any consensus changes should be bundled,
especially when it comes to activation parameters. When we start to bundle
things, we amplify the community resources needed to do review, not reduce
them. I suspect your opinion here is largely informed by your frustration
with the Taproot Activation procedure that you underwent earlier this year.
This is understandable. However, let me present the alternative case. If we
start to bundle features, the review of the features gets significantly
harder. As the Bitcoin project scales, the ability of any one developer to
understand the entire codebase declines. Bundling changes reduces the
number of people who are qualified to review a particular proposal, and
even worse, intimidates people who may be willing and able to review
logically distinct portions of the proposal, resulting in lower amounts of
review overall. This will likely have the opposite effect of what you seem
to desire. BIP8 and BIP9 give us the ability to have multiple independent
soft forks in flight at once. Choosing to bundle them instead makes little
sense when we do not have to. Bundling them will inevitably degenerate into
political horse trading and everyone will be worse off for it.

> part of the network disagrees on whether to activate the consensus
change, part of the network disagrees on how to resist that consensus
change, part of the network disagrees on how to activate that consensus
change etc

Disagreements, and by extension, forks are a part of Bitcoin. What is
important is that they are well defined and clean. This is the reason why
the mandatory signaling period exists in BIP8/9, so that clients that
intend to reject the soft fork change have a very easy means of doing so in
a clean break where consensus is clearly divergent. In accordance with
this, consensus changes should be sequenced so that people can decide which
sides of the forks they want to follow and that the economic reality can
reorganize around that. If choose to bundle them, you have one of two
outcomes: either consensus atomizes into a mist where people have different
ideas of which subsets of a soft fork bundle they want to adopt, or what
likely comes after is a reconvergence on the old client with none of the
soft fork rules in place. This will lead to significantly more confusion as
well given that with sufficient miner consensus some of the rules may stick
anyway even if the rest of the user base reconverges on the old client.

It is quite likely less damaging to consensus to have frequent but strictly
sequenced soft forks so that if one of the new rules is contentious the
break can happen cleanly. That said, if Core or any other client wishes to
cut a release of the software with the parameters bundled into a single
release, that is a significantly more palatable state of affairs, as you
can still pipeline signaling and activation. However, the protocol itself
adopting a tendency to activate unrelated proposals in bundles is a recipe
for disaster.


Respectfully,
Keagan


[1] https://www.truthcoin.info/blog/protocol-upgrade-terminology

On Sat, Oct 16, 2021 at 12:57 PM Michael Folkson via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> > Interesting discussion. Correct me if I'm wrong: but putting too many
> features together in one shot just can't make things harder to debug in
> production if something very unexpected happens. It's a basic principle
> of software engineering.
>
> Soft fork features can (and should) obviously be tested thoroughly on
> testnet, signet, custom signets, sidechains etc on a standalone basis and a
> bundled basis. But whether or not it is a basic principle of general
> software engineering kind of misses the point. Security critical software
> clearly isn't engineered in the same way as a new social media app. Bugs
> are easily reverted in a new social media app. A consensus change is
> extremely hard to revert and probably requires a hard fork, a level of
> central coordination we generally attempt to avoid and a speed of
> deployment that we also attempt to avoid. On top of that we aren't just
> dealing with security critical software. One of the most important
> objectives is to keep all the nodes on the network in consensus.
> Introducing a consensus change before we are comfortable there is community
> consensus for it is a massive effective bug in itself. The network can
> split in multiple ways e.g. part of the network disagrees on whether to
> activate the consensus change, part of the network disagrees on how to
> resist that consensus change, part of the network disagrees on how to
> activate that consensus change etc
>
> In addition, a social media app can experiment in production whether
> Feature A works, whether Feature B works or whether Feature A and B work
> best together. In Bitcoin if we activate consensus Feature A, later decide
> we want consensus Feature B but find out that by previously activating
> Feature A we can't have Feature B (it is now unsafe to activate it) or its
> design now has to be suboptimal because we have to ensure it can safely
> work in the presence of Feature A we have made a mistake by activating
> Feature A in the first place. Decentralized security critical consensus
> changes are an emerging field in itself and really can't be treated like
> any other software project. This will become universally understood I'm
> sure over time.
>
> --Michael Folkson
> Email: michaelfolkson at protonmail.com
> Keybase: michaelfolkson
> PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3
>
>
> ??????? Original Message ???????
> On Friday, October 15th, 2021 at 1:43 AM, Felipe Micaroni Lalli via
> bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
>
> Interesting discussion. Correct me if I'm wrong: but putting too many
> features together in one shot just can't make things harder to debug in
> production if something very unexpected happens. It's a basic principle
> of software engineering.
>
> Change. Deploy. Nothing bad happened? Change it a little more. Deployment.
> Or: Change, change, change. Deploy. Did something bad happen? What change
> caused the problem?
>
> On Thu, Oct 14, 2021 at 8:53 PM Anthony Towns via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> On Mon, Oct 11, 2021 at 12:12:58PM -0700, Jeremy via bitcoin-dev wrote:
>> > > ... in this post I will argue against frequent soft forks with a
>> single or
>> > minimal
>> > > set of features and instead argue for infrequent soft forks with
>> batches
>> > > of features.
>> > I think this type of development has been discussed in the past and has
>> been
>> > rejected.
>>
>> > AJ: - improvements: changes might not make everyone better off, but we
>> >    don't want changes to screw anyone over either -- pareto
>> >    improvements in economics, "first, do no harm", etc. (if we get this
>> >    right, there's no need to make compromises and bundle multiple
>> >    flawed proposals so that everyone's an equal mix of happy and
>> >    miserable)
>>
>> I don't think your conclusion above matches my opinion, for what it's
>> worth.
>>
>> If you've got two features, A and B, where the game theory is:
>>
>>  If A happens, I'm +100, You're -50
>>  If B happens, I'm -50, You're +100
>>
>> then even though A+B is +50, +50, then I do think the answer should
>> generally be "think harder and come up with better proposals" rather than
>> "implement A+B as a bundle that makes us both +50".
>>
>> _But_ if the two features are more like:
>>
>>   If C happens, I'm +100, You're +/- 0
>>   If D happens, I'm +/- 0, You're +100
>>
>> then I don't have a problem with bundling them together as a single
>> simultaneous activation of both C and D.
>>
>> Also, you can have situations where things are better together,
>> that is:
>>
>>   If E happens, we're both at +100
>>   If F happens, we're both at +50
>>   If E+F both happen, we're both at +9000
>>
>> In general, I think combining proposals when the combination is better
>> than the individual proposals were is obviously good; and combining
>> related proposals into a single activation can be good if it is easier
>> to think about the ideas as a set.
>>
>> It's only when you'd be rejecting the proposal on its own merits that
>> I think combining it with others is a bad idea in principle.
>>
>> For specific examples, we bundled schnorr, Taproot, MAST, OP_SUCCESSx
>> and CHECKSIGADD together because they do have synergies like that; we
>> didn't bundle ANYPREVOUT and graftroot despite the potential synergies
>> because those features needed substantially more study.
>>
>> The nulldummy soft-fork (bip 147) was deployed concurrently with
>> the segwit soft-fork (bip 141, 143), but I don't think there was any
>> particular synergy or need for those things to be combined, it just
>> reduced the overhead of two sets of activation signalling to one.
>>
>> Note that the implementation code for nulldummy had already been merged
>> and were applied as relay policy well before activation parameters were
>> defined (May 2014 via PR#3843 vs Sep 2016 for PR#8636) let alone becoming
>> an active soft fork.
>>
>> Cheers,
>> aj
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211230/6bc5d2f3/attachment-0001.html>

From herrytheeagle at gmail.com  Fri Dec 31 16:56:17 2021
From: herrytheeagle at gmail.com (Heritage Samuel Falodun)
Date: Fri, 31 Dec 2021 17:56:17 +0100
Subject: [bitcoin-dev] BIP-119 Deployment and Review Workshops
In-Reply-To: <CAD5xwhjdpmOSdsGZALBq5UZKdG+cu7+-e+4UB8+NpMenrxcR8Q@mail.gmail.com>
References: <CAD5xwhjdpmOSdsGZALBq5UZKdG+cu7+-e+4UB8+NpMenrxcR8Q@mail.gmail.com>
Message-ID: <CAKrV5kCJ=LEjSzMnpf_uCJ8oTfeo-4e-ugBBBJkXkSVd3DEvgw@mail.gmail.com>

Okay Jeremy, I plan to attend.

On Thu, Dec 30, 2021 at 9:29 PM Jeremy via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Dear Bitcoin Developers,
>
> I've selected a time (every 2 weeks on Tuesday 12:00 PM PT starting
> January 11th in Liber ##ctv-bip-review) to host a recurring meeting to
> discuss BIP-119 review and deployment topics.
>
> Before the meeting, please send me any topics you would like to cover & I
> will circulate a preliminary agenda incorporating feedback in advance of
> the meeting.
>
> The rough plan will be to coordinate review, testing, and iron out any
> wrinkles around release timelines and procedures based on my request for
> comment here https://rubin.io/bitcoin/2021/12/24/advent-27/. Meetings
> will be moderated tightly to remain on topic, but all relevant topics will
> be welcome (similar to how I moderated the meetings I hosted for Taproot).
>
> Participation in these meetings does not signify any sort of endorsement
> of BIP-119's inclusion, nor a specific roadmap, but rather to carry on the
> 'gradient descent process' of consensus in an open, interactive, and
> productive format. Logs will be available via gnusha.
>
> Happy New Year,
>
> Jeremy
>
> p.s. If you plan to attend feel free to drop me a private note / reply
> here, otherwise I may ping you to ask if you can attend.
>
> --
> @JeremyRubin <https://twitter.com/JeremyRubin>
> <https://twitter.com/JeremyRubin>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-- 
Heritage Samuel Falodun
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211231/b8b99b7d/attachment.html>

From bram at chia.net  Fri Dec 31 23:22:08 2021
From: bram at chia.net (Bram Cohen)
Date: Fri, 31 Dec 2021 15:22:08 -0800
Subject: [bitcoin-dev] Covenants and capabilities in the UTXO model
Message-ID: <CAHUJnBBFsS597ZRdAtwONMAz1r7gQbrXULzdNtEVxOPENx+tDg@mail.gmail.com>

There are a few different approaches to adding covenants and capabilities
to the UTXO model with varying tradeoffs. It turns out that it can be done
while making very few but not quite zero compromises to practices Bitcoin
has been following so far.

First, the good news: Full support for both capabilities and covenants can
be added without changing the UTXO model whatsoever by adding some more
programmatic capabilities to the language and doing some programmatic
tricks. Since scriptpubkeys/scriptsigs continue to run ephemerally at
validation time full turing completeness is much less dangerous than people
fear. The main thing missing from what's expressed in transactions
themselves is a coherent notion of a single parent of each output instead
of the all-inputs-lead-to-all-outputs approach of transactions currently.
It would also probably be a good idea to add in a bunch of special purpose
opcodes for making coherent statements about transactions since in Bitcoin
they're a very complex and hard to parse format.

Now for the controversial stuff. Once you start implementing complex
general purpose functionality it tends to get very expensive very fast and
is likely impractical unless there's a way to compress or at least
de-duplicate snippets of code which are repeated on chain. Currently
Bitcoin has a strong policy that deciding which transactions to let into a
block for maximum fee is a strictly linear optimization problem and while
it's possible to keep things mostly that way making it completely strict is
unlikely to workable. About as close as you can get is to make it so that
each block can reference code snippets in previous blocks for
deduplication, so at least the optimization is linear for each block by
itself.

Having covenants and capabilities at all is controversial in and of itself.
With covenants the main issue is whether they're opt-in or opt-out. For a
payment to someone to come with a rider where they could accept it and
think their system was working properly for a while until you exercised
some kind of retroactive veto on new action or even clawback would
obviously be unacceptable behavior. But for payments to come with covenants
but the recipient not even be able to parse them unless they're fully
buying into that behavior is much more reasonable.

The main issue which people have raised with capabilities is that if you
were to have colored coins whose value was substantially greater than the
chain they were tokenized on then that could potentially create a business
model for attacking the underlying chain. While this is a real concern
tokenized assets have been out for a while now and have never come close to
causing this to happen, so maybe people aren't so worried about it now.

Given all the above caveats it turns out one weird trick is all you need to
support general purpose capabilities: for a UTXO to have a capability its
scriptpubkey asserts that its parent must either be the originator of that
capability or also conform to the same parent-asserting format. More
complex functionality such as supporting on-chain verifiable colored coins
can also be done but it follows the same pattern: Capabilities are
implemented as backwards pointing covenants.

If you'd like to see a fleshed out implementation of these ideas (albeit in
a slightly different model) there's quite a bit of stuff on chialisp.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211231/dac00321/attachment.html>

