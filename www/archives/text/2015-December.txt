From lf-lists at mattcorallo.com  Tue Dec  1 05:28:42 2015
From: lf-lists at mattcorallo.com (Matt Corallo)
Date: Tue, 01 Dec 2015 05:28:42 +0000
Subject: [bitcoin-dev] [BIP Draft] Datastream compression of Blocks
	and	Transactions
In-Reply-To: <565CD7D8.3070102@gmail.com>
References: <565CD7D8.3070102@gmail.com>
Message-ID: <90EF4E6C-9A71-4A35-A938-EAFC1A24DD24@mattcorallo.com>

I'm really not a fan of this at all. To start with, adding a compression library that is directly accessible to the network on financial software is a really, really scary idea. If there were a massive improvement, I'd find it acceptable, but the improvement you've shown really isn't all that much. The numbers you recently posted show it improving the very beginning of IBD somewhat over high-latency connections, but if we're throughput-limited after the very beginning of IBD, we should fix that, not compress the blocks. Additionally, I'd be very surprised if this had any significant effect on the speed at which new blocks traverse the network (do you have any simulations or other thoughts on this?).

All that said, I'd love a proposal that allows clients to download compressed blocks via an external daemon, especially during IBD. This could help people with very restrictive data caps do IBD instead of being pushed to revert to SPV. Additionally, I think we need more chain sync protocols so that the current P2P protocol isn't consensus-critical anymore.

On November 30, 2015 4:12:24 PM MST, Peter Tschipper via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>@gmaxwell Bip Editor, and the Bitcoin Dev Community,
>
>After several weeks of experimenting and testing with various
>compression libraries I think there is enough evidence to show that
>compressing blocks and transactions is not only beneficial in reducing
>network bandwidth but is also provides a small performance boost when
>there is latency on the network.
>
>The following is a BIP Draft document for your review. 
>(The alignment of the columns in the tables doesn't come out looking
>right in this email but if you cut and paste into a text document they
>are just fine)
>
>
><pre>
>  BIP: ?
>  Title: Datastream compression of Blocks and Tx's
>  Author: Peter Tschipper <peter.tschipper at gmail.com>
>  Status: Draft
>  Type: Standards Track
>  Created: 2015-11-30
></pre>
>
>==Abstract==
>
>To compress blocks and transactions, and to concatenate them together
>when possible, before sending.
>
>==Motivation==
>
>Bandwidth is an issue for users that run nodes in regions where
>bandwidth is expensive and subject to caps, in addition network latency
>in some regions can also be quite high. By compressing data we can
>reduce daily bandwidth used in a significant way while at the same time
>speed up the transmission of data throughout the network. This should
>encourage users to keep their nodes running longer and allow for more
>peer connections with less need for bandwidth throttling and in
>addition, may also encourage users in areas of marginal internet
>connectivity to run nodes where in the past they would not have been
>able to.
>
>==Specification==
>
>Advertise compression using a service bit.  Both peers must have
>compression turned on in order for data to be compressed, sent, and
>decompressed.
>
>Blocks will be sent compressed.
>
>Transactions will be sent compressed with the exception of those less
>than 500 bytes.
>
>Blocks will be concatenated when possible.
>
>Transactions will be concatenated when possible or when a
>MSG_FILTERED_BLOCK is requested.
>
>Compression levels to be specified in "bitcoin.conf".
>
>Compression and decompression can be completely turned off.
>
>Although unlikely, if compression should fail then data will be sent
>uncompressed.
>
>The code for compressing and decompressing will be located in class
>CDataStream.
>
>Compression library LZO1x will be used.
>
>==Rationale==
>
>By using a service bit, compression and decompression can be turned
>on/off completely at both ends with a simple configuration setting. It
>is important to be able to easily turn off compression/decompression as
>a fall back mechanism.  Using a service bit also makes the code fully
>compatible with any node that does not currently support compression. A
>node that do not present the correct service bit will simply receive
>data in standard uncompressed format.
>
>All blocks will be compressed. Even small blocks have been found to
>benefit from compression.
> 
>Multiple block requests that are in queue will be concatenated together
>when possible to increase compressibility of smaller blocks.
>Concatenation will happen only if there are multiple block requests
>from
>the same remote peer.  For example, if peer1 is requesting two blocks
>and they are both in queue then those two blocks will be concatenated.
>However, if peer1 is requesting 1 block and peer2 also one block, and
>they are both in queue, then each peer is sent only its block and no
>concatenation will occur. Up to 16 blocks (the max blocks in flight)
>can
>be concatenated but not exceeding the MAX_PROTOCOL_MESSAGE_LENGTH.
>Concatenated blocks compress better and further reduce bandwidth.
>
>Transactions below 500 bytes do not compress well and will be sent
>uncompressed unless they can be concatenated (see Table 3).
>
>Multiple transaction requests that are in queue will be concatenated
>when possible.  This further reduces bandwidth needs and speeds the
>transfer of large requests for many transactions, such as with
>MSG_FILTERED_BLOCK requests, or when the system gets busy and is
>flooded
>with transactions.  Concatenation happens in the same way as for
>blocks,
>described above.
>
>By allowing for differing compression levels which can be specified in
>the bitcoin.conf file, a node operator can tailor their compression to
>a
>level suitable for their system.
>
>Although unlikely, if compression fails for any reason then blocks and
>transactions will be sent uncompressed.  Therefore, even with
>compression turned on, a node will be able to handle both compressed
>and
>uncompressed data from another peer.
>
>By Abstracting the compression/decompression code into class
>"CDataStream", compression can be easily applied to any datastream.
>
>The compression library LZO1x-1 does not compress to the extent that
>Zlib does but it is clearly the better performer (particularly as file
>sizes get larger), while at the same time providing very good
>compression (see Tables 1 and 2).  Furthermore, LZO1x-999 can provide
>and almost Zlib like compression for those who wish to have more
>compression, although at a cost.
>
>==Test Results==
>
>With the LZO library, current test results show up to a 20% compression
>using LZO1x-1 and up to 27% when using LZO1x-999.  In addition there is
>a marked performance improvement when there is latency on the network.
>From the test results, with a latency of 60ms there is an almost 30%
>improvement in performance when comparing LZO1x-1 compressed blocks
>with
>uncompressed blocks (see Table 5).
>
>The following table shows the percentage that blocks were compressed,
>using two different Zlib and LZO1x compression level settings.
>
>TABLE 1:
>range = data size range
>range           Zlib-1  Zlib-6  LZO1x-1 LZO1x-999
>-----------     ------  ------  ------- --------
>0-250           12.44   12.86   10.79   14.34
>250-500         19.33   12.97   10.34   11.11   
>600-700         16.72   n/a     12.91   17.25
>700-800         6.37    7.65    4.83    8.07
>900-1KB         6.54    6.95    5.64    7.9
>1KB-10KB        25.08   25.65   21.21   22.65
>10KB-100KB      19.77   21.57   4.37    19.02
>100KB-200KB     21.49   23.56   15.37   21.55
>200KB-300KB     23.66   24.18   16.91   22.76
>300KB-400KB     23.4    23.7    16.5    21.38
>400KB-500KB     24.6    24.85   17.56   22.43
>500KB-600KB     25.51   26.55   18.51   23.4
>600KB-700KB     27.25   28.41   19.91   25.46
>700KB-800KB     27.58   29.18   20.26   27.17
>800KB-900KB     27      29.11   20      27.4
>900KB-1MB       28.19   29.38   21.15   26.43
>1MB -2MB        27.41   29.46   21.33   27.73
>
>The following table shows the time in seconds that a block of data
>takes
>to compress using different compression levels.  One can clearly see
>that LZO1x-1 is the fastest and is not as affected when data sizes get
>larger.
>
>TABLE 2:
>range = data size range
>range           Zlib-1  Zlib-6  LZO1x-1 LZO1x-999
>-----------     ------  ------  ------- ---------
>0-250           0.001   0       0       0
>250-500         0       0       0       0.001
>500-1KB         0       0       0       0.001
>1KB-10KB        0.001   0.001   0       0.002
>10KB-100KB      0.004   0.006   0.001   0.017
>100KB-200KB     0.012   0.017   0.002   0.054
>200KB-300KB     0.018   0.024   0.003   0.087
>300KB-400KB     0.022   0.03    0.003   0.121
>400KB-500KB     0.027   0.037   0.004   0.151
>500KB-600KB     0.031   0.044   0.004   0.184
>600KB-700KB     0.035   0.051   0.006   0.211
>700KB-800KB     0.039   0.057   0.006   0.243
>800KB-900KB     0.045   0.064   0.006   0.27
>900KB-1MB       0.049   0.072   0.006   0.307
>
>TABLE 3:
>Compression of Transactions (without concatenation)
>range = block size range
>ubytes = average size of uncompressed transactions
>cbytes = average size of compressed transactions
>cmp% = the percentage amount that the transaction was compressed
>datapoints = number of datapoints taken
>
>range       ubytes    cbytes    cmp%    datapoints
>----------  ------    ------    ------  ----------    
>0-250       220       227       -3.16   23780
>250-500     356       354       0.68    20882
>500-600     534       505       5.29    2772
>600-700     653       608       6.95    1853
>700-800     757       649       14.22   578
>800-900     822       758       7.77    661
>900-1KB     954       862       9.69    906
>1KB-10KB    2698      2222      17.64   3370
>10KB-100KB  15463     12092     21.80   15429
>
>The above table shows that transactions don't compress well below 500
>bytes but do very well beyond 1KB where there are a great deal of those
>large spam type transactions.   However, most transactions happen to be
>in the < 500 byte range.  So the next step was to appy concatenation
>for
>those smaller transactions.  Doing that yielded some very good
>compression results.  Some examples as follows:
>
>The best one that was seen was when 175 transactions were concatenated
>before being compressed.  That yielded a 20% compression ratio, but
>that
>doesn't take into account the savings from the unneeded 174 message
>headers (24 bytes each) as well as 174 TCP ACKs of 52 bytes each which
>yields and additional 76*174 = 13224 byte savings, making for an
>overall
>bandwidth savings of 32%:
>
>     2015-11-18 01:09:09.002061 compressed data from 79890 to 67426
>txcount:175
>
>However, that was an extreme example.  Most transaction aggregates were
>in the 2 to 10 transaction range.  Such as the following:
>
>2015-11-17 21:08:28.469313 compressed data from 3199 to 2876 txcount:10
>
>But even here the savings of 10% was far better than the "nothing" we
>would get without concatenation, but add to that the 76 byte * 9
>transaction savings and we have a total 20% savings in bandwidth for
>transactions that otherwise would not be compressible.  Therefore the
>concatenation of small transactions can also save bandwidth and speed
>up
>the transmission of those transactions through the network while
>keeping
>network and message queue chatter to a minimum.
>
>==Choice of Compression library==
>
>LZO was chosen over Zlib.  LZO is the fastest most scalable option when
>used at the lowest compression setting which will be a performance
>boost
>for users that prefer performance over bandwidth savings. And at the
>higher end, LZO provides good compression (although at a higher cost)
>which approaches that of Zlib.
>
>Other compression libraries investigated were Snappy, LZOf, fastZlib
>and
>LZ4 however none of these were found to be suitable, either because
>they
>were not portable, lacked the flexibility to set compression levels or
>did not provide a useful compression ratio.
>
>The following two tables show results in seconds for syncing the first
>200,000 blocks. Tests were run on a high-speed wireless LAN with very
>little latency, and also run with a 60ms latency which was induced with
>"Netbalancer".
>               
>TABLE 4:
>Results shown in seconds on highspeed wireless LAN (no induced latency)
>Num blks sync'd  Uncmp  Zlib-1  Zlib-6  LZO1x-1  LZO1x-999
>---------------  -----  ------  ------  -------  ---------
>10000            255    232     233     231      257      
>20000            464    414     420     407      453      
>30000            677    594     611     585      650      
>40000            887    787     795     760      849     
>50000            1099   961     977     933      1048   
>60000            1310   1145    1167    1110     1259  
>70000            1512   1330    1362    1291     1470  
>80000            1714   1519    1552    1469     1679   
>90000            1917   1707    1747    1650     1882  
>100000           2122   1905    1950    1843     2111    
>110000           2333   2107    2151    2038     2329  
>120000           2560   2333    2376    2256     2580   
>130000           2835   2656    2679    2558     2921 
>140000           3274   3259    3161    3051     3466   
>150000           3662   3793    3547    3440     3919   
>160000           4040   4172    3937    3767     4416   
>170000           4425   4625    4379    4215     4958   
>180000           4860   5149    4895    4781     5560    
>190000           5855   6160    5898    5805     6557    
>200000           7004   7234    7051    6983     7770   
>
>TABLE 5:
>Results shown in seconds with 60ms of induced latency
>Num blks sync'd  Uncmp  Zlib-1  Zlib-6  LZO1x-1  LZO1x-999
>---------------  -----  ------  ------  -------  ---------
>10000            219    299     296     294      291
>20000            432    568     565     558      548
>30000            652    835     836     819      811
>40000            866    1106    1107    1081     1071
>50000            1082   1372    1381    1341     1333
>60000            1309   1644    1654    1605     1600
>70000            1535   1917    1936    1873     1875
>80000            1762   2191    2210    2141     2141
>90000            1992   2463    2486    2411     2411
>100000           2257   2748    2780    2694     2697
>110000           2627   3034    3076    2970     2983
>120000           3226   3416    3397    3266     3302
>130000           4010   3983    3773    3625     3703
>140000           4914   4503    4292    4127     4287
>150000           5806   4928    4719    4529     4821
>160000           6674   5249    5164    4840     5314
>170000           7563   5603    5669    5289     6002
>180000           8477   6054    6268    5858     6638
>190000           9843   7085    7278    6868     7679
>200000           11338  8215    8433    8044     8795
>
>==Backward compatibility==
>
>Being unable to present the correct service bit, older clients will
>continue to receive standard uncompressed data and will be fully
>compatible with this change.
>
>==Fallback==
>
>It is important to be able to entirely and easily turn off compression
>and decompression as a fall back mechanism. This can be done with a
>simple bitcoin.conf setting of "compressionlevel=0". Only one of the
>two
>connected peers need to set compressionlevel=0 in order to turn off
>compression and decompression completely.
>
>==Deployment==
>
>This enhancement does not require a hard or soft fork.
>
>==Service Bit==
>
>During the testing of this implementation, service bit 28 was used,
>however this enhancement will require a permanently assigned service
>bit.
>
>==Implementation==
>
>This implementation depends on the LZO compression library: lzo-2.09
>
>     https://github.com/ptschip/bitcoin/tree/compress
>
>==Copyright==
>
>This document is placed in the public domain.
>
>
>_______________________________________________
>bitcoin-dev mailing list
>bitcoin-dev at lists.linuxfoundation.org
>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


From Pavel at Janik.cz  Tue Dec  1 20:06:53 2015
From: Pavel at Janik.cz (=?utf-8?Q?Pavel_Jan=C3=ADk?=)
Date: Tue, 1 Dec 2015 21:06:53 +0100
Subject: [bitcoin-dev] [BIP Draft] Datastream compression of Blocks
	and	Transactions
In-Reply-To: <90EF4E6C-9A71-4A35-A938-EAFC1A24DD24@mattcorallo.com>
References: <565CD7D8.3070102@gmail.com>
	<90EF4E6C-9A71-4A35-A938-EAFC1A24DD24@mattcorallo.com>
Message-ID: <04188281-6A0C-4178-B2CA-BDE799C4FE9F@Janik.cz>


> On 01 Dec 2015, at 06:28, Matt Corallo via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
> 
> I'm really not a fan of this at all. To start with, adding a compression library that is directly accessible to the network on financial software is a really, really scary idea.

I have the same opinion.

On the other hand, I can imagine using compression on local blocks storage (be it compressed filesystem, or compression in the user space/in the application - compare with https://github.com/bitcoin/bitcoin/issues/2278). Now that we support pruning and obfuscating, this could be another option. Saving ~20% can be interesting in some usecases.
--  
Pavel Jan?k





From Pavel at Janik.cz  Wed Dec  2 06:47:28 2015
From: Pavel at Janik.cz (=?utf-8?Q?Pavel_Jan=C3=ADk?=)
Date: Wed, 2 Dec 2015 07:47:28 +0100
Subject: [bitcoin-dev] [BIP Draft] Datastream compression of Blocks and
	Transactions
In-Reply-To: <565E30C6.1010002@bitcartel.com>
References: <565CD7D8.3070102@gmail.com>
	<90EF4E6C-9A71-4A35-A938-EAFC1A24DD24@mattcorallo.com>
	<04188281-6A0C-4178-B2CA-BDE799C4FE9F@Janik.cz>
	<565E30C6.1010002@bitcartel.com>
Message-ID: <AF49F870-0600-47D1-8AC6-EEBFAA5B1C24@Janik.cz>


> On 02 Dec 2015, at 00:44, Simon Liu <simon at bitcartel.com> wrote:
> 
> Hi Matt/Pavel,
> 
> Why is it scary/undesirable?  Thanks.

Select your preferable compression library and google for it with +CVE.

E.g. in zlib:

http://www.cvedetails.com/vulnerability-list/vendor_id-72/product_id-1820/GNU-Zlib.html

?allows remote attackers to cause a denial of service (crash) via a crafted compressed stream?
?allows remote attackers to cause a denial of service (application crash)?
etc.

Do you want to expose such lib to the potential attacker?
--  
Pavel Jan?k





From pete at petertodd.org  Wed Dec  2 09:27:39 2015
From: pete at petertodd.org (Peter Todd)
Date: Wed, 2 Dec 2015 17:27:39 +0800
Subject: [bitcoin-dev] Opt-in Full Replace-By-Fee (Full-RBF)
In-Reply-To: <565BC352.4080001@gmail.com>
References: <20151117004218.GB6302@savin.petertodd.org>
	<565BC352.4080001@gmail.com>
Message-ID: <20151202092739.GA19610@muck>

On Sun, Nov 29, 2015 at 10:32:34PM -0500, Chris via bitcoin-dev wrote:
> On 11/16/2015 07:42 PM, Peter Todd via bitcoin-dev wrote:
> > Sequence is used for opting in as it is the only "free-form" field
> > available for that purpose. Opt-in per output was proposed as well by
> > Luke-Jr, however the CTxOut data structure simply doesn't contain any
> > extra fields to use for that purpose.
> What is wrong with using they same scheme as sighash_single?
> 
> If input 0 has nSequence < maxint-1 then output 0 is replaceable.
> 
> For fee bumps you would just stick the change in position zero and
> reduce the value.
> 
> You get FFS functionality without the hassle of addition other inputs.

Again, you're giving the whole world information about what's your
change address; that's simply unacceptable for privacy.

The only way to solve this is by a scheme where you pre-commit via a
hash, and reveal that later, which is extremely complex and not easily
feasible given the current tx data structure.

-- 
'peter'[:-1]@petertodd.org
0000000000000000019a7c015d7b61baa25e8afd4f1dcade4133d8a1d6b7445d
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 650 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151202/2b58fcc7/attachment.sig>

From simon at bitcartel.com  Wed Dec  2 07:33:27 2015
From: simon at bitcartel.com (Simon Liu)
Date: Tue, 1 Dec 2015 23:33:27 -0800
Subject: [bitcoin-dev] [BIP Draft] Datastream compression of Blocks and
 Transactions
In-Reply-To: <AF49F870-0600-47D1-8AC6-EEBFAA5B1C24@Janik.cz>
References: <565CD7D8.3070102@gmail.com>
	<90EF4E6C-9A71-4A35-A938-EAFC1A24DD24@mattcorallo.com>
	<04188281-6A0C-4178-B2CA-BDE799C4FE9F@Janik.cz>
	<565E30C6.1010002@bitcartel.com>
	<AF49F870-0600-47D1-8AC6-EEBFAA5B1C24@Janik.cz>
Message-ID: <565E9EC7.50003@bitcartel.com>

Hi Pavel,

(my earlier email was moderated, so the list can only see it via your
reply),

Yes, an attacker could try and send malicious data to take advantage of
a compression library vulnerability...  but is it that much worse than
existing attack vectors which might also result in denial of service,
crashes, remote execution?

Peter, perhaps your BIP can look at possible ways to isolate the
decompression phase, such as having incoming compressed blocks be saved
to a quarantine folder and an external process/daemon decompress and
verify the block's hash?

Regards,
Simon


On 12/01/2015 10:47 PM, Pavel Jan?k wrote:
> 
>> On 02 Dec 2015, at 00:44, Simon Liu <simon at bitcartel.com> wrote:
>>
>> Hi Matt/Pavel,
>>
>> Why is it scary/undesirable?  Thanks.
> 
> Select your preferable compression library and google for it with +CVE.
> 
> E.g. in zlib:
> 
> http://www.cvedetails.com/vulnerability-list/vendor_id-72/product_id-1820/GNU-Zlib.html
> 
> ?allows remote attackers to cause a denial of service (crash) via a crafted compressed stream?
> ?allows remote attackers to cause a denial of service (application crash)?
> etc.
> 
> Do you want to expose such lib to the potential attacker?
> --  
> Pavel Jan?k
> 
> 
> 
> 

From jgarzik at gmail.com  Wed Dec  2 16:39:57 2015
From: jgarzik at gmail.com (Jeff Garzik)
Date: Wed, 2 Dec 2015 11:39:57 -0500
Subject: [bitcoin-dev] Scaling Bitcoin - summarizing non-jgarzik block size
	BIPs
Message-ID: <CADm_WcYva+CeM2eozBp_iDW6FZP_qCOu0ghVOzE4VzonPi_25w@mail.gmail.com>

To collect things into one place, I was asked by Kanzure to cover
non-jgarzik block size BIPs in a quick summary, and the Scaling Bitcoin
conf folks have graciously allocated a bit of extra time for this.

e.g. BIP 100.5, 103, 105, 106 - "the serious ones"

If there is some input people would like to add to the meat grinder prior
to Dec 7, email jeff at bloq.com

Thanks!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151202/bd1e71c8/attachment.html>

From patrick.strateman at gmail.com  Wed Dec  2 18:45:23 2015
From: patrick.strateman at gmail.com (Patrick Strateman)
Date: Wed, 02 Dec 2015 10:45:23 -0800
Subject: [bitcoin-dev] [BIP Draft] Datastream compression of Blocks and
 Transactions
In-Reply-To: <565E9EC7.50003@bitcartel.com>
References: <565CD7D8.3070102@gmail.com>	<90EF4E6C-9A71-4A35-A938-EAFC1A24DD24@mattcorallo.com>	<04188281-6A0C-4178-B2CA-BDE799C4FE9F@Janik.cz>	<565E30C6.1010002@bitcartel.com>	<AF49F870-0600-47D1-8AC6-EEBFAA5B1C24@Janik.cz>
	<565E9EC7.50003@bitcartel.com>
Message-ID: <565F3C43.3040903@gmail.com>

If compression is to be used a custom compression algorithm should be
written.

Bitcoin data is largely incompressible outside of a tiny subset of fields.

On 12/01/2015 11:33 PM, Simon Liu via bitcoin-dev wrote:
> Hi Pavel,
>
> (my earlier email was moderated, so the list can only see it via your
> reply),
>
> Yes, an attacker could try and send malicious data to take advantage of
> a compression library vulnerability...  but is it that much worse than
> existing attack vectors which might also result in denial of service,
> crashes, remote execution?
>
> Peter, perhaps your BIP can look at possible ways to isolate the
> decompression phase, such as having incoming compressed blocks be saved
> to a quarantine folder and an external process/daemon decompress and
> verify the block's hash?
>
> Regards,
> Simon
>
>
> On 12/01/2015 10:47 PM, Pavel Jan?k wrote:
>>> On 02 Dec 2015, at 00:44, Simon Liu <simon at bitcartel.com> wrote:
>>>
>>> Hi Matt/Pavel,
>>>
>>> Why is it scary/undesirable?  Thanks.
>> Select your preferable compression library and google for it with +CVE.
>>
>> E.g. in zlib:
>>
>> http://www.cvedetails.com/vulnerability-list/vendor_id-72/product_id-1820/GNU-Zlib.html
>>
>> ?allows remote attackers to cause a denial of service (crash) via a crafted compressed stream?
>> ?allows remote attackers to cause a denial of service (application crash)?
>> etc.
>>
>> Do you want to expose such lib to the potential attacker?
>> --  
>> Pavel Jan?k
>>
>>
>>
>>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


From el33th4x0r at gmail.com  Wed Dec  2 18:57:46 2015
From: el33th4x0r at gmail.com (=?UTF-8?Q?Emin_G=C3=BCn_Sirer?=)
Date: Wed, 2 Dec 2015 13:57:46 -0500
Subject: [bitcoin-dev] [BIP Draft] Datastream compression of Blocks and
	Transactions
In-Reply-To: <90EF4E6C-9A71-4A35-A938-EAFC1A24DD24@mattcorallo.com>
References: <565CD7D8.3070102@gmail.com>
	<90EF4E6C-9A71-4A35-A938-EAFC1A24DD24@mattcorallo.com>
Message-ID: <CAPkFh0t9SwVOLrPnL7z80s-Rriezhqxn_3vXKYRxr6JVGNiUZQ@mail.gmail.com>

Thanks Peter for the careful, quantitative work.

I want to bring one additional issue to everyone's consideration, related
to the choice of the Lempel-Ziv family of compressors.

While I'm not familiar with every single compression engine tested, the
Lempel-Ziv family of compressors are generally based on "compression
tables." Essentially, they assign a short unique number to every new
subsequence they encounter, and when they re-encounter a sequence like "ab"
in "abcdfdcdabcdfabcdf" they replace it with that short integer (say, in
this case, 9-bit constant 256). So this example sequence may turn into
"abcdfd<258 for cd><256 for ab><258 for cd>f<261 for abc><259 for df>"
which is slightly shorter than the original (I'm doing this off the top of
my head so the counts may be off, but it's meant to be illustrative). Note
that the sequence "abc" got added into the table only after it was
encountered twice in the input.

This is nice and generic and works well for English text where certain
letter sequences (e.g. "it" "th" "the" "this" "are" "there" etc) are
repeated often, but it is nowhere as compact as it could possibly be for
mostly binary data -- there are opportunities for much better compression,
made possible by the structured reuse of certain byte sequences in the
Bitcoin wire protocol.

On a Bitcoin wire connection, we might see several related transactions
reorganizing cash in a set of addresses, and therefore, several reuses of a
20-byte address. Or we might see a 200-byte transaction get transmitted,
followed by the same transaction, repeated in a block. Ideally, we'd learn
the sequence that may be repeated later on, all at once (e.g. a Bitcoin
address or a transaction), and replace it with a short number, referring
back to the long sequence. In the example above, if we knew that "abcdf"
was a UNIT that would likely be repeated, we would put it into the
compression table as a whole, instead of relying on repetition to get it
into the table one extra byte at a time. That may let us compress the
original sequence down to "abcdfd<257 for cd><256 for abcdf><256 for
abcdf>" from the get go.

Yet the LZ variants I know of will need to see a 200-byte sequence repeated
**199 times** in order to develop a single, reusable, 200-byte long
subsequence in the compression table.

So, a Bitcoin-specific compressor can perhaps do significantly better, but
is it a good idea? Let's argue both sides.

Cons:

On the one hand, Bitcoin-specific compressors will be closely tied to the
contents of messages, which might make it difficult to change the wire
format later on -- changes to the wire format may need corresponding
changes to the compressor.  If the compressor cannot be implemented
cleanly, then the protocol-agnostic, off-the-shelf compressors have a
maintainability edge, which comes at the expense of the compression ratio.

Another argument is that compression algorithms of any kind should be
tested thoroughly before inclusion, and brand new code may lack the
maturity required. While this argument has some merit, all outputs are
verified separately later on during processing, so
compression/decompression errors can potentially be detected. If the
compressor/decompressor can be structured in a way that isolates bitcoind
from failure (e.g. as a separate process for starters), this concern can be
remedied.

Pros:

The nature of LZ compressors leads me to believe that much higher
compression ratios are possible by building a custom, Bitcoin-aware
compressor. If I had to guess, I would venture that compression ratios of
2X or more are possible in some cases. In some sense, the "O(1) block
propagation" idea that Gavin proposed a while ago can be seen as extreme
example of a Bitcoin-specific compressor, albeit one that constrains the
order of transactions in a block.

Compression can buy us some additional throughput at zero cost, modulo code
complexity.
Given the amount of acrimonious debate over the block size we have all had
to endure, it seems
criminal to leave potentially free improvements on the table. Even if the
resulting code is
deemed too complex to include in the production client right now, it would
be good to understand
the potential for improvement.

How to Do It

If we want to compress Bitcoin, a programming challenge/contest would be
one of the best ways to find the best possible, Bitcoin-specific
compressor. This is the kind of self-contained exercise that bright young
hackers love to tackle. It'd bring in new programmers into the ecosystem,
and many of us would love to discover the limits of compressibility for
Bitcoin bits on a wire. And the results would be interesting even if the
final compression engine is not enabled by default, or not even merged.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151202/32591f84/attachment.html>

From peter.tschipper at gmail.com  Wed Dec  2 20:16:19 2015
From: peter.tschipper at gmail.com (Peter Tschipper)
Date: Wed, 2 Dec 2015 12:16:19 -0800
Subject: [bitcoin-dev] [BIP Draft] Datastream compression of Blocks and
 Transactions
In-Reply-To: <CAPkFh0t9SwVOLrPnL7z80s-Rriezhqxn_3vXKYRxr6JVGNiUZQ@mail.gmail.com>
References: <565CD7D8.3070102@gmail.com>
	<90EF4E6C-9A71-4A35-A938-EAFC1A24DD24@mattcorallo.com>
	<CAPkFh0t9SwVOLrPnL7z80s-Rriezhqxn_3vXKYRxr6JVGNiUZQ@mail.gmail.com>
Message-ID: <565F5193.1070802@gmail.com>

Building a compressor from scratch may yeild some better compression
ratios, or not, but having trust and faith in whether it will stand up
against attack vectors another matter.  LZO has been around for 20 years
with very few problems and no current issues.  Maybe something better
can be built, but when and how much testing will need to be done before
it can be trusted?  Right now there is something that provides a benefit
and in the future if something better is found it's not that difficult
to add it.  We could easily support multiple compression libraries.


On 02/12/2015 10:57 AM, Emin G?n Sirer wrote:
> Thanks Peter for the careful, quantitative work.
>
> I want to bring one additional issue to everyone's consideration,
> related to the choice of the Lempel-Ziv family of compressors. 
>
> While I'm not familiar with every single compression engine tested,
> the Lempel-Ziv family of compressors are generally based on
> "compression tables." Essentially, they assign a short unique number
> to every new subsequence they encounter, and when they re-encounter a
> sequence like "ab" in "abcdfdcdabcdfabcdf" they replace it with that
> short integer (say, in this case, 9-bit constant 256). So this example
> sequence may turn into "abcdfd<258 for cd><256 for ab><258 for
> cd>f<261 for abc><259 for df>" which is slightly shorter than the
> original (I'm doing this off the top of my head so the counts may be
> off, but it's meant to be illustrative). Note that the sequence "abc"
> got added into the table only after it was encountered twice in the
> input. 
>
> This is nice and generic and works well for English text where certain
> letter sequences (e.g. "it" "th" "the" "this" "are" "there" etc) are
> repeated often, but it is nowhere as compact as it could possibly be
> for mostly binary data -- there are opportunities for much better
> compression, made possible by the structured reuse of certain byte
> sequences in the Bitcoin wire protocol.
>
> On a Bitcoin wire connection, we might see several related
> transactions reorganizing cash in a set of addresses, and therefore,
> several reuses of a 20-byte address. Or we might see a 200-byte
> transaction get transmitted, followed by the same transaction,
> repeated in a block. Ideally, we'd learn the sequence that may be
> repeated later on, all at once (e.g. a Bitcoin address or a
> transaction), and replace it with a short number, referring back to
> the long sequence. In the example above, if we knew that "abcdf" was a
> UNIT that would likely be repeated, we would put it into the
> compression table as a whole, instead of relying on repetition to get
> it into the table one extra byte at a time. That may let us compress
> the original sequence down to "abcdfd<257 for cd><256 for abcdf><256
> for abcdf>" from the get go.
>
> Yet the LZ variants I know of will need to see a 200-byte sequence
> repeated **199 times** in order to develop a single, reusable,
> 200-byte long subsequence in the compression table. 
>
> So, a Bitcoin-specific compressor can perhaps do significantly better,
> but is it a good idea? Let's argue both sides.
>
> Cons:
>
> On the one hand, Bitcoin-specific compressors will be closely tied to
> the contents of messages, which might make it difficult to change the
> wire format later on -- changes to the wire format may need
> corresponding changes to the compressor.  If the compressor cannot be
> implemented cleanly, then the protocol-agnostic, off-the-shelf
> compressors have a maintainability edge, which comes at the expense of
> the compression ratio. 
>
> Another argument is that compression algorithms of any kind should be
> tested thoroughly before inclusion, and brand new code may lack the
> maturity required. While this argument has some merit, all outputs are
> verified separately later on during processing, so
> compression/decompression errors can potentially be detected. If the
> compressor/decompressor can be structured in a way that isolates
> bitcoind from failure (e.g. as a separate process for starters), this
> concern can be remedied.
>
> Pros:
>
> The nature of LZ compressors leads me to believe that much higher
> compression ratios are possible by building a custom, Bitcoin-aware
> compressor. If I had to guess, I would venture that compression ratios
> of 2X or more are possible in some cases. In some sense, the "O(1)
> block propagation" idea that Gavin proposed a while ago can be seen as
> extreme example of a Bitcoin-specific compressor, albeit one that
> constrains the order of transactions in a block.
>
> Compression can buy us some additional throughput at zero cost, modulo
> code complexity. 
> Given the amount of acrimonious debate over the block size we have all
> had to endure, it seems 
> criminal to leave potentially free improvements on the table. Even if
> the resulting code is
> deemed too complex to include in the production client right now, it
> would be good to understand
> the potential for improvement.
>
> How to Do It
>
> If we want to compress Bitcoin, a programming challenge/contest would
> be one of the best ways to find the best possible, Bitcoin-specific
> compressor. This is the kind of self-contained exercise that bright
> young hackers love to tackle. It'd bring in new programmers into the
> ecosystem, and many of us would love to discover the limits of
> compressibility for Bitcoin bits on a wire. And the results would be
> interesting even if the final compression engine is not enabled by
> default, or not even merged.
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151202/ddbc5702/attachment-0001.html>

From lf-lists at mattcorallo.com  Wed Dec  2 22:23:47 2015
From: lf-lists at mattcorallo.com (Matt Corallo)
Date: Wed, 2 Dec 2015 22:23:47 +0000
Subject: [bitcoin-dev] [BIP Draft] Datastream compression of Blocks and
 Transactions
In-Reply-To: <565F5193.1070802@gmail.com>
References: <565CD7D8.3070102@gmail.com>
	<90EF4E6C-9A71-4A35-A938-EAFC1A24DD24@mattcorallo.com>
	<CAPkFh0t9SwVOLrPnL7z80s-Rriezhqxn_3vXKYRxr6JVGNiUZQ@mail.gmail.com>
	<565F5193.1070802@gmail.com>
Message-ID: <565F6F73.5050906@mattcorallo.com>

My issue is more that its additional complexity and attack surface, and 
for a very minor gain which should disappear with further optimization 
elsewhere and less that we absolutely shouldn't add compression because 
we're definitely gonna have issues.

On 12/02/15 20:16, Peter Tschipper via bitcoin-dev wrote:
> Building a compressor from scratch may yeild some better compression
> ratios, or not, but having trust and faith in whether it will stand up
> against attack vectors another matter.  LZO has been around for 20 years
> with very few problems and no current issues.  Maybe something better
> can be built, but when and how much testing will need to be done before
> it can be trusted?  Right now there is something that provides a benefit
> and in the future if something better is found it's not that difficult
> to add it.  We could easily support multiple compression libraries.
>
>
> On 02/12/2015 10:57 AM, Emin G?n Sirer wrote:
>> Thanks Peter for the careful, quantitative work.
>>
>> I want to bring one additional issue to everyone's consideration,
>> related to the choice of the Lempel-Ziv family of compressors.
>>
>> While I'm not familiar with every single compression engine tested,
>> the Lempel-Ziv family of compressors are generally based on
>> "compression tables." Essentially, they assign a short unique number
>> to every new subsequence they encounter, and when they re-encounter a
>> sequence like "ab" in "abcdfdcdabcdfabcdf" they replace it with that
>> short integer (say, in this case, 9-bit constant 256). So this example
>> sequence may turn into "abcdfd<258 for cd><256 for ab><258 for
>> cd>f<261 for abc><259 for df>" which is slightly shorter than the
>> original (I'm doing this off the top of my head so the counts may be
>> off, but it's meant to be illustrative). Note that the sequence "abc"
>> got added into the table only after it was encountered twice in the
>> input.
>>
>> This is nice and generic and works well for English text where certain
>> letter sequences (e.g. "it" "th" "the" "this" "are" "there" etc) are
>> repeated often, but it is nowhere as compact as it could possibly be
>> for mostly binary data -- there are opportunities for much better
>> compression, made possible by the structured reuse of certain byte
>> sequences in the Bitcoin wire protocol.
>>
>> On a Bitcoin wire connection, we might see several related
>> transactions reorganizing cash in a set of addresses, and therefore,
>> several reuses of a 20-byte address. Or we might see a 200-byte
>> transaction get transmitted, followed by the same transaction,
>> repeated in a block. Ideally, we'd learn the sequence that may be
>> repeated later on, all at once (e.g. a Bitcoin address or a
>> transaction), and replace it with a short number, referring back to
>> the long sequence. In the example above, if we knew that "abcdf" was a
>> UNIT that would likely be repeated, we would put it into the
>> compression table as a whole, instead of relying on repetition to get
>> it into the table one extra byte at a time. That may let us compress
>> the original sequence down to "abcdfd<257 for cd><256 for abcdf><256
>> for abcdf>" from the get go.
>>
>> Yet the LZ variants I know of will need to see a 200-byte sequence
>> repeated **199 times** in order to develop a single, reusable,
>> 200-byte long subsequence in the compression table.
>>
>> So, a Bitcoin-specific compressor can perhaps do significantly better,
>> but is it a good idea? Let's argue both sides.
>>
>> Cons:
>>
>> On the one hand, Bitcoin-specific compressors will be closely tied to
>> the contents of messages, which might make it difficult to change the
>> wire format later on -- changes to the wire format may need
>> corresponding changes to the compressor.  If the compressor cannot be
>> implemented cleanly, then the protocol-agnostic, off-the-shelf
>> compressors have a maintainability edge, which comes at the expense of
>> the compression ratio.
>>
>> Another argument is that compression algorithms of any kind should be
>> tested thoroughly before inclusion, and brand new code may lack the
>> maturity required. While this argument has some merit, all outputs are
>> verified separately later on during processing, so
>> compression/decompression errors can potentially be detected. If the
>> compressor/decompressor can be structured in a way that isolates
>> bitcoind from failure (e.g. as a separate process for starters), this
>> concern can be remedied.
>>
>> Pros:
>>
>> The nature of LZ compressors leads me to believe that much higher
>> compression ratios are possible by building a custom, Bitcoin-aware
>> compressor. If I had to guess, I would venture that compression ratios
>> of 2X or more are possible in some cases. In some sense, the "O(1)
>> block propagation" idea that Gavin proposed a while ago can be seen as
>> extreme example of a Bitcoin-specific compressor, albeit one that
>> constrains the order of transactions in a block.
>>
>> Compression can buy us some additional throughput at zero cost, modulo
>> code complexity.
>> Given the amount of acrimonious debate over the block size we have all
>> had to endure, it seems
>> criminal to leave potentially free improvements on the table. Even if
>> the resulting code is
>> deemed too complex to include in the production client right now, it
>> would be good to understand
>> the potential for improvement.
>>
>> How to Do It
>>
>> If we want to compress Bitcoin, a programming challenge/contest would
>> be one of the best ways to find the best possible, Bitcoin-specific
>> compressor. This is the kind of self-contained exercise that bright
>> young hackers love to tackle. It'd bring in new programmers into the
>> ecosystem, and many of us would love to discover the limits of
>> compressibility for Bitcoin bits on a wire. And the results would be
>> interesting even if the final compression engine is not enabled by
>> default, or not even merged.
>>
>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

From peter.tschipper at gmail.com  Wed Dec  2 23:02:20 2015
From: peter.tschipper at gmail.com (Peter Tschipper)
Date: Wed, 2 Dec 2015 15:02:20 -0800
Subject: [bitcoin-dev] [BIP Draft] Datastream compression of Blocks and
 Transactions
In-Reply-To: <565F6F73.5050906@mattcorallo.com>
References: <565CD7D8.3070102@gmail.com>
	<90EF4E6C-9A71-4A35-A938-EAFC1A24DD24@mattcorallo.com>
	<CAPkFh0t9SwVOLrPnL7z80s-Rriezhqxn_3vXKYRxr6JVGNiUZQ@mail.gmail.com>
	<565F5193.1070802@gmail.com> <565F6F73.5050906@mattcorallo.com>
Message-ID: <565F787C.3080604@gmail.com>

On 02/12/2015 2:23 PM, Matt Corallo wrote:
> My issue is more that its additional complexity and attack surface,
> and for a very minor gain 
What is a minor gain?  15 to 27% compression sounds good to me and the
larger the data the better the compression.  And although there is a
decent peformance gain in proportion to the % of compression, the
original motivation of the BIP was to reduce bandwidth for users in
regions where they are subject to caps. 
> which should disappear with further optimization elsewhere 
Why would the benefit of compressing data disappear with further
optimizations elsewhere, I'm not following you?.  The compression of
data mainly has benefit in the sending of packets over the network.  I
would think the performance gain would be cumulative.  Why would this go
away by optimizing elsewhere?

> and less that we absolutely shouldn't add compression because we're
> definitely gonna have issues.
It's not that difficult to add compression.  Even if there was an issue,
the compression feature can be completely turned off. 

>
> On 12/02/15 20:16, Peter Tschipper via bitcoin-dev wrote:
>> Building a compressor from scratch may yeild some better compression
>> ratios, or not, but having trust and faith in whether it will stand up
>> against attack vectors another matter.  LZO has been around for 20 years
>> with very few problems and no current issues.  Maybe something better
>> can be built, but when and how much testing will need to be done before
>> it can be trusted?  Right now there is something that provides a benefit
>> and in the future if something better is found it's not that difficult
>> to add it.  We could easily support multiple compression libraries.
>>
>>
>> On 02/12/2015 10:57 AM, Emin G?n Sirer wrote:
>>> Thanks Peter for the careful, quantitative work.
>>>
>>> I want to bring one additional issue to everyone's consideration,
>>> related to the choice of the Lempel-Ziv family of compressors.
>>>
>>> While I'm not familiar with every single compression engine tested,
>>> the Lempel-Ziv family of compressors are generally based on
>>> "compression tables." Essentially, they assign a short unique number
>>> to every new subsequence they encounter, and when they re-encounter a
>>> sequence like "ab" in "abcdfdcdabcdfabcdf" they replace it with that
>>> short integer (say, in this case, 9-bit constant 256). So this example
>>> sequence may turn into "abcdfd<258 for cd><256 for ab><258 for
>>> cd>f<261 for abc><259 for df>" which is slightly shorter than the
>>> original (I'm doing this off the top of my head so the counts may be
>>> off, but it's meant to be illustrative). Note that the sequence "abc"
>>> got added into the table only after it was encountered twice in the
>>> input.
>>>
>>> This is nice and generic and works well for English text where certain
>>> letter sequences (e.g. "it" "th" "the" "this" "are" "there" etc) are
>>> repeated often, but it is nowhere as compact as it could possibly be
>>> for mostly binary data -- there are opportunities for much better
>>> compression, made possible by the structured reuse of certain byte
>>> sequences in the Bitcoin wire protocol.
>>>
>>> On a Bitcoin wire connection, we might see several related
>>> transactions reorganizing cash in a set of addresses, and therefore,
>>> several reuses of a 20-byte address. Or we might see a 200-byte
>>> transaction get transmitted, followed by the same transaction,
>>> repeated in a block. Ideally, we'd learn the sequence that may be
>>> repeated later on, all at once (e.g. a Bitcoin address or a
>>> transaction), and replace it with a short number, referring back to
>>> the long sequence. In the example above, if we knew that "abcdf" was a
>>> UNIT that would likely be repeated, we would put it into the
>>> compression table as a whole, instead of relying on repetition to get
>>> it into the table one extra byte at a time. That may let us compress
>>> the original sequence down to "abcdfd<257 for cd><256 for abcdf><256
>>> for abcdf>" from the get go.
>>>
>>> Yet the LZ variants I know of will need to see a 200-byte sequence
>>> repeated **199 times** in order to develop a single, reusable,
>>> 200-byte long subsequence in the compression table.
>>>
>>> So, a Bitcoin-specific compressor can perhaps do significantly better,
>>> but is it a good idea? Let's argue both sides.
>>>
>>> Cons:
>>>
>>> On the one hand, Bitcoin-specific compressors will be closely tied to
>>> the contents of messages, which might make it difficult to change the
>>> wire format later on -- changes to the wire format may need
>>> corresponding changes to the compressor.  If the compressor cannot be
>>> implemented cleanly, then the protocol-agnostic, off-the-shelf
>>> compressors have a maintainability edge, which comes at the expense of
>>> the compression ratio.
>>>
>>> Another argument is that compression algorithms of any kind should be
>>> tested thoroughly before inclusion, and brand new code may lack the
>>> maturity required. While this argument has some merit, all outputs are
>>> verified separately later on during processing, so
>>> compression/decompression errors can potentially be detected. If the
>>> compressor/decompressor can be structured in a way that isolates
>>> bitcoind from failure (e.g. as a separate process for starters), this
>>> concern can be remedied.
>>>
>>> Pros:
>>>
>>> The nature of LZ compressors leads me to believe that much higher
>>> compression ratios are possible by building a custom, Bitcoin-aware
>>> compressor. If I had to guess, I would venture that compression ratios
>>> of 2X or more are possible in some cases. In some sense, the "O(1)
>>> block propagation" idea that Gavin proposed a while ago can be seen as
>>> extreme example of a Bitcoin-specific compressor, albeit one that
>>> constrains the order of transactions in a block.
>>>
>>> Compression can buy us some additional throughput at zero cost, modulo
>>> code complexity.
>>> Given the amount of acrimonious debate over the block size we have all
>>> had to endure, it seems
>>> criminal to leave potentially free improvements on the table. Even if
>>> the resulting code is
>>> deemed too complex to include in the production client right now, it
>>> would be good to understand
>>> the potential for improvement.
>>>
>>> How to Do It
>>>
>>> If we want to compress Bitcoin, a programming challenge/contest would
>>> be one of the best ways to find the best possible, Bitcoin-specific
>>> compressor. This is the kind of self-contained exercise that bright
>>> young hackers love to tackle. It'd bring in new programmers into the
>>> ecosystem, and many of us would love to discover the limits of
>>> compressibility for Bitcoin bits on a wire. And the results would be
>>> interesting even if the final compression engine is not enabled by
>>> default, or not even merged.
>>>
>>
>>
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>


From peter.tschipper at gmail.com  Wed Dec  2 23:05:10 2015
From: peter.tschipper at gmail.com (Peter Tschipper)
Date: Wed, 2 Dec 2015 15:05:10 -0800
Subject: [bitcoin-dev] [BIP Draft] Datastream compression of Blocks and
 Transactions
In-Reply-To: <90EF4E6C-9A71-4A35-A938-EAFC1A24DD24@mattcorallo.com>
References: <565CD7D8.3070102@gmail.com>
	<90EF4E6C-9A71-4A35-A938-EAFC1A24DD24@mattcorallo.com>
Message-ID: <565F7926.103@gmail.com>


On 30/11/2015 9:28 PM, Matt Corallo wrote:
> I'm really not a fan of this at all. To start with, adding a compression library that is directly accessible to the network on financial software is a really, really scary idea. 
Why scary?  LZO has no current security issues, and it will be
configureable by each node operator so it can be turned off completely
if needed or desired. 
> If there were a massive improvement, I'd find it acceptable, but the improvement you've shown really isn't all that much.
Why is 15% at the low end, to 27% at the high end not good?  It sounds
like a very good boost.   
>  The numbers you recently posted show it improving the very beginning of IBD somewhat over high-latency connections, but if we're throughput-limited after the very beginning of IBD, we should fix that, not compress the blocks. 
I only did the compression up to the 200,000 block to better isolate the
transmission of data from the post processing of blocks and determine
whether the compressing of data was adding to much to the total
transmission time.

I think it's clear from the data that as the data (blocks, transactions)
increase in size that (1) they compress better and (2) they have a
bigger and positive impact on improving performance when compressed.

> Additionally, I'd be very surprised if this had any significant effect on the speed at which new blocks traverse the network (do you have any simulations or other thoughts on this?).
>From the table below, at 120000 blocks the time to sync the chain was
roughly the same for compressed vs. uncompressed however after that
point as block sizes start increasing, all compression libraries
peformed much faster than uncompressed. The data provided in this
testing clearly shows that as block size increases, the performance
improvement by compressing data also increases.

TABLE 5:
Results shown in seconds with 60ms of induced latency
Num blks sync'd  Uncmp  Zlib-1  Zlib-6  LZO1x-1  LZO1x-999
---------------  -----  ------  ------  -------  ---------
120000           3226   3416    3397    3266     3302
130000           4010   3983    3773    3625     3703
140000           4914   4503    4292    4127     4287
150000           5806   4928    4719    4529     4821
160000           6674   5249    5164    4840     5314
170000           7563   5603    5669    5289     6002
180000           8477   6054    6268    5858     6638
190000           9843   7085    7278    6868     7679
200000           11338  8215    8433    8044     8795


As far as, what happens after the block is received, then obviously
compression isn't going to help in post processing and validating the
block, but in the pure transmission of the object it most certainly and
logically does and in a fairly direct proportion to the file size (a
file that is 20% smaller will be transmited "at least" 20% faster, you
can use any data transfer time calculator
<http://www.calctool.org/CALC/prof/computing/transfer_time> for that). 
The only issue, that I can see that required testing was to show how
much compression there would be, and how much time the compression of
the data would add to the sending of the data.

 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151202/9212a042/attachment-0001.html>

From dscotese at litmocracy.com  Thu Dec  3 05:52:20 2015
From: dscotese at litmocracy.com (Dave Scotese)
Date: Wed, 2 Dec 2015 21:52:20 -0800
Subject: [bitcoin-dev] [BIP Draft] Datastream compression of Blocks and
	Transactions
In-Reply-To: <565F7926.103@gmail.com>
References: <565CD7D8.3070102@gmail.com>
	<90EF4E6C-9A71-4A35-A938-EAFC1A24DD24@mattcorallo.com>
	<565F7926.103@gmail.com>
Message-ID: <CAGLBAhef0mGKP9iJC-z+qFk4YQSm2kd9Dm2_MyZ3ZjY-ZgeEcQ@mail.gmail.com>

Emin's email presents to me the idea of dictionaries that already contain
the data we'd want to compress.  With 8 bytes of indexing data, we can
refer to a TxID or a Public Key or any existing part of the blockchain.
There are also data sequences like scripts that contain a few variable
chunks and are otherwise identical.  Often, the receiver has the
blockchain, which contains a lot of the data that is in the message being
transmitted.

First, the receiver must indicate that compressed data is preferred and the
height of latest valid block it holds, and the sender must express the
ability to send compressed data.  From this state, the sender sends
messages that are compressed.  Compressed messages are the same as
uncompressed messages except that:

   1. Data read is copied into the decompressed message until the first
   occurrence of 0x00, which is discarded and is followed by compressed data.
   2. Compressed data can use as a dictionary the first 16,777,215 blocks,
   or the last 4,244,635,647 ending with the block at the tip of the
   receiver's chain, or it can specify a run of zero bytes.  The sender and
   receiver must agree on the *receiver's* current block height in order to
   use the last 4B blocks as the dictionary.
   3. Within compressed data, the first byte identifies how to decompress:
      1. 0xFF indicates that the following three bytes are a block height
      with most significant byte 0x00 in network byte order.
      2. 0xFE indicates that the following byte indicates how many zero
      bytes to add to the decompressed data.
      3. 0xFD is an error, so compressed messages are turned off and the
      recipient fails the decompression process.
      4. 0x00 indicates that the zero byte by itself should be added to the
      decompressed data, and the data following is not compressed
(return to step
      1).
      5. All other values represent the most significant byte of a number
      to be subtracted from the receiver's current block height to identify a
      block height (not available until there are least 16,777,216
blocks so that
      this byte can be at least 0x01, since 0x00 would indicate a single zero
      byte, end compressed data, and return to step 1).
   4. If decompression has identified a block height (previous byte was not
   0xFD, 0x00, or 0xFE), then the next four bytes identify a *size *(one
   byte) and a byte index into the block's data (three bytes), and *size *bytes
   from that block are added to the decompressed data.
   5. Steps 3 and 4 process a chunk of compressed data.  If the next byte
   is 0xFD, then decompression goes back to step 1 (add raw bytes until it
   hits a 0x00).  Otherwise, it proceeds through steps 3 (and maybe 4) again.

In Step 3.3, 0xFD causes an error, but it could be used to indicate a
parameterized dictionary entry, for example 0xFD, 0x01 followed by eight
more bytes to be interpreted according to steps 3.1 or 3.5 could mean
OP_DUP OP_HASH160 (20 bytes from the blockchain dictionary) OP_EQUALVERIFY
OP_CHECKSIG, replacing that very common occurrence of 24 bytes with 10
bytes.  Well, 11 if you include the 0x00 required by step5.  But that only
works on addresses that have spent inputs.  Or 0xFD, 0x02 could be
shorthand for the four zeroes of lock_time, followed by Version (1),
followed by 0x01 (for one-input transactions), turning nine bytes into two
for the data at the end of a normal (lock_time = 0) Txn and the beginning
of a single-input Txn.  But I left 0xFD as an error because those gains
didn't seem as frequent as the others.

Dave.

On Wed, Dec 2, 2015 at 3:05 PM, Peter Tschipper via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

>
> On 30/11/2015 9:28 PM, Matt Corallo wrote:
>
> I'm really not a fan of this at all. To start with, adding a compression library that is directly accessible to the network on financial software is a really, really scary idea.
>
> Why scary?  LZO has no current security issues, and it will be
> configureable by each node operator so it can be turned off completely if
> needed or desired.
>
> If there were a massive improvement, I'd find it acceptable, but the improvement you've shown really isn't all that much.
>
> Why is 15% at the low end, to 27% at the high end not good?  It sounds
> like a very good boost.
>
>  The numbers you recently posted show it improving the very beginning of IBD somewhat over high-latency connections, but if we're throughput-limited after the very beginning of IBD, we should fix that, not compress the blocks.
>
> I only did the compression up to the 200,000 block to better isolate the
> transmission of data from the post processing of blocks and determine
> whether the compressing of data was adding to much to the total
> transmission time.
>
> I think it's clear from the data that as the data (blocks, transactions)
> increase in size that (1) they compress better and (2) they have a bigger
> and positive impact on improving performance when compressed.
>
> Additionally, I'd be very surprised if this had any significant effect on the speed at which new blocks traverse the network (do you have any simulations or other thoughts on this?).
>
> From the table below, at 120000 blocks the time to sync the chain was
> roughly the same for compressed vs. uncompressed however after that point
> as block sizes start increasing, all compression libraries peformed much
> faster than uncompressed. The data provided in this testing clearly shows
> that as block size increases, the performance improvement by compressing
> data also increases.
>
> TABLE 5:
> Results shown in seconds with 60ms of induced latency
> Num blks sync'd  Uncmp  Zlib-1  Zlib-6  LZO1x-1  LZO1x-999
> ---------------  -----  ------  ------  -------  ---------
> 120000           3226   3416    3397    3266     3302
> 130000           4010   3983    3773    3625     3703
> 140000           4914   4503    4292    4127     4287
> 150000           5806   4928    4719    4529     4821
> 160000           6674   5249    5164    4840     5314
> 170000           7563   5603    5669    5289     6002
> 180000           8477   6054    6268    5858     6638
> 190000           9843   7085    7278    6868     7679
> 200000           11338  8215    8433    8044     8795
>
>
> As far as, what happens after the block is received, then obviously
> compression isn't going to help in post processing and validating the
> block, but in the pure transmission of the object it most certainly and
> logically does and in a fairly direct proportion to the file size (a file
> that is 20% smaller will be transmited "at least" 20% faster, you can use
> any data transfer time calculator
> <http://www.calctool.org/CALC/prof/computing/transfer_time> for that).
> The only issue, that I can see that required testing was to show how much
> compression there would be, and how much time the compression of the data
> would add to the sending of the data.
>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>


-- 
I like to provide some work at no charge to prove my value. Do you need a
techie?
I own Litmocracy <http://www.litmocracy.com> and Meme Racing
<http://www.memeracing.net> (in alpha).
I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which
now accepts Bitcoin.
I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
"He ought to find it more profitable to play by the rules" - Satoshi
Nakamoto
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151202/c8405e2f/attachment.html>

From gavinandresen at gmail.com  Thu Dec  3 19:14:55 2015
From: gavinandresen at gmail.com (Gavin Andresen)
Date: Thu, 3 Dec 2015 14:14:55 -0500
Subject: [bitcoin-dev] [BIP Draft] Datastream compression of Blocks and
	Transactions
In-Reply-To: <CAPkFh0t9SwVOLrPnL7z80s-Rriezhqxn_3vXKYRxr6JVGNiUZQ@mail.gmail.com>
References: <565CD7D8.3070102@gmail.com>
	<90EF4E6C-9A71-4A35-A938-EAFC1A24DD24@mattcorallo.com>
	<CAPkFh0t9SwVOLrPnL7z80s-Rriezhqxn_3vXKYRxr6JVGNiUZQ@mail.gmail.com>
Message-ID: <CABsx9T3+y6ip3cUW8jP-0n+1FS8t_FdX8fA42JaTO7qW5pMhzA@mail.gmail.com>

On Wed, Dec 2, 2015 at 1:57 PM, Emin G?n Sirer <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> How to Do It
>
> If we want to compress Bitcoin, a programming challenge/contest would be
> one of the best ways to find the best possible, Bitcoin-specific
> compressor. This is the kind of self-contained exercise that bright young
> hackers love to tackle. It'd bring in new programmers into the ecosystem,
> and many of us would love to discover the limits of compressibility for
> Bitcoin bits on a wire. And the results would be interesting even if the
> final compression engine is not enabled by default, or not even merged.
>

I love this idea. Lets build a standardized data set to test against using
real data from the network (has anybody done this yet?).

Something like:

Starting network topology:
list of:  nodeid, nodeid, network latency between the two peers

Changes to network topology:
list of:  nodeid, add/remove nodeid, time of change

Transaction broadcasts:
list of :  transaction, node id that first broadcast, time first broadcast

Block broadcasts:
list of :  block, node id that first broadcast, time first broadcast

Proposed transaction/block optimizations could then be measured against
this standard data set.


-- 
--
Gavin Andresen
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151203/fd6a0dcd/attachment.html>

From rusty at rustcorp.com.au  Thu Dec  3 23:07:56 2015
From: rusty at rustcorp.com.au (Rusty Russell)
Date: Fri, 04 Dec 2015 09:37:56 +1030
Subject: [bitcoin-dev] [BIP Draft] Datastream compression of Blocks
	and	Transactions
In-Reply-To: <CABsx9T3+y6ip3cUW8jP-0n+1FS8t_FdX8fA42JaTO7qW5pMhzA@mail.gmail.com>
References: <565CD7D8.3070102@gmail.com>
	<90EF4E6C-9A71-4A35-A938-EAFC1A24DD24@mattcorallo.com>
	<CAPkFh0t9SwVOLrPnL7z80s-Rriezhqxn_3vXKYRxr6JVGNiUZQ@mail.gmail.com>
	<CABsx9T3+y6ip3cUW8jP-0n+1FS8t_FdX8fA42JaTO7qW5pMhzA@mail.gmail.com>
Message-ID: <87vb8f5dwz.fsf@rustcorp.com.au>

Gavin Andresen via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>
writes:
> On Wed, Dec 2, 2015 at 1:57 PM, Emin G?n Sirer <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> How to Do It
>>
>> If we want to compress Bitcoin, a programming challenge/contest would be
>> one of the best ways to find the best possible, Bitcoin-specific
>> compressor. This is the kind of self-contained exercise that bright young
>> hackers love to tackle. It'd bring in new programmers into the ecosystem,
>> and many of us would love to discover the limits of compressibility for
>> Bitcoin bits on a wire. And the results would be interesting even if the
>> final compression engine is not enabled by default, or not even merged.
>>
>
> I love this idea. Lets build a standardized data set to test against using
> real data from the network (has anybody done this yet?).

https://github.com/rustyrussell/bitcoin-corpus

It includes mempool contents and tx receipt logs for 1 week across 4
nodes.  I vaguely plan to update it every year.

A more ambitious version would add some topology information, but we
need to figure out some anonymization strategy for the data.

Cheers,
Rusty.

From greg at xiph.org  Fri Dec  4 08:26:22 2015
From: greg at xiph.org (Gregory Maxwell)
Date: Fri, 4 Dec 2015 08:26:22 +0000
Subject: [bitcoin-dev] Blockchain verification flag (BIP draft)
Message-ID: <CAAS2fgRwfQNYxCmDPAnVudyAti9v8PPXQjxe9M13pmrFxKcSCQ@mail.gmail.com>

For discussion,

A significant fraction of hashrate currently mines blocks without
verifying them for a span of time after a new block shows up on the
network for economically rational reasons. This otherwise harmful
behavior can be made a beneficial to the whole network; but only if it
is communicated.

This BIP proposal suggests a communication channel and describes its
use and the motivations for it.  I wrote it in response to suggestions
that Bitcoin Core add explicit support for this kind of mining, which
could also implement best in class risk mitigations. I believe
signaling the behavior is a necessary component for risk mitigation
here.

-----------------------------------------------------------------

<pre>
  BIP: draft-maxwell-flagverify
  Title: Blockchain verification flag
  Author: Greg Maxwell <greg at xiph.org>
  Status: Draft
  Type: Standards Track
  Created: 2015-12-02
</pre>

==Abstract==

This BIP describes a flag that the authors of blocks can use to voluntarily
signal that they have completely validated the content of their
block and the blocks before it.

Correct use of this signaling is not enforced internally to the network
but if used it can act as a hint allowing more intelligent risk analysis.

If deployed and adhered to, this mechanism turns otherwise harmful
validation skipping by miners into a behavior which benefits the public.

==Summary==

The version field in a Bitcoin block header is a 32-bit signed integer.

The most significant bit (30) of the block version is defined to signal that
the author of the block has validated the whole chain up to and including the
content of the block.

Conforming miners MUST NOT set this flag when they have not completely
validated the prior block(s) or the content of their own block.  Miners
should continue to try to minimize the amount of time spent mining
on a non-validated chain.  Blocks which extend an invalid chain will
continue to be rejected and ultimately orphaned as validation catches up.

It is recommended, but not required, that miners also not set the flag on blocks
created by the same device which created the block immediately prior.  This
will reduce the incorrect implication of independent validation when the two
most recent blocks are both the product of the same, single, faulty system.

The set state for the bit is defined as verified so that that
un(der)maintained systems do not falsely signal validation.

Non-verifying clients of the network may check this bit (e.g. checking
that the version is >= 1073741824) and use it as an input to their risk
modeling.  It is recommended that once this BIP is widely accepted by the
network that non-full-node wallets refrain from counting confirmations on
blocks where the bit is not set.

The authors of non-verifying clients should keep in mind that this flag
is only correct with the cooperation of the block author, and even then
a validating miner may still accidentally accept or produce an invalid
block due to faulty hardware or software.  Additionally, any miner which
correctly uses this flag could stop doing so at any time, and might
do so intentionally in order to increase the effectiveness of an attack.
As a result of misunderstanding, misconfiguration, laziness, or other
human factors some miners may falsely set the flag.  Because invalid
blocks are rare it may take a long time to detect misuse of the flag.

As such, the accuracy of this field MUST NOT be strongly relied upon.

Especially due to the non-enforceability of the flag, the user community
should keep in mind that both setting the flag correctly and mining
without verification (for brief periods of time) are healthy for the
network.  If participants are punished for following this specification
they will simply lie, and its utility will be diminished.

==Motivation==

Some applications of the Bitcoin system such as thin-client wallets make
a strong assumption that all the authors of the blocks have faithfully
verified the blockchain.  Because many of these applications also take
irreversible actions based on only one or two confirmations and the time
between blocks is often very short, these clients are vulnerable to
even small and short-duration violations of this assumption.

Processing and propagation delays resulting from increased transaction
load contribute to block orphaning when multiple blocks are found at
close to the same time. This has caused some miners to work on extending
the chain with the most proof-of-work prior to validating the latest
block(s).

Although this validation skipping undermines the security assumptions
of thin clients, it also has a beneficial effect: these delays also
make the mining process unfair and cause increased rewards for the
largest miners relative to other miners, resulting in a centralization
pressure.  Deferring validation can reduce this pressure and improve
the security of the Bitcoin system long term.

This BIP seeks to mitigate the harm of breaking the thin client
assumption by allowing miners to efficiently provide additional
information on their level of validation.  By doing so the
network can take advantage of the benefits of bypassed
validation with minimal collateral damage.

==Deployment==

Because there is no consensus enforced behavior there is no special
deployment strategy required.  [BIP 9 will need to be updated.]

==Credits==

Thanks goes to Jeremy Rubin for his two-phase mining suggestion
which inspired this simplified proposal.

==Copyright==

This document is placed in the public domain.

From jannes.faber at gmail.com  Fri Dec  4 12:44:52 2015
From: jannes.faber at gmail.com (Jannes Faber)
Date: Fri, 4 Dec 2015 13:44:52 +0100
Subject: [bitcoin-dev] Blockchain verification flag (BIP draft)
In-Reply-To: <CAAS2fgRwfQNYxCmDPAnVudyAti9v8PPXQjxe9M13pmrFxKcSCQ@mail.gmail.com>
References: <CAAS2fgRwfQNYxCmDPAnVudyAti9v8PPXQjxe9M13pmrFxKcSCQ@mail.gmail.com>
Message-ID: <CABeL=0gWD8Nvp=j7OWKMVeVYH5NA-TBox1UTbyWxmVBf2nSJfQ@mail.gmail.com>

1) (I would assume this is already current default behaviour, but just in
case.) Would it not make sense to *never* send a blockheader to an SPV
client unless the node itself fully validated that block? Regardless of who
mined the block and whether this verification flag has been set or not.

2) Besides having your verification flag in the block, would it not also
make sense to have such a flag in the P2P protocol when blocks (or headers)
are communicated? That way a node could simply do some quick sanity checks
(difficulty as anti-DOS) on an incoming block and then immediately
propagate it to the next (non-SPV) node, but with a flag "Looks good, but I
haven't fully validated it myself, so please don't blame me". And if the
block does turn out to be invalid, the node does not get banned if it was
honest about it.

3) With the above implemented, I can imagine miners running 2 (or more)
nodes side by side, one of them doesn't validate in order to reduce latency
and orphan rates, but the other one does validate and quickly signals the
first one if there's a problem. Both nodes don't necessarily need to be in
the same network or even on the same side of the Great Firewall. Of course
they would be whitelisting each other for trust, or the signal would need
to include some sort of proof.

This probably has been suggested many times already, sorry if this is a
dumb idea.

--
Jannes

On 4 December 2015 at 09:26, Gregory Maxwell via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> For discussion,
>
> A significant fraction of hashrate currently mines blocks without
> verifying them for a span of time after a new block shows up on the
> network for economically rational reasons. This otherwise harmful
> behavior can be made a beneficial to the whole network; but only if it
> is communicated.
>
> This BIP proposal suggests a communication channel and describes its
> use and the motivations for it.  I wrote it in response to suggestions
> that Bitcoin Core add explicit support for this kind of mining, which
> could also implement best in class risk mitigations. I believe
> signaling the behavior is a necessary component for risk mitigation
> here.
>
> -----------------------------------------------------------------
>
> <pre>
>   BIP: draft-maxwell-flagverify
>   Title: Blockchain verification flag
>   Author: Greg Maxwell <greg at xiph.org>
>   Status: Draft
>   Type: Standards Track
>   Created: 2015-12-02
> </pre>
>
> ==Abstract==
>
> This BIP describes a flag that the authors of blocks can use to voluntarily
> signal that they have completely validated the content of their
> block and the blocks before it.
>
> Correct use of this signaling is not enforced internally to the network
> but if used it can act as a hint allowing more intelligent risk analysis.
>
> If deployed and adhered to, this mechanism turns otherwise harmful
> validation skipping by miners into a behavior which benefits the public.
>
> ==Summary==
>
> The version field in a Bitcoin block header is a 32-bit signed integer.
>
> The most significant bit (30) of the block version is defined to signal
> that
> the author of the block has validated the whole chain up to and including
> the
> content of the block.
>
> Conforming miners MUST NOT set this flag when they have not completely
> validated the prior block(s) or the content of their own block.  Miners
> should continue to try to minimize the amount of time spent mining
> on a non-validated chain.  Blocks which extend an invalid chain will
> continue to be rejected and ultimately orphaned as validation catches up.
>
> It is recommended, but not required, that miners also not set the flag on
> blocks
> created by the same device which created the block immediately prior.  This
> will reduce the incorrect implication of independent validation when the
> two
> most recent blocks are both the product of the same, single, faulty system.
>
> The set state for the bit is defined as verified so that that
> un(der)maintained systems do not falsely signal validation.
>
> Non-verifying clients of the network may check this bit (e.g. checking
> that the version is >= 1073741824) and use it as an input to their risk
> modeling.  It is recommended that once this BIP is widely accepted by the
> network that non-full-node wallets refrain from counting confirmations on
> blocks where the bit is not set.
>
> The authors of non-verifying clients should keep in mind that this flag
> is only correct with the cooperation of the block author, and even then
> a validating miner may still accidentally accept or produce an invalid
> block due to faulty hardware or software.  Additionally, any miner which
> correctly uses this flag could stop doing so at any time, and might
> do so intentionally in order to increase the effectiveness of an attack.
> As a result of misunderstanding, misconfiguration, laziness, or other
> human factors some miners may falsely set the flag.  Because invalid
> blocks are rare it may take a long time to detect misuse of the flag.
>
> As such, the accuracy of this field MUST NOT be strongly relied upon.
>
> Especially due to the non-enforceability of the flag, the user community
> should keep in mind that both setting the flag correctly and mining
> without verification (for brief periods of time) are healthy for the
> network.  If participants are punished for following this specification
> they will simply lie, and its utility will be diminished.
>
> ==Motivation==
>
> Some applications of the Bitcoin system such as thin-client wallets make
> a strong assumption that all the authors of the blocks have faithfully
> verified the blockchain.  Because many of these applications also take
> irreversible actions based on only one or two confirmations and the time
> between blocks is often very short, these clients are vulnerable to
> even small and short-duration violations of this assumption.
>
> Processing and propagation delays resulting from increased transaction
> load contribute to block orphaning when multiple blocks are found at
> close to the same time. This has caused some miners to work on extending
> the chain with the most proof-of-work prior to validating the latest
> block(s).
>
> Although this validation skipping undermines the security assumptions
> of thin clients, it also has a beneficial effect: these delays also
> make the mining process unfair and cause increased rewards for the
> largest miners relative to other miners, resulting in a centralization
> pressure.  Deferring validation can reduce this pressure and improve
> the security of the Bitcoin system long term.
>
> This BIP seeks to mitigate the harm of breaking the thin client
> assumption by allowing miners to efficiently provide additional
> information on their level of validation.  By doing so the
> network can take advantage of the benefits of bypassed
> validation with minimal collateral damage.
>
> ==Deployment==
>
> Because there is no consensus enforced behavior there is no special
> deployment strategy required.  [BIP 9 will need to be updated.]
>
> ==Credits==
>
> Thanks goes to Jeremy Rubin for his two-phase mining suggestion
> which inspired this simplified proposal.
>
> ==Copyright==
>
> This document is placed in the public domain.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151204/bbdcc1da/attachment.html>

From lf-lists at mattcorallo.com  Fri Dec  4 13:30:33 2015
From: lf-lists at mattcorallo.com (Matt Corallo)
Date: Fri, 04 Dec 2015 13:30:33 +0000
Subject: [bitcoin-dev] [BIP Draft] Datastream compression of Blocks and
	Transactions
In-Reply-To: <565F787C.3080604@gmail.com>
References: <565CD7D8.3070102@gmail.com>
	<90EF4E6C-9A71-4A35-A938-EAFC1A24DD24@mattcorallo.com>
	<CAPkFh0t9SwVOLrPnL7z80s-Rriezhqxn_3vXKYRxr6JVGNiUZQ@mail.gmail.com>
	<565F5193.1070802@gmail.com> <565F6F73.5050906@mattcorallo.com>
	<565F787C.3080604@gmail.com>
Message-ID: <30817321-B625-44EB-9459-31DFE7C47E5A@mattcorallo.com>



On December 3, 2015 7:02:20 AM GMT+08:00, Peter Tschipper <peter.tschipper at gmail.com> wrote:
>On 02/12/2015 2:23 PM, Matt Corallo wrote:
>> My issue is more that its additional complexity and attack surface,
>> and for a very minor gain 
>What is a minor gain?  15 to 27% compression sounds good to me and the
>larger the data the better the compression.  And although there is a
>decent peformance gain in proportion to the % of compression, the
>original motivation of the BIP was to reduce bandwidth for users in
>regions where they are subject to caps.

Ok. It wasn't clear to me that you weren't also claiming at latency reduction as a result. In any case, the point I was making is that the p2p protocol isn't for every use-case. Indeed, I agree (as noted previously) that we should support people who have very restrictive data usage limits, but I don't think we need to do this in the p2p protocol. Considering we're in desperate need of more ways to sync, supporting syncing over slow and/or very restrictive connections is something maybe better addressed by a sync-over-http-via-cdn protocol than the p2p protocol.

>> which should disappear with further optimization elsewhere 
>Why would the benefit of compressing data disappear with further
>optimizations elsewhere, I'm not following you?.  The compression of
>data mainly has benefit in the sending of packets over the network.  I
>would think the performance gain would be cumulative.  Why would this
>go
>away by optimizing elsewhere?

My point is that, with limited further optimization, and especially after the first hundred thousand blocks, block download should nearly never be the thing limiting IBD speed.

>> and less that we absolutely shouldn't add compression because we're
>> definitely gonna have issues.
>It's not that difficult to add compression.  Even if there was an
>issue,
>the compression feature can be completely turned off. 

No matter how easily you can implement something, complexity always has cost. This is especially true in complicated, incredibly security critical applications exposed to the internet.

>>
>> On 12/02/15 20:16, Peter Tschipper via bitcoin-dev wrote:
>>> Building a compressor from scratch may yeild some better compression
>>> ratios, or not, but having trust and faith in whether it will stand
>up
>>> against attack vectors another matter.  LZO has been around for 20
>years
>>> with very few problems and no current issues.  Maybe something
>better
>>> can be built, but when and how much testing will need to be done
>before
>>> it can be trusted?  Right now there is something that provides a
>benefit
>>> and in the future if something better is found it's not that
>difficult
>>> to add it.  We could easily support multiple compression libraries.
>>>
>>>
>>> On 02/12/2015 10:57 AM, Emin G?n Sirer wrote:
>>>> Thanks Peter for the careful, quantitative work.
>>>>
>>>> I want to bring one additional issue to everyone's consideration,
>>>> related to the choice of the Lempel-Ziv family of compressors.
>>>>
>>>> While I'm not familiar with every single compression engine tested,
>>>> the Lempel-Ziv family of compressors are generally based on
>>>> "compression tables." Essentially, they assign a short unique
>number
>>>> to every new subsequence they encounter, and when they re-encounter
>a
>>>> sequence like "ab" in "abcdfdcdabcdfabcdf" they replace it with
>that
>>>> short integer (say, in this case, 9-bit constant 256). So this
>example
>>>> sequence may turn into "abcdfd<258 for cd><256 for ab><258 for
>>>> cd>f<261 for abc><259 for df>" which is slightly shorter than the
>>>> original (I'm doing this off the top of my head so the counts may
>be
>>>> off, but it's meant to be illustrative). Note that the sequence
>"abc"
>>>> got added into the table only after it was encountered twice in the
>>>> input.
>>>>
>>>> This is nice and generic and works well for English text where
>certain
>>>> letter sequences (e.g. "it" "th" "the" "this" "are" "there" etc)
>are
>>>> repeated often, but it is nowhere as compact as it could possibly
>be
>>>> for mostly binary data -- there are opportunities for much better
>>>> compression, made possible by the structured reuse of certain byte
>>>> sequences in the Bitcoin wire protocol.
>>>>
>>>> On a Bitcoin wire connection, we might see several related
>>>> transactions reorganizing cash in a set of addresses, and
>therefore,
>>>> several reuses of a 20-byte address. Or we might see a 200-byte
>>>> transaction get transmitted, followed by the same transaction,
>>>> repeated in a block. Ideally, we'd learn the sequence that may be
>>>> repeated later on, all at once (e.g. a Bitcoin address or a
>>>> transaction), and replace it with a short number, referring back to
>>>> the long sequence. In the example above, if we knew that "abcdf"
>was a
>>>> UNIT that would likely be repeated, we would put it into the
>>>> compression table as a whole, instead of relying on repetition to
>get
>>>> it into the table one extra byte at a time. That may let us
>compress
>>>> the original sequence down to "abcdfd<257 for cd><256 for
>abcdf><256
>>>> for abcdf>" from the get go.
>>>>
>>>> Yet the LZ variants I know of will need to see a 200-byte sequence
>>>> repeated **199 times** in order to develop a single, reusable,
>>>> 200-byte long subsequence in the compression table.
>>>>
>>>> So, a Bitcoin-specific compressor can perhaps do significantly
>better,
>>>> but is it a good idea? Let's argue both sides.
>>>>
>>>> Cons:
>>>>
>>>> On the one hand, Bitcoin-specific compressors will be closely tied
>to
>>>> the contents of messages, which might make it difficult to change
>the
>>>> wire format later on -- changes to the wire format may need
>>>> corresponding changes to the compressor.  If the compressor cannot
>be
>>>> implemented cleanly, then the protocol-agnostic, off-the-shelf
>>>> compressors have a maintainability edge, which comes at the expense
>of
>>>> the compression ratio.
>>>>
>>>> Another argument is that compression algorithms of any kind should
>be
>>>> tested thoroughly before inclusion, and brand new code may lack the
>>>> maturity required. While this argument has some merit, all outputs
>are
>>>> verified separately later on during processing, so
>>>> compression/decompression errors can potentially be detected. If
>the
>>>> compressor/decompressor can be structured in a way that isolates
>>>> bitcoind from failure (e.g. as a separate process for starters),
>this
>>>> concern can be remedied.
>>>>
>>>> Pros:
>>>>
>>>> The nature of LZ compressors leads me to believe that much higher
>>>> compression ratios are possible by building a custom, Bitcoin-aware
>>>> compressor. If I had to guess, I would venture that compression
>ratios
>>>> of 2X or more are possible in some cases. In some sense, the "O(1)
>>>> block propagation" idea that Gavin proposed a while ago can be seen
>as
>>>> extreme example of a Bitcoin-specific compressor, albeit one that
>>>> constrains the order of transactions in a block.
>>>>
>>>> Compression can buy us some additional throughput at zero cost,
>modulo
>>>> code complexity.
>>>> Given the amount of acrimonious debate over the block size we have
>all
>>>> had to endure, it seems
>>>> criminal to leave potentially free improvements on the table. Even
>if
>>>> the resulting code is
>>>> deemed too complex to include in the production client right now,
>it
>>>> would be good to understand
>>>> the potential for improvement.
>>>>
>>>> How to Do It
>>>>
>>>> If we want to compress Bitcoin, a programming challenge/contest
>would
>>>> be one of the best ways to find the best possible, Bitcoin-specific
>>>> compressor. This is the kind of self-contained exercise that bright
>>>> young hackers love to tackle. It'd bring in new programmers into
>the
>>>> ecosystem, and many of us would love to discover the limits of
>>>> compressibility for Bitcoin bits on a wire. And the results would
>be
>>>> interesting even if the final compression engine is not enabled by
>>>> default, or not even merged.
>>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev at lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>>


From gavinandresen at gmail.com  Fri Dec  4 17:34:27 2015
From: gavinandresen at gmail.com (Gavin Andresen)
Date: Fri, 4 Dec 2015 12:34:27 -0500
Subject: [bitcoin-dev] Blockchain verification flag (BIP draft)
In-Reply-To: <CAAS2fgRwfQNYxCmDPAnVudyAti9v8PPXQjxe9M13pmrFxKcSCQ@mail.gmail.com>
References: <CAAS2fgRwfQNYxCmDPAnVudyAti9v8PPXQjxe9M13pmrFxKcSCQ@mail.gmail.com>
Message-ID: <CABsx9T1vBRMYm6rLuqzvOxD0eABE4saF44JzZjMF3iUU==Nz0Q@mail.gmail.com>

Overall, good idea.

Is there a write-up somewhere describing in detail the 'accidental selfish
mining' problem that this mitigates? I think a link in the BIP to a fuller
description of the problem and how validation-skipping makes it go away
would be helpful.

RE: which bit to use:  the draft versionbits BIP and BIP101 use bit 30; to
avoid confusion, I think it would be better to use bit 0.

I agree with Jannes Faber, behavior with respect to SPV clients should be
to only tell them about fully validated headers. And I also agree that
immediately relaying full-proof-of-work blocks before validation (with an
indication that they haven't been fully validated) is a good idea, but that
discussion didn't reach consensus when I brought it up two years ago (
https://github.com/bitcoin/bitcoin/pull/3580).


-- 
--
Gavin Andresen
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151204/ad61bf07/attachment.html>

From thomas.kerin at gmail.com  Fri Dec  4 16:46:34 2015
From: thomas.kerin at gmail.com (Thomas Kerin)
Date: Fri, 4 Dec 2015 16:46:34 +0000
Subject: [bitcoin-dev] Blockchain verification flag (BIP draft)
In-Reply-To: <CABeL=0gWD8Nvp=j7OWKMVeVYH5NA-TBox1UTbyWxmVBf2nSJfQ@mail.gmail.com>
References: <CAAS2fgRwfQNYxCmDPAnVudyAti9v8PPXQjxe9M13pmrFxKcSCQ@mail.gmail.com>
	<CABeL=0gWD8Nvp=j7OWKMVeVYH5NA-TBox1UTbyWxmVBf2nSJfQ@mail.gmail.com>
Message-ID: <5661C36A.9090509@gmail.com>

1. Not relaying can cause problems. Gossip networks operate by
propagating new information (like a single new header), and refuse to
relay information if it's obviously invalid.

>From the POV of a full node, which will normally hear about the header
first, there's no point to not telling peers about this information.
It's likely in the interest of SPV wallets to hear about EVERY
contending chain, so they can go about their business deciding which is
correct.


2. The only difference between a block and it's header is the list of
transactions. There isn't anywhere else to put the flag but the header's
version. Which is good, because clients usually receive headers first.


3. "Signal would need to include some sort of proof" That's not the
point of this BIP. You can't prove the miner has or hasn't verified the
chain. What purpose would it even serve? If clients accepted this
'proof', they might ignore blocks they should pay attention to.

The BIP doesn't involve proof at all, it's just an indicator you can
chose to use or ignore.


On 04/12/15 12:44, Jannes Faber via bitcoin-dev wrote:
> nodes side by side, one of them doesn't validate in order to reduce latency


From rusty at rustcorp.com.au  Fri Dec  4 22:43:16 2015
From: rusty at rustcorp.com.au (Rusty Russell)
Date: Sat, 05 Dec 2015 09:13:16 +1030
Subject: [bitcoin-dev] Blockchain verification flag (BIP draft)
In-Reply-To: <CABsx9T1vBRMYm6rLuqzvOxD0eABE4saF44JzZjMF3iUU==Nz0Q@mail.gmail.com>
References: <CAAS2fgRwfQNYxCmDPAnVudyAti9v8PPXQjxe9M13pmrFxKcSCQ@mail.gmail.com>
	<CABsx9T1vBRMYm6rLuqzvOxD0eABE4saF44JzZjMF3iUU==Nz0Q@mail.gmail.com>
Message-ID: <871tb16diz.fsf@rustcorp.com.au>

Gavin Andresen via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>
writes:
> Overall, good idea.
>
> Is there a write-up somewhere describing in detail the 'accidental selfish
> mining' problem that this mitigates? I think a link in the BIP to a fuller
> description of the problem and how validation-skipping makes it go away
> would be helpful.
>
> RE: which bit to use:  the draft versionbits BIP and BIP101 use bit 30; to
> avoid confusion, I think it would be better to use bit 0.

Yes, BIP9 need to be adjusted (setting bit 30 means BIP9 counts it as a
vote against all softforks).  BIP101 uses bits 0,1,2 AFAICT, so perhaps
start from the other end and use bit 29?  We can bikeshed that later
though...

> I agree with Jannes Faber, behavior with respect to SPV clients should be
> to only tell them about fully validated headers.

A delicate balance.  If we penalize these blocks too much, it's
disincentive to set the bit.  Fortunately it's easy for SPV clients to
decide for themselves, I think?

Cheers,
Rusty.

From greg at xiph.org  Sun Dec  6 05:13:15 2015
From: greg at xiph.org (Gregory Maxwell)
Date: Sun, 6 Dec 2015 05:13:15 +0000
Subject: [bitcoin-dev] Blockchain verification flag (BIP draft)
In-Reply-To: <871tb16diz.fsf@rustcorp.com.au>
References: <CAAS2fgRwfQNYxCmDPAnVudyAti9v8PPXQjxe9M13pmrFxKcSCQ@mail.gmail.com>
	<CABsx9T1vBRMYm6rLuqzvOxD0eABE4saF44JzZjMF3iUU==Nz0Q@mail.gmail.com>
	<871tb16diz.fsf@rustcorp.com.au>
Message-ID: <CAAS2fgRMi3KKKUW_7eunG9cprOLtz8yrEtA+8ChiCgninjMMZw@mail.gmail.com>

On Fri, Dec 4, 2015 at 10:43 PM, Rusty Russell <rusty at rustcorp.com.au> wrote:
>> I agree with Jannes Faber, behavior with respect to SPV clients should be
>> to only tell them about fully validated headers.
>
> A delicate balance.  If we penalize these blocks too much, it's
> disincentive to set the bit.  Fortunately it's easy for SPV clients to
> decide for themselves, I think?

I think this is orthogonal: You should only tell SPV clients* about
blocks you've fully validated yourself.  The bit in the header doesn't
matter with respect to that. (Effectively, the wallet risk model gets
as input the fact that they got given the block in the first place as
well as the flag where the miner said they were validating things or
not.)

Whatever the ideal behavior is for network nodes towards lite clients;
it's insanely cheap to just spin up a lot of 'nodes' that have
arbitrary behavior; so it shouldn't be relied on too much; but
absolutely they should be filtering to things they've verified
themselves... unlike the mining case, there is no reason not to.

[Specific attacks to consider: You get a broken miner to include both
your payment, and some invalid transaction. Other miners extend it
without verifying. To avoid the fact that nodes sensibly filter
invalid blocks from their lite clients, you simply just run a lot of
'nodes' so that virtually every lite client has a connection to you]

(More specifically, peers should be able to specify that they want to
know about pre-validated blocks and you should be able to fetch blocks
from them which haven't been validated... but no one should get fed
unverified blocks by surprise.)

From james.hilliard1 at gmail.com  Sun Dec  6 02:47:01 2015
From: james.hilliard1 at gmail.com (James Hilliard)
Date: Sat, 5 Dec 2015 20:47:01 -0600
Subject: [bitcoin-dev] Blockchain verification flag (BIP draft)
In-Reply-To: <871tb16diz.fsf@rustcorp.com.au>
References: <CAAS2fgRwfQNYxCmDPAnVudyAti9v8PPXQjxe9M13pmrFxKcSCQ@mail.gmail.com>
	<CABsx9T1vBRMYm6rLuqzvOxD0eABE4saF44JzZjMF3iUU==Nz0Q@mail.gmail.com>
	<871tb16diz.fsf@rustcorp.com.au>
Message-ID: <CADvTj4q_Ubrk6a15Q4uB3MT0PHGnvZt3yPMOWCdPhWf_WGH-tA@mail.gmail.com>

I think something that anyone who isn't validating should be aware of is
that cgminer(which powers the vast majority of the current mining network)
doesn't allow for a pool to revert to mining on the previous block due to
the way chain tracking is implemented.

https://github.com/ckolivas/cgminer/blob/master/cgminer.c#L4727

On Fri, Dec 4, 2015 at 4:43 PM, Rusty Russell via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Gavin Andresen via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>
> writes:
> > Overall, good idea.
> >
> > Is there a write-up somewhere describing in detail the 'accidental
> selfish
> > mining' problem that this mitigates? I think a link in the BIP to a
> fuller
> > description of the problem and how validation-skipping makes it go away
> > would be helpful.
> >
> > RE: which bit to use:  the draft versionbits BIP and BIP101 use bit 30;
> to
> > avoid confusion, I think it would be better to use bit 0.
>
> Yes, BIP9 need to be adjusted (setting bit 30 means BIP9 counts it as a
> vote against all softforks).  BIP101 uses bits 0,1,2 AFAICT, so perhaps
> start from the other end and use bit 29?  We can bikeshed that later
> though...
>
> > I agree with Jannes Faber, behavior with respect to SPV clients should be
> > to only tell them about fully validated headers.
>
> A delicate balance.  If we penalize these blocks too much, it's
> disincentive to set the bit.  Fortunately it's easy for SPV clients to
> decide for themselves, I think?
>
> Cheers,
> Rusty.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151205/4ed6a987/attachment.html>

From greg at xiph.org  Sun Dec  6 06:26:15 2015
From: greg at xiph.org (Gregory Maxwell)
Date: Sun, 6 Dec 2015 06:26:15 +0000
Subject: [bitcoin-dev] Blockchain verification flag (BIP draft)
In-Reply-To: <CADvTj4q_Ubrk6a15Q4uB3MT0PHGnvZt3yPMOWCdPhWf_WGH-tA@mail.gmail.com>
References: <CAAS2fgRwfQNYxCmDPAnVudyAti9v8PPXQjxe9M13pmrFxKcSCQ@mail.gmail.com>
	<CABsx9T1vBRMYm6rLuqzvOxD0eABE4saF44JzZjMF3iUU==Nz0Q@mail.gmail.com>
	<871tb16diz.fsf@rustcorp.com.au>
	<CADvTj4q_Ubrk6a15Q4uB3MT0PHGnvZt3yPMOWCdPhWf_WGH-tA@mail.gmail.com>
Message-ID: <CAAS2fgQQ+ZTUXXXRJWP-RjP+ehsUTrxa7Vy-QrZ-3Q6Offz=5A@mail.gmail.com>

On Sun, Dec 6, 2015 at 2:47 AM, James Hilliard via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> I think something that anyone who isn't validating should be aware of is
> that cgminer(which powers the vast majority of the current mining network)
> doesn't allow for a pool to revert to mining on the previous block due to
> the way chain tracking is implemented.

An interesting potential use for the flag suggested in this draft
would be allowing non-monotone mining for non-verified blocks.

I could add a recommendation for that as well.

From kanzure at gmail.com  Mon Dec  7 06:50:23 2015
From: kanzure at gmail.com (Bryan Bishop)
Date: Mon, 7 Dec 2015 00:50:23 -0600
Subject: [bitcoin-dev] Some transcripts from the Scaling Bitcoin workshops
Message-ID: <CABaSBaxB-chA=QxYGZvDo9JyGiBDrOpQnOBJD8GFiBEGKffrEw@mail.gmail.com>

Hey while I was listening to the talks I also typed most of the words down.

Here are some talks from the Hong Kong workshop:
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/bip99-and-uncontroversial-hard-forks/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/fungibility-and-scalability/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/zero-knowledge-proofs-for-bitcoin-scalability-and-beyond/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/security-assumptions/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/in-adversarial-environments-blockchains-dont-scale/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/why-miners-will-not-voluntarily-individually-produce-smaller-blocks/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/invertible-bloom-lookup-tables-and-weak-block-propagation-performance/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/bip101-block-propagation-data-from-testnet/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/segregated-witness-and-its-impact-on-scalability/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/overview-of-bips-necessary-for-lightning/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/network-topologies-and-their-scalability-implications-on-decentralized-off-chain-networks/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/a-bevy-of-block-size-proposals-bip100-bip102-and-more/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/a-flexible-limit-trading-subsidy-for-larger-blocks/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/validation-cost-metric/

Also, here are some talks from the Montreal workshop:
http://diyhpl.us/wiki/transcripts/scalingbitcoin/alternatives-to-block-size-as-aggregate-resource-limits/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/bitcoin-block-propagation-iblt-rusty-russell/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/bitcoin-failure-modes-and-the-role-of-the-lightning-network/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/bitcoin-load-spike-simulation/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/block-synchronization-time/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/coinscope-andrew-miller/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/competitive-fee-market-urgency/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/issues-impacting-block-size-proposals/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/overview-of-security-concerns/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/relay-network/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/reworking-bitcoin-core-p2p-code-for-robustness-and-event-driven/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/sharding-the-blockchain/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/transaction-fee-estimation/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/validation-costs/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/roundgroup-roundup-1/
http://diyhpl.us/wiki/transcripts/scalingbitcoin/roundgroup-roundup-2/

These are not always exact transcripts because I am typing while I am
listening, thus there are mistakes including typos and listening errors, so
please keep this discrepancy in mind between what's said and what's typed.

- Bryan
http://heybryan.org/
1 512 203 0507
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151207/0d5da746/attachment.html>

From johnathan at corganlabs.com  Mon Dec  7 07:20:19 2015
From: johnathan at corganlabs.com (Johnathan Corgan)
Date: Sun, 6 Dec 2015 23:20:19 -0800
Subject: [bitcoin-dev] Some transcripts from the Scaling Bitcoin
	workshops
In-Reply-To: <CABaSBaxB-chA=QxYGZvDo9JyGiBDrOpQnOBJD8GFiBEGKffrEw@mail.gmail.com>
References: <CABaSBaxB-chA=QxYGZvDo9JyGiBDrOpQnOBJD8GFiBEGKffrEw@mail.gmail.com>
Message-ID: <CALOxbZu467StSj4HLJPonP6=L25kT4n1b_cTF+azBQb43By_Yg@mail.gmail.com>

On Sun, Dec 6, 2015 at 10:50 PM, Bryan Bishop via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:


> Hey while I was listening to the talks I also typed most of the words down
>

?This was clearly a lot of work...thanks again.?

-- 
Johnathan Corgan
Corgan Labs - SDR Training and Development Services
http://corganlabs.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151206/426750af/attachment.html>

From pindar.wong at gmail.com  Mon Dec  7 07:21:58 2015
From: pindar.wong at gmail.com (Pindar Wong)
Date: Mon, 7 Dec 2015 15:21:58 +0800
Subject: [bitcoin-dev] Some transcripts from the Scaling Bitcoin
	workshops
In-Reply-To: <CABaSBaxB-chA=QxYGZvDo9JyGiBDrOpQnOBJD8GFiBEGKffrEw@mail.gmail.com>
References: <CABaSBaxB-chA=QxYGZvDo9JyGiBDrOpQnOBJD8GFiBEGKffrEw@mail.gmail.com>
Message-ID: <CAM7BtUqSPj7ynuj9QMVJDE6=rzuC3Rphk2cbfrGOBi6-jCBSFA@mail.gmail.com>

Awesome...Thank you Bryan!

P.

On Sunday, December 6, 2015, Bryan Bishop via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Hey while I was listening to the talks I also typed most of the words down.
>
> Here are some talks from the Hong Kong workshop:
>
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/bip99-and-uncontroversial-hard-forks/
>
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/fungibility-and-scalability/
>
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/zero-knowledge-proofs-for-bitcoin-scalability-and-beyond/
>
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/security-assumptions/
>
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/in-adversarial-environments-blockchains-dont-scale/
>
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/why-miners-will-not-voluntarily-individually-produce-smaller-blocks/
>
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/invertible-bloom-lookup-tables-and-weak-block-propagation-performance/
>
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/bip101-block-propagation-data-from-testnet/
>
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/segregated-witness-and-its-impact-on-scalability/
>
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/overview-of-bips-necessary-for-lightning/
>
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/network-topologies-and-their-scalability-implications-on-decentralized-off-chain-networks/
>
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/a-bevy-of-block-size-proposals-bip100-bip102-and-more/
>
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/a-flexible-limit-trading-subsidy-for-larger-blocks/
>
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/validation-cost-metric/
>
> Also, here are some talks from the Montreal workshop:
>
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/alternatives-to-block-size-as-aggregate-resource-limits/
>
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/bitcoin-block-propagation-iblt-rusty-russell/
>
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/bitcoin-failure-modes-and-the-role-of-the-lightning-network/
>
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/bitcoin-load-spike-simulation/
>
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/block-synchronization-time/
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/coinscope-andrew-miller/
>
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/competitive-fee-market-urgency/
>
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/issues-impacting-block-size-proposals/
>
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/overview-of-security-concerns/
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/relay-network/
>
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/reworking-bitcoin-core-p2p-code-for-robustness-and-event-driven/
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/sharding-the-blockchain/
>
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/transaction-fee-estimation/
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/validation-costs/
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/roundgroup-roundup-1/
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/roundgroup-roundup-2/
>
> These are not always exact transcripts because I am typing while I am
> listening, thus there are mistakes including typos and listening errors, so
> please keep this discrepancy in mind between what's said and what's typed.
>
> - Bryan
> http://heybryan.org/
> 1 512 203 0507
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151207/a2df4e9f/attachment-0001.html>

From greg at xiph.org  Mon Dec  7 22:02:17 2015
From: greg at xiph.org (Gregory Maxwell)
Date: Mon, 7 Dec 2015 22:02:17 +0000
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
Message-ID: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>

The Scaling Bitcoin Workshop in HK is just wrapping up. Many fascinating
proposals were presented. I think this would be a good time to share my
view of the near term arc for capacity increases in the Bitcoin system. I
believe we?re in a fantastic place right now and that the community
is ready to deliver on a clear forward path with a shared vision that
addresses the needs of the system while upholding its values.

I think it?s important to first clearly express some of the relevant
principles that I think should guide the ongoing development of the
Bitcoin system.

Bitcoin is P2P electronic cash that is valuable over legacy systems
because of the monetary autonomy it brings to its users through
decentralization. Bitcoin seeks to address the root problem with
conventional currency: all the trust that's required to make it work--

-- Not that justified trust is a bad thing, but trust makes systems
brittle, opaque, and costly to operate. Trust failures result in systemic
collapses, trust curation creates inequality and monopoly lock-in, and
naturally arising trust choke-points can be abused to deny access to
due process. Through the use of cryptographic proof and decentralized
networks Bitcoin minimizes and replaces these trust costs.

With the available technology, there are fundamental trade-offs between
scale and decentralization. If the system is too costly people will be
forced to trust third parties rather than independently enforcing the
system's rules. If the Bitcoin blockchain?s resource usage, relative
to the available technology, is too great, Bitcoin loses its competitive
advantages compared to legacy systems because validation will be too
costly (pricing out many users), forcing trust back into the system.
If capacity is too low and our methods of transacting too inefficient,
access to the chain for dispute resolution will be too costly, again
pushing trust back into the system.

Since Bitcoin is an electronic cash, it _isn't_ a generic database;
the demand for cheap highly-replicated perpetual storage is unbounded,
and Bitcoin cannot and will not satisfy that demand for non-ecash
(non-Bitcoin) usage, and there is no shame in that. Fortunately, Bitcoin
can interoperate with other systems that address other applications,
and--with luck and hard work--the Bitcoin system can and will satisfy
the world's demand for electronic cash.

Fortunately, a lot of great technology is in the works that make
navigating the trade-offs easier.

First up: after several years in the making Bitcoin Core has recently
merged libsecp256k1, which results in a huge increase in signature
validation performance. Combined with other recent work we're now getting
ConnectTip performance 7x higher in 0.12 than in prior versions. This
has been a long time coming, and without its anticipation and earlier
work such as headers-first I probably would have been arguing for a
block size decrease last year.  This improvement in the state of the
art for widely available production Bitcoin software sets a stage for
some capacity increases while still catching up on our decentralization
deficit. This shifts the bottlenecks off of CPU and more strongly onto
propagation latency and bandwidth.

Versionbits (BIP9) is approaching maturity and will allow the Bitcoin
network to have multiple in-flight soft-forks. Up until now we?ve had to
completely serialize soft-fork work, and also had no real way to handle
a soft-fork that was merged in core but rejected by the network. All
that is solved in BIP9, which should allow us to pick up the pace of
improvements in the network. It looks like versionbits will be ready
for use in the next soft-fork performed on the network.

The next thing is that, at Scaling Bitcoin Hong Kong, Pieter Wuille
presented on bringing Segregated Witness to Bitcoin. What is proposed
is a _soft-fork_ that increases Bitcoin's scalability and capacity by
reorganizing data in blocks to handle the signatures separately, and in
doing so takes them outside the scope of the current blocksize limit.

The particular proposal amounts to a 4MB blocksize increase at worst. The
separation allows new security models, such as skipping downloading data
you're not going to check and improved performance for lite clients
(especially ones with high privacy). The proposal also includes fraud
proofs which make violations of the Bitcoin system provable with a compact
proof. This completes the vision of "alerts" described in the "Simplified
Payment Verification" section of the Bitcoin whitepaper, and would make it
possible for lite clients to enforce all the rules of the system (under
a new strong assumption that they're not partitioned from someone who
would generate the proofs). The design has numerous other features like
making further enhancements safer and eliminating signature malleability
problems. If widely used this proposal gives a 2x capacity increase
(more if multisig is widely used), but most importantly it makes that
additional capacity--and future capacity beyond it--safer by increasing
efficiency and allowing more trade-offs (in particular, you can use much
less bandwidth in exchange for a strong non-partitioning assumption).

There is a working implementation (though it doesn't yet have the fraud
proofs) at https://github.com/sipa/bitcoin/commits/segwit

(Pieter's talk is at:  transcript:
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/segregated-witness-and-its-impact-on-scalability/
slides:
https://prezi.com/lyghixkrguao/segregated-witness-and-deploying-it-for-bitcoin/
Video: https://www.youtube.com/watch?v=fst1IK_mrng#t=36m )

I had good success deploying an earlier (hard-fork) version of segwit
in the Elements Alpha sidechain; the soft-fork segwit now proposed
is a second-generation design. And I think it's quite reasonable to
get this deployed in a relatively short time frame. The segwit design
calls for a future bitcoinj compatible hardfork to further increase its
efficiency--but it's not necessary to reap most of the benefits,and that
means it can happen on its own schedule and in a non-contentious manner.

Going beyond segwit, there has been some considerable activity brewing
around more efficient block relay.  There is a collection of proposals,
some stemming from a p2pool-inspired informal sketch of mine and some
independently invented, called "weak blocks", "thin blocks" or "soft
blocks".  These proposals build on top of efficient relay techniques
(like the relay network protocol or IBLT) and move virtually all the
transmission time of a block to before the block is found, eliminating
size from the orphan race calculation. We already desperately need this
at the current block sizes. These have not yet been implemented, but
fortunately the path appears clear. I've seen at least one more or less
complete specification, and I expect to see things running using this in a
few months. This tool will remove propagation latency from being a problem
in the absence of strategic behavior by miners.  Better understanding
their behavior when miners behave strategically is an open question.

Concurrently, there is a lot of activity ongoing related to
?non-bandwidth? scaling mechanisms. Non-bandwidth scaling mechanisms
are tools like transaction cut-through and bidirectional payment channels
which increase Bitcoin?s capacity and speed using clever smart contracts
rather than increased bandwidth. Critically, these approaches strike right
at the heart of the capacity vs autotomy trade-off, and may allow us to
achieve very high capacity and very high decentralization. CLTV (BIP65),
deployed a month ago and now active on the network, is very useful for
these techniques (essential for making hold-up refunds work); CSV (BIP68
/ BIP112) is in the pipeline for merge in core and making good progress
(and will likely be ready ahead of segwit). Further Bitcoin protocol
improvements for non-bandwidth scaling are in the works: Many of these
proposals really want anti-malleability fixes (which would be provided
by segwit), and there are checksig flag improvements already tendered and
more being worked on, which would be much easier to deploy with segwit. I
expect that within six months we could have considerably more features
ready for deployment to enable these techniques. Even without them I
believe we?ll be in an acceptable position with respect to capacity
in the near term, but it?s important to enable them for the future.

(http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/overview-of-bips-necessary-for-lightning
is a relevant talk for some of the wanted network features for Lightning,
a bidirectional payment channel proposal which many parties are working
on right now; other non-bandwidth improvements discussed in the past
include transaction cut-through, which I consider a must-read for the
basic intuition about how transaction capacity can be greater than
blockchain capacity: https://bitcointalk.org/index.php?topic=281848.0 ,
though there are many others.)

Further out, there are several proposals related to flex caps or
incentive-aligned dynamic block size controls based on allowing miners
to produce larger blocks at some cost. These proposals help preserve
the alignment of incentives between miners and general node operators,
and prevent defection between the miners from undermining the fee
market behavior that will eventually fund security. I think that right
now capacity is high enough and the needed capacity is low enough that
we don't immediately need these proposals, but they will be critically
important long term. I'm planning to help out and drive towards a more
concrete direction out of these proposals in the following months.

(Relevant talks include
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/a-flexible-limit-trading-subsidy-for-larger-blocks/
)

Finally--at some point the capacity increases from the above may not
be enough.  Delivery on relay improvements, segwit fraud proofs, dynamic
block size controls, and other advances in technology will reduce the risk
and therefore controversy around moderate block size increase proposals
(such as 2/4/8 rescaled to respect segwit's increase). Bitcoin will
be able to move forward with these increases when improvements and
understanding render their risks widely acceptable relative to the
risks of not deploying them. In Bitcoin Core we should keep patches
ready to implement them as the need and the will arises, to keep the
basic software engineering from being the limiting factor.

Our recent and current progress has well positioned the Bitcoin ecosystem
to handle its current capacity needs. I think the above sets out some
clear achievable milestones to continue to advance the art in Bitcoin
capacity while putting us in a good position for further improvement and
evolution.

TL;DR:  I propose we work immediately towards the segwit 4MB block
soft-fork which increases capacity and scalability, and recent speedups
and incoming relay improvements make segwit a reasonable risk. BIP9
and segwit will also make further improvements easier and faster to
deploy. We?ll continue to set the stage for non-bandwidth-increase-based
scaling, while building additional tools that would make bandwidth
increases safer long term. Further work will prepare Bitcoin for further
increases, which will become possible when justified, while also providing
the groundwork to make them justifiable.

Thanks for your time,

From kanzure at gmail.com  Mon Dec  7 22:54:07 2015
From: kanzure at gmail.com (Bryan Bishop)
Date: Mon, 7 Dec 2015 16:54:07 -0600
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
Message-ID: <CABaSBaybA8=R5HtrbEcmvpgyeOA8K3zJUEy55He57-hFkt5PHg@mail.gmail.com>

On Mon, Dec 7, 2015 at 4:02 PM, Gregory Maxwell wrote:
> The Scaling Bitcoin Workshop in HK is just wrapping up. Many fascinating
> proposals were presented. I think this would be a good time to share my
> view of the near term arc for capacity increases in the Bitcoin system. I
> believe we?re in a fantastic place right now and that the community
> is ready to deliver on a clear forward path with a shared vision that
> addresses the needs of the system while upholding its values.

ACK.

One of the interesting take-aways from the workshops for me has been
that there is a large discrepancy between what developers are doing
and what's more widely known. When I was doing initial research and
work for my keynote at the Montreal conference (
http://diyhpl.us/~bryan/irc/bitcoin/scalingbitcoin-review.pdf -- an
attempt at being exhaustive, prior to seeing the workshop proposals ),
what I was most surprised by was the discrepancy between what we think
is being talked about versus what has been emphasized or socially
processed (lots of proposals appear in text, but review efforts are
sometimes "hidden" in corners of github pull request comments, for
example). As another example, the libsecp256k1 testing work reached a
level unseen except perhaps in the aerospace industry, but these sorts
of details are not apparent if you are reading bitcoin-dev archives.
It is very hard to listen to all ideas and find great ideas.
Sometimes, our time can be almost completely exhausted by evaluating
inefficient proposals, so it's not surprising that rough consensus
building could take time. I suspect we will see consensus moving in
positive directions around the proposals you have highlighted.

When Satoshi originally released the Bitcoin whitepaper, practically
everyone-- somehow with the exception of Hal Finney-- didn't look,
because the costs of evaluating cryptographic system proposals is so
high and everyone was jaded and burned out for the past umpteen
decades. (I have IRC logs from January 10th 2009 where I immediately
dismissed Bitcoin after I had seen its announcement on the
p2pfoundation mailing list, perhaps in retrospect I should not let
family tragedy so greatly impact my evaluation of proposals...). It's
hard to evaluate these proposals. Sometimes it may feel like random
proposals are review-resistant, or designed to burn our time up. But I
think this is more reflective of the simple fact that consensus takes
effort, and it's hard work, and this is to be expected in this sort of
system design.

Your email contains a good summary of recent scaling progress and of
efforts presented at the Hong Kong workshop. I like summaries. I have
previously recommended making more summaries and posting them to the
mailing list. In general, it would be good if developers were to write
summaries of recent work and efforts and post them to the bitcoin-dev
mailing list. BIP drafts are excellent. Long-term proposals are
excellent. Short-term coordination happens over IRC, and that makes
sense to me. But I would point out that many of the developments even
from, say, the Montreal workshop were notably absent from the mailing
list. Unless someone was paying close attention, they wouldn't have
noticed some of those efforts which, in some cases, haven't been
mentioned since. I suspect most of this is a matter of attention,
review and keeping track of loose ends, which can be admittedly
difficult.

Short (or even long) summaries in emails are helpful because they
increase the ability of the community to coordinate and figure out
what's going on. Often I will write an email that summarizes some
content simply because I estimate that I am going to forget the
details in the near future, and if I am going to forget them then it
seems likely that others might.... This creates a broad base of
proposals and content to build from when we're doing development work
in the future, making for a much richer community as a consequence.
The contributions from the scalingbitcoin.org workshops are a welcome
addition, and the proposal outlined in the above email contains a good
summary of recent progress. We need more of this sort of synthesis,
we're richer for it. I am excitedly looking forward to the impending
onslaught of Bitcoin progress.

- Bryan
http://heybryan.org/
1 512 203 0507

From cp368202 at ohiou.edu  Mon Dec  7 23:39:12 2015
From: cp368202 at ohiou.edu (Chris Priest)
Date: Mon, 7 Dec 2015 15:39:12 -0800
Subject: [bitcoin-dev] Coalescing Transactions BIP Draft
Message-ID: <CAAcC9yv3Eaqa4af0U4s_V3hpEt7cWn5mLx0DyhqBCK-g32D1yw@mail.gmail.com>

I made a post a few days ago where I laid out a scheme for
implementing "coalescing transactions" using a new opcode. I have
since come to the realization that an opcode is not the best way to do
this. A much better approach I think is a new "transaction type" field
that is split off from the version field. Other uses can come out of
this type field, wildcard inputs is just the first one.

There are two unresolved issues. First, there might need to be a limit
on how many inputs are included in the "coalesce". Lets say you have
an address that has 100,000,000 inputs. If you were to coalesce them
all into one single input, that means that every node has to count of
these 100,000,000 inputs, which could take a long time. But then
again, the total number of inputs a wildcard can cover is limited to
the actual number of UTXOs in the pool, which is very much a
finite/constrained number.

One solution is to limit all wildcard inputs to, say, 10,000 items. If
you have more inputs that you want coalesced, you have to do it in
10,000 chunks, starting from the beginning. I want wildcard inputs to
look as much like normal inputs as much as possible to facilitate
implementation, so embedding a "max search" inside the transaction I
don't think is the best idea. I think if there is going to be a limit,
it should be implied.

The other issue is with limiting wildcard inputs to only inputs that
are confirmed into a fixed number of blocks. Sort of like how coinbase
has to be a certain age before it can be spent, maybe wildcard inputs
should only work on inputs older than a certain block age. Someone
brought up in the last thread that re-orgs can cause problems. I don't
quite see how that could happen, as re-orgs don't really affect
address balances, only block header values, which coalescing
transactions have nothing to do with.

Here is the draft:
https://github.com/priestc/bips/blob/master/bip-coalesc-wildcard.mediawiki

From aj at erisian.com.au  Tue Dec  8 02:42:24 2015
From: aj at erisian.com.au (Anthony Towns)
Date: Tue, 8 Dec 2015 12:42:24 +1000
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
Message-ID: <20151208024224.GA32631@sapphire.erisian.com.au>

On Mon, Dec 07, 2015 at 10:02:17PM +0000, Gregory Maxwell via bitcoin-dev wrote:
> ... bringing Segregated Witness to Bitcoin.
> The particular proposal amounts to a 4MB blocksize increase at worst.

Bit ambiguous what "worst" means here; lots of people would say the
smallest increase is the worst option. :)

By my count, P2PKH transactions get 2x space saving with segwit [0],
while 2-of-2 multisig P2SH transactions (and hence most of the on-chain
lightning transactions) get a 3x space saving [1]. An on-chain HTLC (for
a cross-chain atomic swap eg) would also get 3x space saving [2]. The most
extreme lightning transactions (uncooperative close with bonus anonymity)
could get a 6x saving, but would probably run into SIGOP limits [3].

> If widely used this proposal gives a 2x capacity increase
> (more if multisig is widely used),

So I think it's fair to say that on its own it gives up to a 2x increase
for ordinary pay to public key transactions, and a 3x increase for 2/2
multisig and (on-chain) lightning transactions (which would mean lightning
could scale to ~20M users with 1MB block sizes based on the estimates
from Tadge Dryja's talk). More complicated smart contracts (even just 3
of 5 multisig) presumably benefit even more from this, which seems like
an interesting approach to (part of) jgarzik's "Fidelity problem".

Averaging those numbers as a 2.5x improvement, means that combining
segwit with other proposals would allow you to derate them by a factor
of 2.5, giving:

 BIP-100: maximum of 12.8MB
 BIP-101: 3.2MB in 2016, 6.4MB in 2018, 12.8MB in 2020, 25.6MB in 2022..
 2-4-8:   800kB in 2016, 1.6MB in 2018, 3.2MB in 2020
 BIP-103: 400kB in 2016, 470kB in 2018, 650kB in 2020, 1MB in 2023...

(ie, if BIP-103 had been the "perfect" approach, then post segwit,
it would make sense to put non-consensus soft-limits back in place
for quite a while)

> TL;DR:  I propose we work immediately towards the segwit 4MB block
> soft-fork which increases capacity and scalability, and recent speedups
> and incoming relay improvements make segwit a reasonable risk.

I guess segwit effectively introduces two additional dimensions for
working out how to optimally pack transactions into a block -- there's
the existing constraints on block bytes (<=1MB) and sigops (<=20k), but
there are problably additional constraints on witness bytes (<=3MB) and
there *could* be a different constraint for sigops in witnesses (<=3*20k?
<=4*20k?) compared to sigops in the block while remaining a soft-fork.

It could also be an opportunity to combine the constraints, ie
(segwit_bytes + 50*segwit_sigs < 6M) which would make it easier to avoid
attacks where people try sending transactions with lots of sigops in very
few bytes, filling up blocks by sigops, but only paying fees proportional
to their byte count.

Hmm, after a quick look, I'm not sure if the current segwit branch
actually accounts for sigops in segregated witnesses? If it does, afaics
it simply applies the existing 20k limit to the total, which seems
too low to me?

Having segwit with the current 1MB limit on the traditional block
contents plus an additional 3MB for witness data seems like it would
also give a somewhat gradual increase in transaction volume from the
current 1x rate to an eventual 2x or 3x rate as wallet software upgrades
to support segregated witness transactions. So if problems were found
when block+witness data hit 1.5MB, there'd still be time to roll out
fixes before it got to 1.8MB or 2MB or 3MB. ie this further reduces the
risk compared to a single step increase to 2x capacity.

BTW, it's never been quite clear to me what the risks are precisely.
Here are some:

 - sometime soon, blockchain supply can't meet demand

    + I've never worked out how you'd tell if this is the case;
      there's potentially infinite demand if everything free, so at
      one level it's trivially true, but that's not helpful.

    + Presumably if this were happening in a way that "matters", fees
      would rise precipitously. Perhaps median fees of $2 USD/kB would
      indicate this is happening? If so, it's not here yet and seems
      like it's still a ways off.

    + If it were happening, then, presumably, people become would be
      less optimistic about bitcoin and the price of BTC would drop/not
      rise, but that seems pretty hard to interpret.

 - it becomes harder to build on blocks found by other miners,
   encouraging mining centralisation (which then makes censorship easier,
   and fungibility harder) or forcing trust between miners (eg SPV mining
   empty blocks)

    + latency/bandwidth limitations means miners can't get block
      information quickly enough (mitigated by weak blocks and IBLT)

    + blocks can't be verified quickly enough (due to too many crypto
      ops per block, or because the UTXO set can't be kept in RAM)
      (mitigated by libsecp256k1 improvements, ..?)

    + constructing a new block to mine takes too long

 - it becomes harder to maintain a validating, but non-mining node,
   which in turn makes non-validating nodes harder to run safely (ie,
   Sybil attacks become easier)

    + increased CPU to verify bigger/more complicated blocks (can't keep
      up on a raspberry pi)

    + increased storage (60GB of blockchain might mean it won't fit on
      your laptop)

    + increased bandwidth

    + increased initial sync time (delayed reward = less likely to
      bother)

Cheers,
aj

[0] AIUI, segwit would make the "in block" transactions look like:

     * (4) version
     * (1) input count
     * for each input:
       - (32) tx hash
       - (4) txout index
       - (1) script length = 0
       - (4) sequence number
     * (1) output count
     * for each output:
       - (8) value
       - (1) script length = 34
       - (34) <33 byte push>
     * (4) locktime

    So about 10+41i+43o bytes (with the other information being external to
    the block and the 1MB limit, but committed to via the coinbase).

    A standard pay to public key hash would have a 25 byte output script
    instead of 34 bytes, but also a 105 bytes of input script, so about
    10+146i+34o bytes.

    Over enough transactions inputs and outputs are about equal, so that's
    10+84o versus 10+180o, so a factor of 2x-2.14x in the usual case.

[1] With a P2SH to a 2-of-2 multisig address, the output script would
    be 23 bytes, and the input script would be a 71B redeem script, plus
    two signatures and an OP_0 for about 215B, so totalling 10+256i+32o.

    Again treating i=o over the long term, that's 10+84o version 10+288o,
    so that's a 3.2x-3.4x improvement. 2-of-2 multisig payment would
    cover the normal case for on-chain lightning channel transactions,
    ie where both sides are able to cooperatively close the channel.

[2] A basic HTLC, ie: "pay to A if they know the preimage for X, or pay
    to B after a timeout of T", done by P2SH has about 98B of redeem script
    and either ~105B of signature or ~72B of signature for a total of 203B
    or 170B of input script. So that comes to 10+244i+32o or 10+211i+32o.
    Segwit gives an improvement of 3x-3.3x or 2.7x-2.9x there.

[3] A lightning-style HTLC, which adds a third option of ", or pay to
    B if A was trying to cheat" adds an extra 25 bytes or so to the
    redeem script, changing those numbers to 10+270i+32o and 10+236i+32o,
    and an improvement of 3.3x-3.6x or 2.9x-3.2x.

    A lightning-style HTLC that also uses ecc private keys as the secret
    preimages to be revealed [4] might use an additional ~260 bytes of
    redeem script / script signature, which would make the worst case
    numbers be 10+530i+32o, so 10+562o versus 10+84o, which would be a
    6x-6.7x improvement. But those particular scripts would be constrained
    by consensus sigop limits before the filled up much more than a quarter
    of a block in a segwit/1MB world anyway.

[4] http://lists.linuxfoundation.org/pipermail/lightning-dev/2015-November/000344.html


From aj at erisian.com.au  Tue Dec  8 04:58:03 2015
From: aj at erisian.com.au (Anthony Towns)
Date: Tue, 8 Dec 2015 14:58:03 +1000
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <20151208024224.GA32631@sapphire.erisian.com.au>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208024224.GA32631@sapphire.erisian.com.au>
Message-ID: <20151208045803.GA1042@sapphire.erisian.com.au>

> On Mon, Dec 07, 2015 at 10:02:17PM +0000, Gregory Maxwell wrote:
> > If widely used this proposal gives a 2x capacity increase
> > (more if multisig is widely used),

So from IRC, this doesn't seem quite right -- capacity is constrained as

  base_size + witness_size/4 <= 1MB

rather than

  base_size <= 1MB and base_size + witness_size <= 4MB

or similar. So if you have a 500B transaction and move 250B into the
witness, you're still using up 250B+250B/4 of the 1MB limit, rather than
just 250B of the 1MB limit.

In particular, if you use as many p2pkh transactions as possible, you'd
have 800kB of base data plus 800kB of witness data, and for a block
filled with 2-of-2 multisig p2sh transactions, you'd hit the limit at
670kB of base data and 1.33MB of witness data.

That would be 1.6MB and 2MB of total actual data if you hit the limits
with real transactions, so it's more like a 1.8x increase for real
transactions afaics, even with substantial use of multisig addresses.

The 4MB consensus limit could only be hit by having a single trivial
transaction using as little base data as possible, then a single huge
4MB witness. So people trying to abuse the system have 4x the blocksize
for 1 block's worth of fees, while people using it as intended only get
1.6x or 2x the blocksize... That seems kinda backwards.

Having a cost function rather than separate limits does make it easier to
build blocks (approximately) optimally, though (ie, just divide the fee by
(base_bytes+witness_bytes/4) and sort). Are there any other benefits?

But afaics, you could just have fixed consensus limits and use the cost
function for building blocks, though? ie sort txs by fee divided by [B +
S*50 + W/3] (where B is base bytes, S is sigops and W is witness bytes)
then just fill up the block until one of the three limits (1MB base,
20k sigops, 3MB witness) is hit?

(Doing a hard fork to make *all* the limits -- base data, witness data,
and sigop count -- part of a single cost function might be a win; I'm
just not seeing the gain in forcing witness data to trade off against
block data when filling blocks is already a 2D knapsack problem)

Cheers,
aj

From greg at xiph.org  Tue Dec  8 05:21:18 2015
From: greg at xiph.org (Gregory Maxwell)
Date: Tue, 8 Dec 2015 05:21:18 +0000
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <20151208045803.GA1042@sapphire.erisian.com.au>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208024224.GA32631@sapphire.erisian.com.au>
	<20151208045803.GA1042@sapphire.erisian.com.au>
Message-ID: <CAAS2fgQUJBZqj7Y_k9cUv+dmqnL-aZG6ySKAr=WFWcO4iGUL_w@mail.gmail.com>

On Tue, Dec 8, 2015 at 4:58 AM, Anthony Towns via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> Having a cost function rather than separate limits does make it easier to
> build blocks (approximately) optimally, though (ie, just divide the fee by
> (base_bytes+witness_bytes/4) and sort). Are there any other benefits?

Actually being able to compute fees for your transaction: If there are
multiple limits that are "at play" then how you need to pay would
depend on the entire set of other candidate transactions, which is
unknown to you. Avoiding the need for a fancy solver in the miner is
also virtuous, because requiring software complexity there can make
for centralization advantages or divert development/maintenance cycles
in open source software off to other ends... The multidimensional
optimization is harder to accommodate for improved relay schemes, this
is the same as the "build blocks" but much more critical both because
of the need for consistency and the frequency in which you do it.

These don't, however, apply all that strongly if only one limit is
likely to be the limiting limit... though I am unsure about counting
on that; after all if the other limits wouldn't be limiting, why have
them?

> That seems kinda backwards.

It can seem that way, but all limiting schemes have pathological cases
where someone runs up against the limit in the most costly way.  Keep
in mind that casual pathological behavior can be suppressed via
IsStandard like rules without baking them into consensus; so long as
the candidate attacker isn't miners themselves. Doing so where
possible can help avoid cases like the current sigops limiting which
is just ... pretty broken.

From aj at erisian.com.au  Tue Dec  8 06:54:48 2015
From: aj at erisian.com.au (Anthony Towns)
Date: Tue, 8 Dec 2015 16:54:48 +1000
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAAS2fgQUJBZqj7Y_k9cUv+dmqnL-aZG6ySKAr=WFWcO4iGUL_w@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208024224.GA32631@sapphire.erisian.com.au>
	<20151208045803.GA1042@sapphire.erisian.com.au>
	<CAAS2fgQUJBZqj7Y_k9cUv+dmqnL-aZG6ySKAr=WFWcO4iGUL_w@mail.gmail.com>
Message-ID: <20151208065448.GB1042@sapphire.erisian.com.au>

On Tue, Dec 08, 2015 at 05:21:18AM +0000, Gregory Maxwell via bitcoin-dev wrote:
> On Tue, Dec 8, 2015 at 4:58 AM, Anthony Towns via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org> wrote:
> > Having a cost function rather than separate limits does make it easier to
> > build blocks (approximately) optimally, though (ie, just divide the fee by
> > (base_bytes+witness_bytes/4) and sort). Are there any other benefits?
> Actually being able to compute fees for your transaction: If there are
> multiple limits that are "at play" then how you need to pay would
> depend on the entire set of other candidate transactions, which is
> unknown to you.

Isn't that solvable in the short term, if miners just agree to order
transactions via a cost function, without enforcing it at consensus
level until a later hard fork that can also change the existing limits
to enforce that balance?

(1MB base + 3MB witness + 20k sigops) with segwit initially, to something
like (B + W + 200*U + 40*S < 5e6) where B is base bytes, W is witness
bytes, U is number of UTXOs added (or removed) and S is number of sigops,
or whatever factors actually make sense.

I guess segwit does allow soft-forking more sigops immediately -- segwit
transactions only add sigops into the segregated witness, which doesn't
get counted for existing consensus. So it would be possible to take the
opposite approach, and make the rule immediately be something like:

  50*S < 1M
  B + W/4 + 25*S' < 1M

(where S is sigops in base data, and S' is sigops in witness) and
just rely on S trending to zero (or soft-fork in a requirement that
non-segregated witness transactions have fewer than B/50 sigops) so that
there's only one (linear) equation to optimise, when deciding fees or
creating a block. (I don't see how you could safely set the coefficient
for S' too much smaller though)

B+W/4+25*S' for a 2-in/2-out p2pkh would still be 178+206/4+25*2=280
though, which would allow 3570 transactions per block, versus 2700 now,
which would only be a 32% increase...

> These don't, however, apply all that strongly if only one limit is
> likely to be the limiting limit... though I am unsure about counting
> on that; after all if the other limits wouldn't be limiting, why have
> them?

Sure, but, at least for now, there's already two limits that are being
hit. Having one is *much* better than two, but I don't think two is a
lot better than three?

(Also, the ratio between the parameters doesn't necessary seem like a
constant; it's not clear to me that hardcoding a formula with a single
limit is actually better than hardcoding separate limits, and letting
miners/the market work out coefficients that match the sort of contracts
that are actually being used)

> > That seems kinda backwards.
> It can seem that way, but all limiting schemes have pathological cases
> where someone runs up against the limit in the most costly way. Keep
> in mind that casual pathological behavior can be suppressed via
> IsStandard like rules without baking them into consensus; so long as
> the candidate attacker isn't miners themselves. Doing so where
> possible can help avoid cases like the current sigops limiting which
> is just ... pretty broken.

Sure; it just seems to be halving the increase in block space (60% versus
100% extra for p2pkh, 100% versus 200% for 2/2 multisig p2sh) for what
doesn't actually look like that much of a benefit in fee comparisons?

I mean, as far as I'm concerned, segwit is great even if it doesn't buy
any improvement in transactions/block, so even a 1% gain is brilliant.
I'd just rather the 100%-200% gain I was expecting. :)

Cheers,
aj

From laanwj at gmail.com  Tue Dec  8 11:07:53 2015
From: laanwj at gmail.com (Wladimir J. van der Laan)
Date: Tue, 8 Dec 2015 12:07:53 +0100
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
Message-ID: <20151208110752.GA31180@amethyst.visucore.com>

On Mon, Dec 07, 2015 at 10:02:17PM +0000, Gregory Maxwell via bitcoin-dev wrote:
> The Scaling Bitcoin Workshop in HK is just wrapping up. Many fascinating
> proposals were presented. I think this would be a good time to share my
> view of the near term arc for capacity increases in the Bitcoin system. I
> believe we?re in a fantastic place right now and that the community
> is ready to deliver on a clear forward path with a shared vision that
> addresses the needs of the system while upholding its values.

Thanks for writing this up. Putting the progress, ongoing work and plans related
to scaling in context, in one place, was badly needed.

> TL;DR:  I propose we work immediately towards the segwit 4MB block
> soft-fork which increases capacity and scalability, and recent speedups
> and incoming relay improvements make segwit a reasonable risk. BIP9
> and segwit will also make further improvements easier and faster to
> deploy. We?ll continue to set the stage for non-bandwidth-increase-based
> scaling, while building additional tools that would make bandwidth
> increases safer long term. Further work will prepare Bitcoin for further
> increases, which will become possible when justified, while also providing
> the groundwork to make them justifiable.

Sounds good to me.

There are multiple ways to get involved in ongoing work, where the community
can help to make this happen sooner:

- Review the versionbits BIP https://github.com/bitcoin/bips/blob/master/bip-0009.mediawiki:

  - Compare and test with implementation: https://github.com/bitcoin/bitcoin/pull/6816

- Review CSV BIPs (BIP68 https://github.com/bitcoin/bips/blob/master/bip-0068.mediawiki / 
       BIP112 https://github.com/bitcoin/bips/blob/master/bip-0112.mediawiki),

  - Compare and test implementation: 

    https://github.com/bitcoin/bitcoin/pull/6564  BIP-112: Mempool-only CHECKSEQUENCEVERIFY
    https://github.com/bitcoin/bitcoin/pull/6312  BIP-68: Mempool-only sequence number constraint verification 
    https://github.com/bitcoin/bitcoin/pull/7184  [WIP] Implement SequenceLocks functions for BIP 68

- Segwit BIP is being written, but has not yet been published.

  - Gregory linked to an implementation but as he mentions it is not completely
    finished yet. ETA for a Segwit testnet is later this month, then you can test as well.

Wladimir

From jtimon at jtimon.cc  Tue Dec  8 11:14:32 2015
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Tue, 8 Dec 2015 12:14:32 +0100
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <20151208110752.GA31180@amethyst.visucore.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
Message-ID: <CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>

On Dec 8, 2015 7:08 PM, "Wladimir J. van der Laan via bitcoin-dev" <
bitcoin-dev at lists.linuxfoundation.org> wrote:
>   - Gregory linked to an implementation but as he mentions it is not
completely
>     finished yet. ETA for a Segwit testnet is later this month, then you
can test as well.

Testnet4 ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/bcac0700/attachment.html>

From tobypadilla at gmail.com  Tue Dec  8 02:10:22 2015
From: tobypadilla at gmail.com (Toby Padilla)
Date: Mon, 7 Dec 2015 18:10:22 -0800
Subject: [bitcoin-dev] Key.run: BIP-70 Payments and OP_RETURN
Message-ID: <CAGcHOzy--AmRhBYveFF4Aq1dr=2oB4xuHVZPQnQV4kEXroLWXA@mail.gmail.com>

Hi all. I've been working on a new publication platform based on Bitcoin
called key.run: http://key.run

The high level overview is that key.run stores BitTorrent info hashes
(magnet links) in the blockchain by sending transactions to a "namespace"
Bitcoin address. Using SPV, I reconstitute the key.run db from the
blockchain. This is meant to be an open source and distributed system so
anyone can run the key.run server (and change namespace keys). More info
here: https://git.playgrub.com/toby/keyrun

Now to my issue...

One of the main use cases I wanted to support was people using their
*existing* Bitcoin wallet to encode the key.run transactions on the
blockchain. Practically speaking this meant building the transactions with
the proper OP_RETURN value server-side then passing them to the end user's
wallet via BIP-70. I have this working with Bitcoin Core (there are issues
with other wallets I've tested with BIP-70).

The problem I'm having is that Bitcoin Core will not allow BIP-70
PaymentDetails to contain outputs with zero value. As stated in
https://github.com/bitcoin/bips/blob/master/bip-0070.mediawiki:

"if there are more than one; Outputs with zero amounts shall be ignored"

Bitcoin Core doesn't seem to ignore the output though, it rejects the
transaction and doesn't allow the user to submit it. The key.run
transactions currently work by giving the OP_RETURN outputs a non-zero >
dust value. This value is presumably lost forever.

I think ideally OP_RETURN outputs with zero value WOULD be allowed since
they are valid transactions. Allowing OP_RETURN outputs to be constructed
with BIP-70 Payments is also the only way I can think of to extend the
functionality of existing wallets.

I would love to get feedback on if there is an alternative way to do what I
propose or ideally if BIP-70 could be tweaked to allow OP_RETURN outputs
with zero value.

I'd also love feedback on key.run but this probably isn't the best venue
for that. I've setup #keyrun on Freenode if anyone is interested in
discussing the project.

Regards,
Toby

--
http://twitter.com/toby
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151207/ad78a3e4/attachment-0001.html>

From j at toom.im  Tue Dec  8 12:59:51 2015
From: j at toom.im (Jonathan Toomim)
Date: Tue, 8 Dec 2015 20:59:51 +0800
Subject: [bitcoin-dev] Can kick
Message-ID: <763601A1-03DE-4B7F-A032-C52E0B6C5AA3@toom.im>

I am leaning towards supporting a can kick proposal. Features I think are desirable for this can kick:

0. Block size limit around 2 to 4 MB. Maybe 3 MB? Based on my testnet data, I think 3 MB should be pretty safe.
1. Hard fork with a consensus mechanisms similar to BIP101
2. Approximately 1 or 2 month delay before activation to allow for miners to upgrade their infrastructure.
3. Some form of validation cost metric. BIP101's validation cost metric would be the minimum strictness that I would support, but it would be nice if there were a good UTXO growth metric too. (I do not know enough about the different options to evaluate them right now.)

I will be working on a few improvements to block propagation (especially from China) over the next few months, like blocktorrent and stratum-based GFW penetration. I hope to have these working within a few months. Depending on how those efforts and others (e.g. IBLTs) go, we can look at increasing the block size further, and possibly enacting a long-term scaling roadmap like BIP101.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/75222ee6/attachment.sig>

From vincent.truong at procabiak.com  Tue Dec  8 12:27:27 2015
From: vincent.truong at procabiak.com (Vincent Truong)
Date: Tue, 8 Dec 2015 23:27:27 +1100
Subject: [bitcoin-dev] BIP 9 style version bits for txns
Message-ID: <CACrzPenvAWdkgRG3Y7P31JiNEVRYvd+f1nMp=QRhAp5P_eGRow@mail.gmail.com>

So I have been told more than once that the version announcement in blocks
is not a vote, but a signal for readiness, used in isSupermajority().
Basically, if soft forks (and hard forks) won't activate unless a certain %
of blocks have been flagged with the version up (or bit flipped when
versionbits go live) to signal their readiness, that is a vote against
implementation if they never follow up. I don't like this politically
correct speech because in reality it is a vote... But I'm not here to argue
about that... I would like to see if there are any thoughts on
extending/copying isSupermajority() for a new secondary/non-critical
function to also look for a similar BIP 9 style version bit in txns.
Apologies if already proposed, haven't heard of it anywhere.

If we are looking for a signal of readiness, it is unfair to wallet
developers and exchanges that they are unable to signal if they too are
ready for a change. As more users are going into use SPV or SPV-like
wallets, when a change occurs that makes them incompatible/in need of
upgrade we need to make sure they aren't going to break or introduce
security flaws for users.

If a majority of transactions have been sent are flagged ready, we know
that they're also good to go.

Would you implement the same versionbits for txn's version field, using 3
bits for versioning and 29 bits for flags? This indexing of every txn might
sound insane and computationally expensive for bitcoin Core to run, but if
it isn't critical to upgrade with soft forks, then it can be watched
outside the network by enthusiasts. I believe this is the most politically
correct way to get wallet devs and exchanges involved. (If it were me I
would absolutely try figure out a way to stick it in isSupermajority...)

Miners can watch for readiness flagged by wallets before they themselves
flag ready. We will have to trust miners to not jump the gun, but that's
the trade off.

Thoughts?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/7274423d/attachment.html>

From gavinandresen at gmail.com  Tue Dec  8 15:12:10 2015
From: gavinandresen at gmail.com (Gavin Andresen)
Date: Tue, 8 Dec 2015 10:12:10 -0500
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
Message-ID: <CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>

Thanks for laying out a road-map, Greg.

I'll need to think about it some more, but just a couple of initial
reactions:

Why segwitness as a soft fork? Stuffing the segwitness merkle tree in the
coinbase is messy and will just complicate consensus-critical code (as
opposed to making the right side of the merkle tree in block.version=5
blocks the segwitness data).

It will also make any segwitness fraud proofs significantly larger (merkle
path versus  merkle path to coinbase transactions, plus ENTIRE coinbase
transaction, which might be quite large, plus merkle path up to root).


We also need to fix the O(n^2) sighash problem as an additional BIP for ANY
blocksize increase. That also argues for a hard fork-- it is much easier to
fix it correctly and simplify the consensus code than to continue to apply
band-aid fixes on top of something fundamentally broken.


Segwitness will require a hard or soft-fork rollout, then a significant
fraction of the transaction-producing wallets to upgrade and start
supporting segwitness-style transactions.  I think it will be much quicker
than the P2SH rollout, because the biggest transaction producers have a
strong motivation to lower their fees, and it won't require a new type of
bitcoin address to fund wallets.  But it still feels like it'll be six
months to a year at the earliest before any relief from the current
problems we're seeing from blocks filling up.

Segwitness will make the current bottleneck (block propagation) a little
worse in the short term, because of the extra fraud-proof data.  Benefits
well worth the costs.

------------------

I think a barrier to quickly getting consensus might be a fundamental
difference of opinion on this:
   "Even without them I believe we?ll be in an acceptable position with
respect to capacity in the near term"

The heaviest users of the Bitcoin network (businesses who generate tens of
thousands of transactions per day on behalf of their customers) would
strongly disgree; the current state of affairs is NOT acceptable to them.



-- 
--
Gavin Andresen
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/386ffdbc/attachment.html>

From justus at openbitcoinprivacyproject.org  Tue Dec  8 15:55:57 2015
From: justus at openbitcoinprivacyproject.org (Justus Ranvier)
Date: Tue, 8 Dec 2015 09:55:57 -0600
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
Message-ID: <5666FD8D.8050201@openbitcoinprivacyproject.org>

On 12/08/2015 09:12 AM, Gavin Andresen via bitcoin-dev wrote:
> Stuffing the segwitness merkle tree in the coinbase

If such a change is going to be deployed via a soft fork instead of a
hard fork, then the coinbase is the worst place to put the segwitness
merkle root.

Instead, put it in the first output of the generation transaction as an
OP_RETURN script.

This is a better pattern because coinbase space is limited while output
space is not. The next time there's a good reason to tie another merkle
tree to a block, that proposal can be designated for the second output
of the generation transaction.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0xEAD9E623.asc
Type: application/pgp-keys
Size: 23337 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/a7f58417/attachment-0001.bin>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 801 bytes
Desc: OpenPGP digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/a7f58417/attachment-0001.sig>

From akiva.lichtner at gmail.com  Tue Dec  8 16:27:18 2015
From: akiva.lichtner at gmail.com (Akiva Lichtner)
Date: Tue, 8 Dec 2015 11:27:18 -0500
Subject: [bitcoin-dev] Scaling by Partitioning
Message-ID: <CABCnA7Wqz76m8qo5BYT41Z=hBH+fUfOc4xsFAGg=Niv7Jgkqsg@mail.gmail.com>

Hello,

I am seeking some expert feedback on an idea for scaling Bitcoin. As a
brief introduction: I work in the payment industry and I have twenty years'
experience in development. I have some experience with process groups and
ordering protocols too. I think I understand Satoshi's paper but I admit I
have not read the source code.

The idea is to run more than one simultaneous chain, each chain defeating
double spending on only part of the coin. The coin would be partitioned by
radix (or modulus, not sure what to call it.) For example in order to
multiply throughput by a factor of ten you could run ten parallel chains,
one would work on coin that ends in "0", one on coin that ends in "1", and
so on up to "9".

The number of chains could increase automatically over time based on the
moving average of transaction volume.

Blocks would have to contain the number of the partition they belong to,
and miners would have to round-robin through partitions so that an attacker
would not have an unfair advantage working on just one partition.

I don't think there is much impact to miners, but clients would have to
send more than one message in order to spend money. Client messages will
need to enumerate coin using some sort of compression, to save space. This
seems okay to me since often in computing client software does have to
break things up in equal parts (e.g. memory pages, file system blocks,) and
the client software could hide the details.

Best wishes for continued success to the project.

Regards,
Akiva

P.S. I found a funny anagram for SATOSHI NAKAMOTO: "NSA IS OOOK AT MATH"
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/37780f8d/attachment.html>

From kanzure at gmail.com  Tue Dec  8 16:45:32 2015
From: kanzure at gmail.com (Bryan Bishop)
Date: Tue, 8 Dec 2015 10:45:32 -0600
Subject: [bitcoin-dev] Scaling by Partitioning
In-Reply-To: <CABCnA7Wqz76m8qo5BYT41Z=hBH+fUfOc4xsFAGg=Niv7Jgkqsg@mail.gmail.com>
References: <CABCnA7Wqz76m8qo5BYT41Z=hBH+fUfOc4xsFAGg=Niv7Jgkqsg@mail.gmail.com>
Message-ID: <CABaSBazMd4VYhSgM0-=DWU7_AAGxvXiW2yjsft4bKw8gAtpuNg@mail.gmail.com>

On Tue, Dec 8, 2015 at 10:27 AM, Akiva Lichtner via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> and miners would have to round-robin through partitions

At first glance this proposal seems most similar to the sharding proposals:

http://diyhpl.us/wiki/transcripts/scalingbitcoin/sharding-the-blockchain/
https://github.com/vbuterin/scalability_paper/blob/master/scalability.pdf
https://www.reddit.com/r/Bitcoin/comments/3u1m36/why_arent_we_as_a_community_talking_about/cxbamhn
http://eprint.iacr.org/2015/1168.pdf (committee approach)

> but clients would have to send more than one message in order to spend money

Offloading work to the client for spends has in the past been a
well-received concept, such as the linearized coin history idea.

- Bryan
http://heybryan.org/
1 512 203 0507

From mark at friedenbach.org  Tue Dec  8 17:41:23 2015
From: mark at friedenbach.org (Mark Friedenbach)
Date: Wed, 9 Dec 2015 01:41:23 +0800
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <5666FD8D.8050201@openbitcoinprivacyproject.org>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
	<5666FD8D.8050201@openbitcoinprivacyproject.org>
Message-ID: <CAOG=w-vW36Q5_NMqzZsBgc-p7QEDEYp9OtLkg5tzbAN0YRXFUA@mail.gmail.com>

A far better place than the generation transaction (which I assume means
coinbase transaction?) is the last transaction in the block. That allows
you to save, on average, half of the hashes in the Merkle tree.

On Tue, Dec 8, 2015 at 11:55 PM, Justus Ranvier via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> On 12/08/2015 09:12 AM, Gavin Andresen via bitcoin-dev wrote:
> > Stuffing the segwitness merkle tree in the coinbase
>
> If such a change is going to be deployed via a soft fork instead of a
> hard fork, then the coinbase is the worst place to put the segwitness
> merkle root.
>
> Instead, put it in the first output of the generation transaction as an
> OP_RETURN script.
>
> This is a better pattern because coinbase space is limited while output
> space is not. The next time there's a good reason to tie another merkle
> tree to a block, that proposal can be designated for the second output
> of the generation transaction.
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/040dbf84/attachment.html>

From akiva.lichtner at gmail.com  Tue Dec  8 18:30:01 2015
From: akiva.lichtner at gmail.com (Akiva Lichtner)
Date: Tue, 8 Dec 2015 13:30:01 -0500
Subject: [bitcoin-dev] Scaling by Partitioning
In-Reply-To: <CABaSBazMd4VYhSgM0-=DWU7_AAGxvXiW2yjsft4bKw8gAtpuNg@mail.gmail.com>
References: <CABCnA7Wqz76m8qo5BYT41Z=hBH+fUfOc4xsFAGg=Niv7Jgkqsg@mail.gmail.com>
	<CABaSBazMd4VYhSgM0-=DWU7_AAGxvXiW2yjsft4bKw8gAtpuNg@mail.gmail.com>
Message-ID: <CABCnA7UdOg_3nq2SSuzdAwQMSdmPtr1=f0aRj3=MS7OiqdYVDw@mail.gmail.com>

Thanks for your response and links.

I think the difference is that those proposals all shard the mining work,
whereas what I wrote in my post shards the coin itself. In other words
different parts of the coin space are forever segregated, never ending up
in the same block. It's a big difference conceptually because I could spend
money and a fraction of it makes it into a block in ten minutes and the
rest two hours later.

And I think that's where the potential for the scalability comes in. I am
not really scaling Bitcoin's present requirements, I am relaxing the
requirements in a way that leaves the users and the miners happy, but that
could provoke resistance by the part of of all of us that doesn't want
digital cash as much as it wants to make history.

All the best,
Akiva

P.S. Thanks for pointing out that I hit "reply" instead of "reply all"
earlier ...

On Tue, Dec 8, 2015 at 11:45 AM, Bryan Bishop <kanzure at gmail.com> wrote:

> On Tue, Dec 8, 2015 at 10:27 AM, Akiva Lichtner via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org> wrote:
> > and miners would have to round-robin through partitions
>
> At first glance this proposal seems most similar to the sharding proposals:
>
> http://diyhpl.us/wiki/transcripts/scalingbitcoin/sharding-the-blockchain/
> https://github.com/vbuterin/scalability_paper/blob/master/scalability.pdf
>
> https://www.reddit.com/r/Bitcoin/comments/3u1m36/why_arent_we_as_a_community_talking_about/cxbamhn
> http://eprint.iacr.org/2015/1168.pdf (committee approach)
>
> > but clients would have to send more than one message in order to spend
> money
>
> Offloading work to the client for spends has in the past been a
> well-received concept, such as the linearized coin history idea.
>
> - Bryan
> http://heybryan.org/
> 1 512 203 0507
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/fb4dadb7/attachment.html>

From justus at openbitcoinprivacyproject.org  Tue Dec  8 18:43:40 2015
From: justus at openbitcoinprivacyproject.org (Justus Ranvier)
Date: Tue, 8 Dec 2015 12:43:40 -0600
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAOG=w-vW36Q5_NMqzZsBgc-p7QEDEYp9OtLkg5tzbAN0YRXFUA@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
	<5666FD8D.8050201@openbitcoinprivacyproject.org>
	<CAOG=w-vW36Q5_NMqzZsBgc-p7QEDEYp9OtLkg5tzbAN0YRXFUA@mail.gmail.com>
Message-ID: <566724DC.8000606@openbitcoinprivacyproject.org>

On 12/08/2015 11:41 AM, Mark Friedenbach wrote:
> A far better place than the generation transaction (which I assume means
> coinbase transaction?) is the last transaction in the block. That allows
> you to save, on average, half of the hashes in the Merkle tree.

I don't care what color that bikeshed is painted.

In whatever transaction it is placed, the hash should be on the output
side, That way is more future-proof since it does not crowd out other
hashes which might be equally valuable to commit someday.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0xEAD9E623.asc
Type: application/pgp-keys
Size: 23337 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/fc970ace/attachment-0001.bin>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 801 bytes
Desc: OpenPGP digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/fc970ace/attachment-0001.sig>

From tier.nolan at gmail.com  Tue Dec  8 19:08:57 2015
From: tier.nolan at gmail.com (Tier Nolan)
Date: Tue, 8 Dec 2015 19:08:57 +0000
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAOG=w-vW36Q5_NMqzZsBgc-p7QEDEYp9OtLkg5tzbAN0YRXFUA@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
	<5666FD8D.8050201@openbitcoinprivacyproject.org>
	<CAOG=w-vW36Q5_NMqzZsBgc-p7QEDEYp9OtLkg5tzbAN0YRXFUA@mail.gmail.com>
Message-ID: <CAE-z3OVrpoSHVeJFN-6NzkkZP1y9RmUjKdpxjWN-dJgLK20TBg@mail.gmail.com>

On Tue, Dec 8, 2015 at 5:41 PM, Mark Friedenbach via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> A far better place than the generation transaction (which I assume means
> coinbase transaction?) is the last transaction in the block. That allows
> you to save, on average, half of the hashes in the Merkle tree.
>

This trick can be improved by only using certain tx counts.  If the number
of transactions is limited to a power of 2 (other than the extra
transactions), then you get a path of length zero.

The number of non-zero bits in the tx count determings how many digests are
required.

https://github.com/TierNolan/bips/blob/aux_header/bip-aux-header.mediawiki

This gets the benefit of a soft-fork, while also keeping the proof lengths
small.  The linked bip has a 105 byte overhead for the path.

The cost is that only certain transaction counts are allowed.  In the worst
case, 12.5% of transactions would have to be left in the memory pool.  This
means around 7% of transactions would be delayed until the next block.

Blank transactions (or just transactions with low latency requirements)
could be used to increase the count so that it is raised to one of the
valid numbers.

Managing the UTXO set to ensure that there is at least one output that pays
to OP_TRUE is also a hassle.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/99821607/attachment.html>

From greg at xiph.org  Tue Dec  8 19:31:27 2015
From: greg at xiph.org (Gregory Maxwell)
Date: Tue, 8 Dec 2015 19:31:27 +0000
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <5666FD8D.8050201@openbitcoinprivacyproject.org>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
	<5666FD8D.8050201@openbitcoinprivacyproject.org>
Message-ID: <CAAS2fgTcU-Svd5S3F-xA9+pjYSihdh7jtS6LU4k5enR-8OPESQ@mail.gmail.com>

On Tue, Dec 8, 2015 at 3:55 PM, Justus Ranvier via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> Instead, put it in the first output of the generation transaction as an
> OP_RETURN script.

Pieter was originally putting it in a different location; so it's no
big deal to do so.

But there exists deployed mining hardware that imposes constraints on
the coinbase outputs, unfortunately.

From cp368202 at ohiou.edu  Tue Dec  8 19:40:36 2015
From: cp368202 at ohiou.edu (Chris Priest)
Date: Tue, 8 Dec 2015 11:40:36 -0800
Subject: [bitcoin-dev] BIP 9 style version bits for txns
In-Reply-To: <CACrzPenvAWdkgRG3Y7P31JiNEVRYvd+f1nMp=QRhAp5P_eGRow@mail.gmail.com>
References: <CACrzPenvAWdkgRG3Y7P31JiNEVRYvd+f1nMp=QRhAp5P_eGRow@mail.gmail.com>
Message-ID: <CAAcC9ysviyCaajpwZezzLnPhVVeFxgTKNfFuH6o-CyXNFNBHbA@mail.gmail.com>

I proposed in my Wildcard Inputs BIP that the version field be split
in two. The first 4 bytes are version number (which in practice is
being used for script version), and the second 4 bits are used for
transaction type.

I don't think the BIP9 mechanism really applies to transactions. A
block is essentially a collection of transactions, therefore voting on
the block applies to the many parties who have transactions in the
block. A transaction on the other hand only effects at most two
parties (the sender and the receiver). In other words, block are
"communal" data structures, transactions are individual data
structures. Also, the nature of soft forks are that wallets can choose
to implement a new feature or not. For instance, if no wallets
implement RBF or SW, then those features effectively don't exist,
regardless of how many nodes have upgraded to handle the feature.

Any new transaction feature should get a new "type" number. A new
transaction feature can't happen until the nodes support it.

On 12/8/15, Vincent Truong via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> So I have been told more than once that the version announcement in blocks
> is not a vote, but a signal for readiness, used in isSupermajority().
> Basically, if soft forks (and hard forks) won't activate unless a certain %
> of blocks have been flagged with the version up (or bit flipped when
> versionbits go live) to signal their readiness, that is a vote against
> implementation if they never follow up. I don't like this politically
> correct speech because in reality it is a vote... But I'm not here to argue
> about that... I would like to see if there are any thoughts on
> extending/copying isSupermajority() for a new secondary/non-critical
> function to also look for a similar BIP 9 style version bit in txns.
> Apologies if already proposed, haven't heard of it anywhere.
>
> If we are looking for a signal of readiness, it is unfair to wallet
> developers and exchanges that they are unable to signal if they too are
> ready for a change. As more users are going into use SPV or SPV-like
> wallets, when a change occurs that makes them incompatible/in need of
> upgrade we need to make sure they aren't going to break or introduce
> security flaws for users.
>
> If a majority of transactions have been sent are flagged ready, we know
> that they're also good to go.
>
> Would you implement the same versionbits for txn's version field, using 3
> bits for versioning and 29 bits for flags? This indexing of every txn might
> sound insane and computationally expensive for bitcoin Core to run, but if
> it isn't critical to upgrade with soft forks, then it can be watched
> outside the network by enthusiasts. I believe this is the most politically
> correct way to get wallet devs and exchanges involved. (If it were me I
> would absolutely try figure out a way to stick it in isSupermajority...)
>
> Miners can watch for readiness flagged by wallets before they themselves
> flag ready. We will have to trust miners to not jump the gun, but that's
> the trade off.
>
> Thoughts?
>

From patrick.strateman at gmail.com  Tue Dec  8 20:50:08 2015
From: patrick.strateman at gmail.com (Patrick Strateman)
Date: Tue, 8 Dec 2015 12:50:08 -0800
Subject: [bitcoin-dev] Scaling by Partitioning
In-Reply-To: <CABCnA7Wqz76m8qo5BYT41Z=hBH+fUfOc4xsFAGg=Niv7Jgkqsg@mail.gmail.com>
References: <CABCnA7Wqz76m8qo5BYT41Z=hBH+fUfOc4xsFAGg=Niv7Jgkqsg@mail.gmail.com>
Message-ID: <56674280.3010003@gmail.com>

Payment recipients would need to operate a daemon for each chain, thus
guaranteeing no scaling advantage.

(There are other issues, but I believe that to be enough of a show
stopper not to continue).

On 12/08/2015 08:27 AM, Akiva Lichtner via bitcoin-dev wrote:
> Hello,
>
> I am seeking some expert feedback on an idea for scaling Bitcoin. As a
> brief introduction: I work in the payment industry and I have twenty
> years' experience in development. I have some experience with process
> groups and ordering protocols too. I think I understand Satoshi's
> paper but I admit I have not read the source code.
>
> The idea is to run more than one simultaneous chain, each chain
> defeating double spending on only part of the coin. The coin would be
> partitioned by radix (or modulus, not sure what to call it.) For
> example in order to multiply throughput by a factor of ten you could
> run ten parallel chains, one would work on coin that ends in "0", one
> on coin that ends in "1", and so on up to "9".
>
> The number of chains could increase automatically over time based on
> the moving average of transaction volume.
>
> Blocks would have to contain the number of the partition they belong
> to, and miners would have to round-robin through partitions so that an
> attacker would not have an unfair advantage working on just one partition.
>
> I don't think there is much impact to miners, but clients would have
> to send more than one message in order to spend money. Client messages
> will need to enumerate coin using some sort of compression, to save
> space. This seems okay to me since often in computing client software
> does have to break things up in equal parts (e.g. memory pages, file
> system blocks,) and the client software could hide the details.
>
> Best wishes for continued success to the project.
>
> Regards,
> Akiva
>
> P.S. I found a funny anagram for SATOSHI NAKAMOTO: "NSA IS OOOK AT MATH"
>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/82e0a25d/attachment.html>

From vincent.truong at procabiak.com  Tue Dec  8 21:02:55 2015
From: vincent.truong at procabiak.com (Vincent Truong)
Date: Wed, 9 Dec 2015 08:02:55 +1100
Subject: [bitcoin-dev] BIP 9 style version bits for txns
In-Reply-To: <CAAcC9ysviyCaajpwZezzLnPhVVeFxgTKNfFuH6o-CyXNFNBHbA@mail.gmail.com>
References: <CACrzPenvAWdkgRG3Y7P31JiNEVRYvd+f1nMp=QRhAp5P_eGRow@mail.gmail.com>
	<CAAcC9ysviyCaajpwZezzLnPhVVeFxgTKNfFuH6o-CyXNFNBHbA@mail.gmail.com>
Message-ID: <CACrzPenQadaNMNgnN_1fv5tqspRDtGUgzEmGsDtjMOdjWedZjQ@mail.gmail.com>

I suppose whether the wallet devs want to implement the soft fork or not is
irrelevant. They only need to indicate if they are ready i.e. they've
tested the new soft fork, hard fork or feature and validated that it
doesn't break their nodes or wallet software.
On Dec 9, 2015 6:40 AM, "Chris Priest" <cp368202 at ohiou.edu> wrote:

> I proposed in my Wildcard Inputs BIP that the version field be split
> in two. The first 4 bytes are version number (which in practice is
> being used for script version), and the second 4 bits are used for
> transaction type.
>
> I don't think the BIP9 mechanism really applies to transactions. A
> block is essentially a collection of transactions, therefore voting on
> the block applies to the many parties who have transactions in the
> block. A transaction on the other hand only effects at most two
> parties (the sender and the receiver). In other words, block are
> "communal" data structures, transactions are individual data
> structures. Also, the nature of soft forks are that wallets can choose
> to implement a new feature or not. For instance, if no wallets
> implement RBF or SW, then those features effectively don't exist,
> regardless of how many nodes have upgraded to handle the feature.
>
> Any new transaction feature should get a new "type" number. A new
> transaction feature can't happen until the nodes support it.
>
> On 12/8/15, Vincent Truong via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org> wrote:
> > So I have been told more than once that the version announcement in
> blocks
> > is not a vote, but a signal for readiness, used in isSupermajority().
> > Basically, if soft forks (and hard forks) won't activate unless a
> certain %
> > of blocks have been flagged with the version up (or bit flipped when
> > versionbits go live) to signal their readiness, that is a vote against
> > implementation if they never follow up. I don't like this politically
> > correct speech because in reality it is a vote... But I'm not here to
> argue
> > about that... I would like to see if there are any thoughts on
> > extending/copying isSupermajority() for a new secondary/non-critical
> > function to also look for a similar BIP 9 style version bit in txns.
> > Apologies if already proposed, haven't heard of it anywhere.
> >
> > If we are looking for a signal of readiness, it is unfair to wallet
> > developers and exchanges that they are unable to signal if they too are
> > ready for a change. As more users are going into use SPV or SPV-like
> > wallets, when a change occurs that makes them incompatible/in need of
> > upgrade we need to make sure they aren't going to break or introduce
> > security flaws for users.
> >
> > If a majority of transactions have been sent are flagged ready, we know
> > that they're also good to go.
> >
> > Would you implement the same versionbits for txn's version field, using 3
> > bits for versioning and 29 bits for flags? This indexing of every txn
> might
> > sound insane and computationally expensive for bitcoin Core to run, but
> if
> > it isn't critical to upgrade with soft forks, then it can be watched
> > outside the network by enthusiasts. I believe this is the most
> politically
> > correct way to get wallet devs and exchanges involved. (If it were me I
> > would absolutely try figure out a way to stick it in isSupermajority...)
> >
> > Miners can watch for readiness flagged by wallets before they themselves
> > flag ready. We will have to trust miners to not jump the gun, but that's
> > the trade off.
> >
> > Thoughts?
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/86112498/attachment-0001.html>

From akiva.lichtner at gmail.com  Tue Dec  8 21:23:12 2015
From: akiva.lichtner at gmail.com (Akiva Lichtner)
Date: Tue, 8 Dec 2015 16:23:12 -0500
Subject: [bitcoin-dev] Scaling by Partitioning
In-Reply-To: <56674280.3010003@gmail.com>
References: <CABCnA7Wqz76m8qo5BYT41Z=hBH+fUfOc4xsFAGg=Niv7Jgkqsg@mail.gmail.com>
	<56674280.3010003@gmail.com>
Message-ID: <CABCnA7Vb1JA6E+heXZZ=DKcsK9gusa6tSNEL5AkGRLOT2ZND6w@mail.gmail.com>

It's true that miners would have to be prepared to work on any partition. I
don't see where the number affects defeating double spending, what matters
is the nonce in the block that keeps the next successful miner random.

I expect that the number of miners would be ten times larger as well, so an
attacker would have no advantage working on one partition.

On Tue, Dec 8, 2015 at 3:50 PM, Patrick Strateman via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Payment recipients would need to operate a daemon for each chain, thus
> guaranteeing no scaling advantage.
>
> (There are other issues, but I believe that to be enough of a show stopper
> not to continue).
>
> On 12/08/2015 08:27 AM, Akiva Lichtner via bitcoin-dev wrote:
>
> Hello,
>
> I am seeking some expert feedback on an idea for scaling Bitcoin. As a
> brief introduction: I work in the payment industry and I have twenty years'
> experience in development. I have some experience with process groups and
> ordering protocols too. I think I understand Satoshi's paper but I admit I
> have not read the source code.
>
> The idea is to run more than one simultaneous chain, each chain defeating
> double spending on only part of the coin. The coin would be partitioned by
> radix (or modulus, not sure what to call it.) For example in order to
> multiply throughput by a factor of ten you could run ten parallel chains,
> one would work on coin that ends in "0", one on coin that ends in "1", and
> so on up to "9".
>
> The number of chains could increase automatically over time based on the
> moving average of transaction volume.
>
> Blocks would have to contain the number of the partition they belong to,
> and miners would have to round-robin through partitions so that an attacker
> would not have an unfair advantage working on just one partition.
>
> I don't think there is much impact to miners, but clients would have to
> send more than one message in order to spend money. Client messages will
> need to enumerate coin using some sort of compression, to save space. This
> seems okay to me since often in computing client software does have to
> break things up in equal parts (e.g. memory pages, file system blocks,) and
> the client software could hide the details.
>
> Best wishes for continued success to the project.
>
> Regards,
> Akiva
>
> P.S. I found a funny anagram for SATOSHI NAKAMOTO: "NSA IS OOOK AT MATH"
>
>
>
> _______________________________________________
> bitcoin-dev mailing listbitcoin-dev at lists.linuxfoundation.orghttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/9de15dd2/attachment.html>

From patrick.strateman at gmail.com  Tue Dec  8 21:29:13 2015
From: patrick.strateman at gmail.com (Patrick Strateman)
Date: Tue, 8 Dec 2015 13:29:13 -0800
Subject: [bitcoin-dev] Scaling by Partitioning
In-Reply-To: <CABCnA7Vb1JA6E+heXZZ=DKcsK9gusa6tSNEL5AkGRLOT2ZND6w@mail.gmail.com>
References: <CABCnA7Wqz76m8qo5BYT41Z=hBH+fUfOc4xsFAGg=Niv7Jgkqsg@mail.gmail.com>
	<56674280.3010003@gmail.com>
	<CABCnA7Vb1JA6E+heXZZ=DKcsK9gusa6tSNEL5AkGRLOT2ZND6w@mail.gmail.com>
Message-ID: <56674BA9.8090702@gmail.com>

If partition is selected from a random key (the hash of the output for
example) then payment recipients would need to operate a full node on
each of the chains.

What's the point of partitioning if virtually everybody needs to operate
each partition?

The mining aspect has it's own set of issues, but I'm not going to get
into those.

On 12/08/2015 01:23 PM, Akiva Lichtner wrote:
> It's true that miners would have to be prepared to work on any
> partition. I don't see where the number affects defeating double
> spending, what matters is the nonce in the block that keeps the next
> successful miner random.
>
> I expect that the number of miners would be ten times larger as well,
> so an attacker would have no advantage working on one partition.
>
> On Tue, Dec 8, 2015 at 3:50 PM, Patrick Strateman via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org
> <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:
>
>     Payment recipients would need to operate a daemon for each chain,
>     thus guaranteeing no scaling advantage.
>
>     (There are other issues, but I believe that to be enough of a show
>     stopper not to continue).
>
>     On 12/08/2015 08:27 AM, Akiva Lichtner via bitcoin-dev wrote:
>>     Hello,
>>
>>     I am seeking some expert feedback on an idea for scaling Bitcoin.
>>     As a brief introduction: I work in the payment industry and I
>>     have twenty years' experience in development. I have some
>>     experience with process groups and ordering protocols too. I
>>     think I understand Satoshi's paper but I admit I have not read
>>     the source code.
>>
>>     The idea is to run more than one simultaneous chain, each chain
>>     defeating double spending on only part of the coin. The coin
>>     would be partitioned by radix (or modulus, not sure what to call
>>     it.) For example in order to multiply throughput by a factor of
>>     ten you could run ten parallel chains, one would work on coin
>>     that ends in "0", one on coin that ends in "1", and so on up to "9".
>>
>>     The number of chains could increase automatically over time based
>>     on the moving average of transaction volume.
>>
>>     Blocks would have to contain the number of the partition they
>>     belong to, and miners would have to round-robin through
>>     partitions so that an attacker would not have an unfair advantage
>>     working on just one partition.
>>
>>     I don't think there is much impact to miners, but clients would
>>     have to send more than one message in order to spend money.
>>     Client messages will need to enumerate coin using some sort of
>>     compression, to save space. This seems okay to me since often in
>>     computing client software does have to break things up in equal
>>     parts (e.g. memory pages, file system blocks,) and the client
>>     software could hide the details.
>>
>>     Best wishes for continued success to the project.
>>
>>     Regards,
>>     Akiva
>>
>>     P.S. I found a funny anagram for SATOSHI NAKAMOTO: "NSA IS OOOK
>>     AT MATH"
>>
>>
>>
>>     _______________________________________________
>>     bitcoin-dev mailing list
>>     bitcoin-dev at lists.linuxfoundation.org
>>     <mailto:bitcoin-dev at lists.linuxfoundation.org>
>>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
>     _______________________________________________
>     bitcoin-dev mailing list
>     bitcoin-dev at lists.linuxfoundation.org
>     <mailto:bitcoin-dev at lists.linuxfoundation.org>
>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/e19fe53b/attachment.html>

From cp368202 at ohiou.edu  Tue Dec  8 22:27:48 2015
From: cp368202 at ohiou.edu (Chris Priest)
Date: Tue, 8 Dec 2015 14:27:48 -0800
Subject: [bitcoin-dev] BIP 9 style version bits for txns
In-Reply-To: <CACrzPenQadaNMNgnN_1fv5tqspRDtGUgzEmGsDtjMOdjWedZjQ@mail.gmail.com>
References: <CACrzPenvAWdkgRG3Y7P31JiNEVRYvd+f1nMp=QRhAp5P_eGRow@mail.gmail.com>
	<CAAcC9ysviyCaajpwZezzLnPhVVeFxgTKNfFuH6o-CyXNFNBHbA@mail.gmail.com>
	<CACrzPenQadaNMNgnN_1fv5tqspRDtGUgzEmGsDtjMOdjWedZjQ@mail.gmail.com>
Message-ID: <CAAcC9yuunYdmmpA0qF7ftvOsZ3hGKWKv-FAByA8P4u03vuEudw@mail.gmail.com>

A wallet doesn't receive transactions from other wallets. That is what
a node does. Wallets just make transactions and then sends them to the
nodes. Nodes then send them to other nodes.

In the early days of bitcoin, all wallets were nodes, but now a lot of
wallets are just wallets with out any specific node. For instance, SPV
wallets, they don't get their UTXO data from any one node that can or
can not support a feature. They get UTXO data from many nodes, some of
which could support said feature, others may not.

The nature of the work that nodes perform, they *should* broadcast
what features they support. The only nodes that matter to the network
are nodes that produce blocks. Nodes that don't produce blocks are
kind of just there, serving whoever happens to connect... I guess
nodes could broadcast their supported implementations of via part of
the version message that is part of the p2p handshake process...

On 12/8/15, Vincent Truong <vincent.truong at procabiak.com> wrote:
> I suppose whether the wallet devs want to implement the soft fork or not is
> irrelevant. They only need to indicate if they are ready i.e. they've
> tested the new soft fork, hard fork or feature and validated that it
> doesn't break their nodes or wallet software.
> On Dec 9, 2015 6:40 AM, "Chris Priest" <cp368202 at ohiou.edu> wrote:
>
>> I proposed in my Wildcard Inputs BIP that the version field be split
>> in two. The first 4 bytes are version number (which in practice is
>> being used for script version), and the second 4 bits are used for
>> transaction type.
>>
>> I don't think the BIP9 mechanism really applies to transactions. A
>> block is essentially a collection of transactions, therefore voting on
>> the block applies to the many parties who have transactions in the
>> block. A transaction on the other hand only effects at most two
>> parties (the sender and the receiver). In other words, block are
>> "communal" data structures, transactions are individual data
>> structures. Also, the nature of soft forks are that wallets can choose
>> to implement a new feature or not. For instance, if no wallets
>> implement RBF or SW, then those features effectively don't exist,
>> regardless of how many nodes have upgraded to handle the feature.
>>
>> Any new transaction feature should get a new "type" number. A new
>> transaction feature can't happen until the nodes support it.
>>
>> On 12/8/15, Vincent Truong via bitcoin-dev
>> <bitcoin-dev at lists.linuxfoundation.org> wrote:
>> > So I have been told more than once that the version announcement in
>> blocks
>> > is not a vote, but a signal for readiness, used in isSupermajority().
>> > Basically, if soft forks (and hard forks) won't activate unless a
>> certain %
>> > of blocks have been flagged with the version up (or bit flipped when
>> > versionbits go live) to signal their readiness, that is a vote against
>> > implementation if they never follow up. I don't like this politically
>> > correct speech because in reality it is a vote... But I'm not here to
>> argue
>> > about that... I would like to see if there are any thoughts on
>> > extending/copying isSupermajority() for a new secondary/non-critical
>> > function to also look for a similar BIP 9 style version bit in txns.
>> > Apologies if already proposed, haven't heard of it anywhere.
>> >
>> > If we are looking for a signal of readiness, it is unfair to wallet
>> > developers and exchanges that they are unable to signal if they too are
>> > ready for a change. As more users are going into use SPV or SPV-like
>> > wallets, when a change occurs that makes them incompatible/in need of
>> > upgrade we need to make sure they aren't going to break or introduce
>> > security flaws for users.
>> >
>> > If a majority of transactions have been sent are flagged ready, we know
>> > that they're also good to go.
>> >
>> > Would you implement the same versionbits for txn's version field, using
>> > 3
>> > bits for versioning and 29 bits for flags? This indexing of every txn
>> might
>> > sound insane and computationally expensive for bitcoin Core to run, but
>> if
>> > it isn't critical to upgrade with soft forks, then it can be watched
>> > outside the network by enthusiasts. I believe this is the most
>> politically
>> > correct way to get wallet devs and exchanges involved. (If it were me I
>> > would absolutely try figure out a way to stick it in
>> > isSupermajority...)
>> >
>> > Miners can watch for readiness flagged by wallets before they
>> > themselves
>> > flag ready. We will have to trust miners to not jump the gun, but
>> > that's
>> > the trade off.
>> >
>> > Thoughts?
>> >
>>
>

From j at toom.im  Tue Dec  8 23:40:42 2015
From: j at toom.im (Jonathan Toomim)
Date: Wed, 9 Dec 2015 07:40:42 +0800
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <5666FD8D.8050201@openbitcoinprivacyproject.org>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
	<5666FD8D.8050201@openbitcoinprivacyproject.org>
Message-ID: <2030FF3C-4F65-44E6-A9D5-9CD144179994@toom.im>

Agree. This data does not belong in the coinbase. That space is for miners to use, not devs.

I also think that a hard fork is better for SegWit, as it reduces the size of fraud proofs considerably, makes the whole design more elegant and less kludgey, and is safer for clients who do not upgrade in a timely fashion. I don't like the idea that SegWit would invalidate the security assumptions of non-upgraded clients (including SPV wallets). I think that for these clients, no data is better than invalid data. Better to force them to upgrade by cutting them off the network than to let them think they're validating transactions when they're not.


On Dec 8, 2015, at 11:55 PM, Justus Ranvier via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:

> If such a change is going to be deployed via a soft fork instead of a
> hard fork, then the coinbase is the worst place to put the segwitness
> merkle root.
> 
> Instead, put it in the first output of the generation transaction as an
> OP_RETURN script.
> 
> This is a better pattern because coinbase space is limited while output
> space is not. The next time there's a good reason to tie another merkle
> tree to a block, that proposal can be designated for the second output
> of the generation transaction.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/a4037777/attachment.sig>

From j at toom.im  Tue Dec  8 23:48:58 2015
From: j at toom.im (Jonathan Toomim)
Date: Wed, 9 Dec 2015 07:48:58 +0800
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
Message-ID: <5F73C59C-7939-4937-839D-CA93880CB21F@toom.im>

On Dec 8, 2015, at 6:02 AM, Gregory Maxwell via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:

> The particular proposal amounts to a 4MB blocksize increase at worst.

I understood that SegWit would allow about 1.75 MB of data in the average case while also allowing up to 4 MB of data in the worst case. This means that the mining and block distribution network would need a larger safety factor to deal with worst-case situations, right? If you want to make sure that nothing goes wrong when everything is at its worst, you need to size your network pipes to handle 4 MB in a timely (DoS-resistant) fashion, but you'd normally only be able to use 1.75 MB of it. It seems to me that it would be safer to use a 3 MB limit, and that way you'd also be able to use 3 MB of actual transactions.

As an accounting trick to bypass the 1 MB limit, SegWit sounds like it might make things less well accounted for.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/a0eca647/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/a0eca647/attachment.sig>

From luke at dashjr.org  Tue Dec  8 23:48:53 2015
From: luke at dashjr.org (Luke Dashjr)
Date: Tue, 8 Dec 2015 23:48:53 +0000
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <2030FF3C-4F65-44E6-A9D5-9CD144179994@toom.im>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<5666FD8D.8050201@openbitcoinprivacyproject.org>
	<2030FF3C-4F65-44E6-A9D5-9CD144179994@toom.im>
Message-ID: <201512082348.54788.luke@dashjr.org>

On Tuesday, December 08, 2015 11:40:42 PM Jonathan Toomim via bitcoin-dev 
wrote:
> Agree. This data does not belong in the coinbase. That space is for miners
> to use, not devs.

This has never been guaranteed, nor are softforks a "dev action" in the first 
place.

> I also think that a hard fork is better for SegWit, as it reduces the size
> of fraud proofs considerably, makes the whole design more elegant and less
> kludgey, and is safer for clients who do not upgrade in a timely fashion.

How about we pursue the SegWit softfork, and at the same time* work on a 
hardfork which will simplify the proofs and reduce the kludgeyness of merge-
mining in general? Then, if the hardfork is ready before the softfork, they 
can both go together, but if not, we aren't stuck delaying the improvements of 
SegWit until the hardfork is completed.

* I have been in fact working on such a proposal for a while now, since before 
SegWit.

> I don't like the idea that SegWit would invalidate the security
> assumptions of non-upgraded clients (including SPV wallets). I think that
> for these clients, no data is better than invalid data. Better to force
> them to upgrade by cutting them off the network than to let them think
> they're validating transactions when they're not.

There isn't an option for "no data", as non-upgraded nodes in a hardfork are 
left completely vulnerable to attacking miners, even much lower hashrate than 
the 51% attack risk. So the alternatives are:
- hardfork: complete loss of all security for the old nodes
- softfork: degraded security for old nodes

Luke

From jtimon at jtimon.cc  Tue Dec  8 23:50:35 2015
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Wed, 9 Dec 2015 00:50:35 +0100
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <2030FF3C-4F65-44E6-A9D5-9CD144179994@toom.im>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
	<5666FD8D.8050201@openbitcoinprivacyproject.org>
	<2030FF3C-4F65-44E6-A9D5-9CD144179994@toom.im>
Message-ID: <CABm2gDoUiYb1giLd+=8a-tm+0P0+PLJQpQkffe2DZbN0z887Ww@mail.gmail.com>

On Dec 9, 2015 7:41 AM, "Jonathan Toomim via bitcoin-dev" <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> I also think that a hard fork is better for SegWit, as it reduces the
size of fraud proofs considerably, makes the whole design more elegant and
less kludgey, and is safer for clients who do not upgrade in a timely
fashion.

I agree, although I disagree with the last reason.

> I don't like the idea that SegWit would invalidate the security
assumptions of non-upgraded clients (including SPV wallets). I think that
for these clients, no data is better than invalid data. Better to force
them to upgrade by cutting them off the network than to let them think
they're validating transactions when they're not.

I don't undesrtand. SPV nodes won't think they are validating transactions
with the new version unless they adapt to the new format. They will be
simply unable to receive payments using the new format if it is a softfork
(although as said I agree with making it a hardfork on the simpler design
and smaller fraud proofs grounds alone).

>
> On Dec 8, 2015, at 11:55 PM, Justus Ranvier via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:
>
> > If such a change is going to be deployed via a soft fork instead of a
> > hard fork, then the coinbase is the worst place to put the segwitness
> > merkle root.
> >
> > Instead, put it in the first output of the generation transaction as an
> > OP_RETURN script.
> >
> > This is a better pattern because coinbase space is limited while output
> > space is not. The next time there's a good reason to tie another merkle
> > tree to a block, that proposal can be designated for the second output
> > of the generation transaction.
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/2af9dc6d/attachment.html>

From greg at xiph.org  Tue Dec  8 23:59:33 2015
From: greg at xiph.org (Gregory Maxwell)
Date: Tue, 8 Dec 2015 23:59:33 +0000
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
Message-ID: <CAAS2fgTGYSiAJHZq80rD4UieV8XetS=W0b45b5onWAS9gF-F7g@mail.gmail.com>

On Tue, Dec 8, 2015 at 3:12 PM, Gavin Andresen via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> Why segwitness as a soft fork? Stuffing the segwitness merkle tree in the
> coinbase is messy and will just complicate consensus-critical code (as
> opposed to making the right side of the merkle tree in block.version=5
> blocks the segwitness data).

It's nearly complexity-costless to put it in the coinbase transaction.
Exploring the costs is one of the reasons why this was implemented
first.

We already have consensus critical enforcement there, the height,
which has almost never been problematic. (A popular block explorer
recently misimplemented the var-int decode and suffered an outage).

And most but not all prior commitment proposals have suggested the
same or similar.  The exact location is not that critical, however,
and we do have several soft-fork compatible options.

> It will also make any segwitness fraud proofs significantly larger (merkle
> path versus  merkle path to coinbase transactions, plus ENTIRE coinbase
> transaction, which might be quite large, plus merkle path up to root).

Yes, it will make them larger by log2() the number of transaction in a
block which is-- say-- 448 bytes.

With the coinbase transaction thats another couple kilobytes, I think
this is negligible.

>From a risk reduction perspective, I think it is much preferable to
perform the primary change in a backwards compatible manner, and pick
up the data reorganization in a hardfork if anyone even cares.

I think thats generally a nice cadence to split up risks that way; and
avoid controversy.

> We also need to fix the O(n^2) sighash problem as an additional BIP for ANY
> blocksize increase.

The witness data is never an input to sighash, so no, I don't agree
that this holds for "any" increase.

> Segwitness will make the current bottleneck (block propagation) a little
> worse in the short term, because of the extra fraud-proof data.  Benefits
> well worth the costs.

The fraud proof data is deterministic, full nodes could skip sending
it between each other, if anyone cared; but the overhead is pretty
tiny in any case.

> I think a barrier to quickly getting consensus might be a fundamental
> difference of opinion on this:
>    "Even without them I believe we?ll be in an acceptable position with
> respect to capacity in the near term"
>
> The heaviest users of the Bitcoin network (businesses who generate tens of
> thousands of transactions per day on behalf of their customers) would
> strongly disgree; the current state of affairs is NOT acceptable to them.

My message lays out a plan for several different complementary
capacity advances; it's not referring to the current situation--
though the current capacity situation is no emergency.

I believe it already reflects the emerging consensus in the Bitcoin
Core project; in terms of the overall approach and philosophy, if not
every specific technical detail. It's not a forever plan, but a
pragmatic one that understand that the future is uncertain no matter
what we do; one that trusts that we'll respond to whatever
contingencies surprise us on the road to success.

From greg at xiph.org  Wed Dec  9 00:23:27 2015
From: greg at xiph.org (Gregory Maxwell)
Date: Wed, 9 Dec 2015 00:23:27 +0000
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <5F73C59C-7939-4937-839D-CA93880CB21F@toom.im>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<5F73C59C-7939-4937-839D-CA93880CB21F@toom.im>
Message-ID: <CAAS2fgQEfRCAtHVEbcuyDtcP7kf-HBPtow-qznRBNmeAURMe+w@mail.gmail.com>

On Tue, Dec 8, 2015 at 11:48 PM, Jonathan Toomim <j at toom.im> wrote:
> I understood that SegWit would allow about 1.75 MB of data in the average
> case while also allowing up to 4 MB of data in the worst case. This means
> that the mining and block distribution network would need a larger safety
> factor to deal with worst-case situations, right? If you want to make sure

By contrast it does not reduce the safety factor for the UTXO set at
all; which most hold as a much greater concern in general; and that
isn't something you can say for a block size increase.

With respect to witness safety factor; it's only needed in the case of
strategic or malicious behavior by miners-- both concerns which
several people promoting large block size increases have not only
disregarded but portrayed as unrealistic fear-mongering. Are you
concerned about it?  In any case-- the other improvements described in
my post give me reason to believe that risks created by that
possibility will be addressable.

From j at toom.im  Wed Dec  9 00:40:46 2015
From: j at toom.im (Jonathan Toomim)
Date: Wed, 9 Dec 2015 08:40:46 +0800
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAAS2fgRP8bLWZoKR9-iJS-2RKTGQQ9NG-LpAfa2BOdcR=GuB_A@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<5F73C59C-7939-4937-839D-CA93880CB21F@toom.im>
	<CAAS2fgRP8bLWZoKR9-iJS-2RKTGQQ9NG-LpAfa2BOdcR=GuB_A@mail.gmail.com>
Message-ID: <411150E9-8811-43B9-8285-DC2EB3BD1C50@toom.im>


On Dec 9, 2015, at 8:09 AM, Gregory Maxwell <gmaxwell at gmail.com> wrote:

> On Tue, Dec 8, 2015 at 11:48 PM, Jonathan Toomim <j at toom.im> wrote:
> 
> By contrast it does not reduce the safety factor for the UTXO set at
> all; which most hold as a much greater concern in general;

I don't agree that "most" hold UTXO as a much greater concern in general. I think that it's a concern that has been addressed less, which means it is a more unsolved concern. But it is not currently a bottleneck on block size. Miners can afford way more RAM than 1 GB, and non-mining full nodes don't need to store the UTXO in memory.I think that at the moment, block propagation time is the bottleneck, not UTXO size. It confuses me that SigWit is being pushed as a short-term fix to the capacity issue when it does not address the short-term bottleneck at all.

> and that
> isn't something you can say for a block size increase.

True.

I'd really like to see a grand unified cost metric that includes UTXO expansion. In the mean time, I think miners can use a bit more RAM.

> With respect to witness safety factor; it's only needed in the case of
> strategic or malicious behavior by miners-- both concerns which
> several people promoting large block size increases have not only
> disregarded but portrayed as unrealistic fear-mongering. Are you
> concerned about it?

Some. Much less than e.g. Peter Todd, for example, but when other people see something as a concern that I don't, I try to pay attention to it. I expect Peter wouldn't like the safety factor issue, and I'm surprised he didn't bring it up.

Even if I didn't care about adversarial conditions, it would still interest me to pay attention to the safety factor for political reasons, as it would make subsequent blocksize increases much more difficult. Conspiracy theorists might have a field day with that one...

> In any case-- the other improvements described in
> my post give me reason to believe that risks created by that
> possibility will be addressable.

I'll take a look and try to see which of the worst-case concerns can and cannot be addressed by those improvements.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/c3a9894f/attachment.sig>

From j at toom.im  Wed Dec  9 00:54:38 2015
From: j at toom.im (Jonathan Toomim)
Date: Wed, 9 Dec 2015 08:54:38 +0800
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <201512082348.54788.luke@dashjr.org>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<5666FD8D.8050201@openbitcoinprivacyproject.org>
	<2030FF3C-4F65-44E6-A9D5-9CD144179994@toom.im>
	<201512082348.54788.luke@dashjr.org>
Message-ID: <52A2BDFA-FEEC-459F-A3CB-07F3DFAD0732@toom.im>


On Dec 9, 2015, at 7:48 AM, Luke Dashjr <luke at dashjr.org> wrote:

> How about we pursue the SegWit softfork, and at the same time* work on a
> hardfork which will simplify the proofs and reduce the kludgeyness of merge-
> mining in general? Then, if the hardfork is ready before the softfork, they
> can both go together, but if not, we aren't stuck delaying the improvements of
> SegWit until the hardfork is completed.

So that all our code that parses the blockchain needs to be able to find the sigwit data in both places? That doesn't really sound like an improvement to me. Why not just do it as a hard fork? They're really not that hard to do.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/96a8d6a7/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/96a8d6a7/attachment.sig>

From j at toom.im  Wed Dec  9 00:56:25 2015
From: j at toom.im (Jonathan Toomim)
Date: Wed, 9 Dec 2015 08:56:25 +0800
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CABm2gDoUiYb1giLd+=8a-tm+0P0+PLJQpQkffe2DZbN0z887Ww@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
	<5666FD8D.8050201@openbitcoinprivacyproject.org>
	<2030FF3C-4F65-44E6-A9D5-9CD144179994@toom.im>
	<CABm2gDoUiYb1giLd+=8a-tm+0P0+PLJQpQkffe2DZbN0z887Ww@mail.gmail.com>
Message-ID: <9FA8EA6D-91E0-4DA5-8D1F-094787B22797@toom.im>


On Dec 9, 2015, at 7:50 AM, Jorge Tim?n <jtimon at jtimon.cc> wrote:

> I don't undesrtand. SPV nodes won't think they are validating transactions with the new version unless they adapt to the new format. They will be simply unable to receive payments using the new format if it is a softfork (although as said I agree with making it a hardfork on the simpler design and smaller fraud proofs grounds alone).
> 
Okay, I might just not understand how a sigwit payment would look to current software yet. I'll add learning about that to my to-do list...
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/b3c8f971/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/b3c8f971/attachment.sig>

From jtimon at jtimon.cc  Wed Dec  9 00:58:06 2015
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Wed, 9 Dec 2015 01:58:06 +0100
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAAS2fgTGYSiAJHZq80rD4UieV8XetS=W0b45b5onWAS9gF-F7g@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
	<CAAS2fgTGYSiAJHZq80rD4UieV8XetS=W0b45b5onWAS9gF-F7g@mail.gmail.com>
Message-ID: <CABm2gDrb4ka_sYG-jRzg=gqWvtxLysbRxXCLYL+F5HMqDP0jGg@mail.gmail.com>

On Wed, Dec 9, 2015 at 12:59 AM, Gregory Maxwell via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> On Tue, Dec 8, 2015 at 3:12 PM, Gavin Andresen via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org> wrote:
> We already have consensus critical enforcement there, the height,
> which has almost never been problematic. (A popular block explorer
> recently misimplemented the var-int decode and suffered an outage).

It would be also a nice opportunity to move the height to a more
accessible place.
For example CBlockHeader::hashMerkleRoot (and CBlockIndex's) could be
replaced with a hash of the following struct:

struct hashRootStruct
{
uint256 hashMerkleRoot;
uint256 hashWitnessesRoot;
int32_t nHeight;
}

> From a risk reduction perspective, I think it is much preferable to
> perform the primary change in a backwards compatible manner, and pick
> up the data reorganization in a hardfork if anyone even cares.


But then all wallet software will need to adapt their software twice.
Why introduce technical debt for no good reason?

> I think thats generally a nice cadence to split up risks that way; and
> avoid controversy.

Uncontroversial hardforks can also be deployed with small risks as
described in BIP99.

From jtimon at jtimon.cc  Wed Dec  9 01:02:58 2015
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Wed, 9 Dec 2015 02:02:58 +0100
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CABm2gDrb4ka_sYG-jRzg=gqWvtxLysbRxXCLYL+F5HMqDP0jGg@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
	<CAAS2fgTGYSiAJHZq80rD4UieV8XetS=W0b45b5onWAS9gF-F7g@mail.gmail.com>
	<CABm2gDrb4ka_sYG-jRzg=gqWvtxLysbRxXCLYL+F5HMqDP0jGg@mail.gmail.com>
Message-ID: <CABm2gDrBbCE16rWgRf73=4ZQmkjHKWKccu9wH-V1q4jenjV3_Q@mail.gmail.com>

On Wed, Dec 9, 2015 at 1:58 AM, Jorge Tim?n <jtimon at jtimon.cc> wrote:
> struct hashRootStruct
> {
> uint256 hashMerkleRoot;
> uint256 hashWitnessesRoot;
> int32_t nHeight;
> }

Or better, for forward compatibility (we may want to include more
things apart from nHeight and hashWitnessesRoot in the future):

struct hashRootStruct
{
 uint256 hashMerkleRoot;
 uint256 hashWitnessesRoot;
 uint256 hashextendedHeader;
}

For example, we may want to chose to add an extra nonce there.

From gavinandresen at gmail.com  Wed Dec  9 01:09:16 2015
From: gavinandresen at gmail.com (Gavin Andresen)
Date: Tue, 8 Dec 2015 20:09:16 -0500
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAAS2fgTGYSiAJHZq80rD4UieV8XetS=W0b45b5onWAS9gF-F7g@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
	<CAAS2fgTGYSiAJHZq80rD4UieV8XetS=W0b45b5onWAS9gF-F7g@mail.gmail.com>
Message-ID: <CABsx9T1i50Gvxj18W=n2mYGNpsMrSkDT26CdA3aQqT5FFN86yw@mail.gmail.com>

On Tue, Dec 8, 2015 at 6:59 PM, Gregory Maxwell <greg at xiph.org> wrote:

> > We also need to fix the O(n^2) sighash problem as an additional BIP for
> ANY
> > blocksize increase.
>
> The witness data is never an input to sighash, so no, I don't agree
> that this holds for "any" increase.
>

Here's the attack:

Create a 1-megabyte transaction, with all of it's inputs spending
segwitness-spending SIGHASH_ALL inputs.

Because the segwitness inputs are smaller in the block, you can fit more of
them into 1 megabyte. Each will hash very close to one megabyte of data.

That will be O(n^2) worse than the worst case of a 1-megabyte transaction
with signatures in the scriptSigs.

Did I misunderstand something or miss something about the 1-mb transaction
data and 3-mb segwitness data proposal that would make this attack not
possible?

RE: fraud proof data being deterministic:  yes, I see, the data can be
computed instead of broadcast with the block.

RE: emerging consensus of Core:

I think it is a huge mistake not to "design for success" (see
http://gavinandresen.ninja/designing-for-success ).

I think it is a huge mistake to pile on technical debt in
consensus-critical code. I think we should be working harder to make things
simpler, not more complex, whenever possible.

And I think there are pretty big self-inflicted current problems because
worries about theoretical future problems have prevented us from coming to
consensus on simple solutions.

-- 
--
Gavin Andresen
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/58a8269d/attachment.html>

From greg at xiph.org  Wed Dec  9 01:31:51 2015
From: greg at xiph.org (Gregory Maxwell)
Date: Wed, 9 Dec 2015 01:31:51 +0000
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CABsx9T1i50Gvxj18W=n2mYGNpsMrSkDT26CdA3aQqT5FFN86yw@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
	<CAAS2fgTGYSiAJHZq80rD4UieV8XetS=W0b45b5onWAS9gF-F7g@mail.gmail.com>
	<CABsx9T1i50Gvxj18W=n2mYGNpsMrSkDT26CdA3aQqT5FFN86yw@mail.gmail.com>
Message-ID: <CAAS2fgSxpSat3VOje3-C4zgaRUVJVx-eRJbSYJqhvfR5SvCDwA@mail.gmail.com>

On Wed, Dec 9, 2015 at 1:09 AM, Gavin Andresen <gavinandresen at gmail.com> wrote:
> Create a 1-megabyte transaction, with all of it's inputs spending
> segwitness-spending SIGHASH_ALL inputs.
>
> Because the segwitness inputs are smaller in the block, you can fit more of
> them into 1 megabyte. Each will hash very close to one megabyte of data.

Witness size comes out of the 1MB at a factor of 0.25. It is not
possible to make a block which has signatures with the full 1MB of
data under the sighash while also having signatures externally.  So
every byte moved into the witness and thus only counted as 25% comes
out of the data being hashed and is hashed nInputs (*checksigs) less
times.

> I think it is a huge mistake not to "design for success" (see
> http://gavinandresen.ninja/designing-for-success ).

We are designing for success; including the success of being able to
adapt and cope with uncertainty-- which is the most critical kind of
success we can have in a world where nothing is and can be
predictable.

> I think it is a huge mistake to pile on technical debt in consensus-critical
> code. I think we should be working harder to make things simpler, not more
> complex, whenever possible.

I agree, but nothing I have advocated creates significant technical
debt. It is also a bad engineering practice to combine functional
changes (especially ones with poorly understood system wide
consequences and low user autonomy) with structural tidying.

> And I think there are pretty big self-inflicted current problems because
> worries about theoretical future problems have prevented us from coming to
> consensus on simple solutions.

That isn't my perspective. I believe we've suffered delays because of
a strong desire to be inclusive and hear out all ideas, and not
forestall market adoption, even for ideas that eschewed pragmatism and
tried to build for forever in a single step and which in our hear of
hearts we knew were not the right path today. It's time to move past
that and get back on track with the progress can make and have been
making, in terms of capacity as well as many other areas. I think that
is designing for success.

From akiva.lichtner at gmail.com  Tue Dec  8 21:41:07 2015
From: akiva.lichtner at gmail.com (Akiva Lichtner)
Date: Tue, 8 Dec 2015 16:41:07 -0500
Subject: [bitcoin-dev] Scaling by Partitioning
In-Reply-To: <56674BA9.8090702@gmail.com>
References: <CABCnA7Wqz76m8qo5BYT41Z=hBH+fUfOc4xsFAGg=Niv7Jgkqsg@mail.gmail.com>
	<56674280.3010003@gmail.com>
	<CABCnA7Vb1JA6E+heXZZ=DKcsK9gusa6tSNEL5AkGRLOT2ZND6w@mail.gmail.com>
	<56674BA9.8090702@gmail.com>
Message-ID: <CABCnA7UuYmNDzwyV=7cF2SO3RiVr1Oa2U70bYbgQqes43490_g@mail.gmail.com>

If the system is modified to scale up that means the number of transactions
is going up. That means the number of miners can also go up, and so will
the portion of malicious nodes. At least this seems reasonable. The problem
with partitions is that an attacker can focus on one partition. However
because the number of miners also increases any attacks will fail as long
as the miners are willing to work on any partition, which is easily
accomplished by round-robin.

Since there are N times more miners each miner still does the same amount
of work. The system scales by partitioning the money supply and increasing
the number of miners.



On Tue, Dec 8, 2015 at 4:29 PM, Patrick Strateman via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> If partition is selected from a random key (the hash of the output for
> example) then payment recipients would need to operate a full node on each
> of the chains.
>
> What's the point of partitioning if virtually everybody needs to operate
> each partition?
>
> The mining aspect has it's own set of issues, but I'm not going to get
> into those.
>
> On 12/08/2015 01:23 PM, Akiva Lichtner wrote:
>
> It's true that miners would have to be prepared to work on any partition.
> I don't see where the number affects defeating double spending, what
> matters is the nonce in the block that keeps the next successful miner
> random.
>
> I expect that the number of miners would be ten times larger as well, so
> an attacker would have no advantage working on one partition.
>
> On Tue, Dec 8, 2015 at 3:50 PM, Patrick Strateman via bitcoin-dev <
> <bitcoin-dev at lists.linuxfoundation.org>
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> Payment recipients would need to operate a daemon for each chain, thus
>> guaranteeing no scaling advantage.
>>
>> (There are other issues, but I believe that to be enough of a show
>> stopper not to continue).
>>
>> On 12/08/2015 08:27 AM, Akiva Lichtner via bitcoin-dev wrote:
>>
>> Hello,
>>
>> I am seeking some expert feedback on an idea for scaling Bitcoin. As a
>> brief introduction: I work in the payment industry and I have twenty years'
>> experience in development. I have some experience with process groups and
>> ordering protocols too. I think I understand Satoshi's paper but I admit I
>> have not read the source code.
>>
>> The idea is to run more than one simultaneous chain, each chain defeating
>> double spending on only part of the coin. The coin would be partitioned by
>> radix (or modulus, not sure what to call it.) For example in order to
>> multiply throughput by a factor of ten you could run ten parallel chains,
>> one would work on coin that ends in "0", one on coin that ends in "1", and
>> so on up to "9".
>>
>> The number of chains could increase automatically over time based on the
>> moving average of transaction volume.
>>
>> Blocks would have to contain the number of the partition they belong to,
>> and miners would have to round-robin through partitions so that an attacker
>> would not have an unfair advantage working on just one partition.
>>
>> I don't think there is much impact to miners, but clients would have to
>> send more than one message in order to spend money. Client messages will
>> need to enumerate coin using some sort of compression, to save space. This
>> seems okay to me since often in computing client software does have to
>> break things up in equal parts (e.g. memory pages, file system blocks,) and
>> the client software could hide the details.
>>
>> Best wishes for continued success to the project.
>>
>> Regards,
>> Akiva
>>
>> P.S. I found a funny anagram for SATOSHI NAKAMOTO: "NSA IS OOOK AT MATH"
>>
>>
>>
>> _______________________________________________
>> bitcoin-dev mailing listbitcoin-dev at lists.linuxfoundation.orghttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/bbd37f43/attachment-0001.html>

From aj at erisian.com.au  Wed Dec  9 04:51:39 2015
From: aj at erisian.com.au (Anthony Towns)
Date: Wed, 9 Dec 2015 14:51:39 +1000
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAAS2fgSxpSat3VOje3-C4zgaRUVJVx-eRJbSYJqhvfR5SvCDwA@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
	<CAAS2fgTGYSiAJHZq80rD4UieV8XetS=W0b45b5onWAS9gF-F7g@mail.gmail.com>
	<CABsx9T1i50Gvxj18W=n2mYGNpsMrSkDT26CdA3aQqT5FFN86yw@mail.gmail.com>
	<CAAS2fgSxpSat3VOje3-C4zgaRUVJVx-eRJbSYJqhvfR5SvCDwA@mail.gmail.com>
Message-ID: <20151209045139.GA18566@sapphire.erisian.com.au>

On Wed, Dec 09, 2015 at 01:31:51AM +0000, Gregory Maxwell via bitcoin-dev wrote:
> On Wed, Dec 9, 2015 at 1:09 AM, Gavin Andresen <gavinandresen at gmail.com> wrote:
> > Create a 1-megabyte transaction, with all of it's inputs spending
> > segwitness-spending SIGHASH_ALL inputs.
> > Because the segwitness inputs are smaller in the block, you can fit more of
> > them into 1 megabyte. Each will hash very close to one megabyte of data.
> Witness size comes out of the 1MB at a factor of 0.25. It is not
> possible to make a block which has signatures with the full 1MB of
> data under the sighash while also having signatures externally.  So
> every byte moved into the witness and thus only counted as 25% comes
> out of the data being hashed and is hashed nInputs (*checksigs) less
> times.

So the worst case script I can come up with is:

      <pubkey> 1 0 {2OVER CHECKSIG ADD CODESEP} OP_EQUAL

which (if I didn't mess it up) would give you a redeem script of about
36B plus 4B per sigop, redeemable via a single signature that's valid
for precisely one of the checksigs.

Maxing out 20k sigops gives 80kB of redeemscript in that case; so you
could have to hash 19.9GB of data to fully verify the script with
current bitcoin rules.

Segwit with the 75% factor and the same sigop limit would make that very
slightly worse -- it'd up the hashed data by maybe 1MB in total. Without
a sigop limit at all it'd be severely worse of course -- you could fit
almost 500k sigops in 2MB of witness data, leaving 500kB of base data,
for a total of 250GB of data to hash to verify your 3MB block...

Segwit without the 75% factor, but with a 3MB of witness data limit,
makes that up to three times worse (750k sigops in 3MB of witness data,
with 1MB of base data for 750GB of data to hash), but with any reasonable
sigop limit, afaics it's pretty much the same.

However I think you could add some fairly straightforward (maybe
soft-forking) optimisations to just rule out that sort of (deliberate)
abuse; eg disallowing more than a dozen sigops per input, or just failing
checksigs with the same key in a single input, maybe. So maybe that's
not sufficiently realistic?

I think the only realistic transactions that would cause lots of sigs and
hashing are ones that have lots of inputs that each require a signature
or two, so might happen if a miner is cleaning up dust. In that case,
your 1MB transaction is a single output with a bunch of 41B inputs. If you
have 10k such inputs, that's only 410kB. If each input is a legitimate
2 of 2 multisig, that's about 210 bytes of witness data per input, or
2.1MB, leaving 475kB of base data free, which matches up. 20k sigops by
475kB of data is 9.5GB of hashing.

Switching from 2-of-2 multisig to just a single public key would prevent
you from hitting the sigop limit; I think you could hit 14900 signatures
with about 626kB of base data and 1488kB of witness data, for about
9.3GB of hashed data.

That's a factor of 2x improvement over the deliberately malicious exploit
case above, but it's /only/ a factor of 2x.

I think Rusty's calculation http://rusty.ozlabs.org/?p=522 was that
the worst case for now is hashing about 406kB, 3300 times for 1.34GB of
hashed data [0].

So that's still almost a factor of 4 or 5 worse than what's possible now?
Unless I messed up the maths somewhere?

Cheers,
aj

[0] Though I'm not sure that's correct? Seems like with a 1MB
    transaction with i inputs, each with s bytes of scriptsig, that you're
    hashing (1MB-s*i), and the scriptsig for a p2pkh should only be about
    105B, not 180B.  So maximising i*(1MB-s*i) = 1e6*i - 105*i^2 gives i =
    1e6/210, so 4762 inputs, and hashing 500kB of data each time,
    for about 2.4GB of hashed data total.


From rryananizer at gmail.com  Wed Dec  9 04:44:09 2015
From: rryananizer at gmail.com (Ryan Butler)
Date: Tue, 8 Dec 2015 22:44:09 -0600
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAAS2fgSxpSat3VOje3-C4zgaRUVJVx-eRJbSYJqhvfR5SvCDwA@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
	<CAAS2fgTGYSiAJHZq80rD4UieV8XetS=W0b45b5onWAS9gF-F7g@mail.gmail.com>
	<CABsx9T1i50Gvxj18W=n2mYGNpsMrSkDT26CdA3aQqT5FFN86yw@mail.gmail.com>
	<CAAS2fgSxpSat3VOje3-C4zgaRUVJVx-eRJbSYJqhvfR5SvCDwA@mail.gmail.com>
Message-ID: <CAF_2MyUJMdJyh7FKq6UYCtwJZQ59i-pnWT_tFEK5EQx65iwHDQ@mail.gmail.com>

>I agree, but nothing I have advocated creates significant technical
>debt. It is also a bad engineering practice to combine functional
>changes (especially ones with poorly understood system wide
>consequences and low user autonomy) with structural tidying.

I don't think I would classify placing things in consensus critical code
when it doesn't need to be as "structural tidying".  Gavin said "pile on"
which you took as implying "a lot", he can correct me, but I believe he
meant "add to".

> (especially ones with poorly understood system wide consequences and low
user autonomy)

This implies there you have no confidence in the unit tests and functional
testing around Bitcoin and should not be a reason to avoid refactoring.
It's more a reason to increase testing so that you will have confidence
when you refactor.

Also I don't think Martin Fowler would agree with you...

"Refactoring should be done in conjunction with adding new features."

"Always leave the code better than when you found it."

"Often you start working on adding new functionality and you realize the
existing structures don't play well with what you're about to do.

In this situation it usually pays to begin by refactoring the existing code
into the shape you now know is the right shape for what you're about to do."

-Martin Fowler








On Tue, Dec 8, 2015 at 7:31 PM, Gregory Maxwell via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> On Wed, Dec 9, 2015 at 1:09 AM, Gavin Andresen <gavinandresen at gmail.com>
> wrote:
> > Create a 1-megabyte transaction, with all of it's inputs spending
> > segwitness-spending SIGHASH_ALL inputs.
> >
> > Because the segwitness inputs are smaller in the block, you can fit more
> of
> > them into 1 megabyte. Each will hash very close to one megabyte of data.
>
> Witness size comes out of the 1MB at a factor of 0.25. It is not
> possible to make a block which has signatures with the full 1MB of
> data under the sighash while also having signatures externally.  So
> every byte moved into the witness and thus only counted as 25% comes
> out of the data being hashed and is hashed nInputs (*checksigs) less
> times.
>
> > I think it is a huge mistake not to "design for success" (see
> > http://gavinandresen.ninja/designing-for-success ).
>
> We are designing for success; including the success of being able to
> adapt and cope with uncertainty-- which is the most critical kind of
> success we can have in a world where nothing is and can be
> predictable.
>
> > I think it is a huge mistake to pile on technical debt in
> consensus-critical
> > code. I think we should be working harder to make things simpler, not
> more
> > complex, whenever possible.
>
> I agree, but nothing I have advocated creates significant technical
> debt. It is also a bad engineering practice to combine functional
> changes (especially ones with poorly understood system wide
> consequences and low user autonomy) with structural tidying.
>
> > And I think there are pretty big self-inflicted current problems because
> > worries about theoretical future problems have prevented us from coming
> to
> > consensus on simple solutions.
>
> That isn't my perspective. I believe we've suffered delays because of
> a strong desire to be inclusive and hear out all ideas, and not
> forestall market adoption, even for ideas that eschewed pragmatism and
> tried to build for forever in a single step and which in our hear of
> hearts we knew were not the right path today. It's time to move past
> that and get back on track with the progress can make and have been
> making, in terms of capacity as well as many other areas. I think that
> is designing for success.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151208/ac78b75c/attachment.html>

From greg at xiph.org  Wed Dec  9 06:29:53 2015
From: greg at xiph.org (Gregory Maxwell)
Date: Wed, 9 Dec 2015 06:29:53 +0000
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAF_2MyUJMdJyh7FKq6UYCtwJZQ59i-pnWT_tFEK5EQx65iwHDQ@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
	<CAAS2fgTGYSiAJHZq80rD4UieV8XetS=W0b45b5onWAS9gF-F7g@mail.gmail.com>
	<CABsx9T1i50Gvxj18W=n2mYGNpsMrSkDT26CdA3aQqT5FFN86yw@mail.gmail.com>
	<CAAS2fgSxpSat3VOje3-C4zgaRUVJVx-eRJbSYJqhvfR5SvCDwA@mail.gmail.com>
	<CAF_2MyUJMdJyh7FKq6UYCtwJZQ59i-pnWT_tFEK5EQx65iwHDQ@mail.gmail.com>
Message-ID: <CAAS2fgS-jjEVeHf_LErppTadtAaSeBum+KiGHpoo=Jz5BZArsQ@mail.gmail.com>

On Wed, Dec 9, 2015 at 4:44 AM, Ryan Butler <rryananizer at gmail.com> wrote:
>>I agree, but nothing I have advocated creates significant technical
>>debt. It is also a bad engineering practice to combine functional
>>changes (especially ones with poorly understood system wide
>>consequences and low user autonomy) with structural tidying.
>
> I don't think I would classify placing things in consensus critical code
> when it doesn't need to be as "structural tidying".  Gavin said "pile on"
> which you took as implying "a lot", he can correct me, but I believe he
> meant "add to".

Nothing being discussed would move something from consensus critical
code to not consensus critical.

What was being discussed was the location of the witness commitment;
which is consensus critical regardless of where it is placed. Should
it be placed in an available location which is compatible with the
existing network, or should the block hashing data structure
immediately be changed in an incompatible way to accommodate it in
order to satisfy an ascetic sense of purity and to make fraud proofs
somewhat smaller?

I argue that the size difference in the fraud proofs is not
interesting, the disruption to the network in an incompatible upgrade
is interesting; and that if it really were desirable reorganization to
move the commitment point could be done as part of a separate change
that changes only the location of things (and/or other trivial
adjustments); and that proceeding int this fashion would minimize
disruption and risk... by making the incompatible changes that will
force network wide software updates be as small and as simple as
possible.

>> (especially ones with poorly understood system wide consequences and low
>> user autonomy)
>
> This implies there you have no confidence in the unit tests and functional
> testing around Bitcoin and should not be a reason to avoid refactoring.
> It's more a reason to increase testing so that you will have confidence when
> you refactor.

I am speaking from our engineering experience in a  public,
world-wide, multi-vendor, multi-version, inter-operable, distributed
system which is constantly changing and in production contains private
code, unknown and assorted hardware, mixtures of versions, unreliable
networks, undisclosed usage patterns, and more sources of complex
behavior than can be counted-- including complex economic incentives
and malicious participants.

Even if we knew the complete spectrum of possible states for the
system the combinatioric explosion makes complete testing infeasible.

Though testing is essential one cannot "unit test" away all the risks
related to deploying a new behavior in the network.

From loi.luuthe at gmail.com  Wed Dec  9 06:30:24 2015
From: loi.luuthe at gmail.com (Loi Luu)
Date: Wed, 9 Dec 2015 14:30:24 +0800
Subject: [bitcoin-dev] Scaling by Partitioning
In-Reply-To: <CABCnA7Wqz76m8qo5BYT41Z=hBH+fUfOc4xsFAGg=Niv7Jgkqsg@mail.gmail.com>
References: <CABCnA7Wqz76m8qo5BYT41Z=hBH+fUfOc4xsFAGg=Niv7Jgkqsg@mail.gmail.com>
Message-ID: <CAJmQggC1X5Lgt4xGoMtBZ_v3hC2GXcYaj2FngV2_7A=TDfSuEg@mail.gmail.com>

Dear Akiva,

Its Loi Luu, one of the authors of the SCP protocol (
http://eprint.iacr.org/2015/1168.pdf ).

Before SCP, we had been thinking hard about how to do sharding efficiently
without degrading any security guarantee. A simple solution which splits
the coins, or TXs in to several partitions will just not work. You have to
answer more questions to have a good solutions. For example, I wonder in
your proposal, if a transaction spends a "coin" that ends in "1" and
creates a new coin that ends in "1", which partition should process the
transaction? What is the prior data needed to validate that kind of TXs?

The problem with other proposals, and probably yours as well,  that we see
is that the amount of data that you need to broadcast immediately to the
network increases linearly with the number of TXs that the network can
process. Thus, sharding does not bring any advantage than simply using
other techniques to publish more blocks in one epoch (like Bitcoin-NG,
Ghost). The whole point of using sharding/ partition is to localize
the bandwidth used, and only broadcast only a minimal data to the network.

Clearly we are able to localize the bandwidth used with our SCP protocol.
The cost is that now recipients need to  themselves verify whether a
transaction is double spending. However, we think that it is a reasonable
tradeoff, given the potential scalability that SCP can provides.

Thanks,
Loi Luu.

On Wed, Dec 9, 2015 at 12:27 AM, Akiva Lichtner via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Hello,
>
> I am seeking some expert feedback on an idea for scaling Bitcoin. As a
> brief introduction: I work in the payment industry and I have twenty years'
> experience in development. I have some experience with process groups and
> ordering protocols too. I think I understand Satoshi's paper but I admit I
> have not read the source code.
>
> The idea is to run more than one simultaneous chain, each chain defeating
> double spending on only part of the coin. The coin would be partitioned by
> radix (or modulus, not sure what to call it.) For example in order to
> multiply throughput by a factor of ten you could run ten parallel chains,
> one would work on coin that ends in "0", one on coin that ends in "1", and
> so on up to "9".
>
> The number of chains could increase automatically over time based on the
> moving average of transaction volume.
>
> Blocks would have to contain the number of the partition they belong to,
> and miners would have to round-robin through partitions so that an attacker
> would not have an unfair advantage working on just one partition.
>
> I don't think there is much impact to miners, but clients would have to
> send more than one message in order to spend money. Client messages will
> need to enumerate coin using some sort of compression, to save space. This
> seems okay to me since often in computing client software does have to
> break things up in equal parts (e.g. memory pages, file system blocks,) and
> the client software could hide the details.
>
> Best wishes for continued success to the project.
>
> Regards,
> Akiva
>
> P.S. I found a funny anagram for SATOSHI NAKAMOTO: "NSA IS OOOK AT MATH"
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/65affaa5/attachment-0001.html>

From rryananizer at gmail.com  Wed Dec  9 06:36:22 2015
From: rryananizer at gmail.com (Ryan Butler)
Date: Wed, 9 Dec 2015 00:36:22 -0600
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAAS2fgS-jjEVeHf_LErppTadtAaSeBum+KiGHpoo=Jz5BZArsQ@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
	<CAAS2fgTGYSiAJHZq80rD4UieV8XetS=W0b45b5onWAS9gF-F7g@mail.gmail.com>
	<CABsx9T1i50Gvxj18W=n2mYGNpsMrSkDT26CdA3aQqT5FFN86yw@mail.gmail.com>
	<CAAS2fgSxpSat3VOje3-C4zgaRUVJVx-eRJbSYJqhvfR5SvCDwA@mail.gmail.com>
	<CAF_2MyUJMdJyh7FKq6UYCtwJZQ59i-pnWT_tFEK5EQx65iwHDQ@mail.gmail.com>
	<CAAS2fgS-jjEVeHf_LErppTadtAaSeBum+KiGHpoo=Jz5BZArsQ@mail.gmail.com>
Message-ID: <CAF_2MyWd7a5_yJt8D1uY7V-QqWYCcigR0ubuJwoyxz=yAyMpVw@mail.gmail.com>

I see, thanks for clearing that up, I misread what Gavin stated.

On Wed, Dec 9, 2015 at 12:29 AM, Gregory Maxwell <greg at xiph.org> wrote:

> On Wed, Dec 9, 2015 at 4:44 AM, Ryan Butler <rryananizer at gmail.com> wrote:
> >>I agree, but nothing I have advocated creates significant technical
> >>debt. It is also a bad engineering practice to combine functional
> >>changes (especially ones with poorly understood system wide
> >>consequences and low user autonomy) with structural tidying.
> >
> > I don't think I would classify placing things in consensus critical code
> > when it doesn't need to be as "structural tidying".  Gavin said "pile on"
> > which you took as implying "a lot", he can correct me, but I believe he
> > meant "add to".
>
> Nothing being discussed would move something from consensus critical
> code to not consensus critical.
>
> What was being discussed was the location of the witness commitment;
> which is consensus critical regardless of where it is placed. Should
> it be placed in an available location which is compatible with the
> existing network, or should the block hashing data structure
> immediately be changed in an incompatible way to accommodate it in
> order to satisfy an ascetic sense of purity and to make fraud proofs
> somewhat smaller?
>
> I argue that the size difference in the fraud proofs is not
> interesting, the disruption to the network in an incompatible upgrade
> is interesting; and that if it really were desirable reorganization to
> move the commitment point could be done as part of a separate change
> that changes only the location of things (and/or other trivial
> adjustments); and that proceeding int this fashion would minimize
> disruption and risk... by making the incompatible changes that will
> force network wide software updates be as small and as simple as
> possible.
>
> >> (especially ones with poorly understood system wide consequences and low
> >> user autonomy)
> >
> > This implies there you have no confidence in the unit tests and
> functional
> > testing around Bitcoin and should not be a reason to avoid refactoring.
> > It's more a reason to increase testing so that you will have confidence
> when
> > you refactor.
>
> I am speaking from our engineering experience in a  public,
> world-wide, multi-vendor, multi-version, inter-operable, distributed
> system which is constantly changing and in production contains private
> code, unknown and assorted hardware, mixtures of versions, unreliable
> networks, undisclosed usage patterns, and more sources of complex
> behavior than can be counted-- including complex economic incentives
> and malicious participants.
>
> Even if we knew the complete spectrum of possible states for the
> system the combinatioric explosion makes complete testing infeasible.
>
> Though testing is essential one cannot "unit test" away all the risks
> related to deploying a new behavior in the network.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/21a08ea0/attachment.html>

From mark at friedenbach.org  Wed Dec  9 06:59:43 2015
From: mark at friedenbach.org (Mark Friedenbach)
Date: Wed, 9 Dec 2015 14:59:43 +0800
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAAS2fgS-jjEVeHf_LErppTadtAaSeBum+KiGHpoo=Jz5BZArsQ@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
	<CAAS2fgTGYSiAJHZq80rD4UieV8XetS=W0b45b5onWAS9gF-F7g@mail.gmail.com>
	<CABsx9T1i50Gvxj18W=n2mYGNpsMrSkDT26CdA3aQqT5FFN86yw@mail.gmail.com>
	<CAAS2fgSxpSat3VOje3-C4zgaRUVJVx-eRJbSYJqhvfR5SvCDwA@mail.gmail.com>
	<CAF_2MyUJMdJyh7FKq6UYCtwJZQ59i-pnWT_tFEK5EQx65iwHDQ@mail.gmail.com>
	<CAAS2fgS-jjEVeHf_LErppTadtAaSeBum+KiGHpoo=Jz5BZArsQ@mail.gmail.com>
Message-ID: <CAOG=w-sN5Kc5_W07iSKvZqSz_cNu50rkoQ65cP3_bWeFNcyizA@mail.gmail.com>

Greg, if you have actual data showing that putting the commitment in the
last transaction would be disruptive, and how disruptive, that would be
appreciated. Of the mining hardware I have looked at, none of it cared at
all what transactions other than the coinbase are. You need to provide a
path to the coinbase for extranonce rolling, but the witness commitment
wouldn't need to be updated.

I'm sorry but it's not clear how this would be an incompatible upgrade,
disruptive to anything other than the transaction selection code. Maybe I'm
missing something? I'm not familiar with all the hardware or pooling setups
out there.

On Wed, Dec 9, 2015 at 2:29 PM, Gregory Maxwell via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> On Wed, Dec 9, 2015 at 4:44 AM, Ryan Butler <rryananizer at gmail.com> wrote:
> >>I agree, but nothing I have advocated creates significant technical
> >>debt. It is also a bad engineering practice to combine functional
> >>changes (especially ones with poorly understood system wide
> >>consequences and low user autonomy) with structural tidying.
> >
> > I don't think I would classify placing things in consensus critical code
> > when it doesn't need to be as "structural tidying".  Gavin said "pile on"
> > which you took as implying "a lot", he can correct me, but I believe he
> > meant "add to".
>
> Nothing being discussed would move something from consensus critical
> code to not consensus critical.
>
> What was being discussed was the location of the witness commitment;
> which is consensus critical regardless of where it is placed. Should
> it be placed in an available location which is compatible with the
> existing network, or should the block hashing data structure
> immediately be changed in an incompatible way to accommodate it in
> order to satisfy an ascetic sense of purity and to make fraud proofs
> somewhat smaller?
>
> I argue that the size difference in the fraud proofs is not
> interesting, the disruption to the network in an incompatible upgrade
> is interesting; and that if it really were desirable reorganization to
> move the commitment point could be done as part of a separate change
> that changes only the location of things (and/or other trivial
> adjustments); and that proceeding int this fashion would minimize
> disruption and risk... by making the incompatible changes that will
> force network wide software updates be as small and as simple as
> possible.
>
> >> (especially ones with poorly understood system wide consequences and low
> >> user autonomy)
> >
> > This implies there you have no confidence in the unit tests and
> functional
> > testing around Bitcoin and should not be a reason to avoid refactoring.
> > It's more a reason to increase testing so that you will have confidence
> when
> > you refactor.
>
> I am speaking from our engineering experience in a  public,
> world-wide, multi-vendor, multi-version, inter-operable, distributed
> system which is constantly changing and in production contains private
> code, unknown and assorted hardware, mixtures of versions, unreliable
> networks, undisclosed usage patterns, and more sources of complex
> behavior than can be counted-- including complex economic incentives
> and malicious participants.
>
> Even if we knew the complete spectrum of possible states for the
> system the combinatioric explosion makes complete testing infeasible.
>
> Though testing is essential one cannot "unit test" away all the risks
> related to deploying a new behavior in the network.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/5924c67f/attachment.html>

From greg at xiph.org  Wed Dec  9 07:17:08 2015
From: greg at xiph.org (Gregory Maxwell)
Date: Wed, 9 Dec 2015 07:17:08 +0000
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAOG=w-sN5Kc5_W07iSKvZqSz_cNu50rkoQ65cP3_bWeFNcyizA@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
	<CAAS2fgTGYSiAJHZq80rD4UieV8XetS=W0b45b5onWAS9gF-F7g@mail.gmail.com>
	<CABsx9T1i50Gvxj18W=n2mYGNpsMrSkDT26CdA3aQqT5FFN86yw@mail.gmail.com>
	<CAAS2fgSxpSat3VOje3-C4zgaRUVJVx-eRJbSYJqhvfR5SvCDwA@mail.gmail.com>
	<CAF_2MyUJMdJyh7FKq6UYCtwJZQ59i-pnWT_tFEK5EQx65iwHDQ@mail.gmail.com>
	<CAAS2fgS-jjEVeHf_LErppTadtAaSeBum+KiGHpoo=Jz5BZArsQ@mail.gmail.com>
	<CAOG=w-sN5Kc5_W07iSKvZqSz_cNu50rkoQ65cP3_bWeFNcyizA@mail.gmail.com>
Message-ID: <CAAS2fgTpWCEQxx2PGY4wj085iBG-vNzAx33bZ1ZqgORcdX0S0g@mail.gmail.com>

On Wed, Dec 9, 2015 at 6:59 AM, Mark Friedenbach <mark at friedenbach.org> wrote:
> Greg, if you have actual data showing that putting the commitment in the
> last transaction would be disruptive, and how disruptive, that would be
> appreciated. Of the mining hardware I have looked at, none of it cared at
> all what transactions other than the coinbase are. You need to provide a
> path to the coinbase for extranonce rolling, but the witness commitment
> wouldn't need to be updated.
>
> I'm sorry but it's not clear how this would be an incompatible upgrade,
> disruptive to anything other than the transaction selection code. Maybe I'm
> missing something? I'm not familiar with all the hardware or pooling setups
> out there.

I didn't comment on the transaction output. I have commented on
coinbase outputs and on a hard-fork.

Using an output in the last transaction would break the assumption
that you can truncate a block and still have a valid block. This is
used by some mining setups currently, because GBT does not generate
the coinbase transaction and so cannot know its size; and you may have
to drop the last transaction(s) to make room for it.

That a block can be truncated and still result in a valid block also
seems like a useful property to me.

If the input for that transaction is supposed to be generated from a
coinbase output some blocks earlier, then this may again run into
hardware output constraints in coinbase transactions. (But it may be
better since it wouldn't matter which output created it.). This could
likely be escaped by creating a zero value output only once and just
rolling it forward.

From jtimon at jtimon.cc  Wed Dec  9 07:54:49 2015
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Wed, 9 Dec 2015 08:54:49 +0100
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAAS2fgS-jjEVeHf_LErppTadtAaSeBum+KiGHpoo=Jz5BZArsQ@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
	<CAAS2fgTGYSiAJHZq80rD4UieV8XetS=W0b45b5onWAS9gF-F7g@mail.gmail.com>
	<CABsx9T1i50Gvxj18W=n2mYGNpsMrSkDT26CdA3aQqT5FFN86yw@mail.gmail.com>
	<CAAS2fgSxpSat3VOje3-C4zgaRUVJVx-eRJbSYJqhvfR5SvCDwA@mail.gmail.com>
	<CAF_2MyUJMdJyh7FKq6UYCtwJZQ59i-pnWT_tFEK5EQx65iwHDQ@mail.gmail.com>
	<CAAS2fgS-jjEVeHf_LErppTadtAaSeBum+KiGHpoo=Jz5BZArsQ@mail.gmail.com>
Message-ID: <CABm2gDq4f0ettDhh14jZ0zztSwSJ0Z=KDEeMTOJxTHF8VV+KXw@mail.gmail.com>

On Wed, Dec 9, 2015 at 7:29 AM, Gregory Maxwell via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> What was being discussed was the location of the witness commitment;
> which is consensus critical regardless of where it is placed. Should
> it be placed in an available location which is compatible with the
> existing network, or should the block hashing data structure
> immediately be changed in an incompatible way to accommodate it in
> order to satisfy an ascetic sense of purity and to make fraud proofs
> somewhat smaller?

>From this question one could think that when you said "we can do the
cleanup hardfork later" earlier you didn't really meant it. And that
you will oppose to that hardfork later just like you are opposing to
it now.
As said I disagree that making a softfork first and then move the
commitment is less disruptive (because people will need to adapt their
software twice), but if the intention is to never do the second part
then of course I agree it would be less disruptive.
How long after the softfork would you like to do the hardfork?
1 year after the softfork? 2 years? never?

From greg at xiph.org  Wed Dec  9 08:03:45 2015
From: greg at xiph.org (Gregory Maxwell)
Date: Wed, 9 Dec 2015 08:03:45 +0000
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CABm2gDq4f0ettDhh14jZ0zztSwSJ0Z=KDEeMTOJxTHF8VV+KXw@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
	<CAAS2fgTGYSiAJHZq80rD4UieV8XetS=W0b45b5onWAS9gF-F7g@mail.gmail.com>
	<CABsx9T1i50Gvxj18W=n2mYGNpsMrSkDT26CdA3aQqT5FFN86yw@mail.gmail.com>
	<CAAS2fgSxpSat3VOje3-C4zgaRUVJVx-eRJbSYJqhvfR5SvCDwA@mail.gmail.com>
	<CAF_2MyUJMdJyh7FKq6UYCtwJZQ59i-pnWT_tFEK5EQx65iwHDQ@mail.gmail.com>
	<CAAS2fgS-jjEVeHf_LErppTadtAaSeBum+KiGHpoo=Jz5BZArsQ@mail.gmail.com>
	<CABm2gDq4f0ettDhh14jZ0zztSwSJ0Z=KDEeMTOJxTHF8VV+KXw@mail.gmail.com>
Message-ID: <CAAS2fgTAFgANJ495xiOkiW-OeFA_VZHhhR5uL+jVaoYQz_yBPg@mail.gmail.com>

On Wed, Dec 9, 2015 at 7:54 AM, Jorge Tim?n <jtimon at jtimon.cc> wrote:
> From this question one could think that when you said "we can do the
> cleanup hardfork later" earlier you didn't really meant it. And that
> you will oppose to that hardfork later just like you are opposing to
> it now.
> As said I disagree that making a softfork first and then move the
> commitment is less disruptive (because people will need to adapt their
> software twice), but if the intention is to never do the second part
> then of course I agree it would be less disruptive.
> How long after the softfork would you like to do the hardfork?
> 1 year after the softfork? 2 years? never?

I think it would be logical to do as part of a hardfork that moved
commitments generally; e.g. a better position for merged mining (such
a hardfork was suggested in 2010 as something that could be done if
merged mining was used), room for commitments to additional block
back-references for compact SPV proofs, and/or UTXO set commitments.
Part of the reason to not do it now is that the requirements for the
other things that would be there are not yet well defined. For these
other applications, the additional overhead is actually fairly
meaningful; unlike the fraud proofs.

From mark at friedenbach.org  Wed Dec  9 08:46:31 2015
From: mark at friedenbach.org (Mark Friedenbach)
Date: Wed, 9 Dec 2015 16:46:31 +0800
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAAS2fgTAFgANJ495xiOkiW-OeFA_VZHhhR5uL+jVaoYQz_yBPg@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
	<CAAS2fgTGYSiAJHZq80rD4UieV8XetS=W0b45b5onWAS9gF-F7g@mail.gmail.com>
	<CABsx9T1i50Gvxj18W=n2mYGNpsMrSkDT26CdA3aQqT5FFN86yw@mail.gmail.com>
	<CAAS2fgSxpSat3VOje3-C4zgaRUVJVx-eRJbSYJqhvfR5SvCDwA@mail.gmail.com>
	<CAF_2MyUJMdJyh7FKq6UYCtwJZQ59i-pnWT_tFEK5EQx65iwHDQ@mail.gmail.com>
	<CAAS2fgS-jjEVeHf_LErppTadtAaSeBum+KiGHpoo=Jz5BZArsQ@mail.gmail.com>
	<CABm2gDq4f0ettDhh14jZ0zztSwSJ0Z=KDEeMTOJxTHF8VV+KXw@mail.gmail.com>
	<CAAS2fgTAFgANJ495xiOkiW-OeFA_VZHhhR5uL+jVaoYQz_yBPg@mail.gmail.com>
Message-ID: <CAOG=w-vgY_6JwRYjoSnYjfULu0hspwC1E0uccB_--OHgoZd96A@mail.gmail.com>

My apologies for the apparent miscommunication earlier. It is of interest
to me that the soft-fork be done which is necessary to put a commitment in
the most efficient spot possible, in part because that commitment could be
used for other data such as the merged mining auxiliary blocks, which are
very sensitive to proof size.

Perhaps we have a different view of how the commitment transaction would be
generated. Just as GBT doesn't create the coinbase, it was my expectation
that it wouldn't generate the commitment transaction either -- but
generation of the commitment would be easy, requiring either the coinbase
txid 100 blocks back, or the commitment txid of the prior transaction (note
this impacts SPV mining). The truncation shouldn't be an issue because the
commitment txn would not be part of the list of transactions selected by
GBT, and in any case the truncation would change the witness data which
changes the commitment.

On Wed, Dec 9, 2015 at 4:03 PM, Gregory Maxwell via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> On Wed, Dec 9, 2015 at 7:54 AM, Jorge Tim?n <jtimon at jtimon.cc> wrote:
> > From this question one could think that when you said "we can do the
> > cleanup hardfork later" earlier you didn't really meant it. And that
> > you will oppose to that hardfork later just like you are opposing to
> > it now.
> > As said I disagree that making a softfork first and then move the
> > commitment is less disruptive (because people will need to adapt their
> > software twice), but if the intention is to never do the second part
> > then of course I agree it would be less disruptive.
> > How long after the softfork would you like to do the hardfork?
> > 1 year after the softfork? 2 years? never?
>
> I think it would be logical to do as part of a hardfork that moved
> commitments generally; e.g. a better position for merged mining (such
> a hardfork was suggested in 2010 as something that could be done if
> merged mining was used), room for commitments to additional block
> back-references for compact SPV proofs, and/or UTXO set commitments.
> Part of the reason to not do it now is that the requirements for the
> other things that would be there are not yet well defined. For these
> other applications, the additional overhead is actually fairly
> meaningful; unlike the fraud proofs.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/0736595c/attachment.html>

From jtimon at jtimon.cc  Wed Dec  9 11:08:14 2015
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Wed, 9 Dec 2015 12:08:14 +0100
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAAS2fgTAFgANJ495xiOkiW-OeFA_VZHhhR5uL+jVaoYQz_yBPg@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
	<CAAS2fgTGYSiAJHZq80rD4UieV8XetS=W0b45b5onWAS9gF-F7g@mail.gmail.com>
	<CABsx9T1i50Gvxj18W=n2mYGNpsMrSkDT26CdA3aQqT5FFN86yw@mail.gmail.com>
	<CAAS2fgSxpSat3VOje3-C4zgaRUVJVx-eRJbSYJqhvfR5SvCDwA@mail.gmail.com>
	<CAF_2MyUJMdJyh7FKq6UYCtwJZQ59i-pnWT_tFEK5EQx65iwHDQ@mail.gmail.com>
	<CAAS2fgS-jjEVeHf_LErppTadtAaSeBum+KiGHpoo=Jz5BZArsQ@mail.gmail.com>
	<CABm2gDq4f0ettDhh14jZ0zztSwSJ0Z=KDEeMTOJxTHF8VV+KXw@mail.gmail.com>
	<CAAS2fgTAFgANJ495xiOkiW-OeFA_VZHhhR5uL+jVaoYQz_yBPg@mail.gmail.com>
Message-ID: <CABm2gDratPeF8ibxFO-WYhVeAnT6GG6O0tZenNhfX257+ue0dQ@mail.gmail.com>

Fair enough.
On Dec 9, 2015 4:03 PM, "Gregory Maxwell" <greg at xiph.org> wrote:

> On Wed, Dec 9, 2015 at 7:54 AM, Jorge Tim?n <jtimon at jtimon.cc> wrote:
> > From this question one could think that when you said "we can do the
> > cleanup hardfork later" earlier you didn't really meant it. And that
> > you will oppose to that hardfork later just like you are opposing to
> > it now.
> > As said I disagree that making a softfork first and then move the
> > commitment is less disruptive (because people will need to adapt their
> > software twice), but if the intention is to never do the second part
> > then of course I agree it would be less disruptive.
> > How long after the softfork would you like to do the hardfork?
> > 1 year after the softfork? 2 years? never?
>
> I think it would be logical to do as part of a hardfork that moved
> commitments generally; e.g. a better position for merged mining (such
> a hardfork was suggested in 2010 as something that could be done if
> merged mining was used), room for commitments to additional block
> back-references for compact SPV proofs, and/or UTXO set commitments.
> Part of the reason to not do it now is that the requirements for the
> other things that would be there are not yet well defined. For these
> other applications, the additional overhead is actually fairly
> meaningful; unlike the fraud proofs.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/0845be33/attachment.html>

From ctpacia at gmail.com  Wed Dec  9 14:51:36 2015
From: ctpacia at gmail.com (Chris)
Date: Wed, 9 Dec 2015 09:51:36 -0500
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
Message-ID: <56683FF8.5070003@gmail.com>

On 12/08/2015 10:12 AM, Gavin Andresen via bitcoin-dev wrote:
> Why segwitness as a soft fork? Stuffing the segwitness merkle tree in
> the coinbase is messy and will just complicate consensus-critical code
> (as opposed to making the right side of the merkle tree in
> block.version=5 blocks the segwitness data).
Agreed. I thought the rule was no contentious hark forks. It seems
hardly anyone opposes this change and there seems to be widespread
agreement that the hardfork version would be much cleaner.

From daniele.pinna at gmail.com  Wed Dec  9 12:28:52 2015
From: daniele.pinna at gmail.com (Daniele Pinna)
Date: Wed, 9 Dec 2015 13:28:52 +0100
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
Message-ID: <CAEgR2PHYUMj2NEG9z8nqsGAcKOZQXPHhw8wAgwXK=vWGUrSG2g@mail.gmail.com>

If SegWit were implemented as a hardfork, could the entire blockchain be
reorganized starting from the Genesis block to free up historical space?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/685bb9ff/attachment.html>

From jl2012 at xbt.hk  Wed Dec  9 14:30:23 2015
From: jl2012 at xbt.hk (jl2012 at xbt.hk)
Date: Wed, 09 Dec 2015 09:30:23 -0500
Subject: [bitcoin-dev] Impacts of Segregated Witness softfork
Message-ID: <ed988315078ef8c06f4bd71463440e63@xbt.hk>

Although the plan is to implement SW with softfork, I think many 
important (but non-consensus critical) components of the network would 
be broken and many things have to be redefined.

1. Definition of "Transaction ID". Currently, "Transaction ID" is simply 
a hash of a tx. With SW, we may need to deal with 2 or 3 IDs for each 
tx. Firstly we have the "backward-compatible txid" (bctxid), which has 
exactly the same meaning of the original txid. We also have a "witness 
ID" (wid), which is the hash of the witness. And finally we may need a 
"global txid" (gtxid), which is a hash of bctxid|wid. A gtxid is needed 
mainly for the relay of txs between full nodes. bctxid and wid are 
consensus critical while gtxid is for relay network only.

2. IBLT / Bitcoin relay network: As the "backward-compatible txid" 
defines only part of a tx, any relay protocols between full nodes have 
to use the "global txid" to identify a tx. Malleability attack targeting 
relay network is still possible as the witness is malleable.

3. getblocktemplete has to be upgraded to deal with witness data and 
witness IDs. (Stratum seems to be not affected? I'm not sure)

4. Protocols relying on the coinbase tx (e.g. P2Pool, merged mining): 
depends on the location of witness commitment, these protocols may be 
broken.

Feel free to correct me and add more to the list.





From gavinandresen at gmail.com  Wed Dec  9 16:40:34 2015
From: gavinandresen at gmail.com (Gavin Andresen)
Date: Wed, 9 Dec 2015 11:40:34 -0500
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAAS2fgTAFgANJ495xiOkiW-OeFA_VZHhhR5uL+jVaoYQz_yBPg@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
	<CAAS2fgTGYSiAJHZq80rD4UieV8XetS=W0b45b5onWAS9gF-F7g@mail.gmail.com>
	<CABsx9T1i50Gvxj18W=n2mYGNpsMrSkDT26CdA3aQqT5FFN86yw@mail.gmail.com>
	<CAAS2fgSxpSat3VOje3-C4zgaRUVJVx-eRJbSYJqhvfR5SvCDwA@mail.gmail.com>
	<CAF_2MyUJMdJyh7FKq6UYCtwJZQ59i-pnWT_tFEK5EQx65iwHDQ@mail.gmail.com>
	<CAAS2fgS-jjEVeHf_LErppTadtAaSeBum+KiGHpoo=Jz5BZArsQ@mail.gmail.com>
	<CABm2gDq4f0ettDhh14jZ0zztSwSJ0Z=KDEeMTOJxTHF8VV+KXw@mail.gmail.com>
	<CAAS2fgTAFgANJ495xiOkiW-OeFA_VZHhhR5uL+jVaoYQz_yBPg@mail.gmail.com>
Message-ID: <CABsx9T1pLtOaGOVpVs8URAwpbb884htkrFLWtX8-2gGpS6gPWw@mail.gmail.com>

On Wed, Dec 9, 2015 at 3:03 AM, Gregory Maxwell via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> I think it would be logical to do as part of a hardfork that moved
> commitments generally; e.g. a better position for merged mining (such
> a hardfork was suggested in 2010 as something that could be done if
> merged mining was used), room for commitments to additional block
> back-references for compact SPV proofs, and/or UTXO set commitments.
> Part of the reason to not do it now is that the requirements for the
> other things that would be there are not yet well defined. For these
> other applications, the additional overhead is actually fairly
> meaningful; unlike the fraud proofs.
>

So just design ahead for those future uses. Make the merkle tree:


             root_in_block_header
                     /      \
  tx_data_root      other_root
                               /       \
        segwitness_root     reserved_for_future_use_root

... where reserved_for_future_use is zero until some future block version
(or perhaps better, is just chosen arbitrarily by the miner and sent along
with the block data until some future block version).

That would minimize future disruption of any code that produced or consumed
merkle proofs of the transaction data or segwitness data, especially if the
reserved_for_future_use_root is allowed to be any arbitrary 256-bit value
and not a constant that would get hard-coded into segwitness-proof-checking
code.


-- 
--
Gavin Andresen
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/89ff9b06/attachment.html>

From akiva.lichtner at gmail.com  Wed Dec  9 18:26:06 2015
From: akiva.lichtner at gmail.com (Akiva Lichtner)
Date: Wed, 9 Dec 2015 18:26:06 +0000
Subject: [bitcoin-dev] Scaling by Partitioning
In-Reply-To: <CAJmQggC1X5Lgt4xGoMtBZ_v3hC2GXcYaj2FngV2_7A=TDfSuEg@mail.gmail.com>
References: <CABCnA7Wqz76m8qo5BYT41Z=hBH+fUfOc4xsFAGg=Niv7Jgkqsg@mail.gmail.com>
	<CAJmQggC1X5Lgt4xGoMtBZ_v3hC2GXcYaj2FngV2_7A=TDfSuEg@mail.gmail.com>
Message-ID: <CABCnA7VAO2XKLwd4axaYcttUHzhvXXEvYrwg7XDKH9nfo1k7RA@mail.gmail.com>

Thanks for giving serious consideration to my post.

With regard to your question "if a transaction spends a "coin" that
ends in "1" and creates a new coin that ends in "1", which partition
should process the transaction?", I would answer that only one
partition is involved. In other words, there are N independent block
chains that never cross paths.

With regard to your question "what is the prior data needed to
validate that kind of TXs?" I do not understand what this means. If
you can dumb it down a bit that would be good because there could be
some interesting concern in this question.

Since partitions are completely segregated, there is no need for a
node to work on multiple partitions simultaneously. For attacks to be
defeated a node needs to be able to work on multiple partitions in
turn, not at the same time. The reason is because if the computing
power of the good-faith nodes is unbalanced this gives attackers an
unfair advantage.

On 12/9/15, Loi Luu via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> Dear Akiva,
>
> Its Loi Luu, one of the authors of the SCP protocol (
> http://eprint.iacr.org/2015/1168.pdf ).
>
> Before SCP, we had been thinking hard about how to do sharding efficiently
> without degrading any security guarantee. A simple solution which splits
> the coins, or TXs in to several partitions will just not work. You have to
> answer more questions to have a good solutions. For example, I wonder in
> your proposal, if a transaction spends a "coin" that ends in "1" and
> creates a new coin that ends in "1", which partition should process the
> transaction? What is the prior data needed to validate that kind of TXs?
>
> The problem with other proposals, and probably yours as well,  that we see
> is that the amount of data that you need to broadcast immediately to the
> network increases linearly with the number of TXs that the network can
> process. Thus, sharding does not bring any advantage than simply using
> other techniques to publish more blocks in one epoch (like Bitcoin-NG,
> Ghost). The whole point of using sharding/ partition is to localize
> the bandwidth used, and only broadcast only a minimal data to the network.
>
> Clearly we are able to localize the bandwidth used with our SCP protocol.
> The cost is that now recipients need to  themselves verify whether a
> transaction is double spending. However, we think that it is a reasonable
> tradeoff, given the potential scalability that SCP can provides.
>
> Thanks,
> Loi Luu.
>
> On Wed, Dec 9, 2015 at 12:27 AM, Akiva Lichtner via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> Hello,
>>
>> I am seeking some expert feedback on an idea for scaling Bitcoin. As a
>> brief introduction: I work in the payment industry and I have twenty
>> years'
>> experience in development. I have some experience with process groups and
>> ordering protocols too. I think I understand Satoshi's paper but I admit
>> I
>> have not read the source code.
>>
>> The idea is to run more than one simultaneous chain, each chain defeating
>> double spending on only part of the coin. The coin would be partitioned
>> by
>> radix (or modulus, not sure what to call it.) For example in order to
>> multiply throughput by a factor of ten you could run ten parallel chains,
>> one would work on coin that ends in "0", one on coin that ends in "1",
>> and
>> so on up to "9".
>>
>> The number of chains could increase automatically over time based on the
>> moving average of transaction volume.
>>
>> Blocks would have to contain the number of the partition they belong to,
>> and miners would have to round-robin through partitions so that an
>> attacker
>> would not have an unfair advantage working on just one partition.
>>
>> I don't think there is much impact to miners, but clients would have to
>> send more than one message in order to spend money. Client messages will
>> need to enumerate coin using some sort of compression, to save space.
>> This
>> seems okay to me since often in computing client software does have to
>> break things up in equal parts (e.g. memory pages, file system blocks,)
>> and
>> the client software could hide the details.
>>
>> Best wishes for continued success to the project.
>>
>> Regards,
>> Akiva
>>
>> P.S. I found a funny anagram for SATOSHI NAKAMOTO: "NSA IS OOOK AT MATH"
>>
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>

From loi.luuthe at gmail.com  Wed Dec  9 21:16:00 2015
From: loi.luuthe at gmail.com (Loi Luu)
Date: Thu, 10 Dec 2015 05:16:00 +0800
Subject: [bitcoin-dev] Scaling by Partitioning
In-Reply-To: <CABCnA7VAO2XKLwd4axaYcttUHzhvXXEvYrwg7XDKH9nfo1k7RA@mail.gmail.com>
References: <CABCnA7Wqz76m8qo5BYT41Z=hBH+fUfOc4xsFAGg=Niv7Jgkqsg@mail.gmail.com>
	<CAJmQggC1X5Lgt4xGoMtBZ_v3hC2GXcYaj2FngV2_7A=TDfSuEg@mail.gmail.com>
	<CABCnA7VAO2XKLwd4axaYcttUHzhvXXEvYrwg7XDKH9nfo1k7RA@mail.gmail.com>
Message-ID: <CAJmQggBfYyk3cPAL57NZ3ecPG_ZxffjPftc0-EkTzKagjodgNQ@mail.gmail.com>

I guess the most basic question is how do you define a coin here?

Thanks,
Loi Luu
On 10 Dec 2015 2:26 a.m., "Akiva Lichtner" <akiva.lichtner at gmail.com> wrote:

> Thanks for giving serious consideration to my post.
>
> With regard to your question "if a transaction spends a "coin" that
> ends in "1" and creates a new coin that ends in "1", which partition
> should process the transaction?", I would answer that only one
> partition is involved. In other words, there are N independent block
> chains that never cross paths.
>
> With regard to your question "what is the prior data needed to
> validate that kind of TXs?" I do not understand what this means. If
> you can dumb it down a bit that would be good because there could be
> some interesting concern in this question.
>
> Since partitions are completely segregated, there is no need for a
> node to work on multiple partitions simultaneously. For attacks to be
> defeated a node needs to be able to work on multiple partitions in
> turn, not at the same time. The reason is because if the computing
> power of the good-faith nodes is unbalanced this gives attackers an
> unfair advantage.
>
> On 12/9/15, Loi Luu via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org> wrote:
> > Dear Akiva,
> >
> > Its Loi Luu, one of the authors of the SCP protocol (
> > http://eprint.iacr.org/2015/1168.pdf ).
> >
> > Before SCP, we had been thinking hard about how to do sharding
> efficiently
> > without degrading any security guarantee. A simple solution which splits
> > the coins, or TXs in to several partitions will just not work. You have
> to
> > answer more questions to have a good solutions. For example, I wonder in
> > your proposal, if a transaction spends a "coin" that ends in "1" and
> > creates a new coin that ends in "1", which partition should process the
> > transaction? What is the prior data needed to validate that kind of TXs?
> >
> > The problem with other proposals, and probably yours as well,  that we
> see
> > is that the amount of data that you need to broadcast immediately to the
> > network increases linearly with the number of TXs that the network can
> > process. Thus, sharding does not bring any advantage than simply using
> > other techniques to publish more blocks in one epoch (like Bitcoin-NG,
> > Ghost). The whole point of using sharding/ partition is to localize
> > the bandwidth used, and only broadcast only a minimal data to the
> network.
> >
> > Clearly we are able to localize the bandwidth used with our SCP protocol.
> > The cost is that now recipients need to  themselves verify whether a
> > transaction is double spending. However, we think that it is a reasonable
> > tradeoff, given the potential scalability that SCP can provides.
> >
> > Thanks,
> > Loi Luu.
> >
> > On Wed, Dec 9, 2015 at 12:27 AM, Akiva Lichtner via bitcoin-dev <
> > bitcoin-dev at lists.linuxfoundation.org> wrote:
> >
> >> Hello,
> >>
> >> I am seeking some expert feedback on an idea for scaling Bitcoin. As a
> >> brief introduction: I work in the payment industry and I have twenty
> >> years'
> >> experience in development. I have some experience with process groups
> and
> >> ordering protocols too. I think I understand Satoshi's paper but I admit
> >> I
> >> have not read the source code.
> >>
> >> The idea is to run more than one simultaneous chain, each chain
> defeating
> >> double spending on only part of the coin. The coin would be partitioned
> >> by
> >> radix (or modulus, not sure what to call it.) For example in order to
> >> multiply throughput by a factor of ten you could run ten parallel
> chains,
> >> one would work on coin that ends in "0", one on coin that ends in "1",
> >> and
> >> so on up to "9".
> >>
> >> The number of chains could increase automatically over time based on the
> >> moving average of transaction volume.
> >>
> >> Blocks would have to contain the number of the partition they belong to,
> >> and miners would have to round-robin through partitions so that an
> >> attacker
> >> would not have an unfair advantage working on just one partition.
> >>
> >> I don't think there is much impact to miners, but clients would have to
> >> send more than one message in order to spend money. Client messages will
> >> need to enumerate coin using some sort of compression, to save space.
> >> This
> >> seems okay to me since often in computing client software does have to
> >> break things up in equal parts (e.g. memory pages, file system blocks,)
> >> and
> >> the client software could hide the details.
> >>
> >> Best wishes for continued success to the project.
> >>
> >> Regards,
> >> Akiva
> >>
> >> P.S. I found a funny anagram for SATOSHI NAKAMOTO: "NSA IS OOOK AT MATH"
> >>
> >>
> >> _______________________________________________
> >> bitcoin-dev mailing list
> >> bitcoin-dev at lists.linuxfoundation.org
> >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> >>
> >>
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151210/949b328b/attachment-0001.html>

From xor at freenetproject.org  Wed Dec  9 21:26:05 2015
From: xor at freenetproject.org (xor)
Date: Wed, 09 Dec 2015 22:26:05 +0100
Subject: [bitcoin-dev] "Subsidy fraud" ?
Message-ID: <2770009.yIOdKLpzfC@1337h4x0r>

Pieter Wuille mentions "subsidy fraud" in his recent talk:
https://youtu.be/fst1IK_mrng?t=57m2s

I was unable to google what this is, and the Bitcoin Wiki also does not seem 
to explain it.

If this is a well-known problem, perhaps it would be a good idea to explain it 
somewhere?
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 836 bytes
Desc: This is a digitally signed message part.
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/26a29606/attachment.sig>

From pieter.wuille at gmail.com  Wed Dec  9 21:43:22 2015
From: pieter.wuille at gmail.com (Pieter Wuille)
Date: Wed, 9 Dec 2015 22:43:22 +0100
Subject: [bitcoin-dev] "Subsidy fraud" ?
In-Reply-To: <2770009.yIOdKLpzfC@1337h4x0r>
References: <2770009.yIOdKLpzfC@1337h4x0r>
Message-ID: <CAPg+sBiLmduEM-r_2T713XC1W-R4-Pj85LHo6n14CGYPjeKirA@mail.gmail.com>

I meant a miner claiming more in the coinbase's output than subsidy + fees
allow.
On Dec 10, 2015 5:26 AM, "xor" <xor at freenetproject.org> wrote:

> Pieter Wuille mentions "subsidy fraud" in his recent talk:
> https://youtu.be/fst1IK_mrng?t=57m2s
>
> I was unable to google what this is, and the Bitcoin Wiki also does not
> seem
> to explain it.
>
> If this is a well-known problem, perhaps it would be a good idea to
> explain it
> somewhere?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/1ba8a058/attachment.html>

From onelineproof at gmail.com  Wed Dec  9 22:35:07 2015
From: onelineproof at gmail.com (Andrew)
Date: Wed, 9 Dec 2015 23:35:07 +0100
Subject: [bitcoin-dev] Scaling by Partitioning
In-Reply-To: <CAJmQggC1X5Lgt4xGoMtBZ_v3hC2GXcYaj2FngV2_7A=TDfSuEg@mail.gmail.com>
References: <CABCnA7Wqz76m8qo5BYT41Z=hBH+fUfOc4xsFAGg=Niv7Jgkqsg@mail.gmail.com>
	<CAJmQggC1X5Lgt4xGoMtBZ_v3hC2GXcYaj2FngV2_7A=TDfSuEg@mail.gmail.com>
Message-ID: <CAL8tG=mxYE97iMO05mPq4_f8VcmFBYqAmyPqTs439bPRGhaVqA@mail.gmail.com>

Hi Akiva

I sketched out a similar proposal here:
https://bitcointalk.org/index.php?topic=1083345.0

It's good to see people talking about this :). I'm not quite convinced with
segregated witness, as it might mess up some things, but will take a closer
look.
On Dec 9, 2015 7:32 AM, "Loi Luu via bitcoin-dev" <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Dear Akiva,
>
> Its Loi Luu, one of the authors of the SCP protocol (
> http://eprint.iacr.org/2015/1168.pdf ).
>
> Before SCP, we had been thinking hard about how to do sharding efficiently
> without degrading any security guarantee. A simple solution which splits
> the coins, or TXs in to several partitions will just not work. You have to
> answer more questions to have a good solutions. For example, I wonder in
> your proposal, if a transaction spends a "coin" that ends in "1" and
> creates a new coin that ends in "1", which partition should process the
> transaction? What is the prior data needed to validate that kind of TXs?
>
> The problem with other proposals, and probably yours as well,  that we see
> is that the amount of data that you need to broadcast immediately to the
> network increases linearly with the number of TXs that the network can
> process. Thus, sharding does not bring any advantage than simply using
> other techniques to publish more blocks in one epoch (like Bitcoin-NG,
> Ghost). The whole point of using sharding/ partition is to localize
> the bandwidth used, and only broadcast only a minimal data to the network.
>
> Clearly we are able to localize the bandwidth used with our SCP protocol.
> The cost is that now recipients need to  themselves verify whether a
> transaction is double spending. However, we think that it is a reasonable
> tradeoff, given the potential scalability that SCP can provides.
>
> Thanks,
> Loi Luu.
>
> On Wed, Dec 9, 2015 at 12:27 AM, Akiva Lichtner via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> Hello,
>>
>> I am seeking some expert feedback on an idea for scaling Bitcoin. As a
>> brief introduction: I work in the payment industry and I have twenty years'
>> experience in development. I have some experience with process groups and
>> ordering protocols too. I think I understand Satoshi's paper but I admit I
>> have not read the source code.
>>
>> The idea is to run more than one simultaneous chain, each chain defeating
>> double spending on only part of the coin. The coin would be partitioned by
>> radix (or modulus, not sure what to call it.) For example in order to
>> multiply throughput by a factor of ten you could run ten parallel chains,
>> one would work on coin that ends in "0", one on coin that ends in "1", and
>> so on up to "9".
>>
>> The number of chains could increase automatically over time based on the
>> moving average of transaction volume.
>>
>> Blocks would have to contain the number of the partition they belong to,
>> and miners would have to round-robin through partitions so that an attacker
>> would not have an unfair advantage working on just one partition.
>>
>> I don't think there is much impact to miners, but clients would have to
>> send more than one message in order to spend money. Client messages will
>> need to enumerate coin using some sort of compression, to save space. This
>> seems okay to me since often in computing client software does have to
>> break things up in equal parts (e.g. memory pages, file system blocks,) and
>> the client software could hide the details.
>>
>> Best wishes for continued success to the project.
>>
>> Regards,
>> Akiva
>>
>> P.S. I found a funny anagram for SATOSHI NAKAMOTO: "NSA IS OOOK AT MATH"
>>
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/19e592c7/attachment.html>

From luke.durback at gmail.com  Thu Dec 10 01:35:17 2015
From: luke.durback at gmail.com (Luke Durback)
Date: Wed, 9 Dec 2015 20:35:17 -0500
Subject: [bitcoin-dev] Standard BIP Draft: Turing Pseudo-Completeness
Message-ID: <CAEj3M+wYicoACcpG5YUU6vF8vg98XCcJWmgBiyrJj-xHHxrhig@mail.gmail.com>

Hello Bitcoin-Dev,

I hope this isn't out of line, but I joined the mailing list to try to
start a discussion on adding opcodes to make Script Turing Pseudo-Complete
as Wright suggested is possible.

---

In line with Wright's suggestion, I propose adding a return stack alongside
the, already existing, control stack.

The principle opcodes (excluding conditional versions of call and
return_from) needed are

OP_DEFINITION_START FunctionName:  The code that follows is the definition
of a new function to be named TransactionSenderAddress.FunctionName.  If
this function name is already taken, the transaction is marked invalid.
Within the transaction, the function can be called simply as FunctionName.

OP_DEFINITION_END:  This ends a function definition

OP_FUNCTION_NAME FunctionName:  Gives the current transaction the name
FunctionName (this is necessary to build recursive functions)

---

OP_CALL Namespace.FunctionName Value TransactionFee:  This marks the
transaction as valid.  It also pushes the current execution location onto
the return stack, debits the calling transaction by the TransactionFee and
Value, and creates a new transaction specified by Namespace.FunctionName
with both stacks continued from before (this may be dangerous, but I see no
way around it) with the specified value.

OP_RETURN_FROM_CALL_AND_CONTINUE:  This pops the top value off the return
stack and continues from the specified location with both stacks in tact.

---

It would also be useful if a transaction can create another transaction
arbitrarily, so to prepare for that, I additionally propose

OP_NAMESPACE:  Pushes the current namespace onto the control stack

This, combined with the ability to make new transactions arbitrarily would
allow a function to pay its creator.



I understand that this isn't all that is needed, but I think it's a start.
I hope this proposal has met you all well,

Luke Durback
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/a880be67/attachment.html>

From akiva.lichtner at gmail.com  Thu Dec 10 03:58:10 2015
From: akiva.lichtner at gmail.com (Akiva Lichtner)
Date: Wed, 9 Dec 2015 22:58:10 -0500
Subject: [bitcoin-dev] Scaling by Partitioning
In-Reply-To: <CAL8tG=mxYE97iMO05mPq4_f8VcmFBYqAmyPqTs439bPRGhaVqA@mail.gmail.com>
References: <CABCnA7Wqz76m8qo5BYT41Z=hBH+fUfOc4xsFAGg=Niv7Jgkqsg@mail.gmail.com>
	<CAJmQggC1X5Lgt4xGoMtBZ_v3hC2GXcYaj2FngV2_7A=TDfSuEg@mail.gmail.com>
	<CAL8tG=mxYE97iMO05mPq4_f8VcmFBYqAmyPqTs439bPRGhaVqA@mail.gmail.com>
Message-ID: <CABCnA7W25KoHuSuB3Az250_PiRcFd5MjjKJfrm_qv4oaYUT5mg@mail.gmail.com>

Hi Andrew,

What you suggested is much more sophisticated than what I suggested. I am
strictly talking about independent chains - that's all.

I am struck by the fact that the topic of "scaling bitcoin" seems to be a
mix of different problems, and when people are really trying to solve
different problems, or arguing about applying the same solution in
different settings, it's easy to argue back and forth endlessly. Your post
talks about validating transactions without seeing all transactions. This
is a different problem than what I am addressing. My view of Bitcoin is
colored by my experience with process groups and total ordering. I view
Bitcoin as a timestamp service on all transactions, a total order. A total
order is difficult to scale. Period.

I am just addressing how to change the system so that blocks can be
generated faster if a) the transaction volume increases and b) you are
willing to have more miners. But you are also talking about transaction
verification and making sure that you don't need to verify everything. I
think these are two problems that should have two different names.

Correct me if I am wrong, but the dream of a virtual currency where
everybody is equal and runs the client on their mobile device went out the
window long ago. I think that went out with the special mining hardware. If
my organization had to accept bitcoin payments I would assume that we'll
need a small server farm for transaction verification, and that we would
see all the transactions. Figure 10,000 transactions per second, like VISA.
As far as small organizations or private individuals are concerned, I think
it would be entirely okay for a guy on a smartphone to delegate
verification to a trusted party, as long as the trust chain stops there and
there is plenty of choice.

I am guessing the trustless virtual currency police would get pretty upset
about such a pragmatic approach, but it's not really a choice, the failure
to scale has already occurred. All things considered I think that Bitcoin
will only scale when pragmatic considerations take center stage and the
academic goals take a lower priority. I would think companies would make a
good living out of running trusted verification services.

Once again, it doesn't mean that there is a bank, the network still allows
malicious nodes, but there can be pockets of trust. This is only natural,
most people trust at least one other person, so it's not that weird.

Akiva
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/0844f5e2/attachment-0001.html>

From jgarzik at gmail.com  Thu Dec 10 04:03:30 2015
From: jgarzik at gmail.com (Jeff Garzik)
Date: Thu, 10 Dec 2015 12:03:30 +0800
Subject: [bitcoin-dev] Standard BIP Draft: Turing Pseudo-Completeness
In-Reply-To: <CAEj3M+wYicoACcpG5YUU6vF8vg98XCcJWmgBiyrJj-xHHxrhig@mail.gmail.com>
References: <CAEj3M+wYicoACcpG5YUU6vF8vg98XCcJWmgBiyrJj-xHHxrhig@mail.gmail.com>
Message-ID: <CADm_WcZdC-iNF=tMmYYafsaLaHUcck03zw8cdsSCCPvXdTNyrw@mail.gmail.com>

There is no need for a BIP draft.  "Turing complete" is just a fancy,
executive-impressing term for "it can run any computer program", or put
even more simply, "it can loop"

Furthermore, the specification of such a language is trivial.  It is the
economics of validation that is the complex piece.  Proving whether or not
a program will halt as expected - The Halting Problem - is near impossible
for most complex programs.  As a result, your proof is... running the
program.  That produces enormous validation consequences and costs for
generic-execution scripts when applied to a decentralized network of
validation P2P nodes.

If you need that capability, it is just as easy to use a normal C/C++/etc.
computer language, with your preferred algorithm libraries and development
tools.

See https://github.com/jgarzik/moxiebox for a working example of provable
execution.



On Thu, Dec 10, 2015 at 9:35 AM, Luke Durback via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Hello Bitcoin-Dev,
>
> I hope this isn't out of line, but I joined the mailing list to try to
> start a discussion on adding opcodes to make Script Turing Pseudo-Complete
> as Wright suggested is possible.
>
> ---
>
> In line with Wright's suggestion, I propose adding a return stack
> alongside the, already existing, control stack.
>
> The principle opcodes (excluding conditional versions of call and
> return_from) needed are
>
> OP_DEFINITION_START FunctionName:  The code that follows is the definition
> of a new function to be named TransactionSenderAddress.FunctionName.  If
> this function name is already taken, the transaction is marked invalid.
> Within the transaction, the function can be called simply as FunctionName.
>
> OP_DEFINITION_END:  This ends a function definition
>
> OP_FUNCTION_NAME FunctionName:  Gives the current transaction the name
> FunctionName (this is necessary to build recursive functions)
>
> ---
>
> OP_CALL Namespace.FunctionName Value TransactionFee:  This marks the
> transaction as valid.  It also pushes the current execution location onto
> the return stack, debits the calling transaction by the TransactionFee and
> Value, and creates a new transaction specified by Namespace.FunctionName
> with both stacks continued from before (this may be dangerous, but I see no
> way around it) with the specified value.
>
> OP_RETURN_FROM_CALL_AND_CONTINUE:  This pops the top value off the return
> stack and continues from the specified location with both stacks in tact.
>
> ---
>
> It would also be useful if a transaction can create another transaction
> arbitrarily, so to prepare for that, I additionally propose
>
> OP_NAMESPACE:  Pushes the current namespace onto the control stack
>
> This, combined with the ability to make new transactions arbitrarily would
> allow a function to pay its creator.
>
>
>
> I understand that this isn't all that is needed, but I think it's a
> start.  I hope this proposal has met you all well,
>
> Luke Durback
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151210/1c482e7c/attachment.html>

From dscotese at litmocracy.com  Thu Dec 10 04:08:04 2015
From: dscotese at litmocracy.com (Dave Scotese)
Date: Wed, 9 Dec 2015 20:08:04 -0800
Subject: [bitcoin-dev] Scaling by Partitioning
In-Reply-To: <CAL8tG=mxYE97iMO05mPq4_f8VcmFBYqAmyPqTs439bPRGhaVqA@mail.gmail.com>
References: <CABCnA7Wqz76m8qo5BYT41Z=hBH+fUfOc4xsFAGg=Niv7Jgkqsg@mail.gmail.com>
	<CAJmQggC1X5Lgt4xGoMtBZ_v3hC2GXcYaj2FngV2_7A=TDfSuEg@mail.gmail.com>
	<CAL8tG=mxYE97iMO05mPq4_f8VcmFBYqAmyPqTs439bPRGhaVqA@mail.gmail.com>
Message-ID: <CAGLBAhebzN9FZ0TQO+Xr1SmKH5YFptbQ=M09+c511siAq9dmNQ@mail.gmail.com>

If we partition the work using bits from the TxID (once it is no longer
malleable) or even bits from whatever definition we use for "coin," then
every transaction may have to use all the other partitions to verify that
the incoming coin is good.

If all partitions are involved in validating and storing every transaction,
then we may be doing more work in total, but any one node will only have to
do (and store) a fraction of what it is now.  We would want the current
situation to be identical to one in which all the participants are handling
all the partitions.  So how can we break up the work so that any
participant can handle whatever fraction of the work he or she wants?  One
idea is to use the last bits of the address that will receive the subsidy
and fees.  You solve the block for your partition by determining that all
transactions in the block are valid against the subset of blocks whose
hashes end with the same bits.

This solution is broadcast in the hope that others will start attempting to
validate that same block on their own partition. If they are mining the
same partition, they simply change their subsidy address to work on a
different partition.  Each time a new-but-not-last partition is solved,
everyone working on the block adds the new solver's output address to their
generation transaction with the appropriate fraction of the
reward-plus-subsidy.  In this way, several miners contribute to the
solution of a single block and need only store those blocks that match the
partitions they want to work on.

Suppose we use eight bits so that there are 256 partitions and a miner
wishes to do about 1/5 of the work. That would be 51 partitions.  This is
signaled in the generation transaction, where the bit-pattern of the last
byte of the public key identifies the first partition, and the proportion
of the total reward for the block (51/256) indicates how many partitions a
solution will cover.

Suppose that the last byte of the subsidy address is 0xF0.  This means
there are only 16 partitions left, so we define partition selection to wrap
around.  This 51/256 miner must cover partitions 0xF0 - 0xFF and 0x00 -
0x23. In this way, all mining to date has covered all partitions.

The number of bits to be used might be able to be abstracted out to a
certain level.  Perhaps a miner can indicate how many bits B the
partitioning should use in the CoinBase. The blocks for which a partition
miner claims responsibility are all those with a bit pattern of length B at
the end of their hash matching the the bits at the end of the first
output's public key in the generation transaction, as well as those blocks
with hashes for which the last B bits match any of the next N bit patterns
where for the largest integer N for which the claimed output is not less
than (subsidy+fees)*(N/(2^B)).

If you only store and validate against one partition, and that partition
has a solution already, then you would start working on the next block
(once you've validated the current one against your subset of the
blockchain).  You could even broadcast a solution for that next block
before the previous block is fully solved, thus claiming a piece of the
next block reward (assuming the current block is valid on all partitions).

It seems that a miner who covers only one partition will be at a serious
disadvantage, but as the rate of incoming transactions increases, the
fraction of time he must spend validating (being about half of all other
miners who cover just one more partition) makes up for this disadvantage
somewhat.  He is a "spry" miner and therefore wins more rewards during
times of very dense transaction volume.  If we wish to encourage miners to
work on smaller partitions, we can provide a difficulty break for smaller
fractions of the work.  In fact, the difficulty can be adjusted down for
the first solution, and then slowly back up to full for the last
partition(s).

This proposal has the added benefit of encouraging the assembly of blocks
by miners who work on single partitions to get them out there with a
one-partition solution.

On Wed, Dec 9, 2015 at 2:35 PM, Andrew via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Hi Akiva
>
> I sketched out a similar proposal here:
> https://bitcointalk.org/index.php?topic=1083345.0
>
> It's good to see people talking about this :). I'm not quite convinced
> with segregated witness, as it might mess up some things, but will take a
> closer look.
> On Dec 9, 2015 7:32 AM, "Loi Luu via bitcoin-dev" <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> Dear Akiva,
>>
>> Its Loi Luu, one of the authors of the SCP protocol (
>> http://eprint.iacr.org/2015/1168.pdf ).
>>
>> Before SCP, we had been thinking hard about how to do sharding
>> efficiently without degrading any security guarantee. A simple solution
>> which splits the coins, or TXs in to several partitions will just not work.
>> You have to answer more questions to have a good solutions. For example, I
>> wonder in your proposal, if a transaction spends a "coin" that ends in "1"
>> and creates a new coin that ends in "1", which partition should process the
>> transaction? What is the prior data needed to validate that kind of TXs?
>>
>> The problem with other proposals, and probably yours as well,  that we
>> see is that the amount of data that you need to broadcast immediately to
>> the network increases linearly with the number of TXs that the network can
>> process. Thus, sharding does not bring any advantage than simply using
>> other techniques to publish more blocks in one epoch (like Bitcoin-NG,
>> Ghost). The whole point of using sharding/ partition is to localize
>> the bandwidth used, and only broadcast only a minimal data to the network.
>>
>> Clearly we are able to localize the bandwidth used with our SCP protocol.
>> The cost is that now recipients need to  themselves verify whether a
>> transaction is double spending. However, we think that it is a reasonable
>> tradeoff, given the potential scalability that SCP can provides.
>>
>> Thanks,
>> Loi Luu.
>>
>> On Wed, Dec 9, 2015 at 12:27 AM, Akiva Lichtner via bitcoin-dev <
>> bitcoin-dev at lists.linuxfoundation.org> wrote:
>>
>>> Hello,
>>>
>>> I am seeking some expert feedback on an idea for scaling Bitcoin. As a
>>> brief introduction: I work in the payment industry and I have twenty years'
>>> experience in development. I have some experience with process groups and
>>> ordering protocols too. I think I understand Satoshi's paper but I admit I
>>> have not read the source code.
>>>
>>> The idea is to run more than one simultaneous chain, each chain
>>> defeating double spending on only part of the coin. The coin would be
>>> partitioned by radix (or modulus, not sure what to call it.) For example in
>>> order to multiply throughput by a factor of ten you could run ten parallel
>>> chains, one would work on coin that ends in "0", one on coin that ends in
>>> "1", and so on up to "9".
>>>
>>> The number of chains could increase automatically over time based on the
>>> moving average of transaction volume.
>>>
>>> Blocks would have to contain the number of the partition they belong to,
>>> and miners would have to round-robin through partitions so that an attacker
>>> would not have an unfair advantage working on just one partition.
>>>
>>> I don't think there is much impact to miners, but clients would have to
>>> send more than one message in order to spend money. Client messages will
>>> need to enumerate coin using some sort of compression, to save space. This
>>> seems okay to me since often in computing client software does have to
>>> break things up in equal parts (e.g. memory pages, file system blocks,) and
>>> the client software could hide the details.
>>>
>>> Best wishes for continued success to the project.
>>>
>>> Regards,
>>> Akiva
>>>
>>> P.S. I found a funny anagram for SATOSHI NAKAMOTO: "NSA IS OOOK AT MATH"
>>>
>>>
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev at lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>>>
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>


-- 
I like to provide some work at no charge to prove my value. Do you need a
techie?
I own Litmocracy <http://www.litmocracy.com> and Meme Racing
<http://www.memeracing.net> (in alpha).
I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which
now accepts Bitcoin.
I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
"He ought to find it more profitable to play by the rules" - Satoshi
Nakamoto
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/7030e17e/attachment-0001.html>

From dscotese at litmocracy.com  Thu Dec 10 04:14:17 2015
From: dscotese at litmocracy.com (Dave Scotese)
Date: Wed, 9 Dec 2015 20:14:17 -0800
Subject: [bitcoin-dev] Scaling by Partitioning
In-Reply-To: <CAGLBAhebzN9FZ0TQO+Xr1SmKH5YFptbQ=M09+c511siAq9dmNQ@mail.gmail.com>
References: <CABCnA7Wqz76m8qo5BYT41Z=hBH+fUfOc4xsFAGg=Niv7Jgkqsg@mail.gmail.com>
	<CAJmQggC1X5Lgt4xGoMtBZ_v3hC2GXcYaj2FngV2_7A=TDfSuEg@mail.gmail.com>
	<CAL8tG=mxYE97iMO05mPq4_f8VcmFBYqAmyPqTs439bPRGhaVqA@mail.gmail.com>
	<CAGLBAhebzN9FZ0TQO+Xr1SmKH5YFptbQ=M09+c511siAq9dmNQ@mail.gmail.com>
Message-ID: <CAGLBAhcWfjkhbxU-qxxEbBbpZ9jrbgLUVODth+jYT6t+Ocgyrg@mail.gmail.com>

Edit:
"... as well as those blocks with hashes for which the last B bits match
any of the next N bit patterns where *N is largest* integer for which the
claimed output is not *greater* than (subsidy+fees)*(N/(2^B)).

On Wed, Dec 9, 2015 at 8:08 PM, Dave Scotese <dscotese at litmocracy.com>
wrote:

> If we partition the work using bits from the TxID (once it is no longer
> malleable) or even bits from whatever definition we use for "coin," then
> every transaction may have to use all the other partitions to verify that
> the incoming coin is good.
>
> If all partitions are involved in validating and storing every
> transaction, then we may be doing more work in total, but any one node will
> only have to do (and store) a fraction of what it is now.  We would want
> the current situation to be identical to one in which all the participants
> are handling all the partitions.  So how can we break up the work so that
> any participant can handle whatever fraction of the work he or she wants?
> One idea is to use the last bits of the address that will receive the
> subsidy and fees.  You solve the block for your partition by determining
> that all transactions in the block are valid against the subset of blocks
> whose hashes end with the same bits.
>
> This solution is broadcast in the hope that others will start attempting
> to validate that same block on their own partition. If they are mining the
> same partition, they simply change their subsidy address to work on a
> different partition.  Each time a new-but-not-last partition is solved,
> everyone working on the block adds the new solver's output address to their
> generation transaction with the appropriate fraction of the
> reward-plus-subsidy.  In this way, several miners contribute to the
> solution of a single block and need only store those blocks that match the
> partitions they want to work on.
>
> Suppose we use eight bits so that there are 256 partitions and a miner
> wishes to do about 1/5 of the work. That would be 51 partitions.  This is
> signaled in the generation transaction, where the bit-pattern of the last
> byte of the public key identifies the first partition, and the proportion
> of the total reward for the block (51/256) indicates how many partitions a
> solution will cover.
>
> Suppose that the last byte of the subsidy address is 0xF0.  This means
> there are only 16 partitions left, so we define partition selection to wrap
> around.  This 51/256 miner must cover partitions 0xF0 - 0xFF and 0x00 -
> 0x23. In this way, all mining to date has covered all partitions.
>
> The number of bits to be used might be able to be abstracted out to a
> certain level.  Perhaps a miner can indicate how many bits B the
> partitioning should use in the CoinBase. The blocks for which a partition
> miner claims responsibility are all those with a bit pattern of length B at
> the end of their hash matching the the bits at the end of the first
> output's public key in the generation transaction, as well as those blocks
> with hashes for which the last B bits match any of the next N bit patterns
> where for the largest integer N for which the claimed output is not less
> than (subsidy+fees)*(N/(2^B)).
>
> If you only store and validate against one partition, and that partition
> has a solution already, then you would start working on the next block
> (once you've validated the current one against your subset of the
> blockchain).  You could even broadcast a solution for that next block
> before the previous block is fully solved, thus claiming a piece of the
> next block reward (assuming the current block is valid on all partitions).
>
> It seems that a miner who covers only one partition will be at a serious
> disadvantage, but as the rate of incoming transactions increases, the
> fraction of time he must spend validating (being about half of all other
> miners who cover just one more partition) makes up for this disadvantage
> somewhat.  He is a "spry" miner and therefore wins more rewards during
> times of very dense transaction volume.  If we wish to encourage miners to
> work on smaller partitions, we can provide a difficulty break for smaller
> fractions of the work.  In fact, the difficulty can be adjusted down for
> the first solution, and then slowly back up to full for the last
> partition(s).
>
> This proposal has the added benefit of encouraging the assembly of blocks
> by miners who work on single partitions to get them out there with a
> one-partition solution.
>
> On Wed, Dec 9, 2015 at 2:35 PM, Andrew via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> Hi Akiva
>>
>> I sketched out a similar proposal here:
>> https://bitcointalk.org/index.php?topic=1083345.0
>>
>> It's good to see people talking about this :). I'm not quite convinced
>> with segregated witness, as it might mess up some things, but will take a
>> closer look.
>> On Dec 9, 2015 7:32 AM, "Loi Luu via bitcoin-dev" <
>> bitcoin-dev at lists.linuxfoundation.org> wrote:
>>
>>> Dear Akiva,
>>>
>>> Its Loi Luu, one of the authors of the SCP protocol (
>>> http://eprint.iacr.org/2015/1168.pdf ).
>>>
>>> Before SCP, we had been thinking hard about how to do sharding
>>> efficiently without degrading any security guarantee. A simple solution
>>> which splits the coins, or TXs in to several partitions will just not work.
>>> You have to answer more questions to have a good solutions. For example, I
>>> wonder in your proposal, if a transaction spends a "coin" that ends in "1"
>>> and creates a new coin that ends in "1", which partition should process the
>>> transaction? What is the prior data needed to validate that kind of TXs?
>>>
>>> The problem with other proposals, and probably yours as well,  that we
>>> see is that the amount of data that you need to broadcast immediately to
>>> the network increases linearly with the number of TXs that the network can
>>> process. Thus, sharding does not bring any advantage than simply using
>>> other techniques to publish more blocks in one epoch (like Bitcoin-NG,
>>> Ghost). The whole point of using sharding/ partition is to localize
>>> the bandwidth used, and only broadcast only a minimal data to the network.
>>>
>>> Clearly we are able to localize the bandwidth used with our SCP
>>> protocol. The cost is that now recipients need to  themselves verify
>>> whether a transaction is double spending. However, we think that it is a
>>> reasonable tradeoff, given the potential scalability that SCP can provides.
>>>
>>> Thanks,
>>> Loi Luu.
>>>
>>> On Wed, Dec 9, 2015 at 12:27 AM, Akiva Lichtner via bitcoin-dev <
>>> bitcoin-dev at lists.linuxfoundation.org> wrote:
>>>
>>>> Hello,
>>>>
>>>> I am seeking some expert feedback on an idea for scaling Bitcoin. As a
>>>> brief introduction: I work in the payment industry and I have twenty years'
>>>> experience in development. I have some experience with process groups and
>>>> ordering protocols too. I think I understand Satoshi's paper but I admit I
>>>> have not read the source code.
>>>>
>>>> The idea is to run more than one simultaneous chain, each chain
>>>> defeating double spending on only part of the coin. The coin would be
>>>> partitioned by radix (or modulus, not sure what to call it.) For example in
>>>> order to multiply throughput by a factor of ten you could run ten parallel
>>>> chains, one would work on coin that ends in "0", one on coin that ends in
>>>> "1", and so on up to "9".
>>>>
>>>> The number of chains could increase automatically over time based on
>>>> the moving average of transaction volume.
>>>>
>>>> Blocks would have to contain the number of the partition they belong
>>>> to, and miners would have to round-robin through partitions so that an
>>>> attacker would not have an unfair advantage working on just one partition.
>>>>
>>>> I don't think there is much impact to miners, but clients would have to
>>>> send more than one message in order to spend money. Client messages will
>>>> need to enumerate coin using some sort of compression, to save space. This
>>>> seems okay to me since often in computing client software does have to
>>>> break things up in equal parts (e.g. memory pages, file system blocks,) and
>>>> the client software could hide the details.
>>>>
>>>> Best wishes for continued success to the project.
>>>>
>>>> Regards,
>>>> Akiva
>>>>
>>>> P.S. I found a funny anagram for SATOSHI NAKAMOTO: "NSA IS OOOK AT MATH"
>>>>
>>>>
>>>> _______________________________________________
>>>> bitcoin-dev mailing list
>>>> bitcoin-dev at lists.linuxfoundation.org
>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>>
>>>>
>>>
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev at lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>
>
> --
> I like to provide some work at no charge to prove my value. Do you need a
> techie?
> I own Litmocracy <http://www.litmocracy.com> and Meme Racing
> <http://www.memeracing.net> (in alpha).
> I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com>
> which now accepts Bitcoin.
> I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
> "He ought to find it more profitable to play by the rules" - Satoshi
> Nakamoto
>



-- 
I like to provide some work at no charge to prove my value. Do you need a
techie?
I own Litmocracy <http://www.litmocracy.com> and Meme Racing
<http://www.memeracing.net> (in alpha).
I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which
now accepts Bitcoin.
I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
"He ought to find it more profitable to play by the rules" - Satoshi
Nakamoto
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/45701a43/attachment.html>

From kanzure at gmail.com  Thu Dec 10 04:31:42 2015
From: kanzure at gmail.com (Bryan Bishop)
Date: Wed, 9 Dec 2015 22:31:42 -0600
Subject: [bitcoin-dev] Scaling by Partitioning
In-Reply-To: <CABCnA7W25KoHuSuB3Az250_PiRcFd5MjjKJfrm_qv4oaYUT5mg@mail.gmail.com>
References: <CABCnA7Wqz76m8qo5BYT41Z=hBH+fUfOc4xsFAGg=Niv7Jgkqsg@mail.gmail.com>
	<CAJmQggC1X5Lgt4xGoMtBZ_v3hC2GXcYaj2FngV2_7A=TDfSuEg@mail.gmail.com>
	<CAL8tG=mxYE97iMO05mPq4_f8VcmFBYqAmyPqTs439bPRGhaVqA@mail.gmail.com>
	<CABCnA7W25KoHuSuB3Az250_PiRcFd5MjjKJfrm_qv4oaYUT5mg@mail.gmail.com>
Message-ID: <CABaSBay9G3NQHn0HUkKfenr+e4be6arSBvy6vD=1+M3eSZJHtw@mail.gmail.com>

On Wed, Dec 9, 2015 at 9:58 PM, Akiva Lichtner wrote:
> Correct me if I am wrong, but the dream of a virtual currency where
> everybody is equal and runs the client on their mobile device went out the
> window long ago. I think that went out with the special mining hardware. If

Mining equipment isn't for transaction verification. The mining
equipment is used to work on Proof-of-Work. Thanks.

> my organization had to accept bitcoin payments I would assume that we'll
> need a small server farm for transaction verification, and that we would see

Unfortunately Bitcoin does not work like those centralized systems; it
should not be surprising that a system focused so much on
decentralized and independent verification would have developers
working on so many non-bandwidth scaling solutions. These other
proposals seek to preserve existing properties of Bitcoin (such as
cheap verification, low-bandwidth) while also increasing the amount of
activity that can enjoy the decentralized fruits of Proof-of-Work
labor. But not helpful to assume this can only look like Visa or any
database on a cluster etc...

> would be entirely okay for a guy on a smartphone to delegate verification to
> a trusted party, as long as the trust chain stops there and there is plenty
> of choice.

I don't suppose I could tempt you with probabilistically checkable
proofs, could I? These verify in milliseconds, grow sublinear in size
of the total data, but have no near-term proposal available yet.

> I am guessing the trustless virtual currency police would get pretty upset
> about such a pragmatic approach, but it's not really a choice, the failure
> to scale has already occurred. All things considered I think that Bitcoin

Perhaps instead of failure-to-scale you mean "failure to apply
traditional scaling has already failed", which shouldn't be so
surprising given the different security model that Bitcoin operates
on.

> most people trust at least one other person, so it's not that weird.

see the following recent text,
"""
Bitcoin is P2P electronic cash that is valuable over legacy systems
because of the monetary autonomy it brings to its users through
decentralization. Bitcoin seeks to address the root problem with
conventional currency: all the trust that's required to make it work--

-- Not that justified trust is a bad thing, but trust makes systems
brittle, opaque, and costly to operate. Trust failures result in systemic
collapses, trust curation creates inequality and monopoly lock-in, and
naturally arising trust choke-points can be abused to deny access to
due process. Through the use of cryptographic proof and decentralized
networks Bitcoin minimizes and replaces these trust costs.

With the available technology, there are fundamental trade-offs between
scale and decentralization. If the system is too costly people will be
forced to trust third parties rather than independently enforcing the
system's rules. If the Bitcoin blockchain?s resource usage, relative
to the available technology, is too great, Bitcoin loses its competitive
advantages compared to legacy systems because validation will be too
costly (pricing out many users), forcing trust back into the system.
If capacity is too low and our methods of transacting too inefficient,
access to the chain for dispute resolution will be too costly, again
pushing trust back into the system.
"""

- Bryan
http://heybryan.org/
1 512 203 0507

From akiva.lichtner at gmail.com  Thu Dec 10 04:04:17 2015
From: akiva.lichtner at gmail.com (Akiva Lichtner)
Date: Wed, 9 Dec 2015 23:04:17 -0500
Subject: [bitcoin-dev] Scaling by Partitioning
In-Reply-To: <CAJmQggBfYyk3cPAL57NZ3ecPG_ZxffjPftc0-EkTzKagjodgNQ@mail.gmail.com>
References: <CABCnA7Wqz76m8qo5BYT41Z=hBH+fUfOc4xsFAGg=Niv7Jgkqsg@mail.gmail.com>
	<CAJmQggC1X5Lgt4xGoMtBZ_v3hC2GXcYaj2FngV2_7A=TDfSuEg@mail.gmail.com>
	<CABCnA7VAO2XKLwd4axaYcttUHzhvXXEvYrwg7XDKH9nfo1k7RA@mail.gmail.com>
	<CAJmQggBfYyk3cPAL57NZ3ecPG_ZxffjPftc0-EkTzKagjodgNQ@mail.gmail.com>
Message-ID: <CABCnA7VOknCC6j5HAazW31jx8kGqiM0t0+Qo8QydShzESV2HkQ@mail.gmail.com>

It's an interval (a,b) where a, b are between 0 and 21*10^6*10^8 .

Somebody pointed out that this is not easily accomplished using the current
code because there are no coin ids.


On Wed, Dec 9, 2015 at 4:16 PM, Loi Luu <loi.luuthe at gmail.com> wrote:

> I guess the most basic question is how do you define a coin here?
>
> Thanks,
> Loi Luu
> On 10 Dec 2015 2:26 a.m., "Akiva Lichtner" <akiva.lichtner at gmail.com>
> wrote:
>
>> Thanks for giving serious consideration to my post.
>>
>> With regard to your question "if a transaction spends a "coin" that
>> ends in "1" and creates a new coin that ends in "1", which partition
>> should process the transaction?", I would answer that only one
>> partition is involved. In other words, there are N independent block
>> chains that never cross paths.
>>
>> With regard to your question "what is the prior data needed to
>> validate that kind of TXs?" I do not understand what this means. If
>> you can dumb it down a bit that would be good because there could be
>> some interesting concern in this question.
>>
>> Since partitions are completely segregated, there is no need for a
>> node to work on multiple partitions simultaneously. For attacks to be
>> defeated a node needs to be able to work on multiple partitions in
>> turn, not at the same time. The reason is because if the computing
>> power of the good-faith nodes is unbalanced this gives attackers an
>> unfair advantage.
>>
>> On 12/9/15, Loi Luu via bitcoin-dev
>> <bitcoin-dev at lists.linuxfoundation.org> wrote:
>> > Dear Akiva,
>> >
>> > Its Loi Luu, one of the authors of the SCP protocol (
>> > http://eprint.iacr.org/2015/1168.pdf ).
>> >
>> > Before SCP, we had been thinking hard about how to do sharding
>> efficiently
>> > without degrading any security guarantee. A simple solution which splits
>> > the coins, or TXs in to several partitions will just not work. You have
>> to
>> > answer more questions to have a good solutions. For example, I wonder in
>> > your proposal, if a transaction spends a "coin" that ends in "1" and
>> > creates a new coin that ends in "1", which partition should process the
>> > transaction? What is the prior data needed to validate that kind of TXs?
>> >
>> > The problem with other proposals, and probably yours as well,  that we
>> see
>> > is that the amount of data that you need to broadcast immediately to the
>> > network increases linearly with the number of TXs that the network can
>> > process. Thus, sharding does not bring any advantage than simply using
>> > other techniques to publish more blocks in one epoch (like Bitcoin-NG,
>> > Ghost). The whole point of using sharding/ partition is to localize
>> > the bandwidth used, and only broadcast only a minimal data to the
>> network.
>> >
>> > Clearly we are able to localize the bandwidth used with our SCP
>> protocol.
>> > The cost is that now recipients need to  themselves verify whether a
>> > transaction is double spending. However, we think that it is a
>> reasonable
>> > tradeoff, given the potential scalability that SCP can provides.
>> >
>> > Thanks,
>> > Loi Luu.
>> >
>> > On Wed, Dec 9, 2015 at 12:27 AM, Akiva Lichtner via bitcoin-dev <
>> > bitcoin-dev at lists.linuxfoundation.org> wrote:
>> >
>> >> Hello,
>> >>
>> >> I am seeking some expert feedback on an idea for scaling Bitcoin. As a
>> >> brief introduction: I work in the payment industry and I have twenty
>> >> years'
>> >> experience in development. I have some experience with process groups
>> and
>> >> ordering protocols too. I think I understand Satoshi's paper but I
>> admit
>> >> I
>> >> have not read the source code.
>> >>
>> >> The idea is to run more than one simultaneous chain, each chain
>> defeating
>> >> double spending on only part of the coin. The coin would be partitioned
>> >> by
>> >> radix (or modulus, not sure what to call it.) For example in order to
>> >> multiply throughput by a factor of ten you could run ten parallel
>> chains,
>> >> one would work on coin that ends in "0", one on coin that ends in "1",
>> >> and
>> >> so on up to "9".
>> >>
>> >> The number of chains could increase automatically over time based on
>> the
>> >> moving average of transaction volume.
>> >>
>> >> Blocks would have to contain the number of the partition they belong
>> to,
>> >> and miners would have to round-robin through partitions so that an
>> >> attacker
>> >> would not have an unfair advantage working on just one partition.
>> >>
>> >> I don't think there is much impact to miners, but clients would have to
>> >> send more than one message in order to spend money. Client messages
>> will
>> >> need to enumerate coin using some sort of compression, to save space.
>> >> This
>> >> seems okay to me since often in computing client software does have to
>> >> break things up in equal parts (e.g. memory pages, file system blocks,)
>> >> and
>> >> the client software could hide the details.
>> >>
>> >> Best wishes for continued success to the project.
>> >>
>> >> Regards,
>> >> Akiva
>> >>
>> >> P.S. I found a funny anagram for SATOSHI NAKAMOTO: "NSA IS OOOK AT
>> MATH"
>> >>
>> >>
>> >> _______________________________________________
>> >> bitcoin-dev mailing list
>> >> bitcoin-dev at lists.linuxfoundation.org
>> >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>> >>
>> >>
>> >
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/8f725842/attachment.html>

From luke.durback at gmail.com  Thu Dec 10 04:23:26 2015
From: luke.durback at gmail.com (Luke Durback)
Date: Wed, 9 Dec 2015 23:23:26 -0500
Subject: [bitcoin-dev] Standard BIP Draft: Turing Pseudo-Completeness
In-Reply-To: <CADm_WcZdC-iNF=tMmYYafsaLaHUcck03zw8cdsSCCPvXdTNyrw@mail.gmail.com>
References: <CAEj3M+wYicoACcpG5YUU6vF8vg98XCcJWmgBiyrJj-xHHxrhig@mail.gmail.com>
	<CADm_WcZdC-iNF=tMmYYafsaLaHUcck03zw8cdsSCCPvXdTNyrw@mail.gmail.com>
Message-ID: <CAEj3M+yHnPv0p0icRmFtNxJti2riSGzTPtGCHR_a9dW7qdkFpA@mail.gmail.com>

Mr. Garzik,

Thank you for the prompt response.  I should have explained my proposal a
little better.

First of all, this is not Turing completeness, nor is it pseudo-complete in
the sense of Ethereum's gas economics.

Instead, whenever a function call is encountered, the transaction is
validated and can be included in a block.  The code actually halts many
times.  A new transaction is then produced with the 2 stacks stored in the
transaction data (so that the 2 stacks are saved and execution can be
continued later).  When OP_RETURN_FROM_CALL_AND_CONTINUE is encountered,
the top value of the Return stack is popped and execution continues from
that location until validation/invalidation is reached.  It's not necessary
to check the code to see that it has no infinite loops because any
transaction with infinite loops will run out of BTC with which to fund the
transaction fees of additional function calls.

To reiterate the most important point:  Execution halts every time a
function call is encountered and the transaction can be included in a
block.  A new transaction is then produced that can (if included in a
block) continue execution.


Luke Durback

On Wed, Dec 9, 2015 at 11:03 PM, Jeff Garzik <jgarzik at gmail.com> wrote:

> There is no need for a BIP draft.  "Turing complete" is just a fancy,
> executive-impressing term for "it can run any computer program", or put
> even more simply, "it can loop"
>
> Furthermore, the specification of such a language is trivial.  It is the
> economics of validation that is the complex piece.  Proving whether or not
> a program will halt as expected - The Halting Problem - is near impossible
> for most complex programs.  As a result, your proof is... running the
> program.  That produces enormous validation consequences and costs for
> generic-execution scripts when applied to a decentralized network of
> validation P2P nodes.
>
> If you need that capability, it is just as easy to use a normal C/C++/etc.
> computer language, with your preferred algorithm libraries and development
> tools.
>
> See https://github.com/jgarzik/moxiebox for a working example of provable
> execution.
>
>
>
> On Thu, Dec 10, 2015 at 9:35 AM, Luke Durback via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> Hello Bitcoin-Dev,
>>
>> I hope this isn't out of line, but I joined the mailing list to try to
>> start a discussion on adding opcodes to make Script Turing Pseudo-Complete
>> as Wright suggested is possible.
>>
>> ---
>>
>> In line with Wright's suggestion, I propose adding a return stack
>> alongside the, already existing, control stack.
>>
>> The principle opcodes (excluding conditional versions of call and
>> return_from) needed are
>>
>> OP_DEFINITION_START FunctionName:  The code that follows is the
>> definition of a new function to be named
>> TransactionSenderAddress.FunctionName.  If this function name is already
>> taken, the transaction is marked invalid.  Within the transaction, the
>> function can be called simply as FunctionName.
>>
>> OP_DEFINITION_END:  This ends a function definition
>>
>> OP_FUNCTION_NAME FunctionName:  Gives the current transaction the name
>> FunctionName (this is necessary to build recursive functions)
>>
>> ---
>>
>> OP_CALL Namespace.FunctionName Value TransactionFee:  This marks the
>> transaction as valid.  It also pushes the current execution location onto
>> the return stack, debits the calling transaction by the TransactionFee and
>> Value, and creates a new transaction specified by Namespace.FunctionName
>> with both stacks continued from before (this may be dangerous, but I see no
>> way around it) with the specified value.
>>
>> OP_RETURN_FROM_CALL_AND_CONTINUE:  This pops the top value off the return
>> stack and continues from the specified location with both stacks in tact.
>>
>> ---
>>
>> It would also be useful if a transaction can create another transaction
>> arbitrarily, so to prepare for that, I additionally propose
>>
>> OP_NAMESPACE:  Pushes the current namespace onto the control stack
>>
>> This, combined with the ability to make new transactions arbitrarily
>> would allow a function to pay its creator.
>>
>>
>>
>> I understand that this isn't all that is needed, but I think it's a
>> start.  I hope this proposal has met you all well,
>>
>> Luke Durback
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151209/e1428fac/attachment.html>

From jtimon at jtimon.cc  Thu Dec 10 05:38:01 2015
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Thu, 10 Dec 2015 06:38:01 +0100
Subject: [bitcoin-dev] Standard BIP Draft: Turing Pseudo-Completeness
In-Reply-To: <CAEj3M+wYicoACcpG5YUU6vF8vg98XCcJWmgBiyrJj-xHHxrhig@mail.gmail.com>
References: <CAEj3M+wYicoACcpG5YUU6vF8vg98XCcJWmgBiyrJj-xHHxrhig@mail.gmail.com>
Message-ID: <CABm2gDq3K2uUWx_itZQJH53EFOJKAWOLiy3NdHWGPvUOCm33wA@mail.gmail.com>

On Dec 10, 2015 10:10 AM, "Luke Durback via bitcoin-dev" <
bitcoin-dev at lists.linuxfoundation.org> wrote:
> This, combined with the ability to make new transactions arbitrarily
would allow a function to pay its creator.

I don't understand what you mean by "a function" in this context, I assume
you mean a scriptSig, but then "paying its creator" doesn't make much sense
to me .

Could you provide some high level examples of the use cases you would like
to support with this?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151210/6fab13f9/attachment-0001.html>

From jl2012 at xbt.hk  Thu Dec 10 06:47:35 2015
From: jl2012 at xbt.hk (jl2012 at xbt.hk)
Date: Thu, 10 Dec 2015 01:47:35 -0500
Subject: [bitcoin-dev] Segregated Witness features wish list
Message-ID: <b13f6152767473dcf44a1d8965fdd32c@xbt.hk>

It seems the current consensus is to implement Segregated Witness. SW 
opens many new possibilities but we need a balance between new features 
and deployment time frame. I'm listing by my priority:

1-2 are about scalability and have highest priority

1. Witness size limit: with SW we should allow a bigger overall block 
size. It seems 2MB is considered to be safe for many people. However, 
the exact size and growth of block size should be determined based on 
testing and reasonable projection.

2. Deployment time frame: I prefer as soon as possible, even if none of 
the following new features are implemented. This is not only a technical 
issue but also a response to the community which has been waiting for a 
scaling solution for years

3-6 promote safety and reduce level of trust (higher priority)

3. SIGHASH_WITHINPUTVALUE [1]: there are many SIGHASH proposals but this 
one has the highest priority as it makes offline signing much easier.

4. Sum of fee, sigopcount, size etc as part of the witness hash tree: 
for compact proof of violations in these parameters. I prefer to have 
this feature in SWv1. Otherwise, that would become an ugly softfork in 
SWv2 as we need to maintain one more hash tree

5. Height and position of an input as part of witness will allow compact 
proof of non-existing UTXO. We need this eventually. If it is not done 
in SWv1, we could softfork it nicely in SWv2. I prefer this earlier as 
this is the last puzzle for compact fraud proof.

6. BIP62 and OP_IF malleability fix [2] as standardness rules: 
involuntary malleability may still be a problem in the relay network and 
may make the relay less efficient (need more research)

7-15 are new features and long-term goals (lower priority)

7. Enable OP_CAT etc:
OP_CAT will allow tree signatures described by [3]. Even without Schnorr 
signature, m-of-n multisig will become more efficient if m < n.

OP_SUBSTR/OP_LEFT/OP_RIGHT will allow people to shorten a payment 
address, while sacrificing security.

I'm not sure how those disabled bitwise logic codes could be useful

Multiplication and division may still considered to be risky and not 
very useful?

8. Schnorr signature: for very efficient multisig [3] but could be 
introduced later.

9. Per-input lock-time and relative lock-time: define lock-time and 
relative lock-time in witness, and signed by user. BIP68 is not a very 
ideal solution due to limited lock time length and resolution

10. OP_PUSHLOCKTIME and OP_PUSHRELATIVELOCKTIME: push the lock-time and 
relative lock-time to stack. Will allow more flexibility than OP_CLTV 
and OP_CSV

11. OP_RETURNTURE which allows softfork of any new OP codes [4]. It is 
not really necessary with the version byte design but with OP_RETURNTURE 
we don't need to pump the version byte too frequently.

12. OP_EVAL (BIP12), which enables Merkleized Abstract Syntax Trees 
(MAST) with OP_CAT [5]. This will also obsolete BIP16. Further 
restrictions should be made to make it safe [6]:
a) We may allow at most one OP_EVAL in the scriptPubKey
b) Not allow any OP_EVAL in the serialized script, nor anywhere else in 
the witness (therefore not Turing-complete)
c) In order to maintain the ability to statically analyze scripts, the 
serialized script must be the last push of the witness (or script 
fails), and OP_EVAL must be the last OP code in the scriptPubKey

13. Combo OP codes for more compact scripts, for example:

OP_MERKLEHASH160, if executed, is equivalent to OP_SWAP OP_IF OP_SWAP 
OP_ENDIF OP_CAT OP_HASH160 [3]. Allowing more compact tree-signature and 
MAST scripts.

OP_DUPTOALTSTACK, OP_DUPFROMALTSTACK: copy to / from alt stack without 
removing the item

14. UTXO commitment: good but not in near future

15. Starting as a softfork, moving to a hardfork? SW Softfork is a quick 
but dirty solution. I believe a hardfork is unavoidable in the future, 
as the 1MB limit has to be increased someday. If we could plan it ahead, 
we could have a much cleaner SW hardfork in the future, with codes 
pre-announced for 2 years.


[1] https://bitcointalk.org/index.php?topic=181734.0
[2] 
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-November/011679.html
[3] https://blockstream.com/2015/08/24/treesignatures/
[4] https://bitcointalk.org/index.php?topic=1106586.0
[5] 
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-September/010977.html
[6] https://bitcointalk.org/index.php?topic=58579.msg690093#msg690093

From satoshi at vistomail.com  Thu Dec 10 06:54:46 2015
From: satoshi at vistomail.com (satoshi at vistomail.com)
Date: Thu, 10 Dec 2015 06:54:46 -0000
Subject: [bitcoin-dev] Not this again.
Message-ID: <mailman.3.1449730650.1615.bitcoin-dev@lists.linuxfoundation.org>

I am not Craig Wright. We are all Satoshi.


From luke.durback at gmail.com  Thu Dec 10 06:36:28 2015
From: luke.durback at gmail.com (Luke Durback)
Date: Thu, 10 Dec 2015 01:36:28 -0500
Subject: [bitcoin-dev] Standard BIP Draft: Turing Pseudo-Completeness
In-Reply-To: <CABm2gDq3K2uUWx_itZQJH53EFOJKAWOLiy3NdHWGPvUOCm33wA@mail.gmail.com>
References: <CAEj3M+wYicoACcpG5YUU6vF8vg98XCcJWmgBiyrJj-xHHxrhig@mail.gmail.com>
	<CABm2gDq3K2uUWx_itZQJH53EFOJKAWOLiy3NdHWGPvUOCm33wA@mail.gmail.com>
Message-ID: <CAEj3M+ze9HU1KWoWT2nugw9hYY97jk_xsL8WUWqThq_wrXSAVg@mail.gmail.com>

Tomorrow, I'll work on writing a way to do voting on proposals with BTC
used as voting shares (This will be difficult as I do not know FORTH).
That seems like a fairly simple, useful example that will require loops and
reused functions.  I'll add a fee that goes to the creator.

IMO, if you write a complicated system of scripts that's used frequently,
it makes sense to charge a fee for its usage.  A decentralized exchange
between colored coins, for instance might take a small fee on each trade.


On Dec 10, 2015 10:10 AM, "Luke Durback via bitcoin-dev" <
bitcoin-dev at lists.linuxfoundation.org> wrote:
> This, combined with the ability to make new transactions arbitrarily
would allow a function to pay its creator.

I don't understand what you mean by "a function" in this context, I assume
you mean a scriptSig, but then "paying its creator" doesn't make much sense
to me .

Could you provide some high level examples of the use cases you would like
to support with this?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151210/cad0b36c/attachment.html>

From greg at xiph.org  Thu Dec 10 08:26:04 2015
From: greg at xiph.org (Gregory Maxwell)
Date: Thu, 10 Dec 2015 08:26:04 +0000
Subject: [bitcoin-dev] Segregated Witness features wish list
In-Reply-To: <b13f6152767473dcf44a1d8965fdd32c@xbt.hk>
References: <b13f6152767473dcf44a1d8965fdd32c@xbt.hk>
Message-ID: <CAAS2fgR0X1+-0UTxFWeRba84q3nz1aNHgP4jyw1Gxm+QxH3igg@mail.gmail.com>

On Thu, Dec 10, 2015 at 6:47 AM, jl2012--- via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> It seems the current consensus is to implement Segregated Witness. SW opens
> many new possibilities but we need a balance between new features and
> deployment time frame. I'm listing by my priority:

> 2. Deployment time frame: I prefer as soon as possible, even if none of the following new features are implemented.

Thanks, I agree there.

A point to keep in mind:  Segregated Witness was specifically designed
to make script changes / improvements / additions / total rewrites no
harder to do _after_ SW then they would be do do along with it.  For
many people the "ah ha! lets do this" was realizing it could be a
pretty clean soft-fork.  For me, it was realizing that we could
structure Segwit in a way that radically simply future script updates
... and in doing so avoid a getting trapped by a rush to put in every
script update someone wants.

This is achieved by having the 'version' byte(s) at the start of the
witness program. If the witness program prefix is unrecognized it
means RETURN TRUE.  This recaptures the behavior that seems to have
been intended by OP_VER in the earliest versions of the software, but
actually works instead of giving every user the power to hardfork the
system at any time. :)  This escapes much of the risk in script
changes, as we no longer need to worry about negation, or other
interactions potentially breaking things.  A new version flag can have
its whole design crafted as if it were being created on a clean slate.

Optimizing layout and such I think makes sense, but I think we should
consider any script enhancements completely off the table for SW;
otherwise the binding will delay deployment and increase complexity. I
want most of those things too (a couple I disagree with) and a few of
them we could do quite quickly-- but no need to bind them up; post SW
and esp with version bits we could deploy them quite rapidly and on
their own timeframes.


> Multiplication and division may still considered to be risky and not very useful?

Operations like these make sense with fixed with types, when they are
over arbitrary bignums, they're a complexity nightmare...  as
demonstrated by Bitcoin. :)


RE: OP_DUPTOALTSTACK  yea, I've wanted that several times (really I've
been sad that there isn't just a stack flag on every manipulation
instruction).

From kanzure at gmail.com  Thu Dec 10 08:28:49 2015
From: kanzure at gmail.com (Bryan Bishop)
Date: Thu, 10 Dec 2015 02:28:49 -0600
Subject: [bitcoin-dev] Segregated Witness features wish list
In-Reply-To: <b13f6152767473dcf44a1d8965fdd32c@xbt.hk>
References: <b13f6152767473dcf44a1d8965fdd32c@xbt.hk>
Message-ID: <CABaSBaw1XJkHE77UxSS1=DU6qX9Y5h1DyYJR_EN3ty9KR=4Jvw@mail.gmail.com>

On Thu, Dec 10, 2015 at 12:47 AM, jl2012 wrote:
> 3. SIGHASH_WITHINPUTVALUE [1]: there are many SIGHASH proposals but this one
> has the highest priority as it makes offline signing much easier.

nhashtype proposal:
https://github.com/scmorse/bitcoin-misc/blob/master/sighash_proposal.md

OP_CODESEPARATOR:
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-April/007802.html

summary email about sighash type proposals (which IIRC you saw, so
leaving this link here is mainly for the benefit of others):
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-August/010759.html

- Bryan
http://heybryan.org/
1 512 203 0507

From greg at xiph.org  Thu Dec 10 09:51:23 2015
From: greg at xiph.org (Gregory Maxwell)
Date: Thu, 10 Dec 2015 09:51:23 +0000
Subject: [bitcoin-dev] Segregated Witness features wish list
In-Reply-To: <b13f6152767473dcf44a1d8965fdd32c@xbt.hk>
References: <b13f6152767473dcf44a1d8965fdd32c@xbt.hk>
Message-ID: <CAAS2fgT09Tk+5dBQ4YJA_pwK56xX2mEQe9TkJqgcB0j2BZZJ1g@mail.gmail.com>

On Thu, Dec 10, 2015 at 6:47 AM, jl2012--- via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> 4. Sum of fee, sigopcount, size etc as part of the witness hash tree: for

I should have also commented on this: the block can indicate how many
sum criteria there are; and then additional ones could be soft-forked
in. Haven't tried implementing it yet, but there you go. :)

From tamas at bitsofproof.com  Thu Dec 10 12:54:30 2015
From: tamas at bitsofproof.com (Tamas Blummer)
Date: Thu, 10 Dec 2015 13:54:30 +0100
Subject: [bitcoin-dev] Segregated Witness features wish list
In-Reply-To: <b13f6152767473dcf44a1d8965fdd32c@xbt.hk>
References: <b13f6152767473dcf44a1d8965fdd32c@xbt.hk>
Message-ID: <7D7416E3-0038-484D-BBA9-35FA4C2AE3DC@bitsofproof.com>

Note that the unused space in coin base input script allows us to soft-fork an additional SW Merkle tree root into the design,
therefore please make sure the new SW data structure also has a new slot for future extension.

Tamas Blummer


From jtimon at jtimon.cc  Fri Dec 11 15:36:48 2015
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Fri, 11 Dec 2015 16:36:48 +0100
Subject: [bitcoin-dev] Standard BIP Draft: Turing Pseudo-Completeness
In-Reply-To: <CAEj3M+ze9HU1KWoWT2nugw9hYY97jk_xsL8WUWqThq_wrXSAVg@mail.gmail.com>
References: <CAEj3M+wYicoACcpG5YUU6vF8vg98XCcJWmgBiyrJj-xHHxrhig@mail.gmail.com>
	<CABm2gDq3K2uUWx_itZQJH53EFOJKAWOLiy3NdHWGPvUOCm33wA@mail.gmail.com>
	<CAEj3M+ze9HU1KWoWT2nugw9hYY97jk_xsL8WUWqThq_wrXSAVg@mail.gmail.com>
Message-ID: <CABm2gDr5rKNMerPebJ6b3ayJznEAAvu_zM76syooH-3MepSzXg@mail.gmail.com>

On Dec 10, 2015 7:36 AM, "Luke Durback" <luke.durback at gmail.com> wrote:
>
> Tomorrow, I'll work on writing a way to do voting on proposals with BTC
used as voting shares (This will be difficult as I do not know FORTH).
That seems like a fairly simple, useful example that will require loops and
reused functions.  I'll add a fee that goes to the creator.

If it's voting for something consensus, you will need something special. If
it's not consensus (ie external) thw voting doesn't have to hit the chain
at all.
I don't see how "loops and reused functions" are needed in the scripting
language for this use case, but I'm probably missing some details. Please,
the more concrete you make your example, the easiest it will be for me to
understand.

> IMO, if you write a complicated system of scripts that's used frequently,
it makes sense to charge a fee for its usage.

But each scriptSig is only executed once with its corresponding
scriptPubKey. Are you proposing we change that?

>  A decentralized exchange between colored coins, for instance might take
a small fee on each trade.

I've been researching the topic of decentralized exchange from before the
term "colored coins" was first used (now there's multiple designs and
implementations); contributed to and reviewed many designs: none of them
(colored coins or not) required turing completeness.
I'm sorry, but what you are saying here is too vague for me to concretely
be able to refute the low level "needs" you claim your use cases to have.

> On Dec 10, 2015 10:10 AM, "Luke Durback via bitcoin-dev" <
bitcoin-dev at lists.linuxfoundation.org> wrote:
> > This, combined with the ability to make new transactions arbitrarily
would allow a function to pay its creator.
>
> I don't understand what you mean by "a function" in this context, I
assume you mean a scriptSig, but then "paying its creator" doesn't make
much sense to me .
>
> Could you provide some high level examples of the use cases you would
like to support with this?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151211/af6b721c/attachment.html>

From jtimon at jtimon.cc  Fri Dec 11 15:38:37 2015
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Fri, 11 Dec 2015 16:38:37 +0100
Subject: [bitcoin-dev] Standard BIP Draft: Turing Pseudo-Completeness
In-Reply-To: <CABm2gDr5rKNMerPebJ6b3ayJznEAAvu_zM76syooH-3MepSzXg@mail.gmail.com>
References: <CAEj3M+wYicoACcpG5YUU6vF8vg98XCcJWmgBiyrJj-xHHxrhig@mail.gmail.com>
	<CABm2gDq3K2uUWx_itZQJH53EFOJKAWOLiy3NdHWGPvUOCm33wA@mail.gmail.com>
	<CAEj3M+ze9HU1KWoWT2nugw9hYY97jk_xsL8WUWqThq_wrXSAVg@mail.gmail.com>
	<CABm2gDr5rKNMerPebJ6b3ayJznEAAvu_zM76syooH-3MepSzXg@mail.gmail.com>
Message-ID: <CABm2gDrD2XfhG1VEb6iAb0AoPu9AwQgRjdbv-=cOUhU0_FTrQQ@mail.gmail.com>

well "only executed once" (every time someone verifies that transaction)...
On Dec 11, 2015 4:36 PM, "Jorge Tim?n" <jtimon at jtimon.cc> wrote:

>
> On Dec 10, 2015 7:36 AM, "Luke Durback" <luke.durback at gmail.com> wrote:
> >
> > Tomorrow, I'll work on writing a way to do voting on proposals with BTC
> used as voting shares (This will be difficult as I do not know FORTH).
> That seems like a fairly simple, useful example that will require loops and
> reused functions.  I'll add a fee that goes to the creator.
>
> If it's voting for something consensus, you will need something special.
> If it's not consensus (ie external) thw voting doesn't have to hit the
> chain at all.
> I don't see how "loops and reused functions" are needed in the scripting
> language for this use case, but I'm probably missing some details. Please,
> the more concrete you make your example, the easiest it will be for me to
> understand.
>
> > IMO, if you write a complicated system of scripts that's used
> frequently, it makes sense to charge a fee for its usage.
>
> But each scriptSig is only executed once with its corresponding
> scriptPubKey. Are you proposing we change that?
>
> >  A decentralized exchange between colored coins, for instance might take
> a small fee on each trade.
>
> I've been researching the topic of decentralized exchange from before the
> term "colored coins" was first used (now there's multiple designs and
> implementations); contributed to and reviewed many designs: none of them
> (colored coins or not) required turing completeness.
> I'm sorry, but what you are saying here is too vague for me to concretely
> be able to refute the low level "needs" you claim your use cases to have.
>
> > On Dec 10, 2015 10:10 AM, "Luke Durback via bitcoin-dev" <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
> > > This, combined with the ability to make new transactions arbitrarily
> would allow a function to pay its creator.
> >
> > I don't understand what you mean by "a function" in this context, I
> assume you mean a scriptSig, but then "paying its creator" doesn't make
> much sense to me .
> >
> > Could you provide some high level examples of the use cases you would
> like to support with this?
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151211/7e6ef177/attachment.html>

From jtimon at jtimon.cc  Fri Dec 11 16:18:48 2015
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Fri, 11 Dec 2015 17:18:48 +0100
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CABsx9T1pLtOaGOVpVs8URAwpbb884htkrFLWtX8-2gGpS6gPWw@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
	<CAAS2fgTGYSiAJHZq80rD4UieV8XetS=W0b45b5onWAS9gF-F7g@mail.gmail.com>
	<CABsx9T1i50Gvxj18W=n2mYGNpsMrSkDT26CdA3aQqT5FFN86yw@mail.gmail.com>
	<CAAS2fgSxpSat3VOje3-C4zgaRUVJVx-eRJbSYJqhvfR5SvCDwA@mail.gmail.com>
	<CAF_2MyUJMdJyh7FKq6UYCtwJZQ59i-pnWT_tFEK5EQx65iwHDQ@mail.gmail.com>
	<CAAS2fgS-jjEVeHf_LErppTadtAaSeBum+KiGHpoo=Jz5BZArsQ@mail.gmail.com>
	<CABm2gDq4f0ettDhh14jZ0zztSwSJ0Z=KDEeMTOJxTHF8VV+KXw@mail.gmail.com>
	<CAAS2fgTAFgANJ495xiOkiW-OeFA_VZHhhR5uL+jVaoYQz_yBPg@mail.gmail.com>
	<CABsx9T1pLtOaGOVpVs8URAwpbb884htkrFLWtX8-2gGpS6gPWw@mail.gmail.com>
Message-ID: <CABm2gDp4+V2-Gu3XTcmRxFy_KCfxrptjhmXBxjf04Kdi5Bf+2A@mail.gmail.com>

On Dec 9, 2015 5:40 PM, "Gavin Andresen" <gavinandresen at gmail.com> wrote:
>
> On Wed, Dec 9, 2015 at 3:03 AM, Gregory Maxwell via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:
>>
>> I think it would be logical to do as part of a hardfork that moved
>> commitments generally; e.g. a better position for merged mining (such
>> a hardfork was suggested in 2010 as something that could be done if
>> merged mining was used), room for commitments to additional block
>> back-references for compact SPV proofs, and/or UTXO set commitments.
>> Part of the reason to not do it now is that the requirements for the
>> other things that would be there are not yet well defined. For these
>> other applications, the additional overhead is actually fairly
>> meaningful; unlike the fraud proofs.
>
>
> So just design ahead for those future uses. Make the merkle tree:
>
>
>              root_in_block_header
>                      /      \
>   tx_data_root      other_root
>                                /       \
>         segwitness_root     reserved_for_future_use_root

This is basically what I meant by

struct hashRootStruct
{
uint256 hashMerkleRoot;
uint256 hashWitnessesRoot;
uint256 hashextendedHeader;
}

but my design doesn't calculate other_root as it appears in your tree (is
not necessary).

Since stop requiring bip34 (height in coinbase) is also a hardfork (and a
trivial one) I suggested to move it at the same time. But thinking more
about it, since BIP34 also elegantly solves BIP30, I would keep the height
in the coinbase (even if we move it to the extented header tree as well for
convenience).
That should be able to include future consensus-enforced commitments (extra
back-refs for compact proofs, txo/utxo commitments, etc) or non-consensus
data (merged mining data, miner-published data).
Greg Maxwell suggested to move those later and I answered fair enough. But
thinking more about it, if the extra commitments field is extensible, we
don't need to move anything now, and therefore we don't need for those
designs (extra back-refs for compact proofs, txo/utxo commitments, etc) to
be ready to deploy a hardfork segregated witness: you just need to make
sure that your format is extensible via softfork in the future.

I'm therefore back to the "let's better deploy segregated witness as a
hardfork" position.
The change required to the softfork segregated witnesses implementation
would be relatively small.

Another option would be to deploy both parts (sw and the movement from the
coinbase to the extra header) at the same time but with different
activation conditions, for example:

- For sw: deploy as soon as possible with bip9.
- For the hardfork codebase to extra header movement: 1 year grace + bip9
for later miner upgrade confirmation.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151211/2a7a0a4a/attachment.html>

From gavinandresen at gmail.com  Fri Dec 11 16:43:40 2015
From: gavinandresen at gmail.com (Gavin Andresen)
Date: Fri, 11 Dec 2015 11:43:40 -0500
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CABm2gDp4+V2-Gu3XTcmRxFy_KCfxrptjhmXBxjf04Kdi5Bf+2A@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CABm2gDpcek=u=Rpe68EMOq6M7Bji9J=s5VvoQWKRqaQDAP5kTw@mail.gmail.com>
	<CABsx9T1wga3Tandoe2mVGSKdHJytHoc9Ko7HRm2SvJXABEFk9w@mail.gmail.com>
	<CAAS2fgTGYSiAJHZq80rD4UieV8XetS=W0b45b5onWAS9gF-F7g@mail.gmail.com>
	<CABsx9T1i50Gvxj18W=n2mYGNpsMrSkDT26CdA3aQqT5FFN86yw@mail.gmail.com>
	<CAAS2fgSxpSat3VOje3-C4zgaRUVJVx-eRJbSYJqhvfR5SvCDwA@mail.gmail.com>
	<CAF_2MyUJMdJyh7FKq6UYCtwJZQ59i-pnWT_tFEK5EQx65iwHDQ@mail.gmail.com>
	<CAAS2fgS-jjEVeHf_LErppTadtAaSeBum+KiGHpoo=Jz5BZArsQ@mail.gmail.com>
	<CABm2gDq4f0ettDhh14jZ0zztSwSJ0Z=KDEeMTOJxTHF8VV+KXw@mail.gmail.com>
	<CAAS2fgTAFgANJ495xiOkiW-OeFA_VZHhhR5uL+jVaoYQz_yBPg@mail.gmail.com>
	<CABsx9T1pLtOaGOVpVs8URAwpbb884htkrFLWtX8-2gGpS6gPWw@mail.gmail.com>
	<CABm2gDp4+V2-Gu3XTcmRxFy_KCfxrptjhmXBxjf04Kdi5Bf+2A@mail.gmail.com>
Message-ID: <CABsx9T0nxcqAkEt7+pVm9oZEZH_HCJ9D3J00v0bKJYeUcDv1hQ@mail.gmail.com>

On Fri, Dec 11, 2015 at 11:18 AM, Jorge Tim?n <jtimon at jtimon.cc> wrote:

> This is basically what I meant by
>
> struct hashRootStruct
> {
> uint256 hashMerkleRoot;
> uint256 hashWitnessesRoot;
> uint256 hashextendedHeader;
> }
>
> but my design doesn't calculate other_root as it appears in your tree (is
> not necessary).
>
> It is necessary to maintain compatibility with SPV nodes/wallets.

Any code that just checks merkle paths up into the block header would have
to change if the structure of the merkle tree changed to be three-headed at
the top.

If it remains a binary tree, then it doesn't need to change at all-- the
code that produces the merkle paths will just send a path that is one step
deeper.

Plus, it's just weird to have a merkle tree that isn't a binary tree.....

-- 
--
Gavin Andresen
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151211/2f95a032/attachment-0001.html>

From luke.durback at gmail.com  Fri Dec 11 21:45:44 2015
From: luke.durback at gmail.com (Luke Durback)
Date: Fri, 11 Dec 2015 16:45:44 -0500
Subject: [bitcoin-dev] Standard BIP Draft: Turing Pseudo-Completeness
In-Reply-To: <CABm2gDr5rKNMerPebJ6b3ayJznEAAvu_zM76syooH-3MepSzXg@mail.gmail.com>
References: <CAEj3M+wYicoACcpG5YUU6vF8vg98XCcJWmgBiyrJj-xHHxrhig@mail.gmail.com>
	<CABm2gDq3K2uUWx_itZQJH53EFOJKAWOLiy3NdHWGPvUOCm33wA@mail.gmail.com>
	<CAEj3M+ze9HU1KWoWT2nugw9hYY97jk_xsL8WUWqThq_wrXSAVg@mail.gmail.com>
	<CABm2gDr5rKNMerPebJ6b3ayJznEAAvu_zM76syooH-3MepSzXg@mail.gmail.com>
Message-ID: <CAEj3M+yFPRA8iGzv-D+bQqchxwhqNEdLLwF_KNHGKqVBHNtTXQ@mail.gmail.com>

>If it's voting for something consensus, you will need something special.
If it's not consensus (ie external) thw voting doesn't have to hit the
chain at all.

I had in mind voting for something that can't be trusted if done
externally:  Perhaps BIPs for instance.  People would somehow "mark" their
BTC as being "For Proposition X" (as opposed to all other propositions) and
the vote would be canceled as soon as the BTC is spent again.

Unfortunately, I've spent the past 2 days trying to find a design that
would allow this (I don't think my original suggestion made sense in the
context of how transactions work), and I haven't gotten much yet.

>But each scriptSig is only executed once with its corresponding
scriptPubKey. Are you proposing we change that?

Sorry, I didn't understand Bitcoin's transaction model well enough when I
first made the proposal.  If Turing Pseudo-Completeness is possible with
Bitcoin, then I understand now that it could not require you to execute a
script more than once.  My current thought is that recursion can be
accomplished via checking if the next output's scriptPubKey is identical in
every way to the current scriptPubKey.  Unfortunately, a lot more is needed
than just recursion in order to do on-chain BTC voting the way I have in
mind.  I'll keep working on this.

On Fri, Dec 11, 2015 at 10:36 AM, Jorge Tim?n <jtimon at jtimon.cc> wrote:

>
> On Dec 10, 2015 7:36 AM, "Luke Durback" <luke.durback at gmail.com> wrote:
> >
> > Tomorrow, I'll work on writing a way to do voting on proposals with BTC
> used as voting shares (This will be difficult as I do not know FORTH).
> That seems like a fairly simple, useful example that will require loops and
> reused functions.  I'll add a fee that goes to the creator.
>
> If it's voting for something consensus, you will need something special.
> If it's not consensus (ie external) thw voting doesn't have to hit the
> chain at all.
> I don't see how "loops and reused functions" are needed in the scripting
> language for this use case, but I'm probably missing some details. Please,
> the more concrete you make your example, the easiest it will be for me to
> understand.
>
> > IMO, if you write a complicated system of scripts that's used
> frequently, it makes sense to charge a fee for its usage.
>
> But each scriptSig is only executed once with its corresponding
> scriptPubKey. Are you proposing we change that?
>
> >  A decentralized exchange between colored coins, for instance might take
> a small fee on each trade.
>
> I've been researching the topic of decentralized exchange from before the
> term "colored coins" was first used (now there's multiple designs and
> implementations); contributed to and reviewed many designs: none of them
> (colored coins or not) required turing completeness.
> I'm sorry, but what you are saying here is too vague for me to concretely
> be able to refute the low level "needs" you claim your use cases to have.
>
> > On Dec 10, 2015 10:10 AM, "Luke Durback via bitcoin-dev" <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
> > > This, combined with the ability to make new transactions arbitrarily
> would allow a function to pay its creator.
> >
> > I don't understand what you mean by "a function" in this context, I
> assume you mean a scriptSig, but then "paying its creator" doesn't make
> much sense to me .
> >
> > Could you provide some high level examples of the use cases you would
> like to support with this?
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151211/20ba8605/attachment.html>

From jannes.faber at gmail.com  Sat Dec 12 00:43:11 2015
From: jannes.faber at gmail.com (Jannes Faber)
Date: Sat, 12 Dec 2015 01:43:11 +0100
Subject: [bitcoin-dev] Segregated Witness features wish list
In-Reply-To: <7D7416E3-0038-484D-BBA9-35FA4C2AE3DC@bitsofproof.com>
References: <b13f6152767473dcf44a1d8965fdd32c@xbt.hk>
	<7D7416E3-0038-484D-BBA9-35FA4C2AE3DC@bitsofproof.com>
Message-ID: <CABeL=0hzQeOPcVgRcdM6Q_hqTNevHBOs7d2HMd6yh0o5Du9yAw@mail.gmail.com>

Segregated IBLT

I was just wondering if it would make sense when we have SW to also make
Segregated IBLT? Segregating transactions from signatures and then tune the
parameters such that transactions have a slightly higher guarantee and save
a bit of space on the signatures side.

IBLT should of course, most of the time, convey all transactions _and_ all
signatures. However, in suboptimal situations, at least the receiving miner
will be more likely to have all the transactions, just possibly not all the
signatures.

Assuming the miner was already planning on SPV mining anyway, at least now
she knows which transactions to remove from her mempool, taking away an
excuse to mine an empty block. And she can still verify most of the
signatures too (whatever % could be recovered from the IBLT).

I guess this does not improve the worst adversarial case for IBLT block
propagation, but it should improve the effectiveness in cases where the
"normal" IBLT would fail to deliver all transactions. Transactions without
signatures is better than no transactions at all, for a miner that's eager
to start on the next block, right? In "optimal" cases it would reduce the
size of the IBLT.

Sorry if this was already suggested.


--
Jannes

On 10 December 2015 at 13:54, Tamas Blummer via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Note that the unused space in coin base input script allows us to
> soft-fork an additional SW Merkle tree root into the design,
> therefore please make sure the new SW data structure also has a new slot
> for future extension.
>
> Tamas Blummer
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151212/80703517/attachment.html>

From digitsu at gmail.com  Sat Dec 12 05:13:48 2015
From: digitsu at gmail.com (digitsu at gmail.com)
Date: Fri, 11 Dec 2015 21:13:48 -0800 (PST)
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CABsx9T0nxcqAkEt7+pVm9oZEZH_HCJ9D3J00v0bKJYeUcDv1hQ@mail.gmail.com>
References: <CABsx9T0nxcqAkEt7+pVm9oZEZH_HCJ9D3J00v0bKJYeUcDv1hQ@mail.gmail.com>
Message-ID: <1449897228198.c655b3ae@Nodemailer>

If this means essentially that a soft fork deployment of SegWit will require SPV wallet servers to change their logic (or risk not being able to send payments) then it does seem to me that a hard fork to deploy this non controversial change is not only cleaner (on the data structure side) but safer in terms of the potential to affect the user experience.?






?
Regards,

On Sat, Dec 12, 2015 at 1:43 AM, Gavin Andresen via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:

> On Fri, Dec 11, 2015 at 11:18 AM, Jorge Tim?n <jtimon at jtimon.cc> wrote:
>> This is basically what I meant by
>>
>> struct hashRootStruct
>> {
>> uint256 hashMerkleRoot;
>> uint256 hashWitnessesRoot;
>> uint256 hashextendedHeader;
>> }
>>
>> but my design doesn't calculate other_root as it appears in your tree (is
>> not necessary).
>>
>> It is necessary to maintain compatibility with SPV nodes/wallets.
> Any code that just checks merkle paths up into the block header would have
> to change if the structure of the merkle tree changed to be three-headed at
> the top.
> If it remains a binary tree, then it doesn't need to change at all-- the
> code that produces the merkle paths will just send a path that is one step
> deeper.
> Plus, it's just weird to have a merkle tree that isn't a binary tree.....
> -- 
> --
> Gavin Andresen
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151211/63cf45aa/attachment.html>

From mark at friedenbach.org  Sat Dec 12 15:18:46 2015
From: mark at friedenbach.org (Mark Friedenbach)
Date: Sat, 12 Dec 2015 23:18:46 +0800
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <1449897228198.c655b3ae@Nodemailer>
References: <CABsx9T0nxcqAkEt7+pVm9oZEZH_HCJ9D3J00v0bKJYeUcDv1hQ@mail.gmail.com>
	<1449897228198.c655b3ae@Nodemailer>
Message-ID: <CAOG=w-t=+0Zdoy+d4Y2t9nbRkyO30N_Az9kbRarGo69yHCpSwA@mail.gmail.com>

A segwit supporting server would be required to support relaying segwit
transactions, although a non-segwit server could at least inform a wallet
of segwit txns observed, even if it doesn't relay all information necessary
to validate.

Non segwit servers and wallets would continue operations as if nothing had
occurred.
If this means essentially that a soft fork deployment of SegWit will
require SPV wallet servers to change their logic (or risk not being able to
send payments) then it does seem to me that a hard fork to deploy this non
controversial change is not only cleaner (on the data structure side) but
safer in terms of the potential to affect the user experience.


? Regards,


On Sat, Dec 12, 2015 at 1:43 AM, Gavin Andresen via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> On Fri, Dec 11, 2015 at 11:18 AM, Jorge Tim?n <jtimon at jtimon.cc> wrote:
>
>> This is basically what I meant by
>>
>> struct hashRootStruct
>> {
>> uint256 hashMerkleRoot;
>> uint256 hashWitnessesRoot;
>> uint256 hashextendedHeader;
>> }
>>
>> but my design doesn't calculate other_root as it appears in your tree (is
>> not necessary).
>>
>> It is necessary to maintain compatibility with SPV nodes/wallets.
>
> Any code that just checks merkle paths up into the block header would have
> to change if the structure of the merkle tree changed to be three-headed at
> the top.
>
> If it remains a binary tree, then it doesn't need to change at all-- the
> code that produces the merkle paths will just send a path that is one step
> deeper.
>
> Plus, it's just weird to have a merkle tree that isn't a binary tree.....
>
> --
> --
> Gavin Andresen
>


_______________________________________________
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151212/68adbb6a/attachment.html>

From jtimon at jtimon.cc  Sat Dec 12 20:00:43 2015
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Sat, 12 Dec 2015 21:00:43 +0100
Subject: [bitcoin-dev] Standard BIP Draft: Turing Pseudo-Completeness
In-Reply-To: <CAEj3M+yFPRA8iGzv-D+bQqchxwhqNEdLLwF_KNHGKqVBHNtTXQ@mail.gmail.com>
References: <CAEj3M+wYicoACcpG5YUU6vF8vg98XCcJWmgBiyrJj-xHHxrhig@mail.gmail.com>
	<CABm2gDq3K2uUWx_itZQJH53EFOJKAWOLiy3NdHWGPvUOCm33wA@mail.gmail.com>
	<CAEj3M+ze9HU1KWoWT2nugw9hYY97jk_xsL8WUWqThq_wrXSAVg@mail.gmail.com>
	<CABm2gDr5rKNMerPebJ6b3ayJznEAAvu_zM76syooH-3MepSzXg@mail.gmail.com>
	<CAEj3M+yFPRA8iGzv-D+bQqchxwhqNEdLLwF_KNHGKqVBHNtTXQ@mail.gmail.com>
Message-ID: <CABm2gDp--7Hkecop2h7yZNnJ5D0HnGF6t+uWK_G6-tHkC9q4Fg@mail.gmail.com>

On Fri, Dec 11, 2015 at 10:45 PM, Luke Durback <luke.durback at gmail.com> wrote:
>>If it's voting for something consensus, you will need something special. If
>> it's not consensus (ie external) thw voting doesn't have to hit the chain at
>> all.
>
> I had in mind voting for something that can't be trusted if done externally:
> Perhaps BIPs for instance.  People would somehow "mark" their BTC as being
> "For Proposition X" (as opposed to all other propositions) and the vote
> would be canceled as soon as the BTC is spent again.
>
> Unfortunately, I've spent the past 2 days trying to find a design that would
> allow this (I don't think my original suggestion made sense in the context
> of how transactions work), and I haven't gotten much yet.

Well, as said, if it's for consensus, you will need to adapt the
system in a special way anyway, but I still don't see why turing
completeness is required.
This type of idea is not new. Since miners can censor votes (and
that's undetectable for consensus), several solutions have been
proposed, time lock the votes, for example.

>>But each scriptSig is only executed once with its corresponding
>> scriptPubKey. Are you proposing we change that?
>
> Sorry, I didn't understand Bitcoin's transaction model well enough when I
> first made the proposal.  If Turing Pseudo-Completeness is possible with
> Bitcoin, then I understand now that it could not require you to execute a
> script more than once.  My current thought is that recursion can be
> accomplished via checking if the next output's scriptPubKey is identical in
> every way to the current scriptPubKey.  Unfortunately, a lot more is needed
> than just recursion in order to do on-chain BTC voting the way I have in
> mind.  I'll keep working on this.

What you call "recursion" seems similar to what we usually call "covenants", see

https://bitcointalk.org/index.php?topic=278122.0

Although the thread says "an amusingly bad idea", I think it's
actually a great idea and there's some use cases that are very hard to
support without covenants.
Again, no Turing completeness required for this.

> On Fri, Dec 11, 2015 at 10:36 AM, Jorge Tim?n <jtimon at jtimon.cc> wrote:
>>
>>
>> On Dec 10, 2015 7:36 AM, "Luke Durback" <luke.durback at gmail.com> wrote:
>> >
>> > Tomorrow, I'll work on writing a way to do voting on proposals with BTC
>> > used as voting shares (This will be difficult as I do not know FORTH).  That
>> > seems like a fairly simple, useful example that will require loops and
>> > reused functions.  I'll add a fee that goes to the creator.
>>
>> If it's voting for something consensus, you will need something special.
>> If it's not consensus (ie external) thw voting doesn't have to hit the chain
>> at all.
>> I don't see how "loops and reused functions" are needed in the scripting
>> language for this use case, but I'm probably missing some details. Please,
>> the more concrete you make your example, the easiest it will be for me to
>> understand.
>>
>> > IMO, if you write a complicated system of scripts that's used
>> > frequently, it makes sense to charge a fee for its usage.
>>
>> But each scriptSig is only executed once with its corresponding
>> scriptPubKey. Are you proposing we change that?
>>
>> >  A decentralized exchange between colored coins, for instance might take
>> > a small fee on each trade.
>>
>> I've been researching the topic of decentralized exchange from before the
>> term "colored coins" was first used (now there's multiple designs and
>> implementations); contributed to and reviewed many designs: none of them
>> (colored coins or not) required turing completeness.
>> I'm sorry, but what you are saying here is too vague for me to concretely
>> be able to refute the low level "needs" you claim your use cases to have.
>>
>> > On Dec 10, 2015 10:10 AM, "Luke Durback via bitcoin-dev"
>> > <bitcoin-dev at lists.linuxfoundation.org> wrote:
>> > > This, combined with the ability to make new transactions arbitrarily
>> > > would allow a function to pay its creator.
>> >
>> > I don't understand what you mean by "a function" in this context, I
>> > assume you mean a scriptSig, but then "paying its creator" doesn't make much
>> > sense to me .
>> >
>> > Could you provide some high level examples of the use cases you would
>> > like to support with this?
>
>

From el33th4x0r at gmail.com  Sat Dec 12 21:01:45 2015
From: el33th4x0r at gmail.com (=?UTF-8?Q?Emin_G=C3=BCn_Sirer?=)
Date: Sat, 12 Dec 2015 21:01:45 +0000
Subject: [bitcoin-dev] Standard BIP Draft: Turing Pseudo-Completeness
In-Reply-To: <CABm2gDp--7Hkecop2h7yZNnJ5D0HnGF6t+uWK_G6-tHkC9q4Fg@mail.gmail.com>
References: <CAEj3M+wYicoACcpG5YUU6vF8vg98XCcJWmgBiyrJj-xHHxrhig@mail.gmail.com>
	<CABm2gDq3K2uUWx_itZQJH53EFOJKAWOLiy3NdHWGPvUOCm33wA@mail.gmail.com>
	<CAEj3M+ze9HU1KWoWT2nugw9hYY97jk_xsL8WUWqThq_wrXSAVg@mail.gmail.com>
	<CABm2gDr5rKNMerPebJ6b3ayJznEAAvu_zM76syooH-3MepSzXg@mail.gmail.com>
	<CAEj3M+yFPRA8iGzv-D+bQqchxwhqNEdLLwF_KNHGKqVBHNtTXQ@mail.gmail.com>
	<CABm2gDp--7Hkecop2h7yZNnJ5D0HnGF6t+uWK_G6-tHkC9q4Fg@mail.gmail.com>
Message-ID: <CAPkFh0vk7M_oGR7hQKrgbs3WLdDnqVDagd7kNbVOXzxWkqZXeQ@mail.gmail.com>

.,
,
/.
, /,

,.
   / ,
..
,,,  . // .,      .

_. ...  ..   ._.

,    _


,



,
,
  , , ...     _  _.

,.

.  ,.,    _.
.,    ,  ..
,

,,

._

.  .

_
.
,
,     ,    ,   /..,,

/ ,

.     .

_
.,. _.. ,
,

.. _
   ..

,.,, _
, _
,
///
. ,

   / . ,.
  ,
,.,
. ,
, .,   ,. ._ ,  ,,,//

,        ,
.

,

,
  . . ,

, //  .
,  ,
/

      _,.

, . ,, .

..
  /,/ .
.


  .   .,,_//
,,
.,  .

.  /_. ,
/
.
  /
.._
.
,, / .
   . _ ,
,  ,
/     ,    _ .,
, ,,, ..  ,
  ,

  /.,.
  /. /
. ,/  ,

. .   /,
/,
._
   ,/.
_
.,
,//
, .,,, , ,    , ,
,

,.   ,.,.  .

,  .    ,.  .,   ,
/   _
.
/
  ,.,. ,
,._


,,

, _ _ ,

,
. ,,   ,  _


_..,

  ,
// ,
__ /
!;"$'''. b
    __

On Sat, Dec 12, 2015, 3:01 PM Jorge Tim?n <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> On Fri, Dec 11, 2015 at 10:45 PM, Luke Durback <luke.durback at gmail.com>
> wrote:
> >>If it's voting for something consensus, you will need something special.
> If
> >> it's not consensus (ie external) thw voting doesn't have to hit the
> chain at
> >> all.
> >
> > I had in mind voting for something that can't be trusted if done
> externally:
> > Perhaps BIPs for instance.  People would somehow "mark" their BTC as
> being
> > "For Proposition X" (as opposed to all other propositions) and the vote
> > would be canceled as soon as the BTC is spent again.
> >
> > Unfortunately, I've spent the past 2 days trying to find a design that
> would
> > allow this (I don't think my original suggestion made sense in the
> context
> > of how transactions work), and I haven't gotten much yet.
>
> Well, as said, if it's for consensus, you will need to adapt the
> system in a special way anyway, but I still don't see why turing
> completeness is required.
> This type of idea is not new. Since miners can censor votes (and
> that's undetectable for consensus), several solutions have been
> proposed, time lock the votes, for example.
>
> >>But each scriptSig is only executed once with its corresponding
> >> scriptPubKey. Are you proposing we change that?
> >
> > Sorry, I didn't understand Bitcoin's transaction model well enough when I
> > first made the proposal.  If Turing Pseudo-Completeness is possible with
> > Bitcoin, then I understand now that it could not require you to execute a
> > script more than once.  My current thought is that recursion can be
> > accomplished via checking if the next output's scriptPubKey is identical
> in
> > every way to the current scriptPubKey.  Unfortunately, a lot more is
> needed
> > than just recursion in order to do on-chain BTC voting the way I have in
> > mind.  I'll keep working on this.
>
> What you call "recursion" seems similar to what we usually call
> "covenants", see
>
> https://bitcointalk.org/index.php?topic=278122.0
>
> Although the thread says "an amusingly bad idea", I think it's
> actually a great idea and there's some use cases that are very hard to
> support without covenants.
> Again, no Turing completeness required for this.
>
> > On Fri, Dec 11, 2015 at 10:36 AM, Jorge Tim?n <jtimon at jtimon.cc> wrote:
> >>
> >>
> >> On Dec 10, 2015 7:36 AM, "Luke Durback" <luke.durback at gmail.com> wrote:
> >> >
> >> > Tomorrow, I'll work on writing a way to do voting on proposals with
> BTC
> >> > used as voting shares (This will be difficult as I do not know
> FORTH).  That
> >> > seems like a fairly simple, useful example that will require loops and
> >> > reused functions.  I'll add a fee that goes to the creator.
> >>
> >> If it's voting for something consensus, you will need something special.
> >> If it's not consensus (ie external) thw voting doesn't have to hit the
> chain
> >> at all.
> >> I don't see how "loops and reused functions" are needed in the scripting
> >> language for this use case, but I'm probably missing some details.
> Please,
> >> the more concrete you make your example, the easiest it will be for me
> to
> >> understand.
> >>
> >> > IMO, if you write a complicated system of scripts that's used
> >> > frequently, it makes sense to charge a fee for its usage.
> >>
> >> But each scriptSig is only executed once with its corresponding
> >> scriptPubKey. Are you proposing we change that?
> >>
> >> >  A decentralized exchange between colored coins, for instance might
> take
> >> > a small fee on each trade.
> >>
> >> I've been researching the topic of decentralized exchange from before
> the
> >> term "colored coins" was first used (now there's multiple designs and
> >> implementations); contributed to and reviewed many designs: none of them
> >> (colored coins or not) required turing completeness.
> >> I'm sorry, but what you are saying here is too vague for me to
> concretely
> >> be able to refute the low level "needs" you claim your use cases to
> have.
> >>
> >> > On Dec 10, 2015 10:10 AM, "Luke Durback via bitcoin-dev"
> >> > <bitcoin-dev at lists.linuxfoundation.org> wrote:
> >> > > This, combined with the ability to make new transactions arbitrarily
> >> > > would allow a function to pay its creator.
> >> >
> >> > I don't understand what you mean by "a function" in this context, I
> >> > assume you mean a scriptSig, but then "paying its creator" doesn't
> make much
> >> > sense to me .
> >> >
> >> > Could you provide some high level examples of the use cases you would
> >> > like to support with this?
> >
> >
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151212/a45f8dbe/attachment-0001.html>

From jl2012 at xbt.hk  Sat Dec 12 20:09:03 2015
From: jl2012 at xbt.hk (jl2012 at xbt.hk)
Date: Sat, 12 Dec 2015 15:09:03 -0500
Subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin
Message-ID: <50e629572d8de852eb789d50b34da308@xbt.hk>

It is a common practice in commercial banks that a dormant account might 
be confiscated. Confiscating or deleting dormant UTXOs might be too 
controversial, but allowing the UTXOs set growing without any limit 
might not be a sustainable option. People lose their private keys. 
People do stupid things like sending bitcoin to 1BitcoinEater. We 
shouldn?t be obliged to store everything permanently. This is my 
proposal:

Dormant UTXOs are those UTXOs with 420000 confirmations. In every block 
X after 420000, it will commit to a hash for all UTXOs generated in 
block X-420000. The UTXOs are first serialized into the form: 
txid|index|value|scriptPubKey, then a sorted Merkle hash is calculated. 
After some confirmations, nodes may safely delete the UTXO records of 
block X permanently.

If a user is trying to redeem a dormant UTXO, in addition the signature, 
they have to provide the scriptPubKey, height (X), and UTXO value as 
part of the witness. They also need to provide the Merkle path to the 
dormant UTXO commitment.

To confirm this tx, the miner will calculate a new Merkle hash for the 
block X, with the hash of the spent UTXO replaced by 1, and commit the 
hash to the current block. All full nodes will keep an index of latest 
dormant UTXO commitments so double spending is not possible. (a 
"meta-UTXO set")

If all dormant UTXOs under a Merkle branch are spent, hash of the branch 
will become 1. If all dormant UTXOs in a block are spent, the record for 
this block could be forgotten. Full nodes do not need to remember which 
particular UTXO is spent or not, since any person trying to redeem a 
dormant UTXO has to provide such information.

It becomes the responsibility of dormant coin holders to scan the 
blockchain for the current status of the UTXO commitment for their coin. 
They may also need to pay extra fee for the increased tx size.

This is a softfork if there is no hash collision but this is a 
fundamental assumption in Bitcoin anyway. The proposal also works 
without segregated witness, just by replacing "witness" with "scriptSig"


From kiwigb at yahoo.com  Sat Dec 12 23:01:09 2015
From: kiwigb at yahoo.com (gb)
Date: Sun, 13 Dec 2015 12:01:09 +1300
Subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin
In-Reply-To: <50e629572d8de852eb789d50b34da308@xbt.hk>
References: <50e629572d8de852eb789d50b34da308@xbt.hk>
Message-ID: <1449961269.2210.5.camel@yahoo.com>

The general concept has merit and the basic outline here seems sound
enough. I have harboured a notion for having "archived UTXO" for some
time, this is essentially it. The retrieval from archive cost is on the
UTXO holder not the entire storage network, which is then only bearing
full 'instant' retrieval costs for N blocks.

On Sat, 2015-12-12 at 15:09 -0500, jl2012--- via bitcoin-dev wrote:
> It is a common practice in commercial banks that a dormant account might 
> be confiscated. Confiscating or deleting dormant UTXOs might be too 
> controversial, but allowing the UTXOs set growing without any limit 
> might not be a sustainable option. People lose their private keys. 
> People do stupid things like sending bitcoin to 1BitcoinEater. We 
> shouldn?t be obliged to store everything permanently. This is my 
> proposal:
> 
> Dormant UTXOs are those UTXOs with 420000 confirmations. In every block 
> X after 420000, it will commit to a hash for all UTXOs generated in 
> block X-420000. The UTXOs are first serialized into the form: 
> txid|index|value|scriptPubKey, then a sorted Merkle hash is calculated. 
> After some confirmations, nodes may safely delete the UTXO records of 
> block X permanently.
> 
> If a user is trying to redeem a dormant UTXO, in addition the signature, 
> they have to provide the scriptPubKey, height (X), and UTXO value as 
> part of the witness. They also need to provide the Merkle path to the 
> dormant UTXO commitment.
> 
> To confirm this tx, the miner will calculate a new Merkle hash for the 
> block X, with the hash of the spent UTXO replaced by 1, and commit the 
> hash to the current block. All full nodes will keep an index of latest 
> dormant UTXO commitments so double spending is not possible. (a 
> "meta-UTXO set")
> 
> If all dormant UTXOs under a Merkle branch are spent, hash of the branch 
> will become 1. If all dormant UTXOs in a block are spent, the record for 
> this block could be forgotten. Full nodes do not need to remember which 
> particular UTXO is spent or not, since any person trying to redeem a 
> dormant UTXO has to provide such information.
> 
> It becomes the responsibility of dormant coin holders to scan the 
> blockchain for the current status of the UTXO commitment for their coin. 
> They may also need to pay extra fee for the increased tx size.
> 
> This is a softfork if there is no hash collision but this is a 
> fundamental assumption in Bitcoin anyway. The proposal also works 
> without segregated witness, just by replacing "witness" with "scriptSig"
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev



From vincent.truong at procabiak.com  Sun Dec 13 01:00:26 2015
From: vincent.truong at procabiak.com (Vincent Truong)
Date: Sun, 13 Dec 2015 12:00:26 +1100
Subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin
In-Reply-To: <1449961269.2210.5.camel@yahoo.com>
References: <50e629572d8de852eb789d50b34da308@xbt.hk>
	<1449961269.2210.5.camel@yahoo.com>
Message-ID: <CACrzPenXGQZBrx8QC+1QE2oCE3N=qmfgc_OWrowtjtLjGkZrRA@mail.gmail.com>

Dormant threshold is way too low. There's many news articles about people
forgetting that they used to mine bitcoins and then suddenly remembered.
This will continue to happen for much longer than 8 years as people
rediscover bitcoin when it goes further mainstream. You can't expect them
to have run a node/kept their utxo before they were aware of this change
and then realise miners have discarded their utxo. Oops?

Since we can't predict when mainstream will happen, you instead need a
threshold where the key holder is likely dead. That should be like 80 years
or 120 years, so 4.2m to 6.3m confirmations.

Next paragraph is off topic:

IMO it would be even better for these dormant & dead key holder's utxos to
also re-enter the economy as miner fees; let 1 dormant utxo to be mined per
block. It would need a hard fork. But then maybe people would stop being so
stupid with burning bitcoins/sending it to 1BitcoinEater, or mining a
million bitcoins from day 1 and leaving it, if they know it'll eventually
go into another miner's pockets. This could be used to fund cheap
transactions forever, and miners would be incentivised to hold copies of
these dormant utxos since it could become theirs one day. But this would be
even more controversial than just expiring them as we are in no short
supply of people who believe in Bitcoin's deflationary, fossil fuel
(burnable) economy, rather than a cyclical economy that better resembles
how we treat lost gold today...
On Dec 13, 2015 10:29 AM, "gb via bitcoin-dev" <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> The general concept has merit and the basic outline here seems sound
> enough. I have harboured a notion for having "archived UTXO" for some
> time, this is essentially it. The retrieval from archive cost is on the
> UTXO holder not the entire storage network, which is then only bearing
> full 'instant' retrieval costs for N blocks.
>
> On Sat, 2015-12-12 at 15:09 -0500, jl2012--- via bitcoin-dev wrote:
> > It is a common practice in commercial banks that a dormant account might
> > be confiscated. Confiscating or deleting dormant UTXOs might be too
> > controversial, but allowing the UTXOs set growing without any limit
> > might not be a sustainable option. People lose their private keys.
> > People do stupid things like sending bitcoin to 1BitcoinEater. We
> > shouldn?t be obliged to store everything permanently. This is my
> > proposal:
> >
> > Dormant UTXOs are those UTXOs with 420000 confirmations. In every block
> > X after 420000, it will commit to a hash for all UTXOs generated in
> > block X-420000. The UTXOs are first serialized into the form:
> > txid|index|value|scriptPubKey, then a sorted Merkle hash is calculated.
> > After some confirmations, nodes may safely delete the UTXO records of
> > block X permanently.
> >
> > If a user is trying to redeem a dormant UTXO, in addition the signature,
> > they have to provide the scriptPubKey, height (X), and UTXO value as
> > part of the witness. They also need to provide the Merkle path to the
> > dormant UTXO commitment.
> >
> > To confirm this tx, the miner will calculate a new Merkle hash for the
> > block X, with the hash of the spent UTXO replaced by 1, and commit the
> > hash to the current block. All full nodes will keep an index of latest
> > dormant UTXO commitments so double spending is not possible. (a
> > "meta-UTXO set")
> >
> > If all dormant UTXOs under a Merkle branch are spent, hash of the branch
> > will become 1. If all dormant UTXOs in a block are spent, the record for
> > this block could be forgotten. Full nodes do not need to remember which
> > particular UTXO is spent or not, since any person trying to redeem a
> > dormant UTXO has to provide such information.
> >
> > It becomes the responsibility of dormant coin holders to scan the
> > blockchain for the current status of the UTXO commitment for their coin.
> > They may also need to pay extra fee for the increased tx size.
> >
> > This is a softfork if there is no hash collision but this is a
> > fundamental assumption in Bitcoin anyway. The proposal also works
> > without segregated witness, just by replacing "witness" with "scriptSig"
> >
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev at lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151213/8ec46cd1/attachment.html>

From greg at xiph.org  Sun Dec 13 02:07:36 2015
From: greg at xiph.org (Gregory Maxwell)
Date: Sun, 13 Dec 2015 02:07:36 +0000
Subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin
In-Reply-To: <CACrzPenXGQZBrx8QC+1QE2oCE3N=qmfgc_OWrowtjtLjGkZrRA@mail.gmail.com>
References: <50e629572d8de852eb789d50b34da308@xbt.hk>
	<1449961269.2210.5.camel@yahoo.com>
	<CACrzPenXGQZBrx8QC+1QE2oCE3N=qmfgc_OWrowtjtLjGkZrRA@mail.gmail.com>
Message-ID: <CAAS2fgQi7aiwyOaVBiMbp6t9D58aFAmDdKPzFiscB6ouGzBK6A@mail.gmail.com>

On Sun, Dec 13, 2015 at 1:00 AM, Vincent Truong via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> have run a node/kept their utxo before they were aware of this change and
> then realise miners have discarded their utxo. Oops?

I believe you have misunderstood jl2012's post.  His post does not
cause the outputs to become discarded. They are still spendable,
but the transactions must carry a membership proof to spend them.
They don't have to have stored the data themselves, but they must
get it from somewhere-- including archive nodes that serve this
purpose rather than having every full node carry all that data forever.

Please be conservative with the send button. The list loses its
utility if every moderately complex idea is hit with reflexive
opposition by people who don't understand it.

Peter Todd has proposed something fairly similar with "STXO
commitments". The primary argument against this kind of approach that
I'm aware of is that the membership proofs get pretty big, and if too
aggressive this trades bandwidth for storage, and storage is usually
the cheaper resource. Though at least the membership proofs could be
omitted when transmitting to a node which has signaled that it has
kept the historical data anyways.

From cp368202 at ohiou.edu  Sun Dec 13 08:13:42 2015
From: cp368202 at ohiou.edu (Chris Priest)
Date: Sun, 13 Dec 2015 00:13:42 -0800
Subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin
In-Reply-To: <CAAS2fgQi7aiwyOaVBiMbp6t9D58aFAmDdKPzFiscB6ouGzBK6A@mail.gmail.com>
References: <50e629572d8de852eb789d50b34da308@xbt.hk>
	<1449961269.2210.5.camel@yahoo.com>
	<CACrzPenXGQZBrx8QC+1QE2oCE3N=qmfgc_OWrowtjtLjGkZrRA@mail.gmail.com>
	<CAAS2fgQi7aiwyOaVBiMbp6t9D58aFAmDdKPzFiscB6ouGzBK6A@mail.gmail.com>
Message-ID: <CAAcC9yuSX67ckhBUCsvTk+7PB6vzufuuBsJikSqqqU_4LXoCfA@mail.gmail.com>

I don't like this scheme at all. It doesn't seem to make bitcoin
better, it makes it worse.

Lets say it's 2050 and I want to sweep a paper wallet I created in
2013. I can't just make the TX and send it to the network, I have to
first contact an "archive node" to get the UTXO data in order to make
the TX. How is this better than how the system works today?

Since many people are going to be holding BTC long term (store of
value of a first-class feature of bitcoin), this scheme is going to
effect pretty much all users.

These archive nodes will be essential to network's operation. If there
are no running archive nodes, the effect on the network is the same as
the network today without any full nodes.

Anyways, UTXO size is a function of number of users, rather than a
function of time. If tons of people join the network, UTXO still will
increase no matter what. All this change is going to do is make it
harder for people to use bitcoin. A person can still generate 1GB of
UTXO data, but as long as they spend those UTXOs within the amount
they are still using those resources.

IMO, wildcard inputs is still the best way to limit the UTXO set.


On 12/12/15, Gregory Maxwell via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> On Sun, Dec 13, 2015 at 1:00 AM, Vincent Truong via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org> wrote:
>> have run a node/kept their utxo before they were aware of this change and
>> then realise miners have discarded their utxo. Oops?
>
> I believe you have misunderstood jl2012's post.  His post does not
> cause the outputs to become discarded. They are still spendable,
> but the transactions must carry a membership proof to spend them.
> They don't have to have stored the data themselves, but they must
> get it from somewhere-- including archive nodes that serve this
> purpose rather than having every full node carry all that data forever.
>
> Please be conservative with the send button. The list loses its
> utility if every moderately complex idea is hit with reflexive
> opposition by people who don't understand it.
>
> Peter Todd has proposed something fairly similar with "STXO
> commitments". The primary argument against this kind of approach that
> I'm aware of is that the membership proofs get pretty big, and if too
> aggressive this trades bandwidth for storage, and storage is usually
> the cheaper resource. Though at least the membership proofs could be
> omitted when transmitting to a node which has signaled that it has
> kept the historical data anyways.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

From greg at xiph.org  Sun Dec 13 08:18:57 2015
From: greg at xiph.org (Gregory Maxwell)
Date: Sun, 13 Dec 2015 08:18:57 +0000
Subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin
In-Reply-To: <CAAcC9yuSX67ckhBUCsvTk+7PB6vzufuuBsJikSqqqU_4LXoCfA@mail.gmail.com>
References: <50e629572d8de852eb789d50b34da308@xbt.hk>
	<1449961269.2210.5.camel@yahoo.com>
	<CACrzPenXGQZBrx8QC+1QE2oCE3N=qmfgc_OWrowtjtLjGkZrRA@mail.gmail.com>
	<CAAS2fgQi7aiwyOaVBiMbp6t9D58aFAmDdKPzFiscB6ouGzBK6A@mail.gmail.com>
	<CAAcC9yuSX67ckhBUCsvTk+7PB6vzufuuBsJikSqqqU_4LXoCfA@mail.gmail.com>
Message-ID: <CAAS2fgSchJFk1Ejd8ZfMSzxEO-1TWYR6ag-seQNH_QHrc9Cn3w@mail.gmail.com>

On Sun, Dec 13, 2015 at 8:13 AM, Chris Priest <cp368202 at ohiou.edu> wrote:
> Lets say it's 2050 and I want to sweep a paper wallet I created in
> 2013. I can't just make the TX and send it to the network, I have to
> first contact an "archive node" to get the UTXO data in order to make
> the TX. How is this better than how the system works today?

You already are in that boat. If your paper wallet has only the
private key (as 100% of them do today). You'll have no idea what coins
have been assigned to it, or what their TXids are. You'll need to
contact a public index (which isn't a service existing nodes provide)
or synchronize the full blockchain history to find it. Both are also
sufficient for jl2012's (/Petertodd's STXO), they'd only be providing
you with somewhat more data.  If instead, you insist that you'd
already be running a full node and not have to wait for the sync, then
again you'd also be your own archive. In none of these cases do you
lose anything.

From cp368202 at ohiou.edu  Sun Dec 13 09:17:38 2015
From: cp368202 at ohiou.edu (Chris Priest)
Date: Sun, 13 Dec 2015 01:17:38 -0800
Subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin
In-Reply-To: <CAAS2fgSchJFk1Ejd8ZfMSzxEO-1TWYR6ag-seQNH_QHrc9Cn3w@mail.gmail.com>
References: <50e629572d8de852eb789d50b34da308@xbt.hk>
	<1449961269.2210.5.camel@yahoo.com>
	<CACrzPenXGQZBrx8QC+1QE2oCE3N=qmfgc_OWrowtjtLjGkZrRA@mail.gmail.com>
	<CAAS2fgQi7aiwyOaVBiMbp6t9D58aFAmDdKPzFiscB6ouGzBK6A@mail.gmail.com>
	<CAAcC9yuSX67ckhBUCsvTk+7PB6vzufuuBsJikSqqqU_4LXoCfA@mail.gmail.com>
	<CAAS2fgSchJFk1Ejd8ZfMSzxEO-1TWYR6ag-seQNH_QHrc9Cn3w@mail.gmail.com>
Message-ID: <CAAcC9ysovzcm1SD_4XyxxofmwdXrcQqs0ckQBw626vYsdPftKw@mail.gmail.com>

> In none of these cases do you lose anything.

Nor do you gain anything. Archive nodes will still need to exist
precisely because paper wallets don't include UTXO data. This is like
adding the ability to partially seed a movie with bittorrent. You
still need someone who has the whole thing has to be participating in
order for anyone to play the movie.

This isn't going to kill bitcoin, but it won't make it any better.
Every paper wallet would have to be re-printed with UTXO data
included. It doesn't even solve the core problem because someone can
still flood the network with lots of UTXOs, as long as they spend them
quickly.

On 12/13/15, Gregory Maxwell <greg at xiph.org> wrote:
> On Sun, Dec 13, 2015 at 8:13 AM, Chris Priest <cp368202 at ohiou.edu> wrote:
>> Lets say it's 2050 and I want to sweep a paper wallet I created in
>> 2013. I can't just make the TX and send it to the network, I have to
>> first contact an "archive node" to get the UTXO data in order to make
>> the TX. How is this better than how the system works today?
>
> You already are in that boat. If your paper wallet has only the
> private key (as 100% of them do today). You'll have no idea what coins
> have been assigned to it, or what their TXids are. You'll need to
> contact a public index (which isn't a service existing nodes provide)
> or synchronize the full blockchain history to find it. Both are also
> sufficient for jl2012's (/Petertodd's STXO), they'd only be providing
> you with somewhat more data.  If instead, you insist that you'd
> already be running a full node and not have to wait for the sync, then
> again you'd also be your own archive. In none of these cases do you
> lose anything.
>

From greg at xiph.org  Sun Dec 13 09:24:57 2015
From: greg at xiph.org (Gregory Maxwell)
Date: Sun, 13 Dec 2015 09:24:57 +0000
Subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin
In-Reply-To: <CAAcC9ysovzcm1SD_4XyxxofmwdXrcQqs0ckQBw626vYsdPftKw@mail.gmail.com>
References: <50e629572d8de852eb789d50b34da308@xbt.hk>
	<1449961269.2210.5.camel@yahoo.com>
	<CACrzPenXGQZBrx8QC+1QE2oCE3N=qmfgc_OWrowtjtLjGkZrRA@mail.gmail.com>
	<CAAS2fgQi7aiwyOaVBiMbp6t9D58aFAmDdKPzFiscB6ouGzBK6A@mail.gmail.com>
	<CAAcC9yuSX67ckhBUCsvTk+7PB6vzufuuBsJikSqqqU_4LXoCfA@mail.gmail.com>
	<CAAS2fgSchJFk1Ejd8ZfMSzxEO-1TWYR6ag-seQNH_QHrc9Cn3w@mail.gmail.com>
	<CAAcC9ysovzcm1SD_4XyxxofmwdXrcQqs0ckQBw626vYsdPftKw@mail.gmail.com>
Message-ID: <CAAS2fgQF769ds2F-mcOJ+-5gya-9qW_xcPdGimFE_1SJLmKV=A@mail.gmail.com>

On Sun, Dec 13, 2015 at 9:17 AM, Chris Priest <cp368202 at ohiou.edu> wrote:
>> In none of these cases do you lose anything.
>
> Nor do you gain anything. Archive nodes will still need to exist

Not every node is an archive node; that's even the case today.
Lowering the resource requirements to independently enforce the rules
of the system is highly virtuous.

> precisely because paper wallets don't include UTXO data. This is like
> adding the ability to partially seed a movie with bittorrent.
[...]
> Every paper wallet would have to be re-printed with UTXO data

They are not printed now with UTXO data
(txid:vout:scriptpubkey:amount), and unless you start and fully
synchronize (or are running a full node) you already cannot author a
transaction without that data. The private key is already not enough,
and no Bitcoin node will just give you what you need to know.

The only additional information JL2012's scheme would add would be the
hash tree fragments to show membership; and the same places that
currently give you what is required to author a transaction could
provide it for you.

> included. It doesn't even solve the core problem because someone can
> still flood the network with lots of UTXOs, as long as they spend them
> quickly.

The system already inhibits the rate new UTXO can be added; but we're
still left with the perpetually growing history that contains many
lost and otherwise unspendable outputs.

From danny.thorpe at gmail.com  Sun Dec 13 16:14:21 2015
From: danny.thorpe at gmail.com (Danny Thorpe)
Date: Sun, 13 Dec 2015 08:14:21 -0800
Subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin
In-Reply-To: <50e629572d8de852eb789d50b34da308@xbt.hk>
References: <50e629572d8de852eb789d50b34da308@xbt.hk>
Message-ID: <CAJN5wHWPtDbCHAYOxYLNZykgqt7a2Jefa1ACz-MOT+Ucn6T+dA@mail.gmail.com>

What is the current behavior / cost that this proposal is trying to avoid?
Are ancient utxos required to be kept in memory always in a fully
validating node, or can ancient utxos get pushed out of memory like a
normal LRU caching db?

Thanks,
-Danny
On Dec 12, 2015 1:55 PM, "jl2012--- via bitcoin-dev" <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> It is a common practice in commercial banks that a dormant account might
> be confiscated. Confiscating or deleting dormant UTXOs might be too
> controversial, but allowing the UTXOs set growing without any limit might
> not be a sustainable option. People lose their private keys. People do
> stupid things like sending bitcoin to 1BitcoinEater. We shouldn?t be
> obliged to store everything permanently. This is my proposal:
>
> Dormant UTXOs are those UTXOs with 420000 confirmations. In every block X
> after 420000, it will commit to a hash for all UTXOs generated in block
> X-420000. The UTXOs are first serialized into the form:
> txid|index|value|scriptPubKey, then a sorted Merkle hash is calculated.
> After some confirmations, nodes may safely delete the UTXO records of block
> X permanently.
>
> If a user is trying to redeem a dormant UTXO, in addition the signature,
> they have to provide the scriptPubKey, height (X), and UTXO value as part
> of the witness. They also need to provide the Merkle path to the dormant
> UTXO commitment.
>
> To confirm this tx, the miner will calculate a new Merkle hash for the
> block X, with the hash of the spent UTXO replaced by 1, and commit the hash
> to the current block. All full nodes will keep an index of latest dormant
> UTXO commitments so double spending is not possible. (a "meta-UTXO set")
>
> If all dormant UTXOs under a Merkle branch are spent, hash of the branch
> will become 1. If all dormant UTXOs in a block are spent, the record for
> this block could be forgotten. Full nodes do not need to remember which
> particular UTXO is spent or not, since any person trying to redeem a
> dormant UTXO has to provide such information.
>
> It becomes the responsibility of dormant coin holders to scan the
> blockchain for the current status of the UTXO commitment for their coin.
> They may also need to pay extra fee for the increased tx size.
>
> This is a softfork if there is no hash collision but this is a fundamental
> assumption in Bitcoin anyway. The proposal also works without segregated
> witness, just by replacing "witness" with "scriptSig"
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151213/5df532a2/attachment.html>

From jl2012 at xbt.hk  Sun Dec 13 15:25:10 2015
From: jl2012 at xbt.hk (jl2012 at xbt.hk)
Date: Sun, 13 Dec 2015 10:25:10 -0500
Subject: [bitcoin-dev] Segregated Witness features wish list
In-Reply-To: <CAAS2fgT09Tk+5dBQ4YJA_pwK56xX2mEQe9TkJqgcB0j2BZZJ1g@mail.gmail.com>
References: <b13f6152767473dcf44a1d8965fdd32c@xbt.hk>
	<CAAS2fgT09Tk+5dBQ4YJA_pwK56xX2mEQe9TkJqgcB0j2BZZJ1g@mail.gmail.com>
Message-ID: <2498d6a0691fde6f62453294da6118d0@xbt.hk>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

I'm trying to list the minimal consensus rule changes needed for segwit 
softfork. The list does not cover the changes in non-consensus critical 
behaviors, such as relay of witness data.

1. OP_NOP4 is renamed as OP_SEGWIT
2. A script with OP_SEGWIT must fail if the scriptSig is not completely 
empty
3. If OP_SEGWIT is used in the scriptPubKey, it must be the only and the 
last OP code in the scriptPubKey, or the script must fail
4. The OP_SEGWIT must be preceded by exactly one data push (the 
"serialized script") with at least one byte, or the script must fail
5. The most significant byte of serialized script is the version byte, 
an unsigned number
6. If the version byte is 0x00, the script must fail
7. If the version byte is 0x02 to 0xff, the rest of the serialized 
script is ignored and the output is spendable with any form of witness 
(even if the witness contains something invalid in the current script 
system, e.g. OP_RETURN)
8. If the version byte is 0x01,
8a. rest of the serialized script is deserialized, and be interpreted as 
the scriptPubKey.
8b. the witness is interpreted as the scriptSig.
8c. the script runs with existing rules (including P2SH)
9. If the script fails when OP_SEGWIT is interpreted as a NOP, the 
script must fail. However, this is guaranteed by the rules 2, 3, 4, 6 so 
no additional check is needed.
10. The calculation of Merkle root in the block header remains unchanged
11. The witness commitment is placed somewhere in the block, either in 
coinbase or an OP_RETURN output in a specific tx

Format of the witness commitment:
The witness commitment could be as simple as a hash tree of all witness 
in a block. However, this is bad for further development of sum tree for 
compact SPV fraud proof as we have to maintain one more tree in the 
future. Even if we are not going to implement any sum checking in first 
version of segwit, it's better to make it easier for future softforks. 
(credit: gmaxwell)
12. The block should indicate how many sum criteria there are by 
committing the number following the witness commitment
13. The witness commitment is a hash-sum tree with the number of sum 
criteria committed in rule 12
14. Each sum criterion is a fixed 8 byte signed number (Negative value 
is allowed for use like counting delta-UTXO. 8 bytes is needed for fee 
commitment. Multiple smaller criteria may share a 8 byte slot, as long 
as they do not allow negative value)
15. Nodes will ignore the sum criteria that they do not understand, as 
long as the sum is correctly calculated

Size limit:
16. We need to determine the size limit of witness
17. We also need to have an upper limit for the number of sum criteria, 
or a malicious miner may find a block with many unknown sum criteria and 
feed unlimited amount of garbage to other nodes.

All other functions I mentioned in my wish list could be softforked 
later to this system

To mitigate the risk described in rule 17, we may ask miners to vote for 
an increase (but not decrease) in the number of sum criteria. Initially 
there should be 0 sum criteria. If we would like to introduce a new 
criteria, miners will support the proposal by setting the number of sum 
criteria as 1. However, until 95% of miners support the increase, all 
values of the extra sum criteria must be 0. Therefore, even a malicious 
miner may declare any amount of sum criteria, those criteria unknown to 
the network must be 0, and no extra data is needed to construct the 
block. This voting mechanism could be a softfork over rule 12 and 13, 
and is not necessary in the initial segwit deployment.
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQGcBAEBCAAGBQJWbY0jAAoJEO6eVSA0viTSU1oMAJIrYWerwk84poZBL/ezEsIp
9fCLnFZ4lyO2ijAm5UmwLXGijY03kqp29b0zmyIWV2WuoeW2lN64tLHQRilT0+5R
n5/viQOMv0C0MYs525+/dpNkk2q2MiFmyyozdbU6zcyfdkrkYdChCFOQ9GsxzQHk
n4lL4/RSKdqJZg4x2yEGgdyKA6XrQHaFirdr/K2bhhbk4Q0SOuYjy8Wxa2oCHFCC
WG4K2NBnKCI7DuVXQK+ZC8dYXMwbemeFfPHY6dZVti7j/OFsllyxno/CFKO3rsCs
+uko4XJk6pH0Ncjrc1n0l0v9xIKF5hTqSxFs+GVvhkiBdTDZVe7CdedO9qJWf1hE
bbmLXTURCDQUFe9F3uKsnYfMoD5eniWHx2OQcJcNPlLMJd9zObB3HdgFMW6N53KN
QXLmxobU/xFhmFknz1ShGEIdGSaH0gqnb+WEkO5v5vBO4L6Cikc+lcp7zXqQzWpW
uqm3QSrbKcbR6JEwDFoGQpDkcqpwpTIrOAk4B1jJRg==
=J2KF
-----END PGP SIGNATURE-----


Gregory Maxwell ? 2015-12-10 04:51 ??:
> On Thu, Dec 10, 2015 at 6:47 AM, jl2012--- via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org> wrote:
>> 4. Sum of fee, sigopcount, size etc as part of the witness hash tree: 
>> for
> 
> I should have also commented on this: the block can indicate how many
> sum criteria there are; and then additional ones could be soft-forked
> in. Haven't tried implementing it yet, but there you go. :)


From pieter.wuille at gmail.com  Sun Dec 13 18:07:01 2015
From: pieter.wuille at gmail.com (Pieter Wuille)
Date: Sun, 13 Dec 2015 19:07:01 +0100
Subject: [bitcoin-dev] Segregated Witness features wish list
In-Reply-To: <2498d6a0691fde6f62453294da6118d0@xbt.hk>
References: <b13f6152767473dcf44a1d8965fdd32c@xbt.hk>
	<CAAS2fgT09Tk+5dBQ4YJA_pwK56xX2mEQe9TkJqgcB0j2BZZJ1g@mail.gmail.com>
	<2498d6a0691fde6f62453294da6118d0@xbt.hk>
Message-ID: <CAPg+sBia_rCNkPfKhKxDiOaun5zRKj9LVRkGf_YqVvYWiTox1A@mail.gmail.com>

On Sun, Dec 13, 2015 at 4:25 PM, jl2012--- via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> I'm trying to list the minimal consensus rule changes needed for segwit
> softfork. The list does not cover the changes in non-consensus critical
> behaviors, such as relay of witness data.
>
> 1. OP_NOP4 is renamed as OP_SEGWIT
> 2. A script with OP_SEGWIT must fail if the scriptSig is not completely
> empty
> 3. If OP_SEGWIT is used in the scriptPubKey, it must be the only and the
> last OP code in the scriptPubKey, or the script must fail
> 4. The OP_SEGWIT must be preceded by exactly one data push (the "serialized
> script") with at least one byte, or the script must fail

The use of a NOP opcode to indicate a witness script was something I
considered at first too, but it's not really needed. You wouldn't be
able to use that opcode in any place a normal opcode could occur, as
it needs to be able to inspect the full scriptSig (rather than just
its resulting stack) anyway. So both in practice and conceptually it
is only really working as a template that gets assigned a special
meaning (like P2SH did). We don't need an opcode for that, and instead
we could say that any scriptPubKey (or redeemscript) that consists of
a single push is a witness program.

> 5. The most significant byte of serialized script is the version byte, an
> unsigned number
> 6. If the version byte is 0x00, the script must fail

What is that good for?

> 7. If the version byte is 0x02 to 0xff, the rest of the serialized script is
> ignored and the output is spendable with any form of witness (even if the
> witness contains something invalid in the current script system, e.g.
> OP_RETURN)

Do you mean the scriptPubKey itself, or the script that follows after
the version byte?
* The scriptPubKey itself: that's in contradiction with your rule 4,
as segwit scripts are by definition only a push (+ opcode), so they
can't be an OP_RETURN.
* The script after the version byte: agree - though it doesn't
actually need to be a script at all even (see further).

> 8. If the version byte is 0x01,
> 8a. rest of the serialized script is deserialized, and be interpreted as the
> scriptPubKey.
> 8b. the witness is interpreted as the scriptSig.
> 8c. the script runs with existing rules (including P2SH)

I don't think it's very useful to allow P2SH inside segwit, as we can
actually do better and allow segwit scripts to push the (perhaps 256
bit) hash of the redeemscript in the scriptPubKey, and have the full
redeemscript in the witness. See further for details. The numbers I
showed in the presentation were created using a simulation that used
that model already.

It is useful however to allow segwit inside P2SH (so the witness
program including version byte goes into the redeemscript, inside the
scriptSig). This allows old clients to send to new wallets without any
modifications (at slightly lower efficiency). The rules in this case
must say that the scriptSig is exactly a push of the redeemscript
(which itself contains the witness program), to provide both
compatibility with old consensus rules and malleability protection.

So let me summarize by giving an equivalent to your list above,
reflecting how my current prototype works:
A) A scriptPubKey or P2SH redeemscript that consists of a single push
of 2 to 41 bytes gets a new special meaning, and the byte vector
pushed by it is called the witness program.
A.1) In case the scriptPubKey pushes a witness program directly, the
scriptSig must be exactly empty.
A.2) In case the redeemscript pushes a witness program, the scriptSig
must be exactly the single push of the redeemscript.
B) The first byte of a witness program is the version byte.
B.1) If the witness version byte is 0, the rest of the witness program
is the actual script, which is executed after normal script evaluation
but with data from the witness rather than the scriptSig. The program
must not fail and result in a single TRUE on the stack, and nothing
else (to prevent stuffing the witness with pointless data during relay
of transactions).
B.2) if the witness version byte is 1, the rest of the witness program
must be 32 bytes, and a SHA256 hash of the actual script. The witness
must consist of an input stack to feed to the program, followed by the
serialized program itself (whose hash must match the hash pushed in
the witness program). It is executed after normal script evluation,
and must not fail and result in a single TRUE on the stack, and
nothing else.
B.3) if the witness version byte is 2 or higher, no further
interpretation of the data happens, but can be softforked in.

I'll write a separate mail on the block commitment structure.

-- 
Pieter

From jl2012 at xbt.hk  Sun Dec 13 18:11:41 2015
From: jl2012 at xbt.hk (jl2012 at xbt.hk)
Date: Sun, 13 Dec 2015 13:11:41 -0500
Subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin
In-Reply-To: <CAAcC9ysovzcm1SD_4XyxxofmwdXrcQqs0ckQBw626vYsdPftKw@mail.gmail.com>
References: <50e629572d8de852eb789d50b34da308@xbt.hk>
	<1449961269.2210.5.camel@yahoo.com>
	<CACrzPenXGQZBrx8QC+1QE2oCE3N=qmfgc_OWrowtjtLjGkZrRA@mail.gmail.com>
	<CAAS2fgQi7aiwyOaVBiMbp6t9D58aFAmDdKPzFiscB6ouGzBK6A@mail.gmail.com>
	<CAAcC9yuSX67ckhBUCsvTk+7PB6vzufuuBsJikSqqqU_4LXoCfA@mail.gmail.com>
	<CAAS2fgSchJFk1Ejd8ZfMSzxEO-1TWYR6ag-seQNH_QHrc9Cn3w@mail.gmail.com>
	<CAAcC9ysovzcm1SD_4XyxxofmwdXrcQqs0ckQBw626vYsdPftKw@mail.gmail.com>
Message-ID: <3b28f994d75070ab1fd2d312f29bb706@xbt.hk>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

On Mon, Dec 14, 2015 at 12:14 AM, Danny Thorpe <danny.thorpe at gmail.com> 
wrote:
> What is the current behavior / cost that this proposal is trying to 
> avoid? Are ancient utxos required to be kept in memory always in a 
> fully validating node, or can ancient utxos get pushed out of memory 
> like a normal LRU caching db?

I don't see why it must be kept in memory. But storage is still a 
problem. With the 8 year limit and a fixed max block size, it indirectly 
sets an upper limit for UTXO set.


Chris Priest via bitcoin-dev :
> This isn't going to kill bitcoin, but it won't make it any better.

Do you believe that thousands of volunteer full nodes are obliged to 
store an UTXO record, just because one paid US$0.01 to an anonymous 
miner 100 years ago? It sounds insanely cheap, isn't it? My proposal (or 
similar proposal by Peter Todd) is to solve this problem. Many 
commercial banks have a dormant threshold less than 8 years so I believe 
it is a balanced choice.

Back to the topic, I would like to further elaborate my proposal.

We have 3 types of full nodes:

Archive nodes: full nodes that store the whole blockchain
Full UTXO nodes: full nodes that fully store the latest UTXO state, but 
not the raw blockchain
Lite UTXO nodes: full nodes that store only UTXO created in that past 
420000 blocks

Currently, if one holds nothing but a private key, he must consult 
either an archive node or a full UTXO node for the latest UTXO state to 
spend his coin. We currently do not have any lite UTXO node, and such 
node would not work properly beyond block 420000.

With the softfork I described in my original post, if the UTXO is 
created within the last 420000 blocks, the key holder may consult any 
type of full node, including a lite UTXO node, to create the 
transaction.

If the UTXO has been confirmed by more than 420000 blocks, a lite UTXO 
node obviously can't provide the necessary information to spend the 
coin. However, not even a full UTXO node may do so. A full UTXO node 
could tell the position of the UTXO in the blockchain, but can't provide 
all the information required by my specification. Only an archive node 
may do so.

What extra information is needed?

(1) If your UTXO was generated in block Y, you first need to know the 
TXO state (spent / unspent) of all outputs in block Y at block (Y + 
420000). Only UTXOs at that time are relevant.

(2) You also need to know if there was any spending of any block Y UTXOs 
after block (Y + 420000).

It is not possible to construct the membership prove I require without 
these information. It is designed this way, so that lite UTXO nodes 
won't need to store any dormant UTXO records: not even the hash of 
individual dormant UTXO records. If the blockchain grows to insanely 
big, it may take days or weeks to retrieve to records. However, I don't 
think this is relevant as one has already left his coins dormant for >8 
years. Actually, you don't even need the full blockchain. For (1), all 
you need is the 420000 blocks from Y to Y+420000 minus any witness data, 
as you don't need to do any validation. For (2), you just need the 
coinbase of Y+420001 to present, where any spending would have been 
committed, and retrieve the full block only if a spending is found.

So the Bitcoin Bank (miners) is not going to shred your record and 
confiscate your money. Instead, the Bank throws your record to the 
garage (raw blockchain). You can search for your record by yourself, or 
employ someone (archive node) to search it for you. In any case it 
incurs costs. But as thousands of bankers have kept your record on their 
limited desk space for 8 years for free (though one of them might 
receive a fraction of a penny from you), you shouldn't complain with any 
moral, technical, or legal reason. And no matter what users say, I 
believe something like this will happen when miners and full nodes can't 
handle the UTXO set.

I'd like to see more efficient proposals that archive the same goals.

p.s. there were some typos in my original. The second sentence of the 
second paragraph should be read as "For every block X+420000, it will 
commit to a hash for all UTXOs generated in block X."
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQGcBAEBCAAGBQJWbbR2AAoJEO6eVSA0viTScEoL/RPlsxr0A5wTtgdi+9i4AFlV
Sw/He89+YPGe5VCG74YNAPLEUF1/rICzUJ4DulvNTOo/5xtmkv5ok4bD7v1JZnH3
DE2PExMQYs2X4Qm6mkcwi8IWlMR2U5j5ebUq21Kj4AqVFj9UcQmYGhPehB2f+cM9
Wki/TDwNj5fV8AZ4uR9pPgaf+bvVQQ9BOOLiIMiTbphNCx1hfGfYcsqmXlCbGk9A
PatGR88aQTxpa7PhbCZwwf76cKuOaYYZeHr9jRR9RL5rZVXgE1SI/niBytJhXaP8
lwYtk4Bpz0IGd23v1dArNQQoOp5Xycbeq1l1qyv/qtxju65No+dhqiEcFBZVI1AS
VcndMQ+yvNuxVgib2Ifh9YjXelWAqqLzzoVcz2RxXh6HJ0tVKxBokwdAcsclZb93
zQ1JhDR4vBpLquytZA8lDIxJraNCdB/KEAOAey6ljP3zL7fBLBp1oZw4DDDtFy8V
EMjrOSVnjyuyfey2YXsGnnHuQS0mpwmSroV2400uGQ==
=2xRy
-----END PGP SIGNATURE-----


From jl2012 at xbt.hk  Sun Dec 13 18:41:44 2015
From: jl2012 at xbt.hk (jl2012 at xbt.hk)
Date: Sun, 13 Dec 2015 13:41:44 -0500
Subject: [bitcoin-dev] Segregated Witness features wish list
In-Reply-To: <CAPg+sBia_rCNkPfKhKxDiOaun5zRKj9LVRkGf_YqVvYWiTox1A@mail.gmail.com>
References: <b13f6152767473dcf44a1d8965fdd32c@xbt.hk>
	<CAAS2fgT09Tk+5dBQ4YJA_pwK56xX2mEQe9TkJqgcB0j2BZZJ1g@mail.gmail.com>
	<2498d6a0691fde6f62453294da6118d0@xbt.hk>
	<CAPg+sBia_rCNkPfKhKxDiOaun5zRKj9LVRkGf_YqVvYWiTox1A@mail.gmail.com>
Message-ID: <a46f4277e8859affba4e5cfe5474d53a@xbt.hk>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256

Pieter Wuille 2015-12-13 13:07 :

> The use of a NOP opcode to indicate a witness script was something I
> considered at first too, but it's not really needed. You wouldn't be
> able to use that opcode in any place a normal opcode could occur, as
> it needs to be able to inspect the full scriptSig (rather than just
> its resulting stack) anyway. So both in practice and conceptually it
> is only really working as a template that gets assigned a special
> meaning (like P2SH did). We don't need an opcode for that, and instead
> we could say that any scriptPubKey (or redeemscript) that consists of
> a single push is a witness program.
> 
>> 5. The most significant byte of serialized script is the version byte, 
>> an
>> unsigned number
>> 6. If the version byte is 0x00, the script must fail
> 
> What is that good for?

Just to make sure a script like OP_0 OP_SEGWIT will fail.

Anyway, your design may be better so forget it

>> 7. If the version byte is 0x02 to 0xff, the rest of the serialized 
>> script is
>> ignored and the output is spendable with any form of witness (even if 
>> the
>> witness contains something invalid in the current script system, e.g.
>> OP_RETURN)
> 
> Do you mean the scriptPubKey itself, or the script that follows after
> the version byte?
> * The scriptPubKey itself: that's in contradiction with your rule 4,
> as segwit scripts are by definition only a push (+ opcode), so they
> can't be an OP_RETURN.
> * The script after the version byte: agree - though it doesn't
> actually need to be a script at all even (see further).

I am not referring to the serialized script, but the witness. Basically,
it doesn't care what the content look like.


> It is useful however to allow segwit inside P2SH

Agree

> So let me summarize by giving an equivalent to your list above,
> reflecting how my current prototype works:
> A) A scriptPubKey or P2SH redeemscript that consists of a single push
> of 2 to 41 bytes gets a new special meaning, and the byte vector
> pushed by it is called the witness program.

Why 41 bytes? Do you expect all witness program to be P2SH-like?

> The program
> must not fail and result in a single TRUE on the stack, and nothing
> else (to prevent stuffing the witness with pointless data during relay
> of transactions).

Could we just implement this as standardness rule? It is always possible
to stuff the scriptSig with pointless data so I don't think it's a new
attack vector. What if we want to include the height and tx index of
the input for compact fraud proof? Such fraud proof should not be an
opt-in function and not be dependent on the version byte

For the same reason, we should also allow traditional tx to have data
in the witness field, for any potential softfork upgrade
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQGcBAEBCAAGBQJWbbugAAoJEO6eVSA0viTSD8oMAKFvd/+KZgH13tErEA+iXzF5
pwT4/eoQWSTvxIDVrFN+9wV79ogO4/aiCDEdmNF2IZD3QqmhKl7iOPw2SEseRTbe
e1r5z67yuudXyEQocZvy5+NOUp3N978b8weuRsHWG1HXgxTRmgZTrEeNtbEUs0X2
n5l6e0scnZAu70svBXr8X9HnOm2P/QLxtAqyNW19caCi+Dg/4Curx48tXQ/I9IxT
SYFVzB++FIoua49Cf1RJN+dUfywg67wT5l9NX4uWAX0qNB+p6BPP8df/72G/u564
NIaJs3IFiUaNktXz9aDM4s7pSzR6PlCK6LFKjE52sBY5uREHGU4PnfX9YqtwiEXA
Hr3YoFiepxAwl6icJi3wHKa6i0NGvj1fR1h6xuJ7ulzNv5mwuzXPOgvTDK4wpejl
ee8wsQZwmzchAfgyfPsgSaPh/jjBwm2S+WDMbL4HDmnWqVDl8dG3I/b3XP0aegY9
4RxPhLOA1qToNDGhnm+JNqT60OKgatpDN/4bRgRscA==
=4B1D
-----END PGP SIGNATURE-----


From tier.nolan at gmail.com  Sun Dec 13 21:36:06 2015
From: tier.nolan at gmail.com (Tier Nolan)
Date: Sun, 13 Dec 2015 21:36:06 +0000
Subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin
In-Reply-To: <3b28f994d75070ab1fd2d312f29bb706@xbt.hk>
References: <50e629572d8de852eb789d50b34da308@xbt.hk>
	<1449961269.2210.5.camel@yahoo.com>
	<CACrzPenXGQZBrx8QC+1QE2oCE3N=qmfgc_OWrowtjtLjGkZrRA@mail.gmail.com>
	<CAAS2fgQi7aiwyOaVBiMbp6t9D58aFAmDdKPzFiscB6ouGzBK6A@mail.gmail.com>
	<CAAcC9yuSX67ckhBUCsvTk+7PB6vzufuuBsJikSqqqU_4LXoCfA@mail.gmail.com>
	<CAAS2fgSchJFk1Ejd8ZfMSzxEO-1TWYR6ag-seQNH_QHrc9Cn3w@mail.gmail.com>
	<CAAcC9ysovzcm1SD_4XyxxofmwdXrcQqs0ckQBw626vYsdPftKw@mail.gmail.com>
	<3b28f994d75070ab1fd2d312f29bb706@xbt.hk>
Message-ID: <CAE-z3OXAgvtODDKHTWVyNN1r=T=f-gtGawKtEdOh_H+h-5gqEQ@mail.gmail.com>

On Sun, Dec 13, 2015 at 6:11 PM, jl2012--- via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Back to the topic, I would like to further elaborate my proposal.
>
> We have 3 types of full nodes:
>
> Archive nodes: full nodes that store the whole blockchain
> Full UTXO nodes: full nodes that fully store the latest UTXO state, but
> not the raw blockchain
> Lite UTXO nodes: full nodes that store only UTXO created in that past
> 420000 blocks
>

There is a risk that miners would eventually react by just refusing to
accept blocks that spend dormant outputs.  This is a risk even without the
protocol, but I think if there are already lots of UTXO-lite nodes
deployed, it would be much easier to just define them as the new
(soft-forked) consensus rule.

There is a precedent for things to be disabled rather than fixed when
security problems arise.

Imagine a crisis caused by a security related bug with the revival proofs.
Disabling them is much lower risk than trying to find/fix the bug and then
deploy the fix.  The longer it takes, the longer the security problem
remains.


>
> What extra information is needed?
>
> (1) If your UTXO was generated in block Y, you first need to know the TXO
> state (spent / unspent) of all outputs in block Y at block (Y + 420000).
> Only UTXOs at that time are relevant.
>
> (2) You also need to know if there was any spending of any block Y UTXOs
> after block (Y + 420000).
>

Is this how it works?

Source transaction is included in block Y.

If the output is spent before Y + 420,000, then no further action is taken.

The miner for block Y + 420,000 will include a commitment to
merkle_hash(Block Y's unspent outputs).

It is possible for someone to prove that they didn't spend their
transaction before Y + 420,000.

I think the miners have to remember the "live" UTXO merkle root for every
block?

With the path to the UTXO and the miner can recalculate the root for that
block.

If there were 20 dormant outputs being spent, then the miner would have to
commit to 20 updates.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151213/a4cf6d21/attachment.html>

From rusty at rustcorp.com.au  Sun Dec 13 20:34:48 2015
From: rusty at rustcorp.com.au (Rusty Russell)
Date: Mon, 14 Dec 2015 07:04:48 +1030
Subject: [bitcoin-dev] Segregated Witness features wish list
In-Reply-To: <CABeL=0hzQeOPcVgRcdM6Q_hqTNevHBOs7d2HMd6yh0o5Du9yAw@mail.gmail.com>
References: <b13f6152767473dcf44a1d8965fdd32c@xbt.hk>
	<7D7416E3-0038-484D-BBA9-35FA4C2AE3DC@bitsofproof.com>
	<CABeL=0hzQeOPcVgRcdM6Q_hqTNevHBOs7d2HMd6yh0o5Du9yAw@mail.gmail.com>
Message-ID: <874mfmgk9j.fsf@rustcorp.com.au>

Jannes Faber via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> writes:
> Segregated IBLT
>
> I was just wondering if it would make sense when we have SW to also make
> Segregated IBLT? Segregating transactions from signatures and then tune the
> parameters such that transactions have a slightly higher guarantee and save
> a bit of space on the signatures side.

It just falls out naturally.  If the peer doesn't want the witnesses,
they don't get serialized into the IBLT.

Cheers,
Rusty.

From j at toom.im  Mon Dec 14 11:21:43 2015
From: j at toom.im (Jonathan Toomim)
Date: Mon, 14 Dec 2015 19:21:43 +0800
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAOG=w-t=+0Zdoy+d4Y2t9nbRkyO30N_Az9kbRarGo69yHCpSwA@mail.gmail.com>
References: <CABsx9T0nxcqAkEt7+pVm9oZEZH_HCJ9D3J00v0bKJYeUcDv1hQ@mail.gmail.com>
	<1449897228198.c655b3ae@Nodemailer>
	<CAOG=w-t=+0Zdoy+d4Y2t9nbRkyO30N_Az9kbRarGo69yHCpSwA@mail.gmail.com>
Message-ID: <3292B7BA-13A8-4BD9-AB08-4F5EBE534771@toom.im>

This means that a server supporting SW might only hear of the tx data and not get the signature data for some transactions, depending on how the relay rules worked (e.g. if the SW peers had higher minrelaytxfee settings than the legacy peers). This would complicate fast block relay code like IBLTs, since we now have to check to see that the recipient has both the tx data and the witness/sig data.

The same issue might happen with block relay if we do SW as a soft fork. A SW node might see a block inv from a legacy node first, and might start downloading the block from that node. This block would then be marked as in-flight, and the witness data might not get downloaded. This shouldn't be too hard to fix by creating an inv for the witness data as a separate object, so that a node could download the block from e.g. Peer 1 and the segwit data from Peer 2.

Of course, the code would be simpler if we did this as a hard fork and we could rely on everyone on the segwit fork supporting the segwit data. Although maybe we want to write the interfaces in a way that supports some nodes not downloading the segwit data anyway, just because not every node will want that data.

I haven't had time to read sipa's code yet. I apologize for talking out of a position of ignorance. For anyone who has, do you feel like sharing how it deals with these network relay issues?

By the way, since this thread is really about SegWit and not about any other mechanism for increasing Bitcoin capacity, perhaps we should rename it accordingly?


On Dec 12, 2015, at 11:18 PM, Mark Friedenbach via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:

> A segwit supporting server would be required to support relaying segwit transactions, although a non-segwit server could at least inform a wallet of segwit txns observed, even if it doesn't relay all information necessary to validate.
> 
> Non segwit servers and wallets would continue operations as if nothing had occurred.
> 
> If this means essentially that a soft fork deployment of SegWit will require SPV wallet servers to change their logic (or risk not being able to send payments) then it does seem to me that a hard fork to deploy this non controversial change is not only cleaner (on the data structure side) but safer in terms of the potential to affect the user experience.
> 
> 
> ? Regards,

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151214/b6bcb6cb/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151214/b6bcb6cb/attachment.sig>

From j at toom.im  Mon Dec 14 11:44:34 2015
From: j at toom.im (Jonathan Toomim)
Date: Mon, 14 Dec 2015 19:44:34 +0800
Subject: [bitcoin-dev] Segregated Witness features wish list
In-Reply-To: <b13f6152767473dcf44a1d8965fdd32c@xbt.hk>
References: <b13f6152767473dcf44a1d8965fdd32c@xbt.hk>
Message-ID: <05E42ED2-8F04-4018-B137-74D79BD46348@toom.im>

1. I think we should limit the sum of the block and witness data to nBlockMaxSize*7/4 per block, for a maximum of 1.75 MB total. I don't like the idea that SegWit would give us 1.75 MB of capacity in the typical case, but we have to have hardware capable of 4 MB in adversarial conditions (i.e. intentional multisig). I think a limit to the segwit size allays that concern.

2. I think that segwit is a substantial change to how Bitcoin works, and I very strongly believe that we should not rush this. It changes the block structure, it changes the transaction structure, it changes the network protocol, it changes SPV wallet software, it changes block explorers, and it has changes that affect most other parts of the Bitcoin ecosystem. After we decide to implement it, and have a final version of the code that will be merged, we should give developers of other Bitcoin software time to implement code that supports the new transaction/witness formats.

When you guys say "as soon as possible," what do you mean exactly?

On Dec 10, 2015, at 2:47 PM, jl2012--- via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:

> It seems the current consensus is to implement Segregated Witness. SW opens many new possibilities but we need a balance between new features and deployment time frame. I'm listing by my priority:
> 
> 1-2 are about scalability and have highest priority
> 
> 1. Witness size limit: with SW we should allow a bigger overall block size. It seems 2MB is considered to be safe for many people. However, the exact size and growth of block size should be determined based on testing and reasonable projection.
> 
> 2. Deployment time frame: I prefer as soon as possible, even if none of the following new features are implemented. This is not only a technical issue but also a response to the community which has been waiting for a scaling solution for years
> 

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151214/7a48cdf9/attachment-0001.sig>

From adam at cypherspace.org  Mon Dec 14 12:32:01 2015
From: adam at cypherspace.org (Adam Back)
Date: Mon, 14 Dec 2015 20:32:01 +0800
Subject: [bitcoin-dev] Segregated Witness features wish list
Message-ID: <CALqxMTFY4oAAO4mXVJigEunjPw+q6y3x=zATLEw9ErDcS1RDuw@mail.gmail.com>

I think people have a variety of sequences in mind, eg some would
prefer to deploy versionBits first, so that subsequent features can be
deployed in parallel (currently soft-fork deployment is necessarily
serialised).

Others think do seg-witness first because scale is more important.

Some want to do seg-witness as a hard-fork (personally I think that
would take a bit longer to deploy & activate - the advantage of
soft-fork is that it's lower risk and we have more experience of it).

I've seen a few people want to do BIP 102 first (straight move to 2MB
and only that) and then do seg-witness and other scaling work later.
That's possible also and before Luke observed that you could do a
seg-witness based block-size increase via soft-fork, people had been
working following the summary from the montreal workshop discussion
posted on this list about a loose plan of action, people had been
working on something like BIP 102 to 2-4-8 kind of space, plus
validation cost accounting.

So I think personally soft-fork seg-witness first, but hey I'm not
writing code at present and I'm very happy for wiser and more code and
deployment detail aware minds to figure out the best deployment
strategy, I wouldnt mind any of the above, just think seg-witness
soft-fork is the safest and fastest.  The complexity risk - well on
the plus side it is implemented and it reduces deployment risk, and
it's anyway needed work to have a robust malleability fix which is
needed for a whole range of bitcoin smart-contract, and scaling
features, including for example greenAddress like faster transactions
as used by BitPay?, BitGo and GreenAddress as well as lightning
related proposals and basically any smart-contract use that involves
multiple clauses and dependent transactions.  Also re complexity risk
Greg has highlighted that the complexity and overhead difference is
really minor.  About knock on code changes needed, a bunch of the next
steps for Bitcoin are going to need code changes, I think our scope to
improve and scale Bitcoin are going to be severely hampered if we
restricted ourselves with the pre-condition that we cant make protocol
improvements.  I think people in core would be happy to, and have done
this kind of thing multiple times in the past, to help people for free
on volunteer time integrate and fix up related code in various
languages and FOSS and commercial software that is affected.

As to time-line Luke I saw guestimated by march 2016 on reddit.
Others maybe be more or less conservative.  Probably a BIP and testing
are the main thing, and that can be helped by more eyes.  The one
advantage of BIP 102 like proposal is simplicity if one had to do a
more emergency hard-fork.  Maybe companies and power users, miners and
pool operators could help define some requirements and metrics about
an industry wide service level they are receiving relative to fees.

The other thing which is not protocol related, is that companies can
help themselves and help Bitcoin developers help them, by working to
improve decentralisation with better configurations, more use of
self-hosted and secured full nodes, and decentralisation of policy
control over hashrate.  That might even include buying a nominal (to a
reasonably funded startup) amount of mining equipment.  Or for power
users to do more of that.  Some developers are doing mining.
Blockstream and some employees have a little bit of hashrate.  If we
could define some metrics and best practices and measure the
improvements, that would maybe reduce miners concerns about
centralisation risk and allow a bigger block faster, alongside the
IBLT & weak block network protocol improvements.

Adam

On 14 December 2015 at 19:44, Jonathan Toomim via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> 1. I think we should limit the sum of the block and witness data to nBlockMaxSize*7/4 per block, for a maximum of 1.75 MB total. I don't like the idea that SegWit would give us 1.75 MB of capacity in the typical case, but we have to have hardware capable of 4 MB in adversarial conditions (i.e. intentional multisig). I think a limit to the segwit size allays that concern.
>
> 2. I think that segwit is a substantial change to how Bitcoin works, and I very strongly believe that we should not rush this. It changes the block structure, it changes the transaction structure, it changes the network protocol, it changes SPV wallet software, it changes block explorers, and it has changes that affect most other parts of the Bitcoin ecosystem. After we decide to implement it, and have a final version of the code that will be merged, we should give developers of other Bitcoin software time to implement code that supports the new transaction/witness formats.
>
> When you guys say "as soon as possible," what do you mean exactly?
>
> On Dec 10, 2015, at 2:47 PM, jl2012--- via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> It seems the current consensus is to implement Segregated Witness. SW opens many new possibilities but we need a balance between new features and deployment time frame. I'm listing by my priority:
>>
>> 1-2 are about scalability and have highest priority
>>
>> 1. Witness size limit: with SW we should allow a bigger overall block size. It seems 2MB is considered to be safe for many people. However, the exact size and growth of block size should be determined based on testing and reasonable projection.
>>
>> 2. Deployment time frame: I prefer as soon as possible, even if none of the following new features are implemented. This is not only a technical issue but also a response to the community which has been waiting for a scaling solution for years

From adam at cypherspace.org  Mon Dec 14 12:44:57 2015
From: adam at cypherspace.org (Adam Back)
Date: Mon, 14 Dec 2015 20:44:57 +0800
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <3292B7BA-13A8-4BD9-AB08-4F5EBE534771@toom.im>
References: <CABsx9T0nxcqAkEt7+pVm9oZEZH_HCJ9D3J00v0bKJYeUcDv1hQ@mail.gmail.com>
	<1449897228198.c655b3ae@Nodemailer>
	<CAOG=w-t=+0Zdoy+d4Y2t9nbRkyO30N_Az9kbRarGo69yHCpSwA@mail.gmail.com>
	<3292B7BA-13A8-4BD9-AB08-4F5EBE534771@toom.im>
Message-ID: <CALqxMTFz8Z6u1gRPzORYRiq_NTL_sMuFDd43MqQ+n26oTRZjBQ@mail.gmail.com>

I think someone, maybe Pieter, commented on this relay issue that it
would be likely very transitory, as a lot of stuff would be fairly
quickly upgraded in practice from previous deployment experience, and
I think anyway there is a huge excess connectivity and capacity in the
p2p network vs having a connected network of various versions, and
supporting SPV client load (SPV load is quite low relative to
capacity, even one respectable node can support a large number of SPV
clients).

(Ie so two classes of network node and connectivity wouldnt be a
problem in practice even if it did persist; also the higher capacity
better run nodes are more likely to upgrade due to having more clued
in power user, miner, pool or company operators).

Maybe someone more detailed knowledge could clarify further.

Adam

On 14 December 2015 at 19:21, Jonathan Toomim via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> This means that a server supporting SW might only hear of the tx data and
> not get the signature data for some transactions, depending on how the relay
> rules worked (e.g. if the SW peers had higher minrelaytxfee settings than
> the legacy peers). This would complicate fast block relay code like IBLTs,
> since we now have to check to see that the recipient has both the tx data
> and the witness/sig data.
>
> The same issue might happen with block relay if we do SW as a soft fork. A
> SW node might see a block inv from a legacy node first, and might start
> downloading the block from that node. This block would then be marked as
> in-flight, and the witness data might not get downloaded. This shouldn't be
> too hard to fix by creating an inv for the witness data as a separate
> object, so that a node could download the block from e.g. Peer 1 and the
> segwit data from Peer 2.
>
> Of course, the code would be simpler if we did this as a hard fork and we
> could rely on everyone on the segwit fork supporting the segwit data.
> Although maybe we want to write the interfaces in a way that supports some
> nodes not downloading the segwit data anyway, just because not every node
> will want that data.
>
> I haven't had time to read sipa's code yet. I apologize for talking out of a
> position of ignorance. For anyone who has, do you feel like sharing how it
> deals with these network relay issues?
>
> By the way, since this thread is really about SegWit and not about any other
> mechanism for increasing Bitcoin capacity, perhaps we should rename it
> accordingly?
>
>
> On Dec 12, 2015, at 11:18 PM, Mark Friedenbach via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org> wrote:
>
> A segwit supporting server would be required to support relaying segwit
> transactions, although a non-segwit server could at least inform a wallet of
> segwit txns observed, even if it doesn't relay all information necessary to
> validate.
>
> Non segwit servers and wallets would continue operations as if nothing had
> occurred.
>
> If this means essentially that a soft fork deployment of SegWit will require
> SPV wallet servers to change their logic (or risk not being able to send
> payments) then it does seem to me that a hard fork to deploy this non
> controversial change is not only cleaner (on the data structure side) but
> safer in terms of the potential to affect the user experience.
>
>
> ? Regards,
>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

From j at toom.im  Mon Dec 14 12:50:51 2015
From: j at toom.im (Jonathan Toomim)
Date: Mon, 14 Dec 2015 20:50:51 +0800
Subject: [bitcoin-dev] Segregated Witness features wish list
In-Reply-To: <CALqxMTFY4oAAO4mXVJigEunjPw+q6y3x=zATLEw9ErDcS1RDuw@mail.gmail.com>
References: <CALqxMTFY4oAAO4mXVJigEunjPw+q6y3x=zATLEw9ErDcS1RDuw@mail.gmail.com>
Message-ID: <478742B9-8CEA-467D-957D-3FAAB8AF5337@toom.im>

Off-topic: If you want to decentralize hashing, the best solution is probably to redesign p2pool to use DAGs. p2pool would be great except for the fact that the 30 sec share times are (a) long enough to cause significant reward variance for miners, but (b) short enough to cause hashrate loss from frequent switching on hardware that wasn't designed for it (e.g. Antminers, KNC) and (c) uneven rewards to different miners due to share orphan rates. DAGs can fix all of those issues. I had a talk with some medium-sized Chinese miners on Thursday in which I told them about p2pool, and I got the impression that they would prefer it over their existing pools due to the 0% fees and trustless design if the performance issues were fixed. If anybody is interested in helping with this work, ping me or Bob McElrath backchannel to be included in our conversation.


On Dec 14, 2015, at 8:32 PM, Adam Back <adam at cypherspace.org> wrote:

> The other thing which is not protocol related, is that companies can
> help themselves and help Bitcoin developers help them, by working to
> improve decentralisation with better configurations, more use of
> self-hosted and secured full nodes, and decentralisation of policy
> control over hashrate.  That might even include buying a nominal (to a
> reasonably funded startup) amount of mining equipment.  Or for power
> users to do more of that.  Some developers are doing mining.
> Blockstream and some employees have a little bit of hashrate.  If we
> could define some metrics and best practices and measure the
> improvements, that would maybe reduce miners concerns about
> centralisation risk and allow a bigger block faster, alongside the
> IBLT & weak block network protocol improvements.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151214/8a06afe6/attachment-0001.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151214/8a06afe6/attachment-0001.sig>

From ricardojdfilipe at gmail.com  Sun Dec 13 21:20:21 2015
From: ricardojdfilipe at gmail.com (Ricardo Filipe)
Date: Sun, 13 Dec 2015 21:20:21 +0000
Subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin
In-Reply-To: <3b28f994d75070ab1fd2d312f29bb706@xbt.hk>
References: <50e629572d8de852eb789d50b34da308@xbt.hk>
	<1449961269.2210.5.camel@yahoo.com>
	<CACrzPenXGQZBrx8QC+1QE2oCE3N=qmfgc_OWrowtjtLjGkZrRA@mail.gmail.com>
	<CAAS2fgQi7aiwyOaVBiMbp6t9D58aFAmDdKPzFiscB6ouGzBK6A@mail.gmail.com>
	<CAAcC9yuSX67ckhBUCsvTk+7PB6vzufuuBsJikSqqqU_4LXoCfA@mail.gmail.com>
	<CAAS2fgSchJFk1Ejd8ZfMSzxEO-1TWYR6ag-seQNH_QHrc9Cn3w@mail.gmail.com>
	<CAAcC9ysovzcm1SD_4XyxxofmwdXrcQqs0ckQBw626vYsdPftKw@mail.gmail.com>
	<3b28f994d75070ab1fd2d312f29bb706@xbt.hk>
Message-ID: <CALC81CPN7WmF8sd8jkP7hzPdzzWL6K0SVWpUiXW7dEPpX0DdzA@mail.gmail.com>

I really like ideas that tackle this issue. The question imho is what is
the incentive to run a "Full UTXO node" instead of a pruned or archive node.
For starters, it would be nice to know what would be the savings for Full
UTXO nodes over archive nodes right now.
Also, what advantages would this have over "archive pruned nodes: nodes
that store X blocks of the whole blockchain before 420000". Seems like an
interesting intermediate use case to me too.

2015-12-13 18:11 GMT+00:00 jl2012--- via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org>:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA256
>
> On Mon, Dec 14, 2015 at 12:14 AM, Danny Thorpe <danny.thorpe at gmail.com>
> wrote:
>
>> What is the current behavior / cost that this proposal is trying to
>> avoid? Are ancient utxos required to be kept in memory always in a fully
>> validating node, or can ancient utxos get pushed out of memory like a
>> normal LRU caching db?
>>
>
> I don't see why it must be kept in memory. But storage is still a problem.
> With the 8 year limit and a fixed max block size, it indirectly sets an
> upper limit for UTXO set.
>
>
> Chris Priest via bitcoin-dev :
>
>> This isn't going to kill bitcoin, but it won't make it any better.
>>
>
> Do you believe that thousands of volunteer full nodes are obliged to store
> an UTXO record, just because one paid US$0.01 to an anonymous miner 100
> years ago? It sounds insanely cheap, isn't it? My proposal (or similar
> proposal by Peter Todd) is to solve this problem. Many commercial banks
> have a dormant threshold less than 8 years so I believe it is a balanced
> choice.
>
> Back to the topic, I would like to further elaborate my proposal.
>
> We have 3 types of full nodes:
>
> Archive nodes: full nodes that store the whole blockchain
> Full UTXO nodes: full nodes that fully store the latest UTXO state, but
> not the raw blockchain
> Lite UTXO nodes: full nodes that store only UTXO created in that past
> 420000 blocks
>
> Currently, if one holds nothing but a private key, he must consult either
> an archive node or a full UTXO node for the latest UTXO state to spend his
> coin. We currently do not have any lite UTXO node, and such node would not
> work properly beyond block 420000.
>
> With the softfork I described in my original post, if the UTXO is created
> within the last 420000 blocks, the key holder may consult any type of full
> node, including a lite UTXO node, to create the transaction.
>
> If the UTXO has been confirmed by more than 420000 blocks, a lite UTXO
> node obviously can't provide the necessary information to spend the coin.
> However, not even a full UTXO node may do so. A full UTXO node could tell
> the position of the UTXO in the blockchain, but can't provide all the
> information required by my specification. Only an archive node may do so.
>
> What extra information is needed?
>
> (1) If your UTXO was generated in block Y, you first need to know the TXO
> state (spent / unspent) of all outputs in block Y at block (Y + 420000).
> Only UTXOs at that time are relevant.
>
> (2) You also need to know if there was any spending of any block Y UTXOs
> after block (Y + 420000).
>
> It is not possible to construct the membership prove I require without
> these information. It is designed this way, so that lite UTXO nodes won't
> need to store any dormant UTXO records: not even the hash of individual
> dormant UTXO records. If the blockchain grows to insanely big, it may take
> days or weeks to retrieve to records. However, I don't think this is
> relevant as one has already left his coins dormant for >8 years. Actually,
> you don't even need the full blockchain. For (1), all you need is the
> 420000 blocks from Y to Y+420000 minus any witness data, as you don't need
> to do any validation. For (2), you just need the coinbase of Y+420001 to
> present, where any spending would have been committed, and retrieve the
> full block only if a spending is found.
>
> So the Bitcoin Bank (miners) is not going to shred your record and
> confiscate your money. Instead, the Bank throws your record to the garage
> (raw blockchain). You can search for your record by yourself, or employ
> someone (archive node) to search it for you. In any case it incurs costs.
> But as thousands of bankers have kept your record on their limited desk
> space for 8 years for free (though one of them might receive a fraction of
> a penny from you), you shouldn't complain with any moral, technical, or
> legal reason. And no matter what users say, I believe something like this
> will happen when miners and full nodes can't handle the UTXO set.
>
> I'd like to see more efficient proposals that archive the same goals.
>
> p.s. there were some typos in my original. The second sentence of the
> second paragraph should be read as "For every block X+420000, it will
> commit to a hash for all UTXOs generated in block X."
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v2
>
> iQGcBAEBCAAGBQJWbbR2AAoJEO6eVSA0viTScEoL/RPlsxr0A5wTtgdi+9i4AFlV
> Sw/He89+YPGe5VCG74YNAPLEUF1/rICzUJ4DulvNTOo/5xtmkv5ok4bD7v1JZnH3
> DE2PExMQYs2X4Qm6mkcwi8IWlMR2U5j5ebUq21Kj4AqVFj9UcQmYGhPehB2f+cM9
> Wki/TDwNj5fV8AZ4uR9pPgaf+bvVQQ9BOOLiIMiTbphNCx1hfGfYcsqmXlCbGk9A
> PatGR88aQTxpa7PhbCZwwf76cKuOaYYZeHr9jRR9RL5rZVXgE1SI/niBytJhXaP8
> lwYtk4Bpz0IGd23v1dArNQQoOp5Xycbeq1l1qyv/qtxju65No+dhqiEcFBZVI1AS
> VcndMQ+yvNuxVgib2Ifh9YjXelWAqqLzzoVcz2RxXh6HJ0tVKxBokwdAcsclZb93
> zQ1JhDR4vBpLquytZA8lDIxJraNCdB/KEAOAey6ljP3zL7fBLBp1oZw4DDDtFy8V
> EMjrOSVnjyuyfey2YXsGnnHuQS0mpwmSroV2400uGQ==
> =2xRy
> -----END PGP SIGNATURE-----
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151213/67eaba28/attachment.html>

From jgarzik at gmail.com  Wed Dec 16 14:53:58 2015
From: jgarzik at gmail.com (Jeff Garzik)
Date: Wed, 16 Dec 2015 09:53:58 -0500
Subject: [bitcoin-dev] Block size: It's economics & user preparation & moral
	hazard
Message-ID: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>

All,

Following the guiding WP principle of Assume Good Faith, I've been trying
to boil down the essence of the message following Scaling Bitcoin.  There
are key bitcoin issues that remain outstanding and pressing, that are*
orthogonal to LN & SW*.

I create multiple proposals and try multiple angles because of a few,
notable systemic economic and analysis issues - multiple tries at solving
the same problems.  Why do I do what I do -- Why not try to reboot... just
list those problems?

Definitions:

FE - "Fee Event", the condition where main chain MSG_BLOCK is 95+% to hard
limit for 7 or more days in row, "blocks generally full"   This can also be
induced by a miner squeeze (collective soft limit reduction).


Service - a view of bitcoin as a decentralized, faceless, multi-celled,
amorphous automaton cloud, that provides services in exchange for payment

Users - total [current | future] set of economic actors that pay money to
the Service, and receive value (figuratively or literally) in return

Block Size - This is short hand for MAX_BLOCK_SIZE, the hard limit that
requires, today, a hard fork to increase (excl. extension blocks etc.)


Guiding Principle:

Keep the Service alive, secure, decentralized, and censorship resistant for
as many Users as possible.


Observations on block size (shorthand for MAX_BLOCK_SIZE as noted above):

This is economically modeled as a supply limited resource over time.  On
average, 1M capacity is available every 10 minutes, with variance.


Observations on Users, block size and modern bidding process:

A supermajority of hashpower currently evaluates for block inclusion based,
first-pass, on tx-fee/KB.  Good.

The Service is therefore responsive to the free market and some classes of
DoS.  Good.

Recent mempool changes float relay fee, making the Service more responsive
to fast moving markets and DoS's.  Good progress.


Service provided to Users can be modeled at the bandwidth resource level as
bidding for position in a virtual priority queue, where up-to-1M bursts are
cleared every 10 min (on avg etc.).  Not a perfectly fixed supply,
definitionally, but constrained within a fixed range.


Observations on the state of today's fee market:

On average, blocks are not full.  Economically, this means that fees trend
towards zero, due to theoretically unlimited supply at <1M levels.

Of course, fees are not zero.  The network relay anti-flood limits serve as
an average lower limit for most transactions (excl direct-to-miner).
Wallet software also introduces fee variance in interesting ways.  All this
fee activity is range-bound on the low end.

Let the current set of Users + transaction fee market behavior be TFM
(today's fee market).
Let the post-Fee-Event set of Users + transaction fee market behavior be
FFM (future fee market).

*Key observation:   A Bitcoin Fee Event (see def. at top) is an Economic
Change Event.*

An Economic Change Event is a period of market chaos, where large changes
to prices and sets of economic actors occurs over a short time period.

A Fee Event is a notable Economic Change Event, where a realistic
projection forsees higher fee/KB on average, pricing some economic actors
(bitcoin projects and businesses) out of the system.

*It is a major change to how current Users experience and pay for the
Service*, state change from TFM to FFM.

The game theory bidding behavior is different for a mostly-empty resource
versus a usually-full resource.  Prices are different.  Profitable business
models are different.  Users (the set of economic actors on the network)
are different.


Observation:  Contentious hard fork is an Economic Change Event.

Similarly, a fork that partitions economic actors for an extended period or
permanently is also an Economic Change Event, shuffling prices and economic
actors as the Service dynamically readjusts on both sides of the partition,
and Users-A and Users-B populations change their behavior.



Short-Term Problem #1:  No-action on block size increase leads to an
Economic Change Event.


Failure to increase block size is not obviously-conservative, it is a
conscious choice, electing for one economic state and set of actors and
prices over another.  Choosing FFM over TFM.

*It is rational to reason that maintaining TFM is more conservative* than
enduring an Economic Change Event from TFM to FFM.

*It is rational to reason that maintaining similar prices and economic
actors is less disruptive.*

Failure to increase block size will lead to a Fee Event sooner rather than
later.

Failure to plan ahead for a Fee Event will lead to greater market chaos and
User pain.


Short-Term Problem #2:  Some Developers wish to accelerate the Fee Event,
and a veto can accomplish that.

In the current developer dynamics, 1-2 key developers can and very likely
would veto any block size increase.

Thus a veto (e.g. no-action) can lead to a Fee Event, which leads to
pricing actors out of the system.

A block size veto wields outsize economic power, because it can accelerate
ECE.

*This is an extreme moral hazard:  A few Bitcoin Core committers can veto
increase and thereby reshape bitcoin economics, price some businesses out
of the system.  It is less of a moral hazard to keep the current economics
[by raising block size] and not exercise such power.*


Short-Term Problem #3:  User communication and preparation

The current trajectory of no-block-size-increase can lead to short time
market chaos, actor chaos, businesses no longer viable.

In a $6.6B economy, it is criminal to let the Service undergo an ECE
without warning users loudly, months in advance:  "Dear users, ECE has
accelerated potential due to developers preferring a transition from TFM to
FFM."

As stated, *it is a conscious choice to change bitcoin economics and User
experience* if block size is not advanced with a healthy buffer above
actual average traffic levels.

*Raising block size today, at TFM, produces a smaller fee market delta.*

Further, wallet software User experience is very, very poor in a
hyper-competitive fee market.   (This can and will be improved; that's just
the state of things today)


Short-Term Problem #4:  User/Dev disconnect:   Large mass of users wishes
to push Fee Event into future

Almost all bitcoin businesses, exchanges and miners have stated they want a
block size increase.  See the many media articles, BIP 101 letter, and wiki
e.g.
https://en.bitcoin.it/wiki/Block_size_limit_controversy#Entities_positions

The current apparent-veto on block size increase runs contra to the desires
of many Users.  (note language: "many", not claiming "all")

*It is a valid and rational economic choice to subsidize the system with
lower fees in the beginning*.  Many miners, for example, openly state they
prefer long term system growth over maximizing tiny amounts of current day
income.

Vetoing a block size increase has the effect of eliminating that economic
choice as an option.


It is difficult to measure Users; projecting beyond "businesses and miners"
is near impossible.

Without exaggeration, I have never seen this much disconnect between user
wishes and dev outcomes in 20+ years of open source.


Short-Term Problem #5:  Higher Service prices can negatively impact system
security

Bitcoin depends on a virtuous cycle of users boosting and maintaining
bitcoin's network effect, incentivizing miners, increasing security.

Higher prices that reduce bitcoin's user count and network effect can have
the opposite impact.

(Obviously this is a dynamic system, users and miners react to higher
prices... including actions that then reduce the price)

Short-Term Problem #6:  Post-Fee-Event market reboot problem + general lack
of planning

Game it out:   Blocks are now full (FFM).  Block size kept at 1M.

How full is too full - who and what dictates when 1M should be increased?

The same question remains, yet now economic governance issues are
compounded:  In FFM, the fees are very tightly bound to the upper bound of
the block size.  In TFM, fees are much less sensitive to the upper bound of
block size.


Changing block size, when blocks are full, has a more dramatic effect on
the market - suddenly new supply is magically brought online, and a minor
Economic Change Event occurs.

More generally, the post-Fee-Event next step has not been agreed upon.  Is
it flexcap?  This key "step #2" is just barely at whiteboard stage.


Short-Term Problem #7:   Fee Event timing is unpredictable.

As block size free space gets tighter - that is the trend - and block size
remains at 1M, Users are ever more likely to hit an Economic Change Event.
It could happen in the next 2-6 months.

Today, Users and wallets are not prepared.

It is also understandably a very touchy subject to say "your business or
use case might get priced out of bitcoin"


But it is even worse to let worse let Users run into a Fee Event without
informing the market that the block size will remain at 1M.

Markets function best with maximum knowledge - when they are informed well
in advance of market shifting news and events, giving economic actors time
to prepare.


Short-Term Problem #8:   Very little testing, data, effort put into
blocks-mostly-full economics

*We only know for certain that blocks-mostly-not-full works.*  We do not
know that changing to blocks-mostly-full works.

Changing to a new economic system includes boatloads of risk.

Very little data has been forthcoming from any party on what FFM might look
like, following a Fee Event.


Observation:   In the long run, it is assumed we need a "healthy fee market"

Yes, absolutely.  In the long run, bitcoin was intended to be supported by
transaction fees and not the minting of new supply, and the design of the
system is to slowly wean Users off new supply and onto transaction fees for
supporting the Service.

While agreeing with the goal, it must be acknowledge that this is a vague
and untested goal with many open economic questions -- more of a hope,
really.

It is more conservative to preserve current economics than to change to a
new system with new economics and no notion of what-comes-next (flexcap?)
in terms of system security, healthy sustainable market levels, and impact
of changes during and following an ECE.



Core recommendations:

1) "Short term bump"  Block size increase to maintain buffer.  I've no
special BIP preference.

This avoids moral hazard and avoids a major Economic Change Event, as well
many other risks.


2) If block size stays at 1M, the Bitcoin Core developer team should sign a
collective note stating their desire to transition to a new economic
policy, that of "healthy fee market" and strongly urge users to examine
their fee policies, wallet software, transaction volumes and other possible
User impacting outcomes.


3) Even if can is kicked down the road, Fee Event will come eventually.
Direct research, testing and simulations into the economics and user impact
side of the equation.  Research and experiment with pay-for-burst (pay to
future miner), flexcap and other solutions ASAP.


The worst possible outcome is letting the ecosystem randomly drift into the
first Fee Event without openly stating the new economic policy choices and
consequences.

The simple fact is *inaction* on this supply-limited resource, block size,
will change bitcoin to a new economic shape and with different economic
actors, selecting some and not others.

It is better to kick the can and gather crucial field data, because
next-step (FFM) is very much not fleshed out.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/024095fb/attachment-0001.html>

From pieter.wuille at gmail.com  Wed Dec 16 18:34:32 2015
From: pieter.wuille at gmail.com (Pieter Wuille)
Date: Wed, 16 Dec 2015 19:34:32 +0100
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
Message-ID: <CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>

On Wed, Dec 16, 2015 at 3:53 PM, Jeff Garzik via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> 2) If block size stays at 1M, the Bitcoin Core developer team should sign a
> collective note stating their desire to transition to a new economic policy,
> that of "healthy fee market" and strongly urge users to examine their fee
> policies, wallet software, transaction volumes and other possible User
> impacting outcomes.

You present this as if the Bitcoin Core development team is in charge
of deciding the network consensus rules, and is responsible for making
changes to it in order to satisfy economic demand. If that is the
case, Bitcoin has failed, in my opinion.

What the Bitcoin Core team should do, in my opinion, is merge any
consensus change that is uncontroversial. We can certainly -
individually or not - propose solutions, and express opinions, but as
far as maintainers of the software goes our responsibility is keeping
the system running, and risking either a fork or establishing
ourselves as the de-facto central bank that can make any change to the
system would greatly undermine the system's value.

Hard forking changes require that ultimately every participant in the
system adopts the new rules. I find it immoral and dangerous to merge
such a change without extremely widespread agreement. I am personally
fine with a short-term small block size bump to kick the can down the
road if that is what the ecosystem desires, but I can only agree with
merging it in Core if I'm convinced that there is no strong opposition
to it from others.

Soft forks on the other hand only require a majority of miners to
accept them, and everyone else can upgrade at their leisure or not at
all. Yes, old full nodes after a soft fork are not able to fully
validate the rules new miners enforce anymore, but they do still
verify the rules that their operators opted to enforce. Furthermore,
they can't be prevented. For that reason, I've proposed, and am
working hard, on an approach that includes Segregated Witness as a
first step. It shows the ecosystem that something is being done, it
kicks the can down the road, it solves/issues half a dozen other
issues at the same time, and it does not require the degree of
certainty needed for a hardfork.

-- 
Pieter

From jl2012 at xbt.hk  Wed Dec 16 18:36:00 2015
From: jl2012 at xbt.hk (jl2012)
Date: Wed, 16 Dec 2015 13:36:00 -0500
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
Message-ID: <9a02d94fbc78afaa3e9668e0294eef64@xbt.hk>

I would also like to summarize my observation and thoughts after the 
Hong Kong workshop.

1. I'm so glad that I had this opportunity to meet so many smart 
developers who are dedicated to make Bitcoin better. Regular conference 
like this is very important for a young project, and it is particularly 
important for Bitcoin, with consensus as the core value. I hope such a 
conference could be conducted at least once in 2 years in Hong Kong, 
which is visa-friendly for most people in both East and West.

2. I think some consensus has emerged at/after the conference. There is 
no doubt that segregated witness will be implemented. For block size, I 
believe 2MB as the first step is accepted by the super majority of 
miners, and is generally acceptable / tolerable for devs.

3. Chinese miners are requesting consensus among devs nicely, instead of 
using their majority hashing power to threaten the community. However, 
if I were allowed to speak for them, I think 2MB is what they really 
want, and they believe it is for the best interest of themselves and the 
whole community

4. In the miners round table on the second day, one of the devs 
mentioned that he didn't want to be seen as the decision maker of 
Bitcoin. On the other hand, Chinese miners repeatedly mentioned that 
they want several concrete proposals from devs which they could choose. 
I see no contradiction between these 2 viewpoints.

Below are some of my personal views:

5. Are we going to have a "Fee Event" / "Economic Change Event" in 2-6 
months as Jeff mentioned? Frankly speaking I don't know. As the fee 
starts to increase, spammers will first get squeezed --- which could be 
a good thing. However, I have no idea how many txs on the blockchain are 
spam. We also need to consider the effect of halving in July, which may 
lead to speculation bubble and huge legitimate tx volume.

6. I believe we should avoid a radical "Economic Change Event" at least 
in the next halving cycle, as Bitcoin was designed to bootstrap the 
adoption by high mining reward in the beginning. For this reason, I 
support an early and conservative increase, such as BIP102 or 2-4-8. 2MB 
is accepted by most people and it's better than nothing for BIP101 
proponents. By "early" I mean to be effective by May, at least 2 months 
before the halving.

7. Segregated witness must be done. However, it can't replace a 
short-term block size hardfork for the following reasons:
(a) SW softfork does not allow higher volume if users are not upgrading. 
In order to bootstrap the new tx type, we may need the help of 
altruistic miners to provide a fee discount for SW tx.
(b) In terms of block space saving, SW softfork is most efficient for 
multisig tx, which is still very uncommon
(c) My most optimistic guess is SW will be ready in 6 months, which will 
be very close to halving and potential tx volume burst. And it may not 
be done in 2016, as it does not only involve consensus code, but also 
change in the p2p protocol and wallet design

8. Duplex payment channel / Lightning Network may be viable solutions. 
However, they won't be fully functional until SW is done so they are 
irrelevant in this discussion

9. No matter what is going to be done / not done, I believe we should 
now have a clear road map and schedule for the community: a short-term 
hardfork or not? The timeline of SW? It is bad to leave everything 
uncertain and people can't well prepared for any potential radical 
changes

10. Finally, I hope this discussion remains educated and evidence-based, 
and no circling

From jgarzik at gmail.com  Wed Dec 16 20:38:30 2015
From: jgarzik at gmail.com (Jeff Garzik)
Date: Wed, 16 Dec 2015 15:38:30 -0500
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling Bitcoin
Message-ID: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>

1. Summary

Segregated Witness (SegWitness, SW) is being presented in the context of
Scaling Bitcoin.  It has useful attributes, notably addressing a major
malleability vector, but is not a short term scaling solution.


2. Definitions

Import Fee Event, ECE, TFM, FFM from previous email.

Older clients - Any software not upgraded to SW

Newer clients - Upgraded, SW aware software


Block size - refers to the core block economic resource limited by
MAX_BLOCK_SIZE.  Witness data (or extension block data) is excluded.
Requires a hard fork to change.

Core block - Current bitcoin block, with upper bound MAX_BLOCK_SIZE.  Not
changed by SW.


Extended transaction - Newer, upgraded version of transaction data format.

Extended block - Newer, upgraded version of block data format.


EBS - Extended block size.  Block size seen by newer clients.


3. Context of analysis

One proposal presents SW *in lieu of* a hard fork block size increase.
This email focuses directly on that.

Useful features outside block size context, such as anti-malleability or
fraud proof features, are not covered in depth.


4.1.  Observations on data structure formats and views

SW creates two *views* of each transaction and block.  SW has blocks and
extended blocks.  Similarly, there exists transactions and extended
transactions.

This view is rendered to clients depending on compatibility level.  Newer
clients see extended blocks and extended transactions.  Older clients see
blocks (limit 1M), and do not see extended blocks.  Older clients see
upgraded transactions as unsigned, anyone-can-pay transactions.

Each extended transaction exists in two states, one unsigned and one
signed, each of which passes validation as a valid bitcoin transaction.


4.2.  Observations on behavior of older transaction creation

Transactions created by older clients will not use the extended transaction
format.  All data is stored the standard 1M block as today.


4.3.  Observations on new block economic model

SW complicates block economics by creating two separate, supply limited
resources.

The core block economic resource is heavily contended.  Older clients use
core blocks exclusively.  Newer clients use core blocks more
conservatively, storing as much data as possible in extended blocks.

The extended block economic resource is less heavily contended, though that
of course grows over time as clients upgrade.

Because core blocks are more heavily contended, it is presumed that older
clients will pay a higher fee than newer clients (subject to elasticity
etc.).


5.1.  Problem:  Pace of roll-out will be slow - Whole Ecosystem must be
considered.

The current apparent proposal is to roll out Segregated Witness as a soft
fork, and keep block size at 1M.

The roll-out pace cannot simply be judged by soft fork speed - which is
months at best.  Analysis must the layers above:  Updating bitcoin-core
(JS) and bitcoinj (Java), and then the timelines to roll out those updates
to apps, and then the timeline to update those apps to create extended
transactions.

Overall, wallet software and programmer libraries must be upgraded to make
use of this new format, adding many more months (12+ in some stacks) to the
roll out timeline.  In the meantime, clients continue to contend entirely
for core block space.


5.2.  Problem:   Hard fork to bigger block size Just Works(tm) with most
software, unlike SW.

A simple hard fork such as BIP 102 is automatically compatible with the
vast range of today's ecosystem software.

SW requires merchants to upgrade almost immediately, requires wallet and
other peripheral software upgrades to make use of.  Other updates are
opt-in and occur more slowly.  BIP 70 processors need some updates.

The number of LOC that must change for BIP 102 is very small, and the
problem domain well known, versus SW.


5.3.  Problem:   Due to pace, Fee Event not forestalled.

Even presuming SW is merged into Bitcoin Core tomorrow, this does not
address the risk of a Fee Event and associated Economic Change in the
coming months.


5.4.  Problem:   More complex economic policy, new game theory, new bidding
structure risks.

Splitting blocks into two pieces, each with separate and distinct behaviors
and resource values, creates *two fee markets.*

Having two pricing strata within each block has certainly feasible - that
is the current mining policy of (1) fee/KB followed by (2) priority/age.

Valuable or not - e.g. incentivizing older clients to upgrade - the fact
remains that SW creates a more-complex bidding structure by creating a
second economic resource.

*This is clearly a change to a new economic policy* with standard risks
associated with that.  Will that induce an Economic Change Event (see def
last email)?  *Unlikely*, due to slow rollout pace.


5.5.  Problem:  Current SW mining algorithm needs improvement

Current SW block template maker does a reasonable job, but makes some naive
assumptions about the fee market across an entire extended block.  This is
a mismatch with the economic reality (just described).

5.6.   Problem:  New, under-analyzed attack surfaces

Less significant and fundamental but still worth noting.

This is not a fundamental SW problem, but simply standard complexity risk
factors:  splitting the signatures away from transactions, and creating a
new apparently-unsigned version of the transaction opens the possibility of
some network attacks which cause some clients to degrade down from extended
block to core block mode temporarily.

There is a chance of a failure mode that fools older clients into thinking
fraudulent data is valid (judgement: unlikely vis hashpower but not
impossible)

6. Conclusions and recommendations

It seems unlikely that SW provides scaling in the short term, and SW
introduces new economics complexities.

A "short term bump" hard fork block size increase addresses economic and
ecosystem risks that SW does not.

Bump + SW should proceed in parallel, independent tracks, as orthogonal
issues.


7. Appendix - Other SW comments

Hard forks provide much stronger validation, and ensure the network
operates at a fully trustless level.

SW hard fork is preferred, versus soft fork.  Soft forking SW places a huge
amount of trust on miners to validate transaction signatures, versus the
rest of the network, as the network slowly upgrades to newer clients.

An SW hard fork could also add several zero-filled placeholders in a merkle
tree for future use.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/1c334c8f/attachment-0001.html>

From lf-lists at mattcorallo.com  Wed Dec 16 20:50:15 2015
From: lf-lists at mattcorallo.com (Matt Corallo)
Date: Wed, 16 Dec 2015 20:50:15 +0000
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
	Bitcoin
In-Reply-To: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
Message-ID: <49257841-66C8-4EF7-980B-73DC604CA591@mattcorallo.com>

A large part of your argument is that SW will take longer to deploy than a hard fork, but I completely disagree. Though I do not agree with some people claiming we can deploy SW significantly faster than a hard fork, once the code is ready (probably a six month affair) we can get it deployed very quickly. It's true the ecosystem may take some time to upgrade, but I see that as a feature, not a bug - we can build up some fee pressure with an immediate release valve available for people to use if they want to pay fewer fees.

On the other hand, a hard fork, while simpler for the ecosystem to upgrade to, is a  1-2 year affair (after the code is shipped, so at least 1.5-2.5 from today if we all put off heads down and work). One thing that has concerned me greatly through this whole debate is how quickly people seem to think we can roll out a hard fork. Go look at the distribution of node versions on the network today and work backwards to get nearly every node upgraded... Even with a year between fork-version-release and fork-activation, we'd still kill a bunch of nodes and instead of reducing their security model, lead them to be outright robbed.

On December 16, 2015 12:38:30 PM PST, Jeff Garzik via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
>1. Summary
>
>Segregated Witness (SegWitness, SW) is being presented in the context
>of
>Scaling Bitcoin.  It has useful attributes, notably addressing a major
>malleability vector, but is not a short term scaling solution.
>
>
>2. Definitions
>
>Import Fee Event, ECE, TFM, FFM from previous email.
>
>Older clients - Any software not upgraded to SW
>
>Newer clients - Upgraded, SW aware software
>
>
>Block size - refers to the core block economic resource limited by
>MAX_BLOCK_SIZE.  Witness data (or extension block data) is excluded.
>Requires a hard fork to change.
>
>Core block - Current bitcoin block, with upper bound MAX_BLOCK_SIZE. 
>Not
>changed by SW.
>
>
>Extended transaction - Newer, upgraded version of transaction data
>format.
>
>Extended block - Newer, upgraded version of block data format.
>
>
>EBS - Extended block size.  Block size seen by newer clients.
>
>
>3. Context of analysis
>
>One proposal presents SW *in lieu of* a hard fork block size increase.
>This email focuses directly on that.
>
>Useful features outside block size context, such as anti-malleability
>or
>fraud proof features, are not covered in depth.
>
>
>4.1.  Observations on data structure formats and views
>
>SW creates two *views* of each transaction and block.  SW has blocks
>and
>extended blocks.  Similarly, there exists transactions and extended
>transactions.
>
>This view is rendered to clients depending on compatibility level. 
>Newer
>clients see extended blocks and extended transactions.  Older clients
>see
>blocks (limit 1M), and do not see extended blocks.  Older clients see
>upgraded transactions as unsigned, anyone-can-pay transactions.
>
>Each extended transaction exists in two states, one unsigned and one
>signed, each of which passes validation as a valid bitcoin transaction.
>
>
>4.2.  Observations on behavior of older transaction creation
>
>Transactions created by older clients will not use the extended
>transaction
>format.  All data is stored the standard 1M block as today.
>
>
>4.3.  Observations on new block economic model
>
>SW complicates block economics by creating two separate, supply limited
>resources.
>
>The core block economic resource is heavily contended.  Older clients
>use
>core blocks exclusively.  Newer clients use core blocks more
>conservatively, storing as much data as possible in extended blocks.
>
>The extended block economic resource is less heavily contended, though
>that
>of course grows over time as clients upgrade.
>
>Because core blocks are more heavily contended, it is presumed that
>older
>clients will pay a higher fee than newer clients (subject to elasticity
>etc.).
>
>
>5.1.  Problem:  Pace of roll-out will be slow - Whole Ecosystem must be
>considered.
>
>The current apparent proposal is to roll out Segregated Witness as a
>soft
>fork, and keep block size at 1M.
>
>The roll-out pace cannot simply be judged by soft fork speed - which is
>months at best.  Analysis must the layers above:  Updating bitcoin-core
>(JS) and bitcoinj (Java), and then the timelines to roll out those
>updates
>to apps, and then the timeline to update those apps to create extended
>transactions.
>
>Overall, wallet software and programmer libraries must be upgraded to
>make
>use of this new format, adding many more months (12+ in some stacks) to
>the
>roll out timeline.  In the meantime, clients continue to contend
>entirely
>for core block space.
>
>
>5.2.  Problem:   Hard fork to bigger block size Just Works(tm) with
>most
>software, unlike SW.
>
>A simple hard fork such as BIP 102 is automatically compatible with the
>vast range of today's ecosystem software.
>
>SW requires merchants to upgrade almost immediately, requires wallet
>and
>other peripheral software upgrades to make use of.  Other updates are
>opt-in and occur more slowly.  BIP 70 processors need some updates.
>
>The number of LOC that must change for BIP 102 is very small, and the
>problem domain well known, versus SW.
>
>
>5.3.  Problem:   Due to pace, Fee Event not forestalled.
>
>Even presuming SW is merged into Bitcoin Core tomorrow, this does not
>address the risk of a Fee Event and associated Economic Change in the
>coming months.
>
>
>5.4.  Problem:   More complex economic policy, new game theory, new
>bidding
>structure risks.
>
>Splitting blocks into two pieces, each with separate and distinct
>behaviors
>and resource values, creates *two fee markets.*
>
>Having two pricing strata within each block has certainly feasible -
>that
>is the current mining policy of (1) fee/KB followed by (2)
>priority/age.
>
>Valuable or not - e.g. incentivizing older clients to upgrade - the
>fact
>remains that SW creates a more-complex bidding structure by creating a
>second economic resource.
>
>*This is clearly a change to a new economic policy* with standard risks
>associated with that.  Will that induce an Economic Change Event (see
>def
>last email)?  *Unlikely*, due to slow rollout pace.
>
>
>5.5.  Problem:  Current SW mining algorithm needs improvement
>
>Current SW block template maker does a reasonable job, but makes some
>naive
>assumptions about the fee market across an entire extended block.  This
>is
>a mismatch with the economic reality (just described).
>
>5.6.   Problem:  New, under-analyzed attack surfaces
>
>Less significant and fundamental but still worth noting.
>
>This is not a fundamental SW problem, but simply standard complexity
>risk
>factors:  splitting the signatures away from transactions, and creating
>a
>new apparently-unsigned version of the transaction opens the
>possibility of
>some network attacks which cause some clients to degrade down from
>extended
>block to core block mode temporarily.
>
>There is a chance of a failure mode that fools older clients into
>thinking
>fraudulent data is valid (judgement: unlikely vis hashpower but not
>impossible)
>
>6. Conclusions and recommendations
>
>It seems unlikely that SW provides scaling in the short term, and SW
>introduces new economics complexities.
>
>A "short term bump" hard fork block size increase addresses economic
>and
>ecosystem risks that SW does not.
>
>Bump + SW should proceed in parallel, independent tracks, as orthogonal
>issues.
>
>
>7. Appendix - Other SW comments
>
>Hard forks provide much stronger validation, and ensure the network
>operates at a fully trustless level.
>
>SW hard fork is preferred, versus soft fork.  Soft forking SW places a
>huge
>amount of trust on miners to validate transaction signatures, versus
>the
>rest of the network, as the network slowly upgrades to newer clients.
>
>An SW hard fork could also add several zero-filled placeholders in a
>merkle
>tree for future use.
>
>
>------------------------------------------------------------------------
>
>_______________________________________________
>bitcoin-dev mailing list
>bitcoin-dev at lists.linuxfoundation.org
>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/4c7fc0c2/attachment.html>

From pieter.wuille at gmail.com  Wed Dec 16 20:59:41 2015
From: pieter.wuille at gmail.com (Pieter Wuille)
Date: Wed, 16 Dec 2015 21:59:41 +0100
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
	Bitcoin
In-Reply-To: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
Message-ID: <CAPg+sBhUso0ddfYQMgwF7yX9_VoqP9CZN5h45t3eQi4v3m6f6A@mail.gmail.com>

On Wed, Dec 16, 2015 at 9:38 PM, Jeff Garzik via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> 4.3.  Observations on new block economic model
>
> SW complicates block economics by creating two separate, supply limited
> resources.

Not correct. I propose defining the virtual_block_size as base_size +
witness_size * 0.25, and limiting virtual_block_size to 1M. This
creates a single variable to optimize for. If accepted, miners are
incentived to maximize fee per virtual_block_size instead of per size.

Wallet software can individually choose whether to upgrade or not.
Once they upgrade, they get to perform 1.75x as many transactions for
the same fee (assuming non-complex transactions), and this is
independent of whether anyone else upgrades.

> 5.1.  Problem:  Pace of roll-out will be slow - Whole Ecosystem must be
> considered.
>
> The current apparent proposal is to roll out Segregated Witness as a soft
> fork, and keep block size at 1M.
>
> The roll-out pace cannot simply be judged by soft fork speed - which is
> months at best.  Analysis must the layers above:  Updating bitcoin-core (JS)
> and bitcoinj (Java), and then the timelines to roll out those updates to
> apps, and then the timeline to update those apps to create extended
> transactions.

Agree, however everyone can upgrade whenever they want, and get the
reduced fees as soon as they do. This is contrary to a hard fork,
which forces every full node to upgrade at once (though indeed, light
clients are not necessarily forced to upgrade).

> 5.2.  Problem:   Hard fork to bigger block size Just Works(tm) with most
> software, unlike SW.
>
> A simple hard fork such as BIP 102 is automatically compatible with the vast
> range of today's ecosystem software.
>
> SW requires merchants to upgrade almost immediately, requires wallet and
> other peripheral software upgrades to make use of.  Other updates are opt-in
> and occur more slowly.  BIP 70 processors need some updates.
>
> The number of LOC that must change for BIP 102 is very small, and the
> problem domain well known, versus SW.

It multiplies all current DoS vectors by a factor equal to the
capacity increase factor. SW increases capacity while leaving several
worst-case effects constant.

> 5.4.  Problem:   More complex economic policy, new game theory, new bidding
> structure risks.
>
> Splitting blocks into two pieces, each with separate and distinct behaviors
> and resource values, creates two fee markets.

I believe you have misunderstood the proposal in that case.

> 5.5.  Problem:  Current SW mining algorithm needs improvement
>
> Current SW block template maker does a reasonable job, but makes some naive
> assumptions about the fee market across an entire extended block.  This is a
> mismatch with the economic reality (just described).

Again, I think you misunderstood. The proposal includes a single new
formula for block size, and optimizes for it. In case the proposal is
accepted, the mining code is automatically as optimal as it was
before.

> 6. Conclusions and recommendations
>
> A "short term bump" hard fork block size increase addresses economic and
> ecosystem risks that SW does not.
>
> Bump + SW should proceed in parallel, independent tracks, as orthogonal
> issues.

I agree here. SW is not a replacement for a scale increase. However, I
think it can be adopted much more easily, as it doesn't require the
massively pervasive consensus that a hardfork requires to perform
safely.

> 7. Appendix - Other SW comments
>
> Hard forks provide much stronger validation, and ensure the network operates
> at a fully trustless level.
>
> SW hard fork is preferred, versus soft fork.  Soft forking SW places a huge
> amount of trust on miners to validate transaction signatures, versus the
> rest of the network, as the network slowly upgrades to newer clients.

But old clients may not care about the new rules, and they still
validate the old ones they chose to enforce.

Furthermore, soft forks cannot be prevented: miners can always choose
to enforce stronger rules than the network demands from them.

-- 
Pieter

From jgarzik at gmail.com  Wed Dec 16 21:08:29 2015
From: jgarzik at gmail.com (Jeff Garzik)
Date: Wed, 16 Dec 2015 16:08:29 -0500
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
Message-ID: <CADm_Wcae7OK7kyXkfh+7XFrc2WYsv7T1Va3_E=5om+XYrL9pOw@mail.gmail.com>

On Wed, Dec 16, 2015 at 1:34 PM, Pieter Wuille <pieter.wuille at gmail.com>
wrote:

> On Wed, Dec 16, 2015 at 3:53 PM, Jeff Garzik via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org> wrote:
> > 2) If block size stays at 1M, the Bitcoin Core developer team should
> sign a
> > collective note stating their desire to transition to a new economic
> policy,
> > that of "healthy fee market" and strongly urge users to examine their fee
> > policies, wallet software, transaction volumes and other possible User
> > impacting outcomes.
>
> You present this as if the Bitcoin Core development team is in charge
> of deciding the network consensus rules, and is responsible for making
> changes to it in order to satisfy economic demand. If that is the
> case, Bitcoin has failed, in my opinion.
>

This circles back to Problem #1:   Avoidance of a choice is a still a
choice - failing to ACK a MAX_BLOCK_SIZE increase still creates very real
Economic Change Event risk.

And #3:  If the likely predicted course is that Bitcoin Core will not
accept a protocol change changing MAX_BLOCK_SIZE via hard fork in the short
term, the core dev team should communicate that position clearly to users
and media.

Hitting a Fee Event is market changing, potentially reshuffling economic
actors to a notable degree.  Maintaining a short term economic policy of
fixed 1M supply in the face of rising transaction volume carries risks that
should be analyzed and communicated.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/117c35a7/attachment.html>

From pieter.wuille at gmail.com  Wed Dec 16 21:11:52 2015
From: pieter.wuille at gmail.com (Pieter Wuille)
Date: Wed, 16 Dec 2015 22:11:52 +0100
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <CADm_Wcae7OK7kyXkfh+7XFrc2WYsv7T1Va3_E=5om+XYrL9pOw@mail.gmail.com>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
	<CADm_Wcae7OK7kyXkfh+7XFrc2WYsv7T1Va3_E=5om+XYrL9pOw@mail.gmail.com>
Message-ID: <CAPg+sBimfFVea4Sorgx=DaMPVs1k1DrmTA2ZFdLFtxrqKm23-w@mail.gmail.com>

On Wed, Dec 16, 2015 at 10:08 PM, Jeff Garzik <jgarzik at gmail.com> wrote:
>> You present this as if the Bitcoin Core development team is in charge
>> of deciding the network consensus rules, and is responsible for making
>> changes to it in order to satisfy economic demand. If that is the
>> case, Bitcoin has failed, in my opinion.
>
>
> This circles back to Problem #1:   Avoidance of a choice is a still a choice
> - failing to ACK a MAX_BLOCK_SIZE increase still creates very real Economic
> Change Event risk.

We are not avoiding a choice. We don't have the authority to make a choice.

> And #3:  If the likely predicted course is that Bitcoin Core will not accept
> a protocol change changing MAX_BLOCK_SIZE via hard fork in the short term,
> the core dev team should communicate that position clearly to users and
> media.

I indeed think we can communicate much better that deciding consensus
rules is not within our power.

-- 
Pieter

From jtimon at jtimon.cc  Wed Dec 16 21:24:38 2015
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Wed, 16 Dec 2015 22:24:38 +0100
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <CADm_Wcae7OK7kyXkfh+7XFrc2WYsv7T1Va3_E=5om+XYrL9pOw@mail.gmail.com>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
	<CADm_Wcae7OK7kyXkfh+7XFrc2WYsv7T1Va3_E=5om+XYrL9pOw@mail.gmail.com>
Message-ID: <CABm2gDrQ8X70RGVmboyBzaLmd1ZJ77+-Hdr8PG153FcFJmJ29g@mail.gmail.com>

On Dec 16, 2015 10:08 PM, "Jeff Garzik via bitcoin-dev" <
bitcoin-dev at lists.linuxfoundation.org> wrote:
>
> On Wed, Dec 16, 2015 at 1:34 PM, Pieter Wuille <pieter.wuille at gmail.com>
wrote:
>>
>> On Wed, Dec 16, 2015 at 3:53 PM, Jeff Garzik via bitcoin-dev
>> <bitcoin-dev at lists.linuxfoundation.org> wrote:
>> > 2) If block size stays at 1M, the Bitcoin Core developer team should
sign a
>> > collective note stating their desire to transition to a new economic
policy,
>> > that of "healthy fee market" and strongly urge users to examine their
fee
>> > policies, wallet software, transaction volumes and other possible User
>> > impacting outcomes.
>>
>> You present this as if the Bitcoin Core development team is in charge
>> of deciding the network consensus rules, and is responsible for making
>> changes to it in order to satisfy economic demand. If that is the
>> case, Bitcoin has failed, in my opinion.
>
>
> This circles back to Problem #1:   Avoidance of a choice is a still a
choice - failing to ACK a MAX_BLOCK_SIZE increase still creates very real
Economic Change Event risk.

Unless the community is going to always avoid this "economic change event"
forever (effectively eliminating MAX_BLOCK_SIZE), this is going to happen
at some point. I assume those concerned with the "economic change" are only
scared about it because "nitcoin is still very young" of something like
that.
Since you advocate for delaying this event from happening, can you be
clearer about when do you think it would be ok to let the event happen?
What other event makes this event ok?

> Hitting a Fee Event is market changing, potentially reshuffling economic
actors to a notable degree.  Maintaining a short term economic policy of
fixed 1M supply in the face of rising transaction volume carries risks that
should be analyzed and communicated.

Assuming we adopt bip102, eventually you will be able to say exactly the
same about 2 MB. When does this "let's not change the economics" finishes
(if ever)?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/24def18f/attachment.html>

From jgarzik at gmail.com  Wed Dec 16 21:27:15 2015
From: jgarzik at gmail.com (Jeff Garzik)
Date: Wed, 16 Dec 2015 16:27:15 -0500
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
	Bitcoin
In-Reply-To: <CAPg+sBhUso0ddfYQMgwF7yX9_VoqP9CZN5h45t3eQi4v3m6f6A@mail.gmail.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<CAPg+sBhUso0ddfYQMgwF7yX9_VoqP9CZN5h45t3eQi4v3m6f6A@mail.gmail.com>
Message-ID: <CADm_WcYZq3nzfYMXfzkZsTCsgmzy4L_nYpa5Kax8uF_ajuUTiQ@mail.gmail.com>

On Wed, Dec 16, 2015 at 3:59 PM, Pieter Wuille <pieter.wuille at gmail.com>
wrote:

> On Wed, Dec 16, 2015 at 9:38 PM, Jeff Garzik via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org> wrote:
> > 4.3.  Observations on new block economic model
> >
> > SW complicates block economics by creating two separate, supply limited
> > resources.
>
> Not correct. I propose defining the virtual_block_size as base_size +
> witness_size * 0.25, and limiting virtual_block_size to 1M. This
> creates a single variable to optimize for. If accepted, miners are
> incentived to maximize fee per virtual_block_size instead of per size.
>

It is correct.  There are two separate sets of economic actors and levels
of contention for each set of space.

That is true regardless of the proposed miner selection algorithm.



> > 5.4.  Problem:   More complex economic policy, new game theory, new
> bidding
> > structure risks.
> >
> > Splitting blocks into two pieces, each with separate and distinct
> behaviors
> > and resource values, creates two fee markets.
>
> I believe you have misunderstood the proposal in that case.
>

See above.  There are two separate and distinct resource velocities and
demand levels in reality.  That creates two markets regardless of miner
selection algorithm in the block maker.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/17766776/attachment.html>

From pieter.wuille at gmail.com  Wed Dec 16 21:36:09 2015
From: pieter.wuille at gmail.com (Pieter Wuille)
Date: Wed, 16 Dec 2015 22:36:09 +0100
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
	Bitcoin
In-Reply-To: <CADm_WcYZq3nzfYMXfzkZsTCsgmzy4L_nYpa5Kax8uF_ajuUTiQ@mail.gmail.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<CAPg+sBhUso0ddfYQMgwF7yX9_VoqP9CZN5h45t3eQi4v3m6f6A@mail.gmail.com>
	<CADm_WcYZq3nzfYMXfzkZsTCsgmzy4L_nYpa5Kax8uF_ajuUTiQ@mail.gmail.com>
Message-ID: <CAPg+sBiVVcNNHuV9e1SaPoDSMEwjZHL7tQiszxbE2SQYp1Ongw@mail.gmail.com>

On Wed, Dec 16, 2015 at 10:27 PM, Jeff Garzik <jgarzik at gmail.com> wrote:
>> Not correct. I propose defining the virtual_block_size as base_size +
>> witness_size * 0.25, and limiting virtual_block_size to 1M. This
>> creates a single variable to optimize for. If accepted, miners are
>> incentived to maximize fee per virtual_block_size instead of per size.
>
>
> It is correct.  There are two separate sets of economic actors and levels of
> contention for each set of space.
>
> That is true regardless of the proposed miner selection algorithm.

Maybe I haven't explained this properly, so consider this example:

A miner receives sets of 200 byte transactions with all identical
fees. Non-witness ones (whose virtual size is thus 200 bytes) and a
witness one (where 120 of the 200 bytes are witness data, so its
virtual size is 80 + 120*0.25 = 110 bytes).

The consensus rules would limit 1) the base size to 1000000 bytes 2)
the virtual size to 1000000 bytes. However, as the virtual size is
defined as the sum of the base size plus a non-negative number,
satisfying (2) always implies satisfying (1).

Thus, the miners' best strategy is to accept the witness transactions,
as it allows 1000000/110=9090 transactions rather than
1000000/200=5000.

In fact, the optimal fee maximizing strategy is always to maximize fee
per virtual size.

-- 
Pieter

From jgarzik at gmail.com  Wed Dec 16 21:36:18 2015
From: jgarzik at gmail.com (Jeff Garzik)
Date: Wed, 16 Dec 2015 16:36:18 -0500
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <CABm2gDrQ8X70RGVmboyBzaLmd1ZJ77+-Hdr8PG153FcFJmJ29g@mail.gmail.com>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
	<CADm_Wcae7OK7kyXkfh+7XFrc2WYsv7T1Va3_E=5om+XYrL9pOw@mail.gmail.com>
	<CABm2gDrQ8X70RGVmboyBzaLmd1ZJ77+-Hdr8PG153FcFJmJ29g@mail.gmail.com>
Message-ID: <CADm_WcZncVOV_F7dGp4Xs15xgKwdixdn8wWHML8whRfR_1uNnw@mail.gmail.com>

On Wed, Dec 16, 2015 at 4:24 PM, Jorge Tim?n <jtimon at jtimon.cc> wrote:

> Assuming we adopt bip102, eventually you will be able to say exactly the
> same about 2 MB. When does this "let's not change the economics" finishes
> (if ever)?
>

The question is answered in the original email.

In the short term, the risk of a Fee Event and lack of solid post-Fee-Event
economic plan implies "short term bump" reduces those risks.

It is also true - as noted in [1], a bump does create the danger of long
term moral hazard.

This is why a bump should be accompanied with at least a weak public
commitment to flexcap or a similar long term proposal, as the original
email recommended.  Communicate clearly the conditions for future change,
then iterate as needs and feedback indicate.



[1] http://gtf.org/garzik/bitcoin/BIP100-blocksizechangeproposal.pdf
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/eecf9243/attachment.html>

From jgarzik at gmail.com  Wed Dec 16 22:09:58 2015
From: jgarzik at gmail.com (Jeff Garzik)
Date: Wed, 16 Dec 2015 17:09:58 -0500
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
	Bitcoin
In-Reply-To: <CAPg+sBiVVcNNHuV9e1SaPoDSMEwjZHL7tQiszxbE2SQYp1Ongw@mail.gmail.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<CAPg+sBhUso0ddfYQMgwF7yX9_VoqP9CZN5h45t3eQi4v3m6f6A@mail.gmail.com>
	<CADm_WcYZq3nzfYMXfzkZsTCsgmzy4L_nYpa5Kax8uF_ajuUTiQ@mail.gmail.com>
	<CAPg+sBiVVcNNHuV9e1SaPoDSMEwjZHL7tQiszxbE2SQYp1Ongw@mail.gmail.com>
Message-ID: <CADm_WcZbbv9zy_5kN264GhYC_kBBr+Leoi0y1PA4pm23CaW3QQ@mail.gmail.com>

Maybe a new analogy helps.

SW presents a blended price and blended basket of two goods.  You can
interact with the Service through the blended price, but that does not
erase the fact that the basket contains two separate from similar resources.

A different set of economic actors uses one resource, and/or both.  There
are explicit incentives to shift actors from solely using one resource to
using both.

The fact that separate sets of economic actors and incentives exist is
sufficient to prove it is indeed a basket of goods, not a single good.


On Wed, Dec 16, 2015 at 4:36 PM, Pieter Wuille <pieter.wuille at gmail.com>
wrote:

> Thus, the miners' best strategy is to accept the witness transactions,
> as it allows 1000000/110=9090 transactions rather than
> 1000000/200=5000.
>

Under your blended algorithm, this seems reasonable as a first pass.



> In fact, the optimal fee maximizing strategy is always to maximize fee
> per virtual size.
>

This is a microscopic, not macroscopic analysis.  Externalities and long
term incentives can severely perturb or invalidate that line of thinking.

Typical counter-example:  Many miners are perfectly happy with very low
fees to encourage long term growth of their bitcoin income through network
effect growth -- rendering fee micro-optimizations largely in the realm of
DoS prevention rather than miner incentive.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/0f083114/attachment.html>

From jgarzik at gmail.com  Wed Dec 16 22:10:53 2015
From: jgarzik at gmail.com (Jeff Garzik)
Date: Wed, 16 Dec 2015 17:10:53 -0500
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
	Bitcoin
In-Reply-To: <CADm_WcZbbv9zy_5kN264GhYC_kBBr+Leoi0y1PA4pm23CaW3QQ@mail.gmail.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<CAPg+sBhUso0ddfYQMgwF7yX9_VoqP9CZN5h45t3eQi4v3m6f6A@mail.gmail.com>
	<CADm_WcYZq3nzfYMXfzkZsTCsgmzy4L_nYpa5Kax8uF_ajuUTiQ@mail.gmail.com>
	<CAPg+sBiVVcNNHuV9e1SaPoDSMEwjZHL7tQiszxbE2SQYp1Ongw@mail.gmail.com>
	<CADm_WcZbbv9zy_5kN264GhYC_kBBr+Leoi0y1PA4pm23CaW3QQ@mail.gmail.com>
Message-ID: <CADm_WcYzPvyi1vZmvm0J-JQuzPOqva6N_JLqC-mU4=p4L6FNkw@mail.gmail.com>

On Wed, Dec 16, 2015 at 5:09 PM, Jeff Garzik <jgarzik at gmail.com> wrote:

> SW presents a blended price and blended basket of two goods.  You can
> interact with the Service through the blended price, but that does not
> erase the fact that the basket contains two separate from similar resources.
>

separate-but-similar
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/30baa066/attachment.html>

From jgarzik at gmail.com  Wed Dec 16 22:27:40 2015
From: jgarzik at gmail.com (Jeff Garzik)
Date: Wed, 16 Dec 2015 17:27:40 -0500
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <9a02d94fbc78afaa3e9668e0294eef64@xbt.hk>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<9a02d94fbc78afaa3e9668e0294eef64@xbt.hk>
Message-ID: <CADm_Wca9zTdTc2gvTxrWkFjfA49KhbU_=uNXh_mZ+QYXGZ6wWg@mail.gmail.com>

On Wed, Dec 16, 2015 at 1:36 PM, jl2012 <jl2012 at xbt.hk> wrote:

> 4. In the miners round table on the second day, one of the devs mentioned
> that he didn't want to be seen as the decision maker of Bitcoin. On the
> other hand, Chinese miners repeatedly mentioned that they want several
> concrete proposals from devs which they could choose. I see no
> contradiction between these 2 viewpoints.
>

This was a very interesting dynamic, and seems fair (menu).



> 6. I believe we should avoid a radical "Economic Change Event" at least in
> the next halving cycle, as Bitcoin was designed to bootstrap the adoption
> by high mining reward in the beginning. For this reason, I support an early
> and conservative increase, such as BIP102 or 2-4-8. 2MB is accepted by most
> people and it's better than nothing for BIP101 proponents. By "early" I
> mean to be effective by May, at least 2 months before the halving.
>

That was precisely my logic for picking May 5 as the hard fork date.  Some
buffer before halving, enough for caution and iteration in the meantime.






>
> (c) My most optimistic guess is SW will be ready in 6 months, which will
> be very close to halving and potential tx volume burst. And it may not be
> done in 2016, as it does not only involve consensus code, but also change
> in the p2p protocol and wallet design
>

Not just wallet design -- you have to game through the standard steps of:
 update dev lib (bitcoin-core.js/bitcoinj) + release cycle, update app +
release cycle, for most actors in the ecosystem, on top of the Bitcoin Core
roll out.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/9123a94f/attachment.html>

From lf-lists at mattcorallo.com  Wed Dec 16 22:29:39 2015
From: lf-lists at mattcorallo.com (Matt Corallo)
Date: Wed, 16 Dec 2015 22:29:39 +0000
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
	Bitcoin
In-Reply-To: <CADL_X_dm-Sn2Mo7r746FyH9655cKBajtj-TJnhS47o2Oa-M7yQ@mail.gmail.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<49257841-66C8-4EF7-980B-73DC604CA591@mattcorallo.com>
	<CADL_X_dm-Sn2Mo7r746FyH9655cKBajtj-TJnhS47o2Oa-M7yQ@mail.gmail.com>
Message-ID: <C8923572-D307-423B-8D35-8F1AEA8627E0@mattcorallo.com>

We should probably start by defining "economically important". To me, it's pretty clear that every, or at least around 99% of, " economically important" node have upgraded by the time the fork kicks in, with way more than sufficient time given to everyone to upgrade (minding that this is not an emergency situation and that people have lives and many Bitcoin services are hobby projects and upgrading isn't always as easy as just restarting your node). I'd define "economically important" as any node that is used for anything more than simply "being a node" (ie people who started a node to provide resources to the network, and only using their node for that). Note, of course, that we should avoid breaking all such "non-economically important" nodes, but breaking many of them is not a big deal. Note that my proposal includes nodes such as the one doing transaction selection for the relay network. Though it is not used for payments, if it is not upgraded, things will break.

With this definition in mind, I think a year is an aggressive timeline.

On December 16, 2015 1:51:47 PM PST, Jameson Lopp <jameson.lopp at gmail.com> wrote:
>On Wed, Dec 16, 2015 at 12:50 PM, Matt Corallo via bitcoin-dev <
>bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> A large part of your argument is that SW will take longer to deploy
>than a
>> hard fork, but I completely disagree. Though I do not agree with some
>> people claiming we can deploy SW significantly faster than a hard
>fork,
>> once the code is ready (probably a six month affair) we can get it
>deployed
>> very quickly. It's true the ecosystem may take some time to upgrade,
>but I
>> see that as a feature, not a bug - we can build up some fee pressure
>with
>> an immediate release valve available for people to use if they want
>to pay
>> fewer fees.
>>
>> On the other hand, a hard fork, while simpler for the ecosystem to
>upgrade
>> to, is a 1-2 year affair (after the code is shipped, so at least
>1.5-2.5
>> from today if we all put off heads down and work). One thing that has
>> concerned me greatly through this whole debate is how quickly people
>seem
>> to think we can roll out a hard fork. Go look at the distribution of
>node
>> versions on the network today and work backwards to get nearly every
>node
>> upgraded... Even with a year between fork-version-release and
>> fork-activation, we'd still kill a bunch of nodes and instead of
>reducing
>> their security model, lead them to be outright robbed.
>>
>>
>Over a year seems to be an extraordinarily long time frame is for
>deploying
>a hard fork. It looks like <https://bitnodes.21.co/dashboard/?days=365>
>75%
>of reachable nodes have upgraded in the past 6 months while as much as
>25%
>may not have been upgraded in over a year. However, viewing historical
>stats of version upgrades doesn't seem to be an appropriate comparison
>because node operators have never been faced with the same incentive to
>upgrade. We can point to unintentional forks in the past that have been
>resolved fairly quickly by reaching out to miners, but it's also a poor
>comparison. Unfortunately, we have no way of knowing what percentage of
>nodes are economically important - a great deal of them may be running
>and
>not even be used by the operators.
>
>Perhaps it would be better if we were to formalize the expectations for
>full node operators, but it seems to me that node operators have a
>responsibility to keep themselves informed and decide when it is
>appropriate to update their software. I'm not so sure that it's the
>rest of
>the ecosystem's responsibility to wait around for laggards.
>
>- Jameson
>
>On December 16, 2015 12:38:30 PM PST, Jeff Garzik via bitcoin-dev <
>> bitcoin-dev at lists.linuxfoundation.org> wrote:
>>>
>>>
>>> 1. Summary
>>>
>>> Segregated Witness (SegWitness, SW) is being presented in the
>context of
>>> Scaling Bitcoin.  It has useful attributes, notably addressing a
>major
>>> malleability vector, but is not a short term scaling solution.
>>>
>>>
>>> 2. Definitions
>>>
>>> Import Fee Event, ECE, TFM, FFM from previous email.
>>>
>>> Older clients - Any software not upgraded to SW
>>>
>>> Newer clients - Upgraded, SW aware software
>>>
>>>
>>> Block size - refers to the core block economic resource limit ed by
>>> MAX_BLOCK_SIZE.  Witness data (or extension block data) is excluded.
>>> Requires a hard fork to change.
>>>
>>> Core block - Current bitcoin block, with upper bound MAX_BLOCK_SIZE.
> Not
>>> changed by SW.
>>>
>>>
>>> Extended transaction - Newer, upgraded version of transaction data
>format.
>>>
>>> Extended block - Newer, upgraded version of block data format.
>>>
>>>
>>> EBS - Extended block size.  Block size seen by newer clients.
>>>
>>>
>>> 3. Context of analysis
>>>
>>> One proposal presents SW *in lieu of* a hard fork block size
>increase.
>>> This email focuses directly on that.
>>>
>>> Useful features outside block size context, such as
>anti-malleability or
>>> fraud proof features, are not covered in depth.
>>>
>>>
>>> 4.1.  Observations on data structure formats and views
>>>
>>> SW creates two *views* of each transaction and block.  SW has blocks
>and
>>> extended blocks.  Similarly, there exists transactions and extended
>>> transactions.
>>>
>>> This view is rendered to clients depending on compatibility level. 
>Newer
>>> clients see extended blocks and extended transactions.  Older
>clients see
>>> blocks (limit 1M), and do not see extended blocks.  Older clients
>see
>>> upgraded transactions as unsigned, anyone-can-pay transactions.
>>>
>>> Each extended transaction exists in two states, one unsigned and one
>>> signed, each of which passes validation as a valid bitcoin
>transaction.
>>>
>>>
>>> 4.2.  Observations on behavior of older transaction creation
>>>
>>> Transactions created by older clients will not use the extended
>>> transaction format.  All data is stored the standard 1M block as
>today.
>>>
>>>
>>> 4.3.  Observations on new block economic model
>>>
>>> SW complicates block economics by creating two separate, supply
>limited
>>> resources.
>>>
>>> The core block economic resource is heavily contended.  Older
>clients use
>>> core blocks exclusively.  Newer clients use core block s more
>>> conservatively, storing as much data as possible in extended blocks.
>>>
>>> The extended block economic resource is less heavily contended,
>though
>>> that of course grows over time as clients upgrade.
>>>
>>> Because core blocks are more heavily contended, it is presumed that
>older
>>> clients will pay a higher fee than newer clients (subject to
>elasticity
>>> etc.).
>>>
>>>
>>> 5.1.  Problem:  Pace of roll-out will be slow - Whole Ecosystem must
>be
>>> considered.
>>>
>>> The current apparent proposal is to roll out Segregated Witness as a
>soft
>>> fork, and keep block size at 1M.
>>>
>>> The roll-out pace cannot simply be judged by soft fork speed - which
>is
>>> months at best.  Analysis must the layers above:  Updating
>bitcoin-core
>>> (JS) and bitcoinj (Java), and then the timelines to roll out those
>updates
>>> to apps, and then the timeline to update those apps to create
>extended
>>> transactions.
>>>
>>> Overall, wallet software and programmer libraries must be upgraded
>to
>>> make use of this new format, adding many more months (12+ in some
>stacks)
>>> to the roll out timeline.  In the meantime, clients continue to
>contend
>>> entirely for core block space.
>>>
>>>
>>> 5.2.  Problem:   Hard fork to bigger block size Just Works(tm) with
>most
>>> software, unlike SW.
>>>
>>> A simple hard fork such as BIP 102 is automatically compatible with
>the
>>> vast range of today's ecosystem software.
>>>
>>> SW requires merchants to upgrade almost immediately, requires wallet
>and
>>> other peripheral software upgrades to make use of.  Other updates
>are
>>> opt-in and occur more slowly.  BIP 70 processors need some updates.
>>>
>>> The number of LOC that must change for BIP 102 is very small, and
>the
>>> problem domain well known, versus SW.
>>>
>>>
>>> 5.3.  Problem:   Due to pace, Fee Event not forestalled.
>>>
>>> Even presuming SW is merged into Bitcoin Core tomorrow, this does
>not
>>> address the risk of a Fee Event and associated Economic Change in
>the
>>> coming months.
>>>
>>>
>>> 5.4.  Problem:   More complex economic policy, new game theory, new
>>> bidding structure risks.
>>>
>>> Splitting blocks into two pieces, each with separate and distinct
>>> behaviors and resource values, creates *two fee markets.*
>>>
>>> Having two pricing strata within each block has certainly feasible -
>that
>>> is the current mining policy of (1) fee/KB followed by (2)
>priority/age.
>>>
>>> Valuable or not - e.g. incentivizing older clients to upgrade - the
>fact
>>> remains that SW creates a more-complex bidding structure by creating
>a
>>> second economic resource.
>>>
>>> *This is clearly a change to a new economic policy* with standard
>risks
>>> associated with that.  Will that induce an Economic C hange Event
>(see def
>>> last email)?  *Unlikely*, due to slow rollout pace.
>>>
>>>
>>> 5.5.  Problem:  Current SW mining algorithm needs improvement
>>>
>>> Current SW block template maker does a reasonable job, but makes
>some
>>> naive assumptions about the fee market across an entire extended
>block.
>>> This is a mismatch with the economic reality (just described).
>>>
>>> 5.6.   Problem:  New, under-analyzed attack surfaces
>>>
>>> Less significant and fundamental but still worth noting.
>>>
>>> This is not a fundamental SW problem, but simply standard complexity
>risk
>>> factors:  splitting the signatures away from transactions, and
>creating a
>>> new apparently-unsigned version of the transaction opens t he
>possibility
>>> of some network attacks which cause some clients to degrade down
>from
>>> extended block to core block mode temporarily.
>>>
>>> There is a chance of a failure mode that fools older clients into
>>> thinking fraudulent data is valid (judgement: unlikely vis hashpower
>but
>>> not impossible)
>>>
>>> 6. Conclusions and recommendations
>>>
>>> It seems unlikely that SW provides scaling in the short term, and SW
>>> introduces new economics complexities.
>>>
>>> A "short term bump" hard fork block size increase addresses economic
>and
>>> ecosystem risks that SW does not.
>>>
>>> Bump + SW should proce ed in parallel, independent tracks, as
>orthogonal
>>> issues.
>>>
>>>
>>> 7. Appendix - Other SW comments
>>>
>>> Hard forks provide much stronger validation, and ensure the network
>>> operates at a fully trustless level.
>>>
>>> SW hard fork is preferred, versus soft fork.  Soft forking SW places
>a
>>> huge amount of trust on miners to validate transaction signatures,
>versus
>>> the rest of the network, as the network slowly upgrades to newer
>clients.
>>>
>>> An SW hard fork could also add several zero-filled placeholders in a
>>> merkle tree for future use.
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> ------------------------------
>>>
>>> bitcoin-dev mailing list
>>> bitcoin-dev at lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/d0d4287d/attachment-0001.html>

From lf-lists at mattcorallo.com  Wed Dec 16 22:32:57 2015
From: lf-lists at mattcorallo.com (Matt Corallo)
Date: Wed, 16 Dec 2015 22:32:57 +0000
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
	Bitcoin
In-Reply-To: <CADL_X_dm-Sn2Mo7r746FyH9655cKBajtj-TJnhS47o2Oa-M7yQ@mail.gmail.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<49257841-66C8-4EF7-980B-73DC604CA591@mattcorallo.com>
	<CADL_X_dm-Sn2Mo7r746FyH9655cKBajtj-TJnhS47o2Oa-M7yQ@mail.gmail.com>
Message-ID: <A4D54C2D-3462-486F-9EEE-9BFAA84D4116@mattcorallo.com>

As for "the ecosystem waiting around for laggards", yes, it is absolutely the ecosystems y responsibility to not take actions that will result in people losing money without providing them far more than enough opportunity to fix it. One of the absolute most important features of Bitcoin is that, if you're running a full node, you are provided reasonable security against accepting invalid transactions.

On December 16, 2015 1:51:47 PM PST, Jameson Lopp <jameson.lopp at gmail.com> wrote:
>On Wed, Dec 16, 2015 at 12:50 PM, Matt Corallo via bitcoin-dev <
>bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> A large part of your argument is that SW will take longer to deploy
>than a
>> hard fork, but I completely disagree. Though I do not agree with some
>> people claiming we can deploy SW significantly faster than a hard
>fork,
>> once the code is ready (probably a six month affair) we can get it
>deployed
>> very quickly. It's true the ecosystem may take some time to upgrade,
>but I
>> see that as a feature, not a bug - we can build up some fee pressure
>with
>> an immediate release valve available for people to use if they want
>to pay
>> fewer fees.
>>
>> On the other hand, a hard fork, while simpler for the ecosystem to
>upgrade
>> to, is a 1-2 year affair (after the code is shipped, so at least
>1.5-2.5
>> from today if we all put off heads down and work). One thing that has
>> concerned me greatly through this whole debate is how quickly people
>seem
>> to think we can roll out a hard fork. Go look at the distribution of
>node
>> versions on the network today and work backwards to get nearly every
>node
>> upgraded... Even with a year between fork-version-release and
>> fork-activation, we'd still kill a bunch of nodes and instead of
>reducing
>> their security model, lead them to be outright robbed.
>>
>>
>Over a year seems to be an extraordinarily long time frame is for
>deploying
>a hard fork. It looks like <https://bitnodes.21.co/dashboard/?days=365>
>75%
>of reachable nodes have upgraded in the past 6 months while as much as
>25%
>may not have been upgraded in over a year. However, viewing historical
>stats of version upgrades doesn't seem to be an appropriate comparison
>because node operators have never been faced with the same incentive to
>upgrade. We can point to unintentional forks in the past that have been
>resolved fairly quickly by reaching out to miners, but it's also a poor
>comparison. Unfortunately, we have no way of knowing what percentage of
>nodes are economically important - a great deal of them may be running
>and
>not even be used by the operators.
>
>Perhaps it would be better if we were to formalize the expectations for
>full node operators, but it seems to me that node operators have a
>responsibility to keep themselves informed and decide when it is
>appropriate to update their software. I'm not so sure that it's the
>rest of
>the ecosystem's responsibility to wait around for laggards.
>
>- Jameson
>
>On December 16, 2015 12:38:30 PM PST, Jeff Garzik via bitcoin-dev <
>> bitcoin-dev at lists.linuxfoundation.org> wrote:
>>>
>>>
>>> 1. Summary
>>>
>>> Segregated Witness (SegWitness, SW) is being presented in the
>context of
>>> Scaling Bitcoin.  It has useful attributes, notably addressing a
>major
>>> malleability vector, but is not a short term scaling solution.
>>>
>>>
>>> 2. Definitions
>>>
>>> Import Fee Event, ECE, TFM, FFM from previous email.
>>>
>>> Older clients - Any software not upgraded to SW
>>>
>>> Newer clients - Upgraded, SW aware software
>>>
>>>
>>> Block size - refers to the core block economic resource limit ed by
>>> MAX_BLOCK_SIZE.  Witness data (or extension block data) is excluded.
>>> Requires a hard fork to change.
>>>
>>> Core block - Current bitcoin block, with upper bound MAX_BLOCK_SIZE.
> Not
>>> changed by SW.
>>>
>>>
>>> Extended transaction - Newer, upgraded version of transaction data
>format.
>>>
>>> Extended block - Newer, upgraded version of block data format.
>>>
>>>
>>> EBS - Extended block size.  Block size seen by newer clients.
>>>
>>>
>>> 3. Context of analysis
>>>
>>> One proposal presents SW *in lieu of* a hard fork block size
>increase.
>>> This email focuses directly on that.
>>>
>>> Useful features outside block size context, such as
>anti-malleability or
>>> fraud proof features, are not covered in depth.
>>>
>>>
>>> 4.1.  Observations on data structure formats and views
>>>
>>> SW creates two *views* of each transaction and block.  SW has blocks
>and
>>> extended blocks.  Similarly, there exists transactions and extended
>>> transactions.
>>>
>>> This view is rendered to clients depending on compatibility level. 
>Newer
>>> clients see extended blocks and extended transactions.  Older
>clients see
>>> blocks (limit 1M), and do not see extended blocks.  Older clients
>see
>>> upgraded transactions as unsigned, anyone-can-pay transactions.
>>>
>>> Each extended transaction exists in two states, one unsigned and one
>>> signed, each of which passes validation as a valid bitcoin
>transaction.
>>>
>>>
>>> 4.2.  Observations on behavior of older transaction creation
>>>
>>> Transactions created by older clients will not use the extended
>>> transaction format.  All data is stored the standard 1M block as
>today.
>>>
>>>
>>> 4.3.  Observations on new block economic model
>>>
>>> SW complicates block economics by creating two separate, supply
>limited
>>> resources.
>>>
>>> The core block economic resource is heavily contended.  Older
>clients use
>>> core blocks exclusively.  Newer clients use core block s more
>>> conservatively, storing as much data as possible in extended blocks.
>>>
>>> The extended block economic resource is less heavily contended,
>though
>>> that of course grows over time as clients upgrade.
>>>
>>> Because core blocks are more heavily contended, it is presumed that
>older
>>> clients will pay a higher fee than newer clients (subject to
>elasticity
>>> etc.).
>>>
>>>
>>> 5.1.  Problem:  Pace of roll-out will be slow - Whole Ecosystem must
>be
>>> considered.
>>>
>>> The current apparent proposal is to roll out Segregated Witness as a
>soft
>>> fork, and keep block size at 1M.
>>>
>>> The roll-out pace cannot simply be judged by soft fork speed - which
>is
>>> months at best.  Analysis must the layers above:  Updating
>bitcoin-core
>>> (JS) and bitcoinj (Java), and then the timelines to roll out those
>updates
>>> to apps, and then the timeline to update those apps to create
>extended
>>> transactions.
>>>
>>> Overall, wallet software and programmer libraries must be upgraded
>to
>>> make use of this new format, adding many more months (12+ in some
>stacks)
>>> to the roll out timeline.  In the meantime, clients continue to
>contend
>>> entirely for core block space.
>>>
>>>
>>> 5.2.  Problem:   Hard fork to bigger block size Just Works(tm) with
>most
>>> software, unlike SW.
>>>
>>> A simple hard fork such as BIP 102 is automatically compatible with
>the
>>> vast range of today's ecosystem software.
>>>
>>> SW requires merchants to upgrade almost immediately, requires wallet
>and
>>> other peripheral software upgrades to make use of.  Other updates
>are
>>> opt-in and occur more slowly.  BIP 70 processors need some updates.
>>>
>>> The number of LOC that must change for BIP 102 is very small, and
>the
>>> problem domain well known, versus SW.
>>>
>>>
>>> 5.3.  Problem:   Due to pace, Fee Event not forestalled.
>>>
>>> Even presuming SW is merged into Bitcoin Core tomorrow, this does
>not
>>> address the risk of a Fee Event and associated Economic Change in
>the
>>> coming months.
>>>
>>>
>>> 5.4.  Problem:   More complex economic policy, new game theory, new
>>> bidding structure risks.
>>>
>>> Splitting blocks into two pieces, each with separate and distinct
>>> behaviors and resource values, creates *two fee markets.*
>>>
>>> Having two pricing strata within each block has certainly feasible -
>that
>>> is the current mining policy of (1) fee/KB followed by (2)
>priority/age.
>>>
>>> Valuable or not - e.g. incentivizing older clients to upgrade - the
>fact
>>> remains that SW creates a more-complex bidding structure by creating
>a
>>> second economic resource.
>>>
>>> *This is clearly a change to a new economic policy* with standard
>risks
>>> associated with that.  Will that induce an Economic C hange Event
>(see def
>>> last email)?  *Unlikely*, due to slow rollout pace.
>>>
>>>
>>> 5.5.  Problem:  Current SW mining algorithm needs improvement
>>>
>>> Current SW block template maker does a reasonable job, but makes
>some
>>> naive assumptions about the fee market across an entire extended
>block.
>>> This is a mismatch with the economic reality (just described).
>>>
>>> 5.6.   Problem:  New, under-analyzed attack surfaces
>>>
>>> Less significant and fundamental but still worth noting.
>>>
>>> This is not a fundamental SW problem, but simply standard complexity
>risk
>>> factors:  splitting the signatures away from transactions, and
>creating a
>>> new apparently-unsigned version of the transaction opens t he
>possibility
>>> of some network attacks which cause some clients to degrade down
>from
>>> extended block to core block mode temporarily.
>>>
>>> There is a chance of a failure mode that fools older clients into
>>> thinking fraudulent data is valid (judgement: unlikely vis hashpower
>but
>>> not impossible)
>>>
>>> 6. Conclusions and recommendations
>>>
>>> It seems unlikely that SW provides scaling in the short term, and SW
>>> introduces new economics complexities.
>>>
>>> A "short term bump" hard fork block size increase addresses economic
>and
>>> ecosystem risks that SW does not.
>>>
>>> Bump + SW should proce ed in parallel, independent tracks, as
>orthogonal
>>> issues.
>>>
>>>
>>> 7. Appendix - Other SW comments
>>>
>>> Hard forks provide much stronger validation, and ensure the network
>>> operates at a fully trustless level.
>>>
>>> SW hard fork is preferred, versus soft fork.  Soft forking SW places
>a
>>> huge amount of trust on miners to validate transaction signatures,
>versus
>>> the rest of the network, as the network slowly upgrades to newer
>clients.
>>>
>>> An SW hard fork could also add several zero-filled placeholders in a
>>> merkle tree for future use.
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> ------------------------------
>>>
>>> bitcoin-dev mailing list
>>> bitcoin-dev at lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/f5ad5c11/attachment.html>

From jgarzik at gmail.com  Thu Dec 17 02:21:22 2015
From: jgarzik at gmail.com (Jeff Garzik)
Date: Wed, 16 Dec 2015 21:21:22 -0500
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
	Bitcoin
In-Reply-To: <49257841-66C8-4EF7-980B-73DC604CA591@mattcorallo.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<49257841-66C8-4EF7-980B-73DC604CA591@mattcorallo.com>
Message-ID: <CADm_WcYdAHP95mrxH0CjvxKaV12rXEdBXf-L5QtKHuBL1ndFaQ@mail.gmail.com>

On Wed, Dec 16, 2015 at 3:50 PM, Matt Corallo <lf-lists at mattcorallo.com>
wrote:

> A large part of your argument is that SW will take longer to deploy than a
> hard fork, but I completely disagree. Though I do not agree with some
> people claiming we can deploy SW significantly faster than a hard fork,
> once the code is ready (probably a six month affair) we can get it deployed
> very quickly. It's true the ecosystem may take some time to upgrade, but I
> see that as a feature, not a bug - we can build up some fee pressure with
> an immediate release valve available for people to use if they want to pay
> fewer fees.
>

That's taking a big risk.  "Build up some fee pressure" is essentially
risking a Fee Event if uptake is slower than planned, or traffic is greater
than expected.



>
> On the other hand, a hard fork, while simpler for the ecosystem to upgrade
> to, is a 1-2 year affair (after the code is shipped, so at least 1.5-2.5
> from today if we all put off heads down and work). One thing that has
> concerned me greatly through this whole debate is how quickly people seem
> to think we can roll out a hard fork. Go look at the distribution of node
> versions on the network today and work backwards to get nearly every node
> upgraded... Even with a year between fork-version-release and
> fork-activation, we'd still kill a bunch of nodes and instead of reducing
> their security model, lead them to be outright robbed.
>

A hard fork will never achieve 100%  There are many credible folks and
estimates who feel a May hard fork is reasonable and doable.

Further, hard forks restore the full trustless nature of the post-hard-fork
nodes.  Soft forks continually erode that.  That's why SW should come via
hard fork.  The end result is more secure - 100% validation of witness
transactions.

If regular hard fork plans are proposed in public, many months in advance,
there is plenty of time for the community to react.  Hard forks create a
more predictable market and environment for Users, and a more secure
network.

Further, even if you believe SW makes hard fork unnecessary, it is the
responsible thing to code and communicate to users the plan for a Fee Event
just in case SW uptake and extension block use does not match theoretical
projections of SW proponents.

Finally, SW does not eliminate and is orthogonal to Short Term Problem #1
(orig. email - drift into ECE)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/f044cec8/attachment-0001.html>

From elombrozo at gmail.com  Thu Dec 17 02:44:56 2015
From: elombrozo at gmail.com (Eric Lombrozo)
Date: Wed, 16 Dec 2015 18:44:56 -0800
Subject: [bitcoin-dev] Segregated Witness in the context of
	Scaling	Bitcoin
In-Reply-To: <CADm_WcYdAHP95mrxH0CjvxKaV12rXEdBXf-L5QtKHuBL1ndFaQ@mail.gmail.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<49257841-66C8-4EF7-980B-73DC604CA591@mattcorallo.com>
	<CADm_WcYdAHP95mrxH0CjvxKaV12rXEdBXf-L5QtKHuBL1ndFaQ@mail.gmail.com>
Message-ID: <FC95D096-74AE-4B1C-B11B-1CB718538E7D@gmail.com>

There are no good short-term scaling solutions...this is a very hard problem that necessarily requires a lot of out-of-the-box thinking, something 2015 has seen a LOT of...and I'm optimistic about the ideas presented thus far.

At least SW *is* a scaling solution (albeit most of the important benefits are long term). The issue of fee events has nothing to do with scaling - it has to do with economics...specifically whether we should be subsidizing transactions, who should pay the bill for it, etc. My own personal opinion is that increasing validation costs works against adoption, not for it...even if it artificially keeps fees low - and we'll have to deal with a fee event sooner or later anyhow. You may disagree with my opinion, but please, let's stop confounding the economic issues with actual scaling.

On December 16, 2015 6:21:22 PM PST, Jeff Garzik via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
>On Wed, Dec 16, 2015 at 3:50 PM, Matt Corallo
><lf-lists at mattcorallo.com>
>wrote:
>
>> A large part of your argument is that SW will take longer to deploy
>than a
>> hard fork, but I completely disagree. Though I do not agree with some
>> people claiming we can deploy SW significantly faster than a hard
>fork,
>> once the code is ready (probably a six month affair) we can get it
>deployed
>> very quickly. It's true the ecosystem may take some time to upgrade,
>but I
>> see that as a feature, not a bug - we can build up some fee pressure
>with
>> an immediate release valve available for people to use if they want
>to pay
>> fewer fees.
>>
>
>That's taking a big risk.  "Build up some fee pressure" is essentially
>risking a Fee Event if uptake is slower than planned, or traffic is
>greater
>than expected.
>
>
>
>>
>> On the other hand, a hard fork, while simpler for the ecosystem to
>upgrade
>> to, is a 1-2 year affair (after the code is shipped, so at least
>1.5-2.5
>> from today if we all put off heads down and work). One thing that has
>> concerned me greatly through this whole debate is how quickly people
>seem
>> to think we can roll out a hard fork. Go look at the distribution of
>node
>> versions on the network today and work backwards to get nearly every
>node
>> upgraded... Even with a year between fork-version-release and
>> fork-activation, we'd still kill a bunch of nodes and instead of
>reducing
>> their security model, lead them to be outright robbed.
>>
>
>A hard fork will never achieve 100%  There are many credible folks and
>estimates who feel a May hard fork is reasonable and doable.
>
>Further, hard forks restore the full trustless nature of the
>post-hard-fork
>nodes.  Soft forks continually erode that.  That's why SW should come
>via
>hard fork.  The end result is more secure - 100% validation of witness
>transactions.
>
>If regular hard fork plans are proposed in public, many months in
>advance,
>there is plenty of time for the community to react.  Hard forks create
>a
>more predictable market and environment for Users, and a more secure
>network.
>
>Further, even if you believe SW makes hard fork unnecessary, it is the
>responsible thing to code and communicate to users the plan for a Fee
>Event
>just in case SW uptake and extension block use does not match
>theoretical
>projections of SW proponents.
>
>Finally, SW does not eliminate and is orthogonal to Short Term Problem
>#1
>(orig. email - drift into ECE)
>
>
>------------------------------------------------------------------------
>
>_______________________________________________
>bitcoin-dev mailing list
>bitcoin-dev at lists.linuxfoundation.org
>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/e2c4511c/attachment.html>

From jgarzik at gmail.com  Thu Dec 17 02:58:35 2015
From: jgarzik at gmail.com (Jeff Garzik)
Date: Wed, 16 Dec 2015 21:58:35 -0500
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
	Bitcoin
In-Reply-To: <FC95D096-74AE-4B1C-B11B-1CB718538E7D@gmail.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<49257841-66C8-4EF7-980B-73DC604CA591@mattcorallo.com>
	<CADm_WcYdAHP95mrxH0CjvxKaV12rXEdBXf-L5QtKHuBL1ndFaQ@mail.gmail.com>
	<FC95D096-74AE-4B1C-B11B-1CB718538E7D@gmail.com>
Message-ID: <CADm_WcYhPqZZ5KQ7DxyFgkk5td4ircrXwArg_guWDPWPtnCxhw@mail.gmail.com>

On Wed, Dec 16, 2015 at 9:44 PM, Eric Lombrozo <elombrozo at gmail.com> wrote:

> At least SW *is* a scaling solution (albeit most of the important benefits
> are long term). The issue of fee events has nothing to do with scaling - it
> has to do with economics...specifically whether we should be subsidizing
> transactions, who should pay the bill for it, etc. My own personal opinion
> is that increasing validation costs works against adoption, not for
> it...even if it artificially keeps fees low - and we'll have to deal with a
> fee event sooner or later anyhow. You may disagree with my opinion, but
> please, let's stop confounding the economic issues with actual scaling.
>

At least on my part, the title of the 1st email was "It's economics & ..."
and focused on (a) economics and (b) transition issues.  There was no
confounding.  There was a list of real problems and risks taken when 1M is
not lifted in the short term.

Thus "SW is orthogonal" in these emails, because these problems remain
regardless of SW or no, as the 1st email outlined.

The 2nd email addresses the specific assertion of "no 1M hard fork needed,
because SW."
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/f3dd723a/attachment.html>

From adam at cypherspace.org  Thu Dec 17 03:48:29 2015
From: adam at cypherspace.org (Adam Back)
Date: Thu, 17 Dec 2015 04:48:29 +0100
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
	Bitcoin
In-Reply-To: <CADm_WcYhPqZZ5KQ7DxyFgkk5td4ircrXwArg_guWDPWPtnCxhw@mail.gmail.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<49257841-66C8-4EF7-980B-73DC604CA591@mattcorallo.com>
	<CADm_WcYdAHP95mrxH0CjvxKaV12rXEdBXf-L5QtKHuBL1ndFaQ@mail.gmail.com>
	<FC95D096-74AE-4B1C-B11B-1CB718538E7D@gmail.com>
	<CADm_WcYhPqZZ5KQ7DxyFgkk5td4ircrXwArg_guWDPWPtnCxhw@mail.gmail.com>
Message-ID: <CALqxMTEqFR_HzXaBDfU8qe6KR3AJ6BziqtXHn5-dnSQ-eU8yqA@mail.gmail.com>

There are a range of opinions about input assumptions by different
people.  In each case, short of misunderstanding, if we have the same
input assumptions we're going to reach the same conclusions.  This is
the way of the world in a meritocracy.  The interesting point is to
compare the input assumptions and try to figure out which are more
realistic, pragmatic and achieve the best outcome.

It might be instructive to re-read Greg's roadmap and others to
re-read Jeff's original post (I will).

There is a proposed roadmap and soft-fork block-size increase and code
that Pieter is working on.  There has been rationale described for
this approach, and it achieves many useful things both short, mid and
long term for scale and other issues.

There seem to be a range of opinions on the fee market, and one
question is when do we deem it safe to aim to be prepared to support a
fee market.

How elastic is block-size demand?  (I think there is evidence of some
elasticity which indicates a partly working fee market already).  What
I mean by elasticity of block-size demand is there are off-chain
transactions and people make an economic choice of whether to go on
chain or not, and the vast majority of transactions, all told, are
off-chain.  Clearly it is ideal if they all go on chain, scale
permitting.

If we look at the roadmap at high-level:

1) bump (seg-wit or ...)
2) network improvements (IBLT/weak-block/other)
3) longer term dynamic block-size (flexcap)
4) write-cache (lightning)

It would probably be good to see some work on preparing for fee
markets.  That has happened somewhat recently in response to the
stress tests.  We do have an observed problem that if there is no
incentive to prepare, the improvements dont happen, and so we can
never be ready for a fee market.  That's kind of how we got here,
people were talking about fee-estimation and dynamic fees several
years ago before the block-size went from 250kB to 750kB, and then
lost interest as there was another 500kB to play with.  There could be
a best practice doc written asking people to prepare.  That might
help.

Presumably it's good if we do see the fee market more, for it to come
in gradually.  Flexcap probably helps there because the block-size
itself becomes elastic to demand (pay for bigger blocks).

If we want to avoid a fee market for the immediate term, are we more
worried about period 1, or period 2 or 3.  Probably 2 is more of a
worry as we're scaling in 1 where in period 2 we're preparing for
scaling and more time has passed for demand to grow.  That might for
example argue for seg-wit because it brings us closer to 4) and if we
spread things out we might delay the possibility to do lightning as
there is only so many cycles for forks (hard or soft) in testing,
deployment planning etc so it can be good to have a holistic view.

Also the question of time-frame that is safe for soft-forks or
hard-forks is another input where views seem to vary.  I think some
people are more optimistic about being able to avoid people losing
money in fast hard-forks.  One lesson on users, is users find failure
modes that testing cant, or do things you would expect them not to do.

Also we're calling hard-forks things that are really soft-forks to SPV
clients, and hard-forks only to full-nodes.  If we wanted to make a
real economic choice, we could artificially make an SPV hard-fork,
however that would make upgrade harder.

As I said in an earlier email I think everyone is empathetic to user
requirements, including economic desires - but Bitcoin has inherent
constraints that are complex to improve.  Each proposal is trying to
best meet those holistic user requirements.  There are no free lunches
and we dont want to economically hurt anyone in total or as a group or
type of use.  Not all requirements can be met, they are in a trade
off, so that calls for balance, planning and transparency.

This is also a market, we can discuss protocol tradeoffs without being
melodramatic - would be kind of undesirable if a dramatic or emotive
way to express something as easily or more clearly expressed in
technical constructive words is moving the price around.

Adam

On 17 December 2015 at 03:58, Jeff Garzik via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> On Wed, Dec 16, 2015 at 9:44 PM, Eric Lombrozo <elombrozo at gmail.com> wrote:
>>
>> At least SW *is* a scaling solution (albeit most of the important benefits
>> are long term). The issue of fee events has nothing to do with scaling - it
>> has to do with economics...specifically whether we should be subsidizing
>> transactions, who should pay the bill for it, etc. My own personal opinion
>> is that increasing validation costs works against adoption, not for
>> it...even if it artificially keeps fees low - and we'll have to deal with a
>> fee event sooner or later anyhow. You may disagree with my opinion, but
>> please, let's stop confounding the economic issues with actual scaling.
>
>
> At least on my part, the title of the 1st email was "It's economics & ..."
> and focused on (a) economics and (b) transition issues.  There was no
> confounding.  There was a list of real problems and risks taken when 1M is
> not lifted in the short term.
>
> Thus "SW is orthogonal" in these emails, because these problems remain
> regardless of SW or no, as the 1st email outlined.
>
> The 2nd email addresses the specific assertion of "no 1M hard fork needed,
> because SW."
>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

From aj at erisian.com.au  Thu Dec 17 03:52:22 2015
From: aj at erisian.com.au (Anthony Towns)
Date: Thu, 17 Dec 2015 13:52:22 +1000
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
 Bitcoin
In-Reply-To: <CAPg+sBiVVcNNHuV9e1SaPoDSMEwjZHL7tQiszxbE2SQYp1Ongw@mail.gmail.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<CAPg+sBhUso0ddfYQMgwF7yX9_VoqP9CZN5h45t3eQi4v3m6f6A@mail.gmail.com>
	<CADm_WcYZq3nzfYMXfzkZsTCsgmzy4L_nYpa5Kax8uF_ajuUTiQ@mail.gmail.com>
	<CAPg+sBiVVcNNHuV9e1SaPoDSMEwjZHL7tQiszxbE2SQYp1Ongw@mail.gmail.com>
Message-ID: <20151217035222.GA7713@sapphire.erisian.com.au>

On Wed, Dec 16, 2015 at 10:36:09PM +0100, Pieter Wuille via bitcoin-dev wrote:
> On Wed, Dec 16, 2015 at 10:27 PM, Jeff Garzik <jgarzik at gmail.com> wrote:
> >> Not correct. I propose defining the virtual_block_size as base_size +
> >> witness_size * 0.25, and limiting virtual_block_size to 1M. This
> >> creates a single variable to optimize for. If accepted, miners are
> >> incentived to maximize fee per virtual_block_size instead of per size.
> > It is correct.  There are two separate sets of economic actors and levels of
> > contention for each set of space.
> > That is true regardless of the proposed miner selection algorithm.

You're right that the miner selection algorithm doesn't force it to be
the way Pieter describe. But your claim is still incorrect. :)

> Maybe I haven't explained this properly, so consider this example:

Alternatively:

With Pieter's segwit proposal (as it stands), there are two
consensus-limited resources: number of signature ops in the base block
must be no more than 20k, and the virtual block size must be no more
than 1MB (where virtual block size = base block size plus a quarter of
the witness data size).

Nodes and miners have other constraints -- bandwidth, storage, CPU, etc,
such that they might not want to max out these limits for whatever reason,
but those limits aren't enforced by consensus, so can be adjusted as
technology improves just by individual miner policy.

> In fact, the optimal fee maximizing strategy is always to maximize fee
> per virtual size.

(modulo sigop constraints, same as today for fee per base block size)

That's on the "supply" side (ie, miners are forced to be a single group
of economic actors with alighned constraints due to consensus rules).

On the demand side, there might be people who are able to trade off
witness data for base data at different ratios. For most, it's just 1:1
up to a limit as they move scriptsig to witness data, and obviously if
you have to trade 1B of base data for more than 4B of witness data it's
uneconomic. But since the ratio is fixed, there's no bartering to be
done, it's just the same simple calculation for everyone -- does 1B of
base convert to <4B of witness? then do it; otherwise, don't. But once
they've selected a tradeoff, all they can do is choose an absolute fee
value for their transaction, and then you're just back to having some
people who are willing to pay higher fees per virtual block size, and
others who are only willing to pay lower fees.

Cheers,
aj


From jl2012 at xbt.hk  Thu Dec 17 05:32:11 2015
From: jl2012 at xbt.hk (jl2012)
Date: Thu, 17 Dec 2015 00:32:11 -0500
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
 Bitcoin
In-Reply-To: <49257841-66C8-4EF7-980B-73DC604CA591@mattcorallo.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<49257841-66C8-4EF7-980B-73DC604CA591@mattcorallo.com>
Message-ID: <9869fe48a4fc53fc355a35cead73fca2@xbt.hk>

There are at least 2 proposals on the table:

1. SWSF (segwit soft fork) with 1MB virtual block limit, approximately 
equals to 2MB actual limit

2. BIP102: 2MB actual limit

Since the actual limits for both proposals are approximately the same, 
it is not a determining factor in this discussion

The biggest advantage of SWSF is its softfork nature. However, its 
complexity is not comparable with any previous softforks we had. It is 
reasonable to doubt if it could be ready in 6 months

For BIP102, although it is a hardfork, it is a very simple one and could 
be deployed with ISM in less than a month. It is even simpler than 
BIP34, 66, and 65.

So we have a very complicated softfork vs. a very simple hardfork. The 
only reason makes BIP102 not easy is the fact that it's a hardfork.

The major criticism for a hardfork is requiring everyone to upgrade. Is 
that really a big problem?

First of all, hardfork is not a totally unknown territory. BIP50 was a 
hardfork. The accident happened on 13 March 2013. Bitcoind 0.8.1 was 
released on 18 March, which only gave 2 months of grace period for 
everyone to upgrade. The actual hardfork happened on 16 August. 
Everything completed in 5 months without any panic or chaos. This 
experience strongly suggests that 5 months is already safe for a simple 
hardfork. (in terms of simplicity, I believe BIP102 is even simpler than 
BIP50)

Another experience is from BIP66. The 0.10.0 was released on 16 Feb 
2015, exactly 10 months ago. I analyze the data on 
https://bitnodes.21.co and found that 4600 out of 5090 nodes (90.4%) 
indicate BIP66 support. Considering this is a softfork, I consider this 
as very good adoption already.

With the evidence from BIP50 and BIP66, I believe a 5 months 
pre-announcement is good enough for BIP102. As the vast majority of 
miners have declared their support for a 2MB solution, the legacy 1MB 
fork will certainly be abandoned and no one will get robbed.


My primary proposal:

Now - 15 Jan 2016: formally consult the major miners and merchants if 
they support an one-off rise to 2MB. I consider approximately 80% of 
mining power and 80% of trading volume would be good enough

16 - 31 Jan 2016: release 0.11.3 with BIP102 with ISM vote requiring 80% 
of hashing power

1 Jun 2016: the first day a 2MB block may be allowed

Before 31 Dec 2016: release SWSF



My secondary proposal:

Now: Work on SWSF in a turbo mode and have a deadline of 1 Jun 2016

1 Jun 2016: release SWSF

What if the deadline is not met? Maybe pushing an urgent BIP102 if 
things become really bad.


In any case, I hope a clear decision and road map could be made now. 
This topic has been discussed to death. We are just bringing further 
uncertainty if we keep discussing.


Matt Corallo via bitcoin-dev ? 2015-12-16 15:50 ??:
> A large part of your argument is that SW will take longer to deploy
> than a hard fork, but I completely disagree. Though I do not agree
> with some people claiming we can deploy SW significantly faster than a
> hard fork, once the code is ready (probably a six month affair) we can
> get it deployed very quickly. It's true the ecosystem may take some
> time to upgrade, but I see that as a feature, not a bug - we can build
> up some fee pressure with an immediate release valve available for
> people to use if they want to pay fewer fees.
> 
>  On the other hand, a hard fork, while simpler for the ecosystem to
> upgrade to, is a 1-2 year affair (after the code is shipped, so at
> least 1.5-2.5 from today if we all put off heads down and work). One
> thing that has concerned me greatly through this whole debate is how
> quickly people seem to think we can roll out a hard fork. Go look at
> the distribution of node versions on the network today and work
> backwards to get nearly every node upgraded... Even with a year
> between fork-version-release and fork-activation, we'd still kill a
> bunch of nodes and instead of reducing their security model, lead them
> to be outright robbed.
> 


From dscotese at litmocracy.com  Thu Dec 17 06:12:41 2015
From: dscotese at litmocracy.com (Dave Scotese)
Date: Wed, 16 Dec 2015 22:12:41 -0800
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <CADm_Wca9zTdTc2gvTxrWkFjfA49KhbU_=uNXh_mZ+QYXGZ6wWg@mail.gmail.com>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<9a02d94fbc78afaa3e9668e0294eef64@xbt.hk>
	<CADm_Wca9zTdTc2gvTxrWkFjfA49KhbU_=uNXh_mZ+QYXGZ6wWg@mail.gmail.com>
Message-ID: <CAGLBAhfWS4089mcN8FmFcu19r9EwyBMS=TO_2ak3g28UHNXnvg@mail.gmail.com>

On Wed, Dec 16, 2015 at 1:11 PM, Pieter Wuille via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:
> I indeed think we can communicate much better that deciding consensus
> rules is not within our power.

I'm not a core dev, so maybe I have the power to change the consensus
rules.  No one has that power, actually, at least not legitimately.  All we
can do is build it and hope enough people find it acceptable to adopt.  Who
doesn't want to hard fork to 2MB blocks on May 5th and why not?

I have a bitcoin to be split up among all those who suffer from a May 5,
2016 hardfork to 2MB blocks if they can prove it to me, or prove it to
enough engineers that I succumb to peer pressure.  I would have to respect
the engineers though.

There!

Now that we've agreed to have a hard fork on May 5th, 2016, we might decide
to implement some other methods of avoiding the FFM, like braiding the
blockchain or flexcap, or just let anyone mining on top of a block make one
that is a five or ten Kb larger.

notplato

On Wed, Dec 16, 2015 at 2:27 PM, Jeff Garzik via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> On Wed, Dec 16, 2015 at 1:36 PM, jl2012 <jl2012 at xbt.hk> wrote:
>
>> 4. In the miners round table on the second day, one of the devs mentioned
>> that he didn't want to be seen as the decision maker of Bitcoin. On the
>> other hand, Chinese miners repeatedly mentioned that they want several
>> concrete proposals from devs which they could choose. I see no
>> contradiction between these 2 viewpoints.
>>
>
> This was a very interesting dynamic, and seems fair (menu).
>
>
>
>> 6. I believe we should avoid a radical "Economic Change Event" at least
>> in the next halving cycle, as Bitcoin was designed to bootstrap the
>> adoption by high mining reward in the beginning. For this reason, I support
>> an early and conservative increase, such as BIP102 or 2-4-8. 2MB is
>> accepted by most people and it's better than nothing for BIP101 proponents.
>> By "early" I mean to be effective by May, at least 2 months before the
>> halving.
>>
>
> That was precisely my logic for picking May 5 as the hard fork date.  Some
> buffer before halving, enough for caution and iteration in the meantime.
>
>
>
>
>
>
>>
>> (c) My most optimistic guess is SW will be ready in 6 months, which will
>> be very close to halving and potential tx volume burst. And it may not be
>> done in 2016, as it does not only involve consensus code, but also change
>> in the p2p protocol and wallet design
>>
>
> Not just wallet design -- you have to game through the standard steps of:
>  update dev lib (bitcoin-core.js/bitcoinj) + release cycle, update app +
> release cycle, for most actors in the ecosystem, on top of the Bitcoin Core
> roll out.
>
>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>


-- 
I like to provide some work at no charge to prove my value. Do you need a
techie?
I own Litmocracy <http://www.litmocracy.com> and Meme Racing
<http://www.memeracing.net> (in alpha).
I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which
now accepts Bitcoin.
I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
"He ought to find it more profitable to play by the rules" - Satoshi
Nakamoto
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/73119590/attachment.html>

From mark at friedenbach.org  Thu Dec 17 09:33:26 2015
From: mark at friedenbach.org (Mark Friedenbach)
Date: Thu, 17 Dec 2015 17:33:26 +0800
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
	Bitcoin
In-Reply-To: <9869fe48a4fc53fc355a35cead73fca2@xbt.hk>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<49257841-66C8-4EF7-980B-73DC604CA591@mattcorallo.com>
	<9869fe48a4fc53fc355a35cead73fca2@xbt.hk>
Message-ID: <CAOG=w-sMdp1+mcNZc_yfwx59EAwSkFUHekRrqcYXsuLVoCw=1A@mail.gmail.com>

There are many reasons to support segwit beyond it being a soft-fork. For
example:

* the limitation of non-witness data to no more than 1MB makes the
quadratic scaling costs in large transaction validation no worse than they
currently are;
* redeem scripts in witness use a more accurate cost accounting than
non-witness data (further improvements to this beyond what Pieter has
implemented are possible); and
* segwit provides features (e.g. opt-in malleability protection) which are
required by higher-level scaling solutions.

With that in mind I really don't understand the viewpoint that it would be
better to engage a strictly inferior proposal such as a simple adjustment
of the block size to 2MB.

On Thu, Dec 17, 2015 at 1:32 PM, jl2012 via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> There are at least 2 proposals on the table:
>
> 1. SWSF (segwit soft fork) with 1MB virtual block limit, approximately
> equals to 2MB actual limit
>
> 2. BIP102: 2MB actual limit
>
> Since the actual limits for both proposals are approximately the same, it
> is not a determining factor in this discussion
>
> The biggest advantage of SWSF is its softfork nature. However, its
> complexity is not comparable with any previous softforks we had. It is
> reasonable to doubt if it could be ready in 6 months
>
> For BIP102, although it is a hardfork, it is a very simple one and could
> be deployed with ISM in less than a month. It is even simpler than BIP34,
> 66, and 65.
>
> So we have a very complicated softfork vs. a very simple hardfork. The
> only reason makes BIP102 not easy is the fact that it's a hardfork.
>
> The major criticism for a hardfork is requiring everyone to upgrade. Is
> that really a big problem?
>
> First of all, hardfork is not a totally unknown territory. BIP50 was a
> hardfork. The accident happened on 13 March 2013. Bitcoind 0.8.1 was
> released on 18 March, which only gave 2 months of grace period for everyone
> to upgrade. The actual hardfork happened on 16 August. Everything completed
> in 5 months without any panic or chaos. This experience strongly suggests
> that 5 months is already safe for a simple hardfork. (in terms of
> simplicity, I believe BIP102 is even simpler than BIP50)
>
> Another experience is from BIP66. The 0.10.0 was released on 16 Feb 2015,
> exactly 10 months ago. I analyze the data on https://bitnodes.21.co and
> found that 4600 out of 5090 nodes (90.4%) indicate BIP66 support.
> Considering this is a softfork, I consider this as very good adoption
> already.
>
> With the evidence from BIP50 and BIP66, I believe a 5 months
> pre-announcement is good enough for BIP102. As the vast majority of miners
> have declared their support for a 2MB solution, the legacy 1MB fork will
> certainly be abandoned and no one will get robbed.
>
>
> My primary proposal:
>
> Now - 15 Jan 2016: formally consult the major miners and merchants if they
> support an one-off rise to 2MB. I consider approximately 80% of mining
> power and 80% of trading volume would be good enough
>
> 16 - 31 Jan 2016: release 0.11.3 with BIP102 with ISM vote requiring 80%
> of hashing power
>
> 1 Jun 2016: the first day a 2MB block may be allowed
>
> Before 31 Dec 2016: release SWSF
>
>
>
> My secondary proposal:
>
> Now: Work on SWSF in a turbo mode and have a deadline of 1 Jun 2016
>
> 1 Jun 2016: release SWSF
>
> What if the deadline is not met? Maybe pushing an urgent BIP102 if things
> become really bad.
>
>
> In any case, I hope a clear decision and road map could be made now. This
> topic has been discussed to death. We are just bringing further uncertainty
> if we keep discussing.
>
>
> Matt Corallo via bitcoin-dev ? 2015-12-16 15:50 ??:
>
>> A large part of your argument is that SW will take longer to deploy
>> than a hard fork, but I completely disagree. Though I do not agree
>> with some people claiming we can deploy SW significantly faster than a
>> hard fork, once the code is ready (probably a six month affair) we can
>> get it deployed very quickly. It's true the ecosystem may take some
>> time to upgrade, but I see that as a feature, not a bug - we can build
>> up some fee pressure with an immediate release valve available for
>> people to use if they want to pay fewer fees.
>>
>>  On the other hand, a hard fork, while simpler for the ecosystem to
>> upgrade to, is a 1-2 year affair (after the code is shipped, so at
>> least 1.5-2.5 from today if we all put off heads down and work). One
>> thing that has concerned me greatly through this whole debate is how
>> quickly people seem to think we can roll out a hard fork. Go look at
>> the distribution of node versions on the network today and work
>> backwards to get nearly every node upgraded... Even with a year
>> between fork-version-release and fork-activation, we'd still kill a
>> bunch of nodes and instead of reducing their security model, lead them
>> to be outright robbed.
>>
>>
> _______________________________________________
>
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151217/49c13a41/attachment-0001.html>

From aj at erisian.com.au  Thu Dec 17 10:57:13 2015
From: aj at erisian.com.au (Anthony Towns)
Date: Thu, 17 Dec 2015 20:57:13 +1000
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
 Bitcoin
In-Reply-To: <9869fe48a4fc53fc355a35cead73fca2@xbt.hk>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<49257841-66C8-4EF7-980B-73DC604CA591@mattcorallo.com>
	<9869fe48a4fc53fc355a35cead73fca2@xbt.hk>
Message-ID: <20151217105713.GA16561@sapphire.erisian.com.au>

On Thu, Dec 17, 2015 at 12:32:11AM -0500, jl2012 via bitcoin-dev wrote:
> There are at least 2 proposals on the table:
> 1. SWSF (segwit soft fork) with 1MB virtual block limit, approximately
>    equals to 2MB actual limit
> 2. BIP102: 2MB actual limit

I think there's a few variants of (2) -- there's "just 2MB", "2MB now,
4MB in a while, 8MB after that", "1MB for a while longer, then 2MB,
then 4MB" (halved from 2/4/8 since segwit gives 1.6x-2x benefit), and
variations of those with different dates, whether or not to smooth out
the increases to avoid economic shocks, and how to determine activation
(flag day? miner consensus? combination?).

> Since the actual limits for both proposals are approximately the same, it is
> not a determining factor in this discussion

That's true on the benefit side (both give about double the number of
ordinary transactions per block; though segregated witness has other
benefits). On the cost side, the limits are different:

 * worst case block data size is 2x for BIP102, 4x for segwit (affecting
   bandwidth, latency and storage costs for nodes)

 * worst case sigops is 2x for BIP102, but the same as today for segwit
   (affecting block validation time)

 * worst case bytes to hash a block is 4x for BIP102 (doubling block
   size and sigops), but the same as today for segwit (again affecting
   block validation time)

 * worst case UTXO bloat is 2x for BIP102, but the same as today for
   segwit (affecting memory usage, and validation time)

In the "expected" case (where people aren't attacking the blockchain)
I think they're the same on all these metrics. But increasing the
limits could easily make attacks more common, especially if it makes
them more effective.

I think the main attack vector of these is that increasing block
validation time via too many (active) sigops or too many bytes hashed
allows a selfish mining attack, but I'm not clear enough on how that
would work exactly to estimate where the boundary between acceptable and
unacceptable risk is (and how feasible non-consensus-level countermeasures
might be).

But at 1x sigops, you can already (accidently!) construct blocks that
take minutes to verify; and at 4x you can probably already construct a
block that takes 10 minutes to verify, which would probably be pretty
bad... But I'm not sure this isn't already exploitable, so maybe we're
already assuming a certain level of altruism and making things worse
doesn't actually make them worse?

I think it would be good for BIP102 or similar to include an evaluation
of that risk before being deployed [0].

> The major criticism for a hardfork is requiring everyone to upgrade. Is that
> really a big problem?

Yes. That doesn't mean it's not worth it, though.

(The 2-month timeline for the BIP50 accidental hardfork to be accepted
on the network seems persuasive to me that it's possible to roll out a
deliberate, SPV-compatible, hardfork on today's network in 3-6 months)

> My primary proposal:
> 1 Jun 2016: the first day a 2MB block may be allowed
> 
> My secondary proposal:
> 1 Jun 2016: release SWSF

I think it makes sense to just do both of these independently; ie:

 * release segwit via softfork ASAP (perhaps targetting March or April
   to get it included in bitcoin, activation a month or three
   afterwards?), with virtual block size calculated as proposed and
   capped by MAX_BLOCK_SIZE [1]

 * increase MAX_BLOCK_SIZE via hardfork to 2MB after block 420,000
   (phased in gradually? with future scheduled increases to 4MB or 8MB?)

If segwit gets delayed because it's complicated, that's okay; if
it comes out earlier, that's okay too. If the hardfork gets delayed
because miners aren't ready or because it's better to introduce it in a
staggered fashion, or because there's no clear evaluation of the risks,
that's okay too.

But more importantly, it allows evaluate the pros and cons of each
implementation separately and on its own merits, rather than arguing
against working on one just because you're in favour of doing the
other ASAP.


They have benefits if you combine them too; for instance, if the
MAX_BLOCK_SIZE increase is phased in rather than done as a step increase
(ie block x's limit is 1MB, block x+1's limit is 1.005MB or similar,
and block x+2's limit is 1.01MB, etc) having segwit available in parallel
could provide a helpful escape valve: if an individual bitcoin user has
been dying for more capacity, they can spend the time/effort to update
their software for segwit and get it immediately without having to wait
as the consensus limits rise.

Conversely, having both segwit and a phased increase to MAX_BLOCK_SIZE
means that miners generally won't be immediately mining 2MB (or 4MB)
blocks halfway through the year, which should avoid both technological
shocks (bandwidth just doubled!) or economic shocks (supply has increased
so fees have plummeted), which could be good.


FWIW, the worst case scenarios are:

 * block data size:
     BIP102:  2x   (worst/avg)
     segwit:  4x   (worst, ~2x avg)
     both:    8x   (worst, ~4x avg)
     BIP101:  8x   (worst/avg)

 * sigops per block:
     BIP102:  2x
     segwit:  1x
     both:    2x
     BIP101:  8x

 * bytes hashed per block:
     BIP102:  4x
     segwit:  1x
     both:    4x
     BIP101:  64x

 * UTXO rate of increase:
     BIP102:  2x
     segwit:  1x
     both:    2x
     BIP101:  8x

Compared to the (expected, eventual, near-term) benefits:

 * transactions per block:
     BIP102:  2x
     segwit:  1.6x-2x
     both:    3.2x-4x
     BIP101:  8x

 * misc:
     BIP101/2: planned hardforks are possible, bitcoin community governance
       is demonstrably working, etc
     segwit: malleability fixes, script improvements, lightning
       enablement, etc

The block data is the only case where the average case is already just
about the worst case; for the others, as long as the worst case doesn't
inspire new attacks, the future average case should just increase in
proportion to the additional transactions.

Cheers,
aj

[0] (and segwit should actually account for sigops in witness data before
     being deployed...)

[1] If segwit warrants a hardfork to clean up data structures, I think
    that should be deferred until well after the MAX_BLOCK_SIZE hardfork,
    rather than trying to do it at the same time. As such, doing segwit by
    soft fork in the short term seems to make sense, since it also helps
    with transaction malleability and further improvements to script.


From marcel at jamin.net  Thu Dec 17 06:14:13 2015
From: marcel at jamin.net (Marcel Jamin)
Date: Thu, 17 Dec 2015 07:14:13 +0100
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
	Bitcoin
In-Reply-To: <49257841-66C8-4EF7-980B-73DC604CA591@mattcorallo.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<49257841-66C8-4EF7-980B-73DC604CA591@mattcorallo.com>
Message-ID: <CAAUq486TDe4eRZMFYx4gkmHDU4sCEeoBZqN-MVsdFbsM9rQ6TQ@mail.gmail.com>

Maybe we should first gather concrete estimates about and roughly agree on

- how long SW (SF) development will probably take

- how long the ecosystem needs to prepare for a hardfork (SW (HF) or a
simple can kicking block size increase)

Opinions differ wildly from what it looks like, but maybe we can get to
estimates that the majority here can accept.

---

Personally, I think that the disclaimer "Bitcoin is an experiment" is
pervasive. It's still a pre-release, even with a $6bn vote of confidence.
If you don't follow developments in this phase, don't upgrade and then have
an elevated risk of losing money by getting scammed, then that's a little
bit your fault. I'd absolutely support a change in mentality on that once
1.0.0 arrives, but until then is bitcoin a work-in-progress experiment and
a high risk investment.

A planned hard-fork is an experiment that needs to be run anyway. When do
we want to do that, if not now?


2015-12-16 21:50 GMT+01:00 Matt Corallo via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org>:

> A large part of your argument is that SW will take longer to deploy than a
> hard fork, but I completely disagree. Though I do not agree with some
> people claiming we can deploy SW significantly faster than a hard fork,
> once the code is ready (probably a six month affair) we can get it deployed
> very quickly. It's true the ecosystem may take some time to upgrade, but I
> see that as a feature, not a bug - we can build up some fee pressure with
> an immediate release valve available for people to use if they want to pay
> fewer fees.
>
> On the other hand, a hard fork, while simpler for the ecosystem to upgrade
> to, is a 1-2 year affair (after the code is shipped, so at least 1.5-2.5
> from today if we all put off heads down and work). One thing that has
> concerned me greatly through this whole debate is how quickly people seem
> to think we can roll out a hard fork. Go look at the distribution of node
> versions on the network today and work backwards to get nearly every node
> upgraded... Even with a year between fork-version-release and
> fork-activation, we'd still kill a bunch of nodes and instead of reducing
> their security model, lead them to be outright robbed.
>
> On December 16, 2015 12:38:30 PM PST, Jeff Garzik via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>>
>>
>> 1. Summary
>>
>> Segregated Witness (SegWitness, SW) is being presented in the context of
>> Scaling Bitcoin.  It has useful attributes, notably addressing a major
>> malleability vector, but is not a short term scaling solution.
>>
>>
>> 2. Definitions
>>
>> Import Fee Event, ECE, TFM, FFM from previous email.
>>
>> Older clients - Any software not upgraded to SW
>>
>> Newer clients - Upgraded, SW aware software
>>
>>
>> Block size - refers to the core block economic resource limit ed by
>> MAX_BLOCK_SIZE.  Witness data (or extension block data) is excluded.
>> Requires a hard fork to change.
>>
>> Core block - Current bitcoin block, with upper bound MAX_BLOCK_SIZE.  Not
>> changed by SW.
>>
>>
>> Extended transaction - Newer, upgraded version of transaction data format.
>>
>> Extended block - Newer, upgraded version of block data format.
>>
>>
>> EBS - Extended block size.  Block size seen by newer clients.
>>
>>
>> 3. Context of analysis
>>
>> One proposal presents SW *in lieu of* a hard fork block size increase.
>> This email focuses directly on that.
>>
>> Useful features outside block size context, such as anti-malleability or
>> fraud proof features, are not covered in depth.
>>
>>
>> 4.1.  Observations on data structure formats and views
>>
>> SW creates two *views* of each transaction and block.  SW has blocks and
>> extended blocks.  Similarly, there exists transactions and extended
>> transactions.
>>
>> This view is rendered to clients depending on compatibility level.  Newer
>> clients see extended blocks and extended transactions.  Older clients see
>> blocks (limit 1M), and do not see extended blocks.  Older clients see
>> upgraded transactions as unsigned, anyone-can-pay transactions.
>>
>> Each extended transaction exists in two states, one unsigned and one
>> signed, each of which passes validation as a valid bitcoin transaction.
>>
>>
>> 4.2.  Observations on behavior of older transaction creation
>>
>> Transactions created by older clients will not use the extended
>> transaction format.  All data is stored the standard 1M block as today.
>>
>>
>> 4.3.  Observations on new block economic model
>>
>> SW complicates block economics by creating two separate, supply limited
>> resources.
>>
>> The core block economic resource is heavily contended.  Older clients use
>> core blocks exclusively.  Newer clients use core block s more
>> conservatively, storing as much data as possible in extended blocks.
>>
>> The extended block economic resource is less heavily contended, though
>> that of course grows over time as clients upgrade.
>>
>> Because core blocks are more heavily contended, it is presumed that older
>> clients will pay a higher fee than newer clients (subject to elasticity
>> etc.).
>>
>>
>> 5.1.  Problem:  Pace of roll-out will be slow - Whole Ecosystem must be
>> considered.
>>
>> The current apparent proposal is to roll out Segregated Witness as a soft
>> fork, and keep block size at 1M.
>>
>> The roll-out pace cannot simply be judged by soft fork speed - which is
>> months at best.  Analysis must the layers above:  Updating bitcoin-core
>> (JS) and bitcoinj (Java), and then the timelines to roll out those updates
>> to apps, and then the timeline to update those apps to create extended
>> transactions.
>>
>> Overall, wallet software and programmer libraries must be upgraded to
>> make use of this new format, adding many more months (12+ in some stacks)
>> to the roll out timeline.  In the meantime, clients continue to contend
>> entirely for core block space.
>>
>>
>> 5.2.  Problem:   Hard fork to bigger block size Just Works(tm) with most
>> software, unlike SW.
>>
>> A simple hard fork such as BIP 102 is automatically compatible with the
>> vast range of today's ecosystem software.
>>
>> SW requires merchants to upgrade almost immediately, requires wallet and
>> other peripheral software upgrades to make use of.  Other updates are
>> opt-in and occur more slowly.  BIP 70 processors need some updates.
>>
>> The number of LOC that must change for BIP 102 is very small, and the
>> problem domain well known, versus SW.
>>
>>
>> 5.3.  Problem:   Due to pace, Fee Event not forestalled.
>>
>> Even presuming SW is merged into Bitcoin Core tomorrow, this does not
>> address the risk of a Fee Event and associated Economic Change in the
>> coming months.
>>
>>
>> 5.4.  Problem:   More complex economic policy, new game theory, new
>> bidding structure risks.
>>
>> Splitting blocks into two pieces, each with separate and distinct
>> behaviors and resource values, creates *two fee markets.*
>>
>> Having two pricing strata within each block has certainly feasible - that
>> is the current mining policy of (1) fee/KB followed by (2) priority/age.
>>
>> Valuable or not - e.g. incentivizing older clients to upgrade - the fact
>> remains that SW creates a more-complex bidding structure by creating a
>> second economic resource.
>>
>> *This is clearly a change to a new economic policy* with standard risks
>> associated with that.  Will that induce an Economic C hange Event (see def
>> last email)?  *Unlikely*, due to slow rollout pace.
>>
>>
>> 5.5.  Problem:  Current SW mining algorithm needs improvement
>>
>> Current SW block template maker does a reasonable job, but makes some
>> naive assumptions about the fee market across an entire extended block.
>> This is a mismatch with the economic reality (just described).
>>
>> 5.6.   Problem:  New, under-analyzed attack surfaces
>>
>> Less significant and fundamental but still worth noting.
>>
>> This is not a fundamental SW problem, but simply standard complexity risk
>> factors:  splitting the signatures away from transactions, and creating a
>> new apparently-unsigned version of the transaction opens t he possibility
>> of some network attacks which cause some clients to degrade down from
>> extended block to core block mode temporarily.
>>
>> There is a chance of a failure mode that fools older clients into
>> thinking fraudulent data is valid (judgement: unlikely vis hashpower but
>> not impossible)
>>
>> 6. Conclusions and recommendations
>>
>> It seems unlikely that SW provides scaling in the short term, and SW
>> introduces new economics complexities.
>>
>> A "short term bump" hard fork block size increase addresses economic and
>> ecosystem risks that SW does not.
>>
>> Bump + SW should proce ed in parallel, independent tracks, as orthogonal
>> issues.
>>
>>
>> 7. Appendix - Other SW comments
>>
>> Hard forks provide much stronger validation, and ensure the network
>> operates at a fully trustless level.
>>
>> SW hard fork is preferred, versus soft fork.  Soft forking SW places a
>> huge amount of trust on miners to validate transaction signatures, versus
>> the rest of the network, as the network slowly upgrades to newer clients.
>>
>> An SW hard fork could also add several zero-filled placeholders in a
>> merkle tree for future use.
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>> ------------------------------
>>
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151217/689f5cee/attachment.html>

From jl2012 at xbt.hk  Thu Dec 17 10:00:24 2015
From: jl2012 at xbt.hk (jl2012)
Date: Thu, 17 Dec 2015 05:00:24 -0500
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
 Bitcoin
In-Reply-To: <CAOG=w-sMdp1+mcNZc_yfwx59EAwSkFUHekRrqcYXsuLVoCw=1A@mail.gmail.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<49257841-66C8-4EF7-980B-73DC604CA591@mattcorallo.com>
	<9869fe48a4fc53fc355a35cead73fca2@xbt.hk>
	<CAOG=w-sMdp1+mcNZc_yfwx59EAwSkFUHekRrqcYXsuLVoCw=1A@mail.gmail.com>
Message-ID: <18f0e80b0a55272a61d547f59efc6c9d@xbt.hk>

I know my reply is a long one but please read before you hit send. I 
have 2 proposals: fast BIP102 + slow SWSF and fast SWSF only. I guess no 
one here is arguing for not doing segwit; and it is on the top of my 
wish list. My main argument (maybe also Jeff's) is that segwit is too 
complicated and may not be a viable short term solution (with the 
reasons I listed that I don't want to repeat)

And also I don't agree with you that BIP102 is *strictly* inferior than 
segwit. We never had a complex softfork like segwit, but we did have a 
successful simple hardfork (BIP50), and BIP102 is very simple. (Details 
in my last post. I'm not going to repeat)

Mark Friedenbach ? 2015-12-17 04:33 ??:
> There are many reasons to support segwit beyond it being a soft-fork.
> For example:
> 
> * the limitation of non-witness data to no more than 1MB makes the
> quadratic scaling costs in large transaction validation no worse than
> they currently are;
> * redeem scripts in witness use a more accurate cost accounting than
> non-witness data (further improvements to this beyond what Pieter has
> implemented are possible); and
> * segwit provides features (e.g. opt-in malleability protection) which
> are required by higher-level scaling solutions.
> 
> With that in mind I really don't understand the viewpoint that it
> would be better to engage a strictly inferior proposal such as a
> simple adjustment of the block size to 2MB.


From jameson.lopp at gmail.com  Thu Dec 17 02:06:08 2015
From: jameson.lopp at gmail.com (Jameson Lopp)
Date: Wed, 16 Dec 2015 18:06:08 -0800
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <CAPg+sBimfFVea4Sorgx=DaMPVs1k1DrmTA2ZFdLFtxrqKm23-w@mail.gmail.com>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
	<CADm_Wcae7OK7kyXkfh+7XFrc2WYsv7T1Va3_E=5om+XYrL9pOw@mail.gmail.com>
	<CAPg+sBimfFVea4Sorgx=DaMPVs1k1DrmTA2ZFdLFtxrqKm23-w@mail.gmail.com>
Message-ID: <CADL_X_eWNuMJn_UBm3YYg_HjHmyDj3J85xp02L-0N-x6bmPZLA@mail.gmail.com>

On Wed, Dec 16, 2015 at 1:11 PM, Pieter Wuille via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> On Wed, Dec 16, 2015 at 10:08 PM, Jeff Garzik <jgarzik at gmail.com> wrote:
> >> You present this as if the Bitcoin Core development team is in charge
> >> of deciding the network consensus rules, and is responsible for making
> >> changes to it in order to satisfy economic demand. If that is the
> >> case, Bitcoin has failed, in my opinion.
> >
> >
> > This circles back to Problem #1:   Avoidance of a choice is a still a
> choice
> > - failing to ACK a MAX_BLOCK_SIZE increase still creates very real
> Economic
> > Change Event risk.
>
> We are not avoiding a choice. We don't have the authority to make a choice.
>
> > And #3:  If the likely predicted course is that Bitcoin Core will not
> accept
> > a protocol change changing MAX_BLOCK_SIZE via hard fork in the short
> term,
> > the core dev team should communicate that position clearly to users and
> > media.
>
> I indeed think we can communicate much better that deciding consensus
> rules is not within our power.
>

Indeed, because I sometimes find these statements to be confusing as well -
I can completely understand what you mean if you're speaking from a moral
standpoint. If you're saying that it's unacceptable for the Bitcoin Core
developers to force consensus changes upon the system, I agree. But
thankfully the design of the system does not allow the developers to do so.
Developers can commit amazing code or terrible code, but it must be
voluntarily adopted by the rest of the ecosystem. Core developers can't
decide these changes, they merely propose them to the ecosystem by writing
and releasing code.

I agree that Core developers have no authority to make these decisions on
behalf of all of the network participants. However, they are in a position
of authority when it comes to proposing changes. One of my takeaways from
Hong Kong was that most miners have little interest in taking
responsibility for consensus changes - they trust the Core developers to
use their expertise to propose changes that will result in the continued
operation of the network and not endanger their business operations.

A non-trivial portion of the ecosystem is requesting that the Core
developers make a proposal so that the network participants can make a
choice. Jeff noted that we can expect for the economic conditions of the
network to change significantly in 2016, barring higher throughput
capacity. If the year+ deployment timeframe for hard forks proposed by Matt
on another thread is what we can expect for any proposed consensus change,
then it should be non-contentious to announce that there will be no hard
fork in 2016. This will give clarity to the rest of the ecosystem as to how
they should prepare.


> --
> Pieter
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/ba83fcba/attachment.html>

From jameson.lopp at gmail.com  Wed Dec 16 21:51:47 2015
From: jameson.lopp at gmail.com (Jameson Lopp)
Date: Wed, 16 Dec 2015 13:51:47 -0800
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
	Bitcoin
In-Reply-To: <49257841-66C8-4EF7-980B-73DC604CA591@mattcorallo.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<49257841-66C8-4EF7-980B-73DC604CA591@mattcorallo.com>
Message-ID: <CADL_X_dm-Sn2Mo7r746FyH9655cKBajtj-TJnhS47o2Oa-M7yQ@mail.gmail.com>

On Wed, Dec 16, 2015 at 12:50 PM, Matt Corallo via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> A large part of your argument is that SW will take longer to deploy than a
> hard fork, but I completely disagree. Though I do not agree with some
> people claiming we can deploy SW significantly faster than a hard fork,
> once the code is ready (probably a six month affair) we can get it deployed
> very quickly. It's true the ecosystem may take some time to upgrade, but I
> see that as a feature, not a bug - we can build up some fee pressure with
> an immediate release valve available for people to use if they want to pay
> fewer fees.
>
> On the other hand, a hard fork, while simpler for the ecosystem to upgrade
> to, is a 1-2 year affair (after the code is shipped, so at least 1.5-2.5
> from today if we all put off heads down and work). One thing that has
> concerned me greatly through this whole debate is how quickly people seem
> to think we can roll out a hard fork. Go look at the distribution of node
> versions on the network today and work backwards to get nearly every node
> upgraded... Even with a year between fork-version-release and
> fork-activation, we'd still kill a bunch of nodes and instead of reducing
> their security model, lead them to be outright robbed.
>
>
Over a year seems to be an extraordinarily long time frame is for deploying
a hard fork. It looks like <https://bitnodes.21.co/dashboard/?days=365> 75%
of reachable nodes have upgraded in the past 6 months while as much as 25%
may not have been upgraded in over a year. However, viewing historical
stats of version upgrades doesn't seem to be an appropriate comparison
because node operators have never been faced with the same incentive to
upgrade. We can point to unintentional forks in the past that have been
resolved fairly quickly by reaching out to miners, but it's also a poor
comparison. Unfortunately, we have no way of knowing what percentage of
nodes are economically important - a great deal of them may be running and
not even be used by the operators.

Perhaps it would be better if we were to formalize the expectations for
full node operators, but it seems to me that node operators have a
responsibility to keep themselves informed and decide when it is
appropriate to update their software. I'm not so sure that it's the rest of
the ecosystem's responsibility to wait around for laggards.

- Jameson

On December 16, 2015 12:38:30 PM PST, Jeff Garzik via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>>
>>
>> 1. Summary
>>
>> Segregated Witness (SegWitness, SW) is being presented in the context of
>> Scaling Bitcoin.  It has useful attributes, notably addressing a major
>> malleability vector, but is not a short term scaling solution.
>>
>>
>> 2. Definitions
>>
>> Import Fee Event, ECE, TFM, FFM from previous email.
>>
>> Older clients - Any software not upgraded to SW
>>
>> Newer clients - Upgraded, SW aware software
>>
>>
>> Block size - refers to the core block economic resource limit ed by
>> MAX_BLOCK_SIZE.  Witness data (or extension block data) is excluded.
>> Requires a hard fork to change.
>>
>> Core block - Current bitcoin block, with upper bound MAX_BLOCK_SIZE.  Not
>> changed by SW.
>>
>>
>> Extended transaction - Newer, upgraded version of transaction data format.
>>
>> Extended block - Newer, upgraded version of block data format.
>>
>>
>> EBS - Extended block size.  Block size seen by newer clients.
>>
>>
>> 3. Context of analysis
>>
>> One proposal presents SW *in lieu of* a hard fork block size increase.
>> This email focuses directly on that.
>>
>> Useful features outside block size context, such as anti-malleability or
>> fraud proof features, are not covered in depth.
>>
>>
>> 4.1.  Observations on data structure formats and views
>>
>> SW creates two *views* of each transaction and block.  SW has blocks and
>> extended blocks.  Similarly, there exists transactions and extended
>> transactions.
>>
>> This view is rendered to clients depending on compatibility level.  Newer
>> clients see extended blocks and extended transactions.  Older clients see
>> blocks (limit 1M), and do not see extended blocks.  Older clients see
>> upgraded transactions as unsigned, anyone-can-pay transactions.
>>
>> Each extended transaction exists in two states, one unsigned and one
>> signed, each of which passes validation as a valid bitcoin transaction.
>>
>>
>> 4.2.  Observations on behavior of older transaction creation
>>
>> Transactions created by older clients will not use the extended
>> transaction format.  All data is stored the standard 1M block as today.
>>
>>
>> 4.3.  Observations on new block economic model
>>
>> SW complicates block economics by creating two separate, supply limited
>> resources.
>>
>> The core block economic resource is heavily contended.  Older clients use
>> core blocks exclusively.  Newer clients use core block s more
>> conservatively, storing as much data as possible in extended blocks.
>>
>> The extended block economic resource is less heavily contended, though
>> that of course grows over time as clients upgrade.
>>
>> Because core blocks are more heavily contended, it is presumed that older
>> clients will pay a higher fee than newer clients (subject to elasticity
>> etc.).
>>
>>
>> 5.1.  Problem:  Pace of roll-out will be slow - Whole Ecosystem must be
>> considered.
>>
>> The current apparent proposal is to roll out Segregated Witness as a soft
>> fork, and keep block size at 1M.
>>
>> The roll-out pace cannot simply be judged by soft fork speed - which is
>> months at best.  Analysis must the layers above:  Updating bitcoin-core
>> (JS) and bitcoinj (Java), and then the timelines to roll out those updates
>> to apps, and then the timeline to update those apps to create extended
>> transactions.
>>
>> Overall, wallet software and programmer libraries must be upgraded to
>> make use of this new format, adding many more months (12+ in some stacks)
>> to the roll out timeline.  In the meantime, clients continue to contend
>> entirely for core block space.
>>
>>
>> 5.2.  Problem:   Hard fork to bigger block size Just Works(tm) with most
>> software, unlike SW.
>>
>> A simple hard fork such as BIP 102 is automatically compatible with the
>> vast range of today's ecosystem software.
>>
>> SW requires merchants to upgrade almost immediately, requires wallet and
>> other peripheral software upgrades to make use of.  Other updates are
>> opt-in and occur more slowly.  BIP 70 processors need some updates.
>>
>> The number of LOC that must change for BIP 102 is very small, and the
>> problem domain well known, versus SW.
>>
>>
>> 5.3.  Problem:   Due to pace, Fee Event not forestalled.
>>
>> Even presuming SW is merged into Bitcoin Core tomorrow, this does not
>> address the risk of a Fee Event and associated Economic Change in the
>> coming months.
>>
>>
>> 5.4.  Problem:   More complex economic policy, new game theory, new
>> bidding structure risks.
>>
>> Splitting blocks into two pieces, each with separate and distinct
>> behaviors and resource values, creates *two fee markets.*
>>
>> Having two pricing strata within each block has certainly feasible - that
>> is the current mining policy of (1) fee/KB followed by (2) priority/age.
>>
>> Valuable or not - e.g. incentivizing older clients to upgrade - the fact
>> remains that SW creates a more-complex bidding structure by creating a
>> second economic resource.
>>
>> *This is clearly a change to a new economic policy* with standard risks
>> associated with that.  Will that induce an Economic C hange Event (see def
>> last email)?  *Unlikely*, due to slow rollout pace.
>>
>>
>> 5.5.  Problem:  Current SW mining algorithm needs improvement
>>
>> Current SW block template maker does a reasonable job, but makes some
>> naive assumptions about the fee market across an entire extended block.
>> This is a mismatch with the economic reality (just described).
>>
>> 5.6.   Problem:  New, under-analyzed attack surfaces
>>
>> Less significant and fundamental but still worth noting.
>>
>> This is not a fundamental SW problem, but simply standard complexity risk
>> factors:  splitting the signatures away from transactions, and creating a
>> new apparently-unsigned version of the transaction opens t he possibility
>> of some network attacks which cause some clients to degrade down from
>> extended block to core block mode temporarily.
>>
>> There is a chance of a failure mode that fools older clients into
>> thinking fraudulent data is valid (judgement: unlikely vis hashpower but
>> not impossible)
>>
>> 6. Conclusions and recommendations
>>
>> It seems unlikely that SW provides scaling in the short term, and SW
>> introduces new economics complexities.
>>
>> A "short term bump" hard fork block size increase addresses economic and
>> ecosystem risks that SW does not.
>>
>> Bump + SW should proce ed in parallel, independent tracks, as orthogonal
>> issues.
>>
>>
>> 7. Appendix - Other SW comments
>>
>> Hard forks provide much stronger validation, and ensure the network
>> operates at a fully trustless level.
>>
>> SW hard fork is preferred, versus soft fork.  Soft forking SW places a
>> huge amount of trust on miners to validate transaction signatures, versus
>> the rest of the network, as the network slowly upgrades to newer clients.
>>
>> An SW hard fork could also add several zero-filled placeholders in a
>> merkle tree for future use.
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>> ------------------------------
>>
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/3bfa78f6/attachment-0001.html>

From corey3 at gmail.com  Thu Dec 17 07:54:41 2015
From: corey3 at gmail.com (Corey Haddad)
Date: Wed, 16 Dec 2015 23:54:41 -0800
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
	Bitcoin
In-Reply-To: <9869fe48a4fc53fc355a35cead73fca2@xbt.hk>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<49257841-66C8-4EF7-980B-73DC604CA591@mattcorallo.com>
	<9869fe48a4fc53fc355a35cead73fca2@xbt.hk>
Message-ID: <CAK_HAC-QmFiQGePpPH7n7qV-A4mkQdsWmgwA__mc1GBkTa6oFA@mail.gmail.com>

A planned hardfork, similar to certain softforks, leaves users with some
reduction in security.  It does not leave them defenseless.  Consider the
following:

1: Hard to be robbed on the basis of hashpower.

In reality the old chain will see mining all but stop, and blocks would be
hours to days apart even if a couple percentage points of hashpower failed
to switch over.  Six confirmations would certainly take days.  If the fork
can be scheduled at the beginning of a difficulty period, the old chain
would almost certainly not even ever make it to the next retargeting.

2: Hard to be robber on the basis of awareness.

Expect there to be fairly widespread coverage in the Bitcoin press, and as
the fork draws near, maybe coverage in business and tech publications.
Further, the alert keys will certainly be used, so node operators will get
the message directly.

3: There still needs to be a targeted attack by a fraudster on an unaware
node operator.

To fall victim, one needs to give up something of value to an attacker in
exchange for Bitcoins (on the old chain).  The typical uninitiated
full-node user (probably a small subset anyway) is typically going to be
buying bitcoin from a trusted source, and then saving or spending them, or
perhaps gambling.  They are not, typically, going to be providing a service
or selling goods in exchange for Bitcoin unless they are at least somewhat
aware of what is going on in the Bitcoin space.  It's possible, of course,
but we are talking about small numbers here of people who fit the above.

All three parts of the above would have to go perfectly wrong for someone
to loose out.  Someone somewhere will probably get scammed as a result of a
hardfork.  That stinks, and we should make reasonable efforts to help them
avoid that fate.  But at this point in Bitcoin's development, it is still
in beta, it's still an economic experiment, and we can't allow the software
to become hamstrung out of fear that some inattentive user might bungle
their security.  If they merely waited for 6 confirmations, as is the
standard advice, they would be waiting for days.  If that along doesn't
give them a hint that something is wrong, it might still be too early days
for them to be playing with Bitcoin for anything important.

I support a hardfork deployment that takes 80% of hashpower activate + a
4-month delay.

On Wed, Dec 16, 2015 at 9:32 PM, jl2012 via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> There are at least 2 proposals on the table:
>
> 1. SWSF (segwit soft fork) with 1MB virtual block limit, approximately
> equals to 2MB actual limit
>
> 2. BIP102: 2MB actual limit
>
> Since the actual limits for both proposals are approximately the same, it
> is not a determining factor in this discussion
>
> The biggest advantage of SWSF is its softfork nature. However, its
> complexity is not comparable with any previous softforks we had. It is
> reasonable to doubt if it could be ready in 6 months
>
> For BIP102, although it is a hardfork, it is a very simple one and could
> be deployed with ISM in less than a month. It is even simpler than BIP34,
> 66, and 65.
>
> So we have a very complicated softfork vs. a very simple hardfork. The
> only reason makes BIP102 not easy is the fact that it's a hardfork.
>
> The major criticism for a hardfork is requiring everyone to upgrade. Is
> that really a big problem?
>
> First of all, hardfork is not a totally unknown territory. BIP50 was a
> hardfork. The accident happened on 13 March 2013. Bitcoind 0.8.1 was
> released on 18 March, which only gave 2 months of grace period for everyone
> to upgrade. The actual hardfork happened on 16 August. Everything completed
> in 5 months without any panic or chaos. This experience strongly suggests
> that 5 months is already safe for a simple hardfork. (in terms of
> simplicity, I believe BIP102 is even simpler than BIP50)
>
> Another experience is from BIP66. The 0.10.0 was released on 16 Feb 2015,
> exactly 10 months ago. I analyze the data on https://bitnodes.21.co and
> found that 4600 out of 5090 nodes (90.4%) indicate BIP66 support.
> Considering this is a softfork, I consider this as very good adoption
> already.
>
> With the evidence from BIP50 and BIP66, I believe a 5 months
> pre-announcement is good enough for BIP102. As the vast majority of miners
> have declared their support for a 2MB solution, the legacy 1MB fork will
> certainly be abandoned and no one will get robbed.
>
>
> My primary proposal:
>
> Now - 15 Jan 2016: formally consult the major miners and merchants if they
> support an one-off rise to 2MB. I consider approximately 80% of mining
> power and 80% of trading volume would be good enough
>
> 16 - 31 Jan 2016: release 0.11.3 with BIP102 with ISM vote requiring 80%
> of hashing power
>
> 1 Jun 2016: the first day a 2MB block may be allowed
>
> Before 31 Dec 2016: release SWSF
>
>
>
> My secondary proposal:
>
> Now: Work on SWSF in a turbo mode and have a deadline of 1 Jun 2016
>
> 1 Jun 2016: release SWSF
>
> What if the deadline is not met? Maybe pushing an urgent BIP102 if things
> become really bad.
>
>
> In any case, I hope a clear decision and road map could be made now. This
> topic has been discussed to death. We are just bringing further uncertainty
> if we keep discussing.
>
>
> Matt Corallo via bitcoin-dev ? 2015-12-16 15:50 ??:
>
>> A large part of your argument is that SW will take longer to deploy
>> than a hard fork, but I completely disagree. Though I do not agree
>> with some people claiming we can deploy SW significantly faster than a
>> hard fork, once the code is ready (probably a six month affair) we can
>> get it deployed very quickly. It's true the ecosystem may take some
>> time to upgrade, but I see that as a feature, not a bug - we can build
>> up some fee pressure with an immediate release valve available for
>> people to use if they want to pay fewer fees.
>>
>>  On the other hand, a hard fork, while simpler for the ecosystem to
>> upgrade to, is a 1-2 year affair (after the code is shipped, so at
>> least 1.5-2.5 from today if we all put off heads down and work). One
>> thing that has concerned me greatly through this whole debate is how
>> quickly people seem to think we can roll out a hard fork. Go look at
>> the distribution of node versions on the network today and work
>> backwards to get nearly every node upgraded... Even with a year
>> between fork-version-release and fork-activation, we'd still kill a
>> bunch of nodes and instead of reducing their security model, lead them
>> to be outright robbed.
>>
>>
> _______________________________________________
>
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151216/5e55d210/attachment.html>

From jtimon at jtimon.cc  Thu Dec 17 13:09:05 2015
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Thu, 17 Dec 2015 14:09:05 +0100
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
	Bitcoin
In-Reply-To: <CAK_HAC-QmFiQGePpPH7n7qV-A4mkQdsWmgwA__mc1GBkTa6oFA@mail.gmail.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<49257841-66C8-4EF7-980B-73DC604CA591@mattcorallo.com>
	<9869fe48a4fc53fc355a35cead73fca2@xbt.hk>
	<CAK_HAC-QmFiQGePpPH7n7qV-A4mkQdsWmgwA__mc1GBkTa6oFA@mail.gmail.com>
Message-ID: <CABm2gDp+UFua=ZqzDFhZ7F6MeLbc_fBv13WYcpttSP1Lyy1ngg@mail.gmail.com>

Although I agree that how safe a pre-hardfork upgrade period is depends on
the complexity of the changes (we should assume everyone may need time to
reimplementat it themselves in their own implementations, not just upgrade
bitcoin core) and bip102 is indeed a very simple hardfork, I think less
than 6 months for a hardfork is starting to push it too much.
For a more complex hardfork (say, a SW hardfork or a collection of many
little fixes) I believe 1 year or more would make more sense.

BIP99 recommends a time threshold (height or median time) + 95% miner
upgrade confirmation with BIP9 (version bits).
So how about the following plan?

1) Deploy BIP102 when its ready + 6 median time months + 95% miner upgrade
confirmation

2) Deploy SW when it's ready + 95% miner upgrade confirmation via bip9.

Note that both "when it's ready" depend on something we are not paying a
lot of attention to: bip9's implementation (just like bip113, bip68-112,
bip99, the coinbase-commitments-cleanup post-SW uncontroversial hardfork,
etc).

Unless I'm missing something, 2 mb x4 = 8mb, so bip102 + SW is already
equivalent to the 2-4-8 "compromise" proposal (which by the way I never
liked, because I don't think anybody should be in a position to
"compromise" anything and because I don't see how "let's avoid an
unavoidable economic change for a little bit longer" arguments can
reasoably claim that "we need to kick the can down the road exactly 3 more
times" or whatever is the reasoning behind it).
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151217/e44914d7/attachment.html>

From sickpig at gmail.com  Thu Dec 17 15:51:19 2015
From: sickpig at gmail.com (sickpig at gmail.com)
Date: Thu, 17 Dec 2015 16:51:19 +0100
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
	Bitcoin
In-Reply-To: <CABm2gDp+UFua=ZqzDFhZ7F6MeLbc_fBv13WYcpttSP1Lyy1ngg@mail.gmail.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<49257841-66C8-4EF7-980B-73DC604CA591@mattcorallo.com>
	<9869fe48a4fc53fc355a35cead73fca2@xbt.hk>
	<CAK_HAC-QmFiQGePpPH7n7qV-A4mkQdsWmgwA__mc1GBkTa6oFA@mail.gmail.com>
	<CABm2gDp+UFua=ZqzDFhZ7F6MeLbc_fBv13WYcpttSP1Lyy1ngg@mail.gmail.com>
Message-ID: <CA+c4Zow4qnhQZFgaY-hOJA4LUtuM_rb1xRbMAOD7gW3i2KzB9A@mail.gmail.com>

On Thu, Dec 17, 2015 at 2:09 PM, Jorge Tim?n <
bitcoin-dev at lists.linuxfoundation.org> wrote:

>
> Unless I'm missing something, 2 mb x4 = 8mb, so bip102 + SW is already
> equivalent to the 2-4-8 "compromise" proposal (which by the way I never
> liked, because I don't think anybody should be in a position to
> "compromise" anything and because I don't see how "let's avoid an
> unavoidable economic change for a little bit longer" arguments can
> reasoably claim that "we need to kick the can down the road exactly 3 more
> times" or whatever is the reasoning behind it).
>

isn't SegWit gain ~75%? hence 2mb x 1.75 = 3.5.

4x is theoric gain you get in case of 2-2 multisig txs.

am I missign something obvious?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151217/54f03105/attachment-0001.html>

From tier.nolan at gmail.com  Thu Dec 17 16:58:02 2015
From: tier.nolan at gmail.com (Tier Nolan)
Date: Thu, 17 Dec 2015 16:58:02 +0000
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <CAPg+sBimfFVea4Sorgx=DaMPVs1k1DrmTA2ZFdLFtxrqKm23-w@mail.gmail.com>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
	<CADm_Wcae7OK7kyXkfh+7XFrc2WYsv7T1Va3_E=5om+XYrL9pOw@mail.gmail.com>
	<CAPg+sBimfFVea4Sorgx=DaMPVs1k1DrmTA2ZFdLFtxrqKm23-w@mail.gmail.com>
Message-ID: <CAE-z3OVD67rDefFzbpuzTO0=54_hJzSfSPg735zk_vjsANmEhQ@mail.gmail.com>

On Wed, Dec 16, 2015 at 9:11 PM, Pieter Wuille via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> We are not avoiding a choice. We don't have the authority to make a choice.
>

This is really the most important question.

Bitcoin is kind of like a republic where there is separation of powers
between various groups.

The power blocs in the process include

- Core Devs
- Miners
- Exchanges
- Merchants
- Customers

Complete agreement is not required for a change.  If merchants and their
customers were to switch to different software, then there is little any of
the other groups could do.

Consensus is nice, certainly, and it is a good social norm to seek
widespread agreement before committing to a decision above objection.
Committing to no block increase is also committing to a decision against
objections.

Having said that, each of the groups are not equal in power and
organisation.

Merchants and their customers have potentially a large amount of power, but
they are disorganised.  There is little way for them to formally express a
view, much less put their power behind making a change.  Their potential
power is crippled by public action problems.

On the other extreme is the core devs. Their power is based on legitimacy
due to having a line of succession starting with Satoshi and respect gained
due to technical and political competence.  Being a small group, they are
organised and they are also more directly involved.

The miners are less centralised, but statements supported by the majority
of the hashing power are regularly made.  The miners' position is that they
want dev consensus.  This means that they have delegated their decision
making to the core devs.

The means that the two most powerful groups in Bitcoin have given the core
devs the authority to make the decision.  They don't have carte blanche
from the miners.

If the core devs made the 2MB hard-fork with a 75% miner threshold, it is
highly likely that the other groups would accept it.

That is the only authority that exists in Bitcoin.  The check is that if
the authority is abused, the other groups can simply leave (or use
checkpointing)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151217/35172889/attachment.html>

From aj at erisian.com.au  Thu Dec 17 17:55:41 2015
From: aj at erisian.com.au (Anthony Towns)
Date: Fri, 18 Dec 2015 03:55:41 +1000
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
 Bitcoin
In-Reply-To: <CA+c4Zow4qnhQZFgaY-hOJA4LUtuM_rb1xRbMAOD7gW3i2KzB9A@mail.gmail.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<49257841-66C8-4EF7-980B-73DC604CA591@mattcorallo.com>
	<9869fe48a4fc53fc355a35cead73fca2@xbt.hk>
	<CAK_HAC-QmFiQGePpPH7n7qV-A4mkQdsWmgwA__mc1GBkTa6oFA@mail.gmail.com>
	<CABm2gDp+UFua=ZqzDFhZ7F6MeLbc_fBv13WYcpttSP1Lyy1ngg@mail.gmail.com>
	<CA+c4Zow4qnhQZFgaY-hOJA4LUtuM_rb1xRbMAOD7gW3i2KzB9A@mail.gmail.com>
Message-ID: <20151217175541.GA10809@sapphire.erisian.com.au>

On Thu, Dec 17, 2015 at 04:51:19PM +0100, sickpig--- via bitcoin-dev wrote:
> On Thu, Dec 17, 2015 at 2:09 PM, Jorge Tim?n wrote:
> > Unless I'm missing something, 2 mb x4 = 8mb, so bip102 + SW is already
> > equivalent to the 2-4-8 "compromise" proposal [...]
> isn't SegWit gain ~75%? hence 2mb x 1.75 = 3.5.

Segwit as proposed gives a 75% *discount* to witness data with the
same limit, so at a 1MB limit, that might give you (eg) 2.05MB made up
of 650kB of base block data plus 1.4MB of witness data; where 650kB +
1.4MB/4 = 1MB at the 1MB limit; or 4.1MB made up of 1.3MB of base plus
2.8MB of witness, for 1.3MB+2.8MB/4 = 2MB at a 2MB limit.

> 4x is theoric gain you get in case of 2-2 multisig txs.

With segregated witness, 2-2 multisig transactions are made up of 94B
of base data, plus about 214B of witness data; discounting the witness
data by 75% gives 94+214/4=148 bytes. That compares to about 301B for
a 2-2 multisig transaction with P2SH rather than segwit, and 301/148
gives about a 2.03x gain, not a 4x gain. A 2.05x gain is what I assumed
to get the numbers above.

You get further improvements with, eg, 3-of-3 multisig, but to get
the full, theoretical 4x gain you'd need a fairly degenerate looking
transaction.

Pay to public key hash with segwit lets you move about half the
transaction data into the witness, giving about a 1.6x improvement by
my count (eg 1.6MB = 800kB of base data plus 800kB of witness data,
where 800kB+800kB/4=1MB), so I think a gain of between 1.6 and 2.0 is
a reasonable expectation to have for the proposed segwit scheme overall.

Cheers,
aj


From jgarzik at gmail.com  Thu Dec 17 18:27:13 2015
From: jgarzik at gmail.com (Jeff Garzik)
Date: Thu, 17 Dec 2015 13:27:13 -0500
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
	Bitcoin
In-Reply-To: <CADm_WcZbbv9zy_5kN264GhYC_kBBr+Leoi0y1PA4pm23CaW3QQ@mail.gmail.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<CAPg+sBhUso0ddfYQMgwF7yX9_VoqP9CZN5h45t3eQi4v3m6f6A@mail.gmail.com>
	<CADm_WcYZq3nzfYMXfzkZsTCsgmzy4L_nYpa5Kax8uF_ajuUTiQ@mail.gmail.com>
	<CAPg+sBiVVcNNHuV9e1SaPoDSMEwjZHL7tQiszxbE2SQYp1Ongw@mail.gmail.com>
	<CADm_WcZbbv9zy_5kN264GhYC_kBBr+Leoi0y1PA4pm23CaW3QQ@mail.gmail.com>
Message-ID: <CADm_WcbiLCU3yuSfWEJbLDWhfc-9kYFJFCo+fRYyENAsvParng@mail.gmail.com>

On Wed, Dec 16, 2015 at 5:09 PM, Jeff Garzik <jgarzik at gmail.com> wrote:

> SW presents a blended price and blended basket of two goods.  You can
> interact with the Service through the blended price, but that does not
> erase the fact that the basket contains two separate from similar resources.
>
> A different set of economic actors uses one resource, and/or both.  There
> are explicit incentives to shift actors from solely using one resource to
> using both.
>

Illustration:  If SW is deployed via soft fork, the count of nodes that
validate witness data is significantly lower than the count of nodes that
validate non-witness data.  Soft forks are not trustless operation, they
depend on miner trust, slowly eroding the trustless validation of older
nodes over time.

Higher security in one data area versus another produces another economic
value distinction between the two goods in the basket, and creates a "pay
more for higher security in core block, pay less for lower security in
witness" dynamic.

This economic distinction is not present if SW is deployed via hard fork.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151217/f0a3b264/attachment.html>

From jgarzik at gmail.com  Thu Dec 17 18:52:39 2015
From: jgarzik at gmail.com (Jeff Garzik)
Date: Thu, 17 Dec 2015 13:52:39 -0500
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
	Bitcoin
In-Reply-To: <2402050984d0076bf0a4556e10962722@xbt.hk>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<CAPg+sBhUso0ddfYQMgwF7yX9_VoqP9CZN5h45t3eQi4v3m6f6A@mail.gmail.com>
	<CADm_WcYZq3nzfYMXfzkZsTCsgmzy4L_nYpa5Kax8uF_ajuUTiQ@mail.gmail.com>
	<CAPg+sBiVVcNNHuV9e1SaPoDSMEwjZHL7tQiszxbE2SQYp1Ongw@mail.gmail.com>
	<CADm_WcZbbv9zy_5kN264GhYC_kBBr+Leoi0y1PA4pm23CaW3QQ@mail.gmail.com>
	<CADm_WcbiLCU3yuSfWEJbLDWhfc-9kYFJFCo+fRYyENAsvParng@mail.gmail.com>
	<2402050984d0076bf0a4556e10962722@xbt.hk>
Message-ID: <CADm_WcbtOE-mxE=nYEAkn84q4eZHMQ7jCpLLrL4EoLguiZNHNg@mail.gmail.com>

On Thu, Dec 17, 2015 at 1:46 PM, jl2012 <jl2012 at xbt.hk> wrote:

> This is not correct.
>
> As only about 1/3 of nodes support BIP65 now, would you consider CLTV tx
> are less secure than others? I don't think so. Since one invalid CLTV tx
> will make the whole block invalid. Having more nodes to fully validate
> non-CLTV txs won't make them any safer. The same logic also applies to SW
> softfork.
>


Yes - the logic applies to all soft forks.  Each soft fork degrades the
security of non-upgraded nodes.

The core design of bitcoin is that trustless nodes validate the work of
miners, not trust them.

Soft forks move in the opposite direction.  Each new soft-forked feature
leans very heavily on miner trust rather than P2P network validation.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151217/b7b9e80f/attachment.html>

From jl2012 at xbt.hk  Thu Dec 17 18:46:06 2015
From: jl2012 at xbt.hk (jl2012)
Date: Thu, 17 Dec 2015 13:46:06 -0500
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
 Bitcoin
In-Reply-To: <CADm_WcbiLCU3yuSfWEJbLDWhfc-9kYFJFCo+fRYyENAsvParng@mail.gmail.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<CAPg+sBhUso0ddfYQMgwF7yX9_VoqP9CZN5h45t3eQi4v3m6f6A@mail.gmail.com>
	<CADm_WcYZq3nzfYMXfzkZsTCsgmzy4L_nYpa5Kax8uF_ajuUTiQ@mail.gmail.com>
	<CAPg+sBiVVcNNHuV9e1SaPoDSMEwjZHL7tQiszxbE2SQYp1Ongw@mail.gmail.com>
	<CADm_WcZbbv9zy_5kN264GhYC_kBBr+Leoi0y1PA4pm23CaW3QQ@mail.gmail.com>
	<CADm_WcbiLCU3yuSfWEJbLDWhfc-9kYFJFCo+fRYyENAsvParng@mail.gmail.com>
Message-ID: <2402050984d0076bf0a4556e10962722@xbt.hk>

This is not correct.

As only about 1/3 of nodes support BIP65 now, would you consider CLTV tx 
are less secure than others? I don't think so. Since one invalid CLTV tx 
will make the whole block invalid. Having more nodes to fully validate 
non-CLTV txs won't make them any safer. The same logic also applies to 
SW softfork.

You may argue that a softfork would make the network as a whole less 
secure, as old nodes have to trust new nodes. However, the security of 
all content in the same block must be the same, by definition.

Anyway, I support SW softfork at the beginning, and eventually (~2 
years) moving to a hardfork with higher block size limit and better 
commitment structure.

Jeff Garzik via bitcoin-dev ? 2015-12-17 13:27 ??:

> 
> Illustration:  If SW is deployed via soft fork, the count of nodes
> that validate witness data is significantly lower than the count of
> nodes that validate non-witness data.  Soft forks are not trustless
> operation, they depend on miner trust, slowly eroding the trustless
> validation of older nodes over time.
> 
> Higher security in one data area versus another produces another
> economic value distinction between the two goods in the basket, and
> creates a "pay more for higher security in core block, pay less for
> lower security in witness" dynamic.
> 
> This economic distinction is not present if SW is deployed via hard
> fork.
> 
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


From pete at petertodd.org  Thu Dec 17 19:44:08 2015
From: pete at petertodd.org (Peter Todd)
Date: Thu, 17 Dec 2015 11:44:08 -0800
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <CAE-z3OVD67rDefFzbpuzTO0=54_hJzSfSPg735zk_vjsANmEhQ@mail.gmail.com>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
	<CADm_Wcae7OK7kyXkfh+7XFrc2WYsv7T1Va3_E=5om+XYrL9pOw@mail.gmail.com>
	<CAPg+sBimfFVea4Sorgx=DaMPVs1k1DrmTA2ZFdLFtxrqKm23-w@mail.gmail.com>
	<CAE-z3OVD67rDefFzbpuzTO0=54_hJzSfSPg735zk_vjsANmEhQ@mail.gmail.com>
Message-ID: <20151217194407.GB1351@muck>

On Thu, Dec 17, 2015 at 04:58:02PM +0000, Tier Nolan via bitcoin-dev wrote:
> This is really the most important question.
> 
> Bitcoin is kind of like a republic where there is separation of powers
> between various groups.
> 
> The power blocs in the process include
> 
> - Core Devs
> - Miners
> - Exchanges
> - Merchants
> - Customers
> 
> Complete agreement is not required for a change.  If merchants and their
> customers were to switch to different software, then there is little any of
> the other groups could do.

If Bitcoin remains decentralized, miners have veto power over any
blocksize increases. You can always soft-fork in a blocksize reduction
in a decentralized blockchain that actually works.

-- 
'peter'[:-1]@petertodd.org
000000000000000001bd68962863e6fa34e9776df361d4926912f52fc5f4b618
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 650 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151217/704b0752/attachment.sig>

From elombrozo at gmail.com  Thu Dec 17 21:18:57 2015
From: elombrozo at gmail.com (Eric Lombrozo)
Date: Thu, 17 Dec 2015 13:18:57 -0800
Subject: [bitcoin-dev] Segregated Witness in the context of
	Scaling	Bitcoin
In-Reply-To: <CADm_WcbtOE-mxE=nYEAkn84q4eZHMQ7jCpLLrL4EoLguiZNHNg@mail.gmail.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<CAPg+sBhUso0ddfYQMgwF7yX9_VoqP9CZN5h45t3eQi4v3m6f6A@mail.gmail.com>
	<CADm_WcYZq3nzfYMXfzkZsTCsgmzy4L_nYpa5Kax8uF_ajuUTiQ@mail.gmail.com>
	<CAPg+sBiVVcNNHuV9e1SaPoDSMEwjZHL7tQiszxbE2SQYp1Ongw@mail.gmail.com>
	<CADm_WcZbbv9zy_5kN264GhYC_kBBr+Leoi0y1PA4pm23CaW3QQ@mail.gmail.com>
	<CADm_WcbiLCU3yuSfWEJbLDWhfc-9kYFJFCo+fRYyENAsvParng@mail.gmail.com>
	<2402050984d0076bf0a4556e10962722@xbt.hk>
	<CADm_WcbtOE-mxE=nYEAkn84q4eZHMQ7jCpLLrL4EoLguiZNHNg@mail.gmail.com>
Message-ID: <FEEEAAAD-8981-4E81-9D06-C5746A1ED123@gmail.com>

Doesn't a good soft fork signaling mechanism along with an activation warning system for non-upgraded nodes (i.e. BIP9, or even block version ISM for that matter) essentially fix this? I don't quite get why this should be an issue.

On December 17, 2015 10:52:39 AM PST, Jeff Garzik via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
>On Thu, Dec 17, 2015 at 1:46 PM, jl2012 <jl2012 at xbt.hk> wrote:
>
>> This is not correct.
>>
>> As only about 1/3 of nodes support BIP65 now, would you consider CLTV
>tx
>> are less secure than others? I don't think so. Since one invalid CLTV
>tx
>> will make the whole block invalid. Having more nodes to fully
>validate
>> non-CLTV txs won't make them any safer. The same logic also applies
>to SW
>> softfork.
>>
>
>
>Yes - the logic applies to all soft forks.  Each soft fork degrades the
>security of non-upgraded nodes.
>
>The core design of bitcoin is that trustless nodes validate the work of
>miners, not trust them.
>
>Soft forks move in the opposite direction.  Each new soft-forked
>feature
>leans very heavily on miner trust rather than P2P network validation.
>
>
>------------------------------------------------------------------------
>
>_______________________________________________
>bitcoin-dev mailing list
>bitcoin-dev at lists.linuxfoundation.org
>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151217/1cefd38f/attachment.html>

From adam at cypherspace.org  Thu Dec 17 21:31:07 2015
From: adam at cypherspace.org (Adam Back)
Date: Thu, 17 Dec 2015 22:31:07 +0100
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
	Bitcoin
In-Reply-To: <CADm_WcbtOE-mxE=nYEAkn84q4eZHMQ7jCpLLrL4EoLguiZNHNg@mail.gmail.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<CAPg+sBhUso0ddfYQMgwF7yX9_VoqP9CZN5h45t3eQi4v3m6f6A@mail.gmail.com>
	<CADm_WcYZq3nzfYMXfzkZsTCsgmzy4L_nYpa5Kax8uF_ajuUTiQ@mail.gmail.com>
	<CAPg+sBiVVcNNHuV9e1SaPoDSMEwjZHL7tQiszxbE2SQYp1Ongw@mail.gmail.com>
	<CADm_WcZbbv9zy_5kN264GhYC_kBBr+Leoi0y1PA4pm23CaW3QQ@mail.gmail.com>
	<CADm_WcbiLCU3yuSfWEJbLDWhfc-9kYFJFCo+fRYyENAsvParng@mail.gmail.com>
	<2402050984d0076bf0a4556e10962722@xbt.hk>
	<CADm_WcbtOE-mxE=nYEAkn84q4eZHMQ7jCpLLrL4EoLguiZNHNg@mail.gmail.com>
Message-ID: <CALqxMTH27TYy+=FriA2Pa56Pzj-aLGDVeY0T7iBga7_3VoAS3g@mail.gmail.com>

While it is interesting to contemplate moving to a world with
hard-fork only upgrades (deprecate soft-forks), now is possibly not
the time to consider that.  Someone can take that topic and make a
what-if sketch for how it could work and put it on the wishlist wiki
if its not already there.

We want to be pragmatic and constructive to reach consensus and that
takes not mixing in what-ifs or orthogonal long standing problems into
the mix, as needing to be fixed now.

Adam


On 17 December 2015 at 19:52, Jeff Garzik via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>
> On Thu, Dec 17, 2015 at 1:46 PM, jl2012 <jl2012 at xbt.hk> wrote:
>>
>> This is not correct.
>>
>> As only about 1/3 of nodes support BIP65 now, would you consider CLTV tx
>> are less secure than others? I don't think so. Since one invalid CLTV tx
>> will make the whole block invalid. Having more nodes to fully validate
>> non-CLTV txs won't make them any safer. The same logic also applies to SW
>> softfork.
>
>
>
> Yes - the logic applies to all soft forks.  Each soft fork degrades the
> security of non-upgraded nodes.
>
> The core design of bitcoin is that trustless nodes validate the work of
> miners, not trust them.
>
> Soft forks move in the opposite direction.  Each new soft-forked feature
> leans very heavily on miner trust rather than P2P network validation.
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

From pieter.wuille at gmail.com  Fri Dec 18 02:30:38 2015
From: pieter.wuille at gmail.com (Pieter Wuille)
Date: Fri, 18 Dec 2015 03:30:38 +0100
Subject: [bitcoin-dev] On the security of softforks
Message-ID: <CAPg+sBjJcqeqGLHnPyWt23z3YoCRGozQupuMxy51J_-hdkKBSA@mail.gmail.com>

Hello all,

For a long time, I was personally of the opinion that soft forks
constituted a mild security reduction for old full nodes, albeit one
that was preferable to hard forks due to being far less risky, easier,
and less forceful to deploy.

After thinking more about this, I'm not convinced that it is even that anymore.

Let's analyze all failure modes (and feel free to let me know whether
I've missed any specific ones):

1) The risk of an old full node wallet accepting a transaction that is
invalid to the new rules.

The receiver wallet chooses what address/script to accept coins on.
They'll upgrade to the new softfork rules before creating an address
that depends on the softfork's features.

So, not a problem.

2) The risk of an old full node wallet accepting a transaction whose
coins passed through a script that depends on the softforked rules.

It is reasonable that the receiver of a transaction places some trust
in the sender, and on the basis of that, decides to reduce the number
of confirmations before acceptance. In case the transaction indirectly
depends on a low-confirmation transaction using softforked rules, it
may be treated as an anyone-can-spend transaction. Obviously, no trust
can be placed in such a transactions not being reorged out and
replaced with an incompatible one.

However, this problem is common for all anyonecanspend transactions,
which are perfectly legal today in the blockchain. So, if this is a
worry, we can solve it by marking incoming transactions as "uncertain
history" in the wallet if they have an anyonecanspend transaction with
less than 6 confirmations in its history. In fact, the same problem to
a lesser extent exists if coins pass through a 1-of-N multisig or so,
because you're not only trusting the (indirect) senders, but also
their potential cosigners.

3) The risk of an SPV node wallet accepting an unconfirmed transaction
which is invalid to new nodes.

Defrauding an SPV wallet with an invalid unconfirmed transaction
doesn't change with the introduction of new consensus rules, as they
don't validate them anyway.

In the case the client trusts the full node peer(s) it is connected to
to do validation before relay, nodes can either indicate (service bit
or new p2p message) which softforks are accepted (as it only matters
to SPV wallets that wish to accept transactions using new style script
anyway), or wallets can rely on the new rules being non-standard even
to old full nodes (which is typically aimed for in softforks).

4) The risk of an SPV node wallet accepting a confirmed transaction
which is invalid to new nodes

Miners can of course construct an invalid block purely for defrauding
SPV nodes, without intending to get that block accepted by full nodes.
That is expensive (no subsidy/fee income for those blocks) and more
importantly it isn't in any way affected by softforks.

So the only place where this matters is where miners create a block
chain that violates the new rules, and still get it accepted. This
requires a hash rate majority, and sufficiently few economically
important full nodes that forking them off is a viable approach.

It's interesting that even though it requires forking off full nodes
(who will notice, there will be an invalid majority hash rate chain to
them), the attack only allows defrauding SPV nodes. It can't be used
to bypass any of the economic properties of the system (as subsidy and
other resource limits are still enforced by old nodes, and invalid
scripts will either not be accepted by old full nodes wallets, or are
as vulnerable as unrelated anyonecanspends).

Furthermore, it's easily preventable by not using the feature in SPV
wallets until a sufficient amount of economically relevant full nodes
are known to have upgraded, or by just waiting for enough
confirmations.



So, we'd of course prefer to have all full nodes enforce all rules,
but the security reduction is not large. On the other hand, there are
also security advantages that softforks offer:

A) Softforks do not require the pervasive consensus that hardforks
need. Soft forks can be deployed without knowing when all full nodes
will adopt the rule, or even whether they will ever adopt it at all.

B) Keeping up with hard forking changes puts load on full node
operators, who may choose to instead switch to delegating full
validation to third parties, which is worse than just validating the
old rules.

C) Hardfork coordination has a centralizing effect on development. As
hardforks can only be deployed with sufficient node deployment, they
can't just be triggered by miner votes. This requires central
coordination to determine flag times, which is incompatible with
having multiple independent consensus changes being proposed. For
softforks, something like BIP9 supports having multiple independent
softforks in flight, that nodes can individually chose to accept or
not, only requiring coordination to not choose clashing bit numbers.
For hardforks, there is effectively no choice but having every
codebase deployed at a particular point in time to support every
possible hard forks (there can still be an additional hashpower based
trigger conditions for hardforks, but all nodes need to support the
fork at the earliest time it can happen, or risk being forked off).

D) If you are concerned about the security degradation a soft fork
might bring, you can always configure your node to treat a (signalled)
softfork as a hardfork, and stop processing blocks if a sortfork
condition is detected. The other direction is not possible.

-- 
Pieter

From j at toom.im  Fri Dec 18 02:47:14 2015
From: j at toom.im (Jonathan Toomim)
Date: Fri, 18 Dec 2015 10:47:14 +0800
Subject: [bitcoin-dev] On the security of softforks
In-Reply-To: <CAPg+sBjJcqeqGLHnPyWt23z3YoCRGozQupuMxy51J_-hdkKBSA@mail.gmail.com>
References: <CAPg+sBjJcqeqGLHnPyWt23z3YoCRGozQupuMxy51J_-hdkKBSA@mail.gmail.com>
Message-ID: <E76D5BF9-41BF-4AF5-BBAC-06F4EF574EBE@toom.im>


On Dec 18, 2015, at 10:30 AM, Pieter Wuille via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:

> 1) The risk of an old full node wallet accepting a transaction that is
> invalid to the new rules.
> 
> The receiver wallet chooses what address/script to accept coins on.
> They'll upgrade to the new softfork rules before creating an address
> that depends on the softfork's features.
> 
> So, not a problem.


Mallory wants to defraud Bob with a 1 BTC payment for some beer. Bob runs the old rules. Bob creates a p2pkh address for Mallory to use. Mallory takes 1 BTC, and creates an invalid SegWit transaction that Bob cannot properly validate and that pays into one of Mallory's wallets. Mallory then immediately spends the unconfirmed transaction into Bob's address. Bob sees what appears to be a valid transaction chain which is not actually valid.

Clueless Carol is one of the 4.9% of miners who forgot to upgrade her mining node. Carol sees that Mallory included an enormous fee in his transactions, so Carol makes sure to include both transactions in her block.

Mallory gets free beer.

Anything I'm missing?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/4389e72a/attachment-0001.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/4389e72a/attachment-0001.sig>

From elombrozo at gmail.com  Fri Dec 18 03:02:36 2015
From: elombrozo at gmail.com (Eric Lombrozo)
Date: Fri, 18 Dec 2015 03:02:36 +0000
Subject: [bitcoin-dev] On the security of softforks
In-Reply-To: <E76D5BF9-41BF-4AF5-BBAC-06F4EF574EBE@toom.im>
Message-ID: <em9d607452-50c0-4aa2-941e-7b637a287a70@platinum>

First of all, that's an expensive beer!

Second of all, any consensus rule change risks non-full-validating or 
non-upgraded nodes seeing invalid confirmations...but assuming a large 
supermajority (i.e. > 95%) of hashing power is behind the new rule, it 
is extremely unlikely that very many invalid confirmations will ever be 
seen by anyone. The number of confirmations you require depends on your 
use case security requirements...and especially during a new rule 
activation, it is probably not a good idea for non-validating nodes or 
non-upgraded nodes to accept coins with low confirmation counts unless 
the risk is accounted for in the use case (i.e. a web hosting provider 
that can shut the user out if fraud is later detected).

Third of all, as long as the rule change activation is signaled in 
blocks, even old nodes will be able to detect that something is fishy 
and warn users to be more cautious (i.e. wait more confirmations or 
immediately upgrade or connect to a different node that has upgraded, 
etc...)

I honestly don't see an issue here - unless you're already violating 
fundamental security assumptions that would make you vulnerable to 
exploitation even without rule changes.

- Eric

------ Original Message ------
From: "Jonathan Toomim via bitcoin-dev" 
<bitcoin-dev at lists.linuxfoundation.org>
To: "Pieter Wuille" <pieter.wuille at gmail.com>
Cc: "Bitcoin Dev" <bitcoin-dev at lists.linuxfoundation.org>
Sent: 12/17/2015 6:47:14 PM
Subject: Re: [bitcoin-dev] On the security of softforks

>
>On Dec 18, 2015, at 10:30 AM, Pieter Wuille via bitcoin-dev 
><bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>>1) The risk of an old full node wallet accepting a transaction that is
>>invalid to the new rules.
>>
>>The receiver wallet chooses what address/script to accept coins on.
>>They'll upgrade to the new softfork rules before creating an address
>>that depends on the softfork's features.
>>
>>So, not a problem.
>
>Mallory wants to defraud Bob with a 1 BTC payment for some beer. Bob 
>runs the old rules. Bob creates a p2pkh address for Mallory to use. 
>Mallory takes 1 BTC, and creates an invalid SegWit transaction that Bob 
>cannot properly validate and that pays into one of Mallory's wallets. 
>Mallory then immediately spends the unconfirmed transaction into Bob's 
>address. Bob sees what appears to be a valid transaction chain which is 
>not actually valid.
>
>Clueless Carol is one of the 4.9% of miners who forgot to upgrade her 
>mining node. Carol sees that Mallory included an enormous fee in his 
>transactions, so Carol makes sure to include both transactions in her 
>block.
>
>Mallory gets free beer.
>
>Anything I'm missing?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/a519b123/attachment.html>

From jl2012 at xbt.hk  Fri Dec 18 03:10:02 2015
From: jl2012 at xbt.hk (jl2012)
Date: Thu, 17 Dec 2015 22:10:02 -0500
Subject: [bitcoin-dev] On the security of softforks
In-Reply-To: <E76D5BF9-41BF-4AF5-BBAC-06F4EF574EBE@toom.im>
References: <CAPg+sBjJcqeqGLHnPyWt23z3YoCRGozQupuMxy51J_-hdkKBSA@mail.gmail.com>
	<E76D5BF9-41BF-4AF5-BBAC-06F4EF574EBE@toom.im>
Message-ID: <6bc6451f3d9cfb5e5a1ba15356c229bc@xbt.hk>

Jonathan Toomim via bitcoin-dev ? 2015-12-17 21:47 ??:
> Mallory wants to defraud Bob with a 1 BTC payment for some beer. Bob
> runs the old rules. Bob creates a p2pkh address for Mallory to use.
> Mallory takes 1 BTC, and creates an invalid SegWit transaction that
> Bob cannot properly validate and that pays into one of Mallory's
> wallets. Mallory then immediately spends the unconfirmed transaction
> into Bob's address. Bob sees what appears to be a valid transaction
> chain which is not actually valid.
> 
> Clueless Carol is one of the 4.9% of miners who forgot to upgrade her
> mining node. Carol sees that Mallory included an enormous fee in his
> transactions, so Carol makes sure to include both transactions in her
> block.
> 
> Mallory gets free beer.
> 
> Anything I'm missing?

You miss the fact that 0-conf is not safe, neither 1-conf. What you are 
suggesting is just a variation of Finney attack.

From jgarzik at gmail.com  Fri Dec 18 05:11:48 2015
From: jgarzik at gmail.com (Jeff Garzik)
Date: Fri, 18 Dec 2015 00:11:48 -0500
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
Message-ID: <CADm_WcbrMyk-=OnQ-3UvnF_8brhn+X2NqRPbo5xUXsbcZpc0=Q@mail.gmail.com>

On Wed, Dec 16, 2015 at 1:34 PM, Pieter Wuille <pieter.wuille at gmail.com>
wrote:

> On Wed, Dec 16, 2015 at 3:53 PM, Jeff Garzik via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org> wrote:
> > 2) If block size stays at 1M, the Bitcoin Core developer team should
> sign a
> > collective note stating their desire to transition to a new economic
> policy,
> > that of "healthy fee market" and strongly urge users to examine their fee
> > policies, wallet software, transaction volumes and other possible User
> > impacting outcomes.
>
> You present this as if the Bitcoin Core development team is in charge
> of deciding the network consensus rules, and is responsible for making
> changes to it in order to satisfy economic demand. If that is the
> case, Bitcoin has failed, in my opinion.
>

Diverging from the satoshi block size change plan[1] and current economics
would seem to require a high level of months-ahead communication to users.




> all. Yes, old full nodes after a soft fork are not able to fully
> validate the rules new miners enforce anymore, but they do still
> verify the rules that their operators opted to enforce. Furthermore,
> they can't be prevented. For that reason, I've proposed, and am
> working hard, on an approach that includes Segregated Witness as a
> first step. It shows the ecosystem that something is being done, it
> kicks the can down the road, it solves/issues half a dozen other
> issues at the same time, and it does not require the degree of
> certainty needed for a hardfork.
>

Segregated Witness does not kick the can, it solves none of the problems
#1, #3 - #8 explicitly defined and listed in email #1.

1)  A plan of "SW + no hard fork" is gambling with ECE risk, gambling there
will be no Fee Event, because the core block size is still heavily
contended -- 100% contended at time out SW rollout.

2) We are only 100% certain that bitcoin works in the
blocks-not-full-on-avg state, where there is a healthy buffer between the
hard limit and the average block size.

There is remains major ECE risk due to the core block size freeze, possibly
pushing the system into a new, untried economic state and causing major
market and actor disruption.  Users of the Service can still drift randomly
and unpredictably into a Fee Event.

SW mitigates this
- only after several months
- only assuming robust adoption rates by up-layer ecosystem software, and
- only assuming transaction volume growth is flat or sub-linear

Those conditions *must* go as planned to fulfill "SW kicked the can" -- a
lot of if's.

As stated, SW is orthogonal to the drift-into-uncharted-waters problem
outlined in email #1, which a short term bump does address.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/df9746ce/attachment.html>

From jtimon at jtimon.cc  Fri Dec 18 05:23:25 2015
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Fri, 18 Dec 2015 06:23:25 +0100
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <20151217194407.GB1351@muck>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
	<CADm_Wcae7OK7kyXkfh+7XFrc2WYsv7T1Va3_E=5om+XYrL9pOw@mail.gmail.com>
	<CAPg+sBimfFVea4Sorgx=DaMPVs1k1DrmTA2ZFdLFtxrqKm23-w@mail.gmail.com>
	<CAE-z3OVD67rDefFzbpuzTO0=54_hJzSfSPg735zk_vjsANmEhQ@mail.gmail.com>
	<20151217194407.GB1351@muck>
Message-ID: <CABm2gDpvW-s0dqvzxukWiaTwtPxAn9Pj2jHxLDz+ARUzwnTE9w@mail.gmail.com>

On Thu, Dec 17, 2015 at 8:44 PM, Peter Todd via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> If Bitcoin remains decentralized, miners have veto power over any
> blocksize increases. You can always soft-fork in a blocksize reduction
> in a decentralized blockchain that actually works.

You can always schism hardfork miners out...

From jtimon at jtimon.cc  Fri Dec 18 05:32:31 2015
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Fri, 18 Dec 2015 06:32:31 +0100
Subject: [bitcoin-dev] On the security of softforks
In-Reply-To: <6bc6451f3d9cfb5e5a1ba15356c229bc@xbt.hk>
References: <CAPg+sBjJcqeqGLHnPyWt23z3YoCRGozQupuMxy51J_-hdkKBSA@mail.gmail.com>
	<E76D5BF9-41BF-4AF5-BBAC-06F4EF574EBE@toom.im>
	<6bc6451f3d9cfb5e5a1ba15356c229bc@xbt.hk>
Message-ID: <CABm2gDqo_W-T3n78O-4wLAv7C2iyJFXo_r6JHxdaStBz9aPocA@mail.gmail.com>

To me it's getting clearer and clearer that th frintier between
softforks and hardforks it's softer than we thought.
Aoftforks should start having a minimum median time deplayment day (be
it height or median time, I don't care, just not header.nTime).
TYDGFHdfthfg64565$%^$

On Fri, Dec 18, 2015 at 4:10 AM, jl2012 via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> Jonathan Toomim via bitcoin-dev ? 2015-12-17 21:47 ??:
>>
>> Mallory wants to defraud Bob with a 1 BTC payment for some beer. Bob
>> runs the old rules. Bob creates a p2pkh address for Mallory to use.
>> Mallory takes 1 BTC, and creates an invalid SegWit transaction that
>> Bob cannot properly validate and that pays into one of Mallory's
>> wallets. Mallory then immediately spends the unconfirmed transaction
>> into Bob's address. Bob sees what appears to be a valid transaction
>> chain which is not actually valid.
>>
>> Clueless Carol is one of the 4.9% of miners who forgot to upgrade her
>> mining node. Carol sees that Mallory included an enormous fee in his
>> transactions, so Carol makes sure to include both transactions in her
>> block.
>>
>> Mallory gets free beer.
>>
>> Anything I'm missing?
>
>
> You miss the fact that 0-conf is not safe, neither 1-conf. What you are
> suggesting is just a variation of Finney attack.
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From aj at erisian.com.au  Fri Dec 18 06:12:23 2015
From: aj at erisian.com.au (Anthony Towns)
Date: Fri, 18 Dec 2015 16:12:23 +1000
Subject: [bitcoin-dev] On the security of softforks
In-Reply-To: <E76D5BF9-41BF-4AF5-BBAC-06F4EF574EBE@toom.im>
References: <CAPg+sBjJcqeqGLHnPyWt23z3YoCRGozQupuMxy51J_-hdkKBSA@mail.gmail.com>
	<E76D5BF9-41BF-4AF5-BBAC-06F4EF574EBE@toom.im>
Message-ID: <20151218061223.GA22650@sapphire.erisian.com.au>

On Fri, Dec 18, 2015 at 10:47:14AM +0800, Jonathan Toomim via bitcoin-dev wrote:
> On Dec 18, 2015, at 10:30 AM, Pieter Wuille via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
> > 1) The risk of an old full node wallet accepting a transaction that is
> > invalid to the new rules.
> > The receiver wallet chooses what address/script to accept coins on.
> > They'll upgrade to the new softfork rules before creating an address
> > that depends on the softfork's features.
> > So, not a problem.
> Mallory wants to defraud Bob with a 1 BTC payment for some beer. Bob
> runs the old rules. Bob creates a p2pkh address for Mallory to
> use. Mallory takes 1 BTC, and creates an invalid SegWit transaction
> that Bob cannot properly validate and that pays into one of Mallory's
> wallets. [...]
> Clueless Carol is one of the 4.9% of miners who forgot to upgrade
> her mining node. Carol sees that Mallory included an enormous fee in
> his transactions, so Carol makes sure to include both transactions in
> her block.

For it to be a "safe" soft fork, the "invalid segwit transaction" should
look non-standard to Carol, and as such she should refuse to mine it.

I think the attack has to go like this:

 * segwit activates; 5% of miners fail to upgrade however

 * Mallory creates a transaction paying to a segwit script
   (ie scriptPubKey is just a 33 byte push) [0]

 * non-upgraded nodes and miners will refuse to forward or mine
   this transaction (a non-p2sh scriptPubKey that just pushes data is
   non-standard) but the upgraded nodes and miners will forward and mine
   it. it will be included in the blockchain by upgraded miners fairly
   soon, and will then be in the UTXO set of non-upgraded miners and
   nodes too.

 * Mallory creates a segwit-invalid spend back to himself (or directly
   to Bob for the 1BTC), ie provides empty scriptSig, but no
   witness data. Upgraded miners and nodes reject the transaction,
   but non-upgraded nodes will relay and mine it afaics.

I *think* that transaction will fail the AreInputsStandard() test on
non-upgraded nodes, and thus still won't be accepted to the mempool
or mined by non-upgraded nodes, and thus no one will see it, or any
descendent transactions. (Upgraded nodes will reject it because it's
segwit invalid, of course)

If it is accepted by some old nodes, that transaction won't ever get many
confirmations -- if Carol mines it, her block will be orphaned by the
upgraded mining majority after the next two or three blocks are found.
With only 5% of hashpower, it will take around three hours for Carol
and friends to find a block in general.

Also, the fact that segwit outputs are "anyone can spend" maybe mitigates
this further -- you could have a vigilante node that creates invalid
segwit txns for every segwit output that just spends the entire thing
to fees. Even if the vigilante's transactions get rejected by nodes who
see Mallory's attempt first, that should still be enough to trigger any
sort of double-spend alerts I can think of, at least if anyone at all
is altruistic enough to run a vigilante node like that in the first place.

> Mallory gets free beer.
> Anything I'm missing?

So I think the only way Mallory gets free beer from you with segwit
soft-fork is if:

 - you're running out of date software and you're ignoring warnings to
   upgrade (block versions have bumped)
 - you've turned off standardness checks
 - you're accepting low-confirmation transactions
 - you're not using any double-spend detection service

If you're not accepting zero-confirmation transactions straight from
the mempool, (ie you require 1 or 2 confirmations) you also need:

 - some non-upgraded miners who have turned off standardness checks
 - your business is setup that an attacker can happily wait hours for
   the transaction to be included in a block before trying to get beer
   from you

In general (IMO), just leaving standardness checks turned on (and waiting
for 6 confirmations before accepting any non-standard transaction) should
be enough to keep you safe from any attack a soft-fork might enable.

Upgrading your software regularly should also be enough to keep you safe
for any soft-fork, and also for any hard-fork, obviously.

Cheers,
aj

[0] Actually, for this attack Mallory could use *any* segwit payment, it
    doesn't have to be his bitcoins to start with, he just has to make
    it look like they finish up with him, which is trivial if segwit
    looks like anyone can spend. Having it be his segwit payment in the
    first place makes it a little easier to ensure his payment is seen
    as the original and not the doublespend though.


From pieter.wuille at gmail.com  Fri Dec 18 07:56:58 2015
From: pieter.wuille at gmail.com (Pieter Wuille)
Date: Fri, 18 Dec 2015 08:56:58 +0100
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <CADm_WcbrMyk-=OnQ-3UvnF_8brhn+X2NqRPbo5xUXsbcZpc0=Q@mail.gmail.com>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
	<CADm_WcbrMyk-=OnQ-3UvnF_8brhn+X2NqRPbo5xUXsbcZpc0=Q@mail.gmail.com>
Message-ID: <CAPg+sBjbATqf8DXGF7obw9a=371zQ_S0EgTapnUmukAVenTneQ@mail.gmail.com>

On Fri, Dec 18, 2015 at 6:11 AM, Jeff Garzik <jgarzik at gmail.com> wrote:
>> You present this as if the Bitcoin Core development team is in charge
>> of deciding the network consensus rules, and is responsible for making
>> changes to it in order to satisfy economic demand. If that is the
>> case, Bitcoin has failed, in my opinion.
>
> Diverging from the satoshi block size change plan[1] and current economics
> would seem to require a high level of months-ahead communication to users.

I don't see any plan, but will you say the same thing when the subsidy
dwindles, and mining income seems to become uncertain? It will equally
be an economic change, which equally well will have been predictable,
and it will equally well be treatable with a hardfork to increase the
subsidy.

Yes, I'm aware the argument above is a straw man, because there was
clear expectation that the subsidy would go down asymptotically, and
much less an expectation that the blocksize would remain fixed
forever.

But I am not against a block size increase hard fork. My talk on
segregated witness even included proposed pursuing a hard fork at a
slightly later stage.

But what you're arguing for is that - despite being completely
expected - blocks grew fuller, and people didn't adapt to block size
pressure and a fee market, so the Core committee now needs to kick the
can down the road, because we can't accept the risk of economic
change. That sounds very much like a bailout to me.

Again. I am not against growth, but increasing in response to fear of
economic change is the wrong approach. Economic change is inevitable.

> Segregated Witness does not kick the can, it solves none of the problems #1,
> #3 - #8 explicitly defined and listed in email #1.
>
> 1)  A plan of "SW + no hard fork" is gambling with ECE risk, gambling there
> will be no Fee Event, because the core block size is still heavily contended
> -- 100% contended at time out SW rollout.

That is an assumption. I expect demand for transactions at a given
feerate to stop growing at a certain contention level (and we've
reached some level of contention already, with mempools not being
cleared for significant amounts of time already).

> SW mitigates this
> - only after several months

That is assuming a hard fork consensus forming, deployment,
activation, ... goes faster than a softfork.

> - only assuming robust adoption rates by up-layer ecosystem software, and

That's not required. Everyone who individually switches to new
transactions gets to do 1.75x more transactions for the same price
(and at the same time gets safer contracts, better script
upgradability, and more security models at their disposal), completely
independent of whether anyone else in the ecosystem does the same.

> - only assuming transaction volume growth is flat or sub-linear

The only question is how many transactions for what price. Contention
always happens at a specific feerate level anyway.

> Those conditions must go as planned to fulfill "SW kicked the can" -- a lot
> of if's.
>
> As stated, SW is orthogonal to the drift-into-uncharted-waters problem
> outlined in email #1, which a short term bump does address.

Both SW and a short bump (which is apparently not what BIP102 does
anymore?) increase capacity available per price, and yes, they are
completely orthogonal.

My only disagreement is the motivation (avoiding economic change, as
opposed to aiming for safe growth) and the claim that a capacity
increase hardfork is easier and safe(r) to roll out quickly than
sortfork SW.

Apart from that, we clearly need to do both at some point.

-- 
Pieter

From tier.nolan at gmail.com  Fri Dec 18 09:44:36 2015
From: tier.nolan at gmail.com (Tier Nolan)
Date: Fri, 18 Dec 2015 09:44:36 +0000
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <20151217194407.GB1351@muck>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
	<CADm_Wcae7OK7kyXkfh+7XFrc2WYsv7T1Va3_E=5om+XYrL9pOw@mail.gmail.com>
	<CAPg+sBimfFVea4Sorgx=DaMPVs1k1DrmTA2ZFdLFtxrqKm23-w@mail.gmail.com>
	<CAE-z3OVD67rDefFzbpuzTO0=54_hJzSfSPg735zk_vjsANmEhQ@mail.gmail.com>
	<20151217194407.GB1351@muck>
Message-ID: <CAE-z3OXQpHFXY_vbTRMMcFSfP4tiFAsmCxkkZ+=Bfkppg=ciMQ@mail.gmail.com>

On Thu, Dec 17, 2015 at 7:44 PM, Peter Todd <pete at petertodd.org> wrote:

> If Bitcoin remains decentralized, miners have veto power over any
> blocksize increases. You can always soft-fork in a blocksize reduction
> in a decentralized blockchain that actually works.
>

The actual users of the system have significant power, if they (could)
choose to use it.  There are "chicken" effects though.  They can impose
costs on the other participants but using those options harms themselves.
If the cost of inaction is greater than the costs of action, then the
chicken effects go away.

In the extreme, they could move away from decentralisation and the concept
of miners and have a centralised checkpointing system.  This would be a
bankrupting cost to miners but at the cost to the users of the
decentralised nature of the system.

At a lower extreme, they could change the mining hash function.  This would
devalue all of the miner's investments.  A whole new program of ASIC
investments would have to happen and the new miners would be significantly
different.  It would also establish that merchants and users are not to be
ignored.  On the other hand, bankrupting miners would make it harder to
convince new miners to make the actual investments in ASICs required to
establish security.

As a gesture, if merchants and exchanges wanted to get their "seat" at the
table, they could create a representative group that insists on a trivial
soft fork.  For example, they could say that they will not accept any block
from block N to block N + 5000 that doesn't have a specific bit set in the
version.

Miners have an advantage where they can say that they have the majority of
the hashing power.  As part of the public action problem that merchants
face, there is no equivalent metric.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/52161768/attachment.html>

From sickpig at gmail.com  Fri Dec 18 10:01:52 2015
From: sickpig at gmail.com (sickpig at gmail.com)
Date: Fri, 18 Dec 2015 11:01:52 +0100
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
	Bitcoin
In-Reply-To: <20151217175541.GA10809@sapphire.erisian.com.au>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<49257841-66C8-4EF7-980B-73DC604CA591@mattcorallo.com>
	<9869fe48a4fc53fc355a35cead73fca2@xbt.hk>
	<CAK_HAC-QmFiQGePpPH7n7qV-A4mkQdsWmgwA__mc1GBkTa6oFA@mail.gmail.com>
	<CABm2gDp+UFua=ZqzDFhZ7F6MeLbc_fBv13WYcpttSP1Lyy1ngg@mail.gmail.com>
	<CA+c4Zow4qnhQZFgaY-hOJA4LUtuM_rb1xRbMAOD7gW3i2KzB9A@mail.gmail.com>
	<20151217175541.GA10809@sapphire.erisian.com.au>
Message-ID: <CA+c4Zoxp91rpcKFqs_FJD_o1e6QzUH0Hk+jm1r9ZVsL4so_VHA@mail.gmail.com>

Anthony,


On Thu, Dec 17, 2015 at 6:55 PM, Anthony Towns via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> On Thu, Dec 17, 2015 at 04:51:19PM +0100, sickpig--- via bitcoin-dev wrote:
> > On Thu, Dec 17, 2015 at 2:09 PM, Jorge Tim?n wrote:
> > > Unless I'm missing something, 2 mb x4 = 8mb, so bip102 + SW is already
> > > equivalent to the 2-4-8 "compromise" proposal [...]
> > isn't SegWit gain ~75%? hence 2mb x 1.75 = 3.5.
>
> Segwit as proposed gives a 75% *discount* to witness data with the
> same limit, so at a 1MB limit, that might give you (eg) 2.05MB made up
> of 650kB of base block data plus 1.4MB of witness data; where 650kB +
> 1.4MB/4 = 1MB at the 1MB limit; or 4.1MB made up of 1.3MB of base plus
> 2.8MB of witness, for 1.3MB+2.8MB/4 = 2MB at a 2MB limit.
>
> > 4x is theoric gain you get in case of 2-2 multisig txs.
>
> With segregated witness, 2-2 multisig transactions are made up of 94B
> of base data, plus about 214B of witness data; discounting the witness
> data by 75% gives 94+214/4=148 bytes. That compares to about 301B for
> a 2-2 multisig transaction with P2SH rather than segwit, and 301/148
> gives about a 2.03x gain, not a 4x gain. A 2.05x gain is what I assumed
> to get the numbers above.
>
> You get further improvements with, eg, 3-of-3 multisig, but to get
> the full, theoretical 4x gain you'd need a fairly degenerate looking
> transaction.
>
> Pay to public key hash with segwit lets you move about half the
> transaction data into the witness, giving about a 1.6x improvement by
> my count (eg 1.6MB = 800kB of base data plus 800kB of witness data,
> where 800kB+800kB/4=1MB), so I think a gain of between 1.6 and 2.0 is
> a reasonable expectation to have for the proposed segwit scheme overall.
>
>
many thanks for the explanation.

so it should be fair to say that BIP 102 + SW would bring a gain between
2*1.6 and 2*2.

Just for the sake of simplicity if we take the middle of the interval we
could say
that BIP102 + SW will bring us a max block (virtual) size equal to 1MB * 2
* 1.8 = 3.6

Is it right?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/7a7b71da/attachment-0001.html>

From sickpig at gmail.com  Fri Dec 18 10:13:33 2015
From: sickpig at gmail.com (sickpig at gmail.com)
Date: Fri, 18 Dec 2015 11:13:33 +0100
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <CAPg+sBjbATqf8DXGF7obw9a=371zQ_S0EgTapnUmukAVenTneQ@mail.gmail.com>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
	<CADm_WcbrMyk-=OnQ-3UvnF_8brhn+X2NqRPbo5xUXsbcZpc0=Q@mail.gmail.com>
	<CAPg+sBjbATqf8DXGF7obw9a=371zQ_S0EgTapnUmukAVenTneQ@mail.gmail.com>
Message-ID: <CA+c4Zozac8=aMrAJ1N_6SR9eBD+w0e70cEnk9CG_2oZ72AS-8g@mail.gmail.com>

On Fri, Dec 18, 2015 at 8:56 AM, Pieter Wuille via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

>
> > - only assuming robust adoption rates by up-layer ecosystem software, and
>
> That's not required. Everyone who individually switches to new
> transactions gets to do 1.75x more transactions for the same price
> (and at the same time gets safer contracts, better script
> upgradability, and more security models at their disposal), completely
> independent of whether anyone else in the ecosystem does the same.
>
>
So hypothetically if wallets/payments processors/full nodes adoption
will take 6 month to get to 50% after the segwit soft-fork activation, this
means that actual network capacity will be increased by:

1.75 x 0.5 + 1 x 0.5 = 1.375

after six month.

An hard-fork on the others side would bring 1.75 since the activation, am I
right?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/79231514/attachment.html>

From pete at petertodd.org  Fri Dec 18 12:18:45 2015
From: pete at petertodd.org (Peter Todd)
Date: Fri, 18 Dec 2015 04:18:45 -0800
Subject: [bitcoin-dev] On the security of softforks
In-Reply-To: <em9d607452-50c0-4aa2-941e-7b637a287a70@platinum>
References: <E76D5BF9-41BF-4AF5-BBAC-06F4EF574EBE@toom.im>
	<em9d607452-50c0-4aa2-941e-7b637a287a70@platinum>
Message-ID: <20151218121845.GB22789@muck>

On Fri, Dec 18, 2015 at 03:02:36AM +0000, Eric Lombrozo via bitcoin-dev wrote:
> First of all, that's an expensive beer!
> 
> Second of all, any consensus rule change risks non-full-validating
> or non-upgraded nodes seeing invalid confirmations...but assuming a
> large supermajority (i.e. > 95%) of hashing power is behind the new
> rule, it is extremely unlikely that very many invalid confirmations
> will ever be seen by anyone. The number of confirmations you require

To clarify, because the 95% of upgraded hashing power is creating valid
blocks from the point of view of the remaining 5%, that 95% majority
will continually reorg the 5% non-upgrading chain. This ensures that the
invalid chain remains short, and thus the # of invalid confirmations
possible remains small. For instance, the chance of getting one invalid
confirmation is 0.05^1 = 5%, two invalid confirmations 0.05^2 = 0.25%, three
0.05^3 = 0.01% etc.

Whereas with a hard fork, the 5% of miners will continue mining on their
own chain. While that chain's length will increase more slowly than
normal, the # of confirmations that non-upgraded clients will see on it
are unbounded.


Anyway, we should write this up as a BIP - there's been a tremendous
amount of misinformation, even flat out lies, floating around on this
subject.

-- 
'peter'[:-1]@petertodd.org
000000000000000001bd68962863e6fa34e9776df361d4926912f52fc5f4b618
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 650 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/6607180d/attachment.sig>

From jgarzik at gmail.com  Fri Dec 18 13:56:45 2015
From: jgarzik at gmail.com (Jeff Garzik)
Date: Fri, 18 Dec 2015 08:56:45 -0500
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <CAPg+sBjbATqf8DXGF7obw9a=371zQ_S0EgTapnUmukAVenTneQ@mail.gmail.com>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
	<CADm_WcbrMyk-=OnQ-3UvnF_8brhn+X2NqRPbo5xUXsbcZpc0=Q@mail.gmail.com>
	<CAPg+sBjbATqf8DXGF7obw9a=371zQ_S0EgTapnUmukAVenTneQ@mail.gmail.com>
Message-ID: <CADm_WcaxTmnAQR6+fejEEFB8bGQviHEE8_G5SQRygXOX1c6XSQ@mail.gmail.com>

On Fri, Dec 18, 2015 at 2:56 AM, Pieter Wuille <pieter.wuille at gmail.com>
wrote:

> On Fri, Dec 18, 2015 at 6:11 AM, Jeff Garzik <jgarzik at gmail.com> wrote:
> >> You present this as if the Bitcoin Core development team is in charge
> >> of deciding the network consensus rules, and is responsible for making
> >> changes to it in order to satisfy economic demand. If that is the
> >> case, Bitcoin has failed, in my opinion.
> >
> > Diverging from the satoshi block size change plan[1] and current
> economics
> > would seem to require a high level of months-ahead communication to
> users.
>
> I don't see any plan, but will you say the same thing when the subsidy
>

Yes, I forgot the link:

[1] https://bitcointalk.org/index.php?topic=1347.msg15366#msg15366



> dwindles, and mining income seems to become uncertain? It will equally
> be an economic change, which equally well will have been predictable,
> and it will equally well be treatable with a hardfork to increase the
> subsidy.
>

That is a red herring.  Nobody I know has proposed this, and I am opposed
to changing that fundamental.

It is well known that the 1M limit was never intended to stay, unlike 21M
coin limit etc.

1M was set high in the beginning because it is a DoS engineering limit, not
an [accidental] economic policy tool.




> But I am not against a block size increase hard fork. My talk on
> segregated witness even included proposed pursuing a hard fork at a
> slightly later stage.
>

Great!



> But what you're arguing for is that - despite being completely
> expected - blocks grew fuller, and people didn't adapt to block size
> pressure and a fee market, so the Core committee now needs to kick the
> can down the road, because we can't accept the risk of economic
> change. That sounds very much like a bailout to me.
>

I am arguing for continuing what we know works.  We are 100% certain
blocks-not-full-on-avg works, where a "buffer" of space exists between avg
block size and hard limit.

Any other avenue is by definition speculation and risk.  You _think_ you
know what a healthy fee market _should_ be.  Massive damage occurs to
bitcoin if you are wrong - and I listed several

vis expectation, there is clear consensus and expectation that block size
would increase, from 2010 onward.  It was always a question of _when_ not
if.

Sticking with 1M presents clear risk of (a) economic fracture and (b)
community fracture.  It quite clearly risks massive change to an unknown
system at an unknown, unpredictable date in the future.

BIP 102 presents an expected upgrade at a predictable date in the future.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/3a05c528/attachment.html>

From pieter.wuille at gmail.com  Fri Dec 18 15:48:43 2015
From: pieter.wuille at gmail.com (Pieter Wuille)
Date: Fri, 18 Dec 2015 16:48:43 +0100
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <CA+c4Zozac8=aMrAJ1N_6SR9eBD+w0e70cEnk9CG_2oZ72AS-8g@mail.gmail.com>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
	<CADm_WcbrMyk-=OnQ-3UvnF_8brhn+X2NqRPbo5xUXsbcZpc0=Q@mail.gmail.com>
	<CAPg+sBjbATqf8DXGF7obw9a=371zQ_S0EgTapnUmukAVenTneQ@mail.gmail.com>
	<CA+c4Zozac8=aMrAJ1N_6SR9eBD+w0e70cEnk9CG_2oZ72AS-8g@mail.gmail.com>
Message-ID: <CAPg+sBhsKD8jd9Y9+ngXY5tKUheO3d4P1b47eYL=Uzpat+KJ2w@mail.gmail.com>

On Dec 18, 2015 2:13 AM, "sickpig at gmail.com" <sickpig at gmail.com> wrote:
> 1.75 x 0.5 + 1 x 0.5 = 1.375
>
> after six month.
>
> An hard-fork on the others side would bring 1.75 since the activation, am
I right?

Yes.

However, SW immediately gives a 1.75 capacity increase for anyone who
adopts it, after the softfork, instantly. They don't need to wait for
anyone else.

A hard fork is an orthogonal improvement, which is also needed if we don't
want to be stuck with a constant maximum ultimately.

Hardforks can however only be deployed at a time when all full node
software can reasonably have agreed to upgrade, while a softfork can be
deployed much earlier.

They are independent improvements, and we need both. I am however of the
opinion that hard forks need a much clearer consensus and much longer
rollout timeframes to be safe (see my thread on the security of softforks).

-- 
Pieter
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/5a9ace37/attachment.html>

From 1240902 at gmail.com  Fri Dec 18 19:17:03 2015
From: 1240902 at gmail.com (Chun Wang)
Date: Sat, 19 Dec 2015 03:17:03 +0800
Subject: [bitcoin-dev] The increase of max block size should be determined
 by block height instead of block time
Message-ID: <CAFzgq-xNZmWrdwCDv3twdsqSWk-FyMuLYJjZ_bA42_5Po0mgEg@mail.gmail.com>

In many BIPs we have seen, include the latest BIP202, it is the block
time that determine the max block size. From from pool's point of
view, it cannot issue a job with a fixed ntime due to the existence of
ntime roll. It is hard to issue a job with the max block size unknown.
For developers, it is also easier to implement if max block size is a
function of block height instead of time. Block height is also much
more simple and elegant than time.

From jtimon at jtimon.cc  Fri Dec 18 19:52:19 2015
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Fri, 18 Dec 2015 20:52:19 +0100
Subject: [bitcoin-dev] The increase of max block size should be
 determined by block height instead of block time
In-Reply-To: <CAFzgq-xNZmWrdwCDv3twdsqSWk-FyMuLYJjZ_bA42_5Po0mgEg@mail.gmail.com>
References: <CAFzgq-xNZmWrdwCDv3twdsqSWk-FyMuLYJjZ_bA42_5Po0mgEg@mail.gmail.com>
Message-ID: <CABm2gDqJgPM1KRRSR3wSEhQ77Oq6P_VVvHwc3Yt4qnkAr7d2nA@mail.gmail.com>

I agree that nHeight is the simplest option and is my preference.
Another option is to use the median time from the previous block (thus you
know whether or not the next block should start the miner confirmation or
not). In fact, if we're going to use bip9  for 95% miner upgrade
confirmation, it would be nice to always pick a difficulty retarget block
(ie block.nHeight % DifficultyAdjustmentInterval == 0).
Actually I would always have an initial height in bip9, for softforks too.
I would also use the sign bit as the "hardfork bit" that gets activated for
the next diff interval after 95% is reached and a hardfork becomes active
(that way even SPV nodes will notice when a softfork  or hardfork happens
and also be able to tell which one is it).
I should update bip99 with all this. And if the 2 mb bump is
uncontroversial, maybe I can add that to the timewarp fix and th recovery
of the other 2 bits in block.nVersion (given that bip102 doesn't seem to
follow bip99's recommendations and doesn't want to give 6 full months as
the pre activation grace period).
On Dec 18, 2015 8:17 PM, "Chun Wang via bitcoin-dev" <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> In many BIPs we have seen, include the latest BIP202, it is the block
> time that determine the max block size. From from pool's point of
> view, it cannot issue a job with a fixed ntime due to the existence of
> ntime roll. It is hard to issue a job with the max block size unknown.
> For developers, it is also easier to implement if max block size is a
> function of block height instead of time. Block height is also much
> more simple and elegant than time.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/80de6201/attachment-0001.html>

From jgarzik at gmail.com  Fri Dec 18 20:02:24 2015
From: jgarzik at gmail.com (Jeff Garzik)
Date: Fri, 18 Dec 2015 15:02:24 -0500
Subject: [bitcoin-dev] The increase of max block size should be
 determined by block height instead of block time
In-Reply-To: <CABm2gDqJgPM1KRRSR3wSEhQ77Oq6P_VVvHwc3Yt4qnkAr7d2nA@mail.gmail.com>
References: <CAFzgq-xNZmWrdwCDv3twdsqSWk-FyMuLYJjZ_bA42_5Po0mgEg@mail.gmail.com>
	<CABm2gDqJgPM1KRRSR3wSEhQ77Oq6P_VVvHwc3Yt4qnkAr7d2nA@mail.gmail.com>
Message-ID: <CADm_WcYFmvu+_OXjm53DHV_q2m8z7Q9zd7QaTrs-uqfiK62CAQ@mail.gmail.com>

>From a code standpoint, based off height is easy.

My first internal version triggered on block 406,800 (~May 5), and each
block increased by 20 bytes thereafter.

It was changed to time, because time was the standard used in years past
for other changes; MTP flag day is more stable than block height.

It is preferred to have a single flag trigger (height or time), rather than
the more complex trigger-on-time, increment-on-height, but any combination
of those will work.

Easy to change code back to height-based...



On Fri, Dec 18, 2015 at 2:52 PM, Jorge Tim?n <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> I agree that nHeight is the simplest option and is my preference.
> Another option is to use the median time from the previous block (thus you
> know whether or not the next block should start the miner confirmation or
> not). In fact, if we're going to use bip9  for 95% miner upgrade
> confirmation, it would be nice to always pick a difficulty retarget block
> (ie block.nHeight % DifficultyAdjustmentInterval == 0).
> Actually I would always have an initial height in bip9, for softforks too.
> I would also use the sign bit as the "hardfork bit" that gets activated
> for the next diff interval after 95% is reached and a hardfork becomes
> active (that way even SPV nodes will notice when a softfork  or hardfork
> happens and also be able to tell which one is it).
> I should update bip99 with all this. And if the 2 mb bump is
> uncontroversial, maybe I can add that to the timewarp fix and th recovery
> of the other 2 bits in block.nVersion (given that bip102 doesn't seem to
> follow bip99's recommendations and doesn't want to give 6 full months as
> the pre activation grace period).
> On Dec 18, 2015 8:17 PM, "Chun Wang via bitcoin-dev" <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> In many BIPs we have seen, include the latest BIP202, it is the block
>> time that determine the max block size. From from pool's point of
>> view, it cannot issue a job with a fixed ntime due to the existence of
>> ntime roll. It is hard to issue a job with the max block size unknown.
>> For developers, it is also easier to implement if max block size is a
>> function of block height instead of time. Block height is also much
>> more simple and elegant than time.
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/770de789/attachment.html>

From jtimon at jtimon.cc  Fri Dec 18 20:10:02 2015
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Fri, 18 Dec 2015 21:10:02 +0100
Subject: [bitcoin-dev] The increase of max block size should be
 determined by block height instead of block time
In-Reply-To: <CADm_WcYFmvu+_OXjm53DHV_q2m8z7Q9zd7QaTrs-uqfiK62CAQ@mail.gmail.com>
References: <CAFzgq-xNZmWrdwCDv3twdsqSWk-FyMuLYJjZ_bA42_5Po0mgEg@mail.gmail.com>
	<CABm2gDqJgPM1KRRSR3wSEhQ77Oq6P_VVvHwc3Yt4qnkAr7d2nA@mail.gmail.com>
	<CADm_WcYFmvu+_OXjm53DHV_q2m8z7Q9zd7QaTrs-uqfiK62CAQ@mail.gmail.com>
Message-ID: <CABm2gDoyzLErwA0g624A2aPUqSi3gXTgcmC7TTTUNDKyecDpuA@mail.gmail.com>

Well, if it's not going to be height, I think median time of the previous
block is better than the time of the current one, and would also solve Chun
Wang's concerns.
But as said I prefer to use heights that correspond to diff recalculation
(because that's the window that bip9 will use for the later 95%
confirmation anyway).
On Dec 18, 2015 9:02 PM, "Jeff Garzik" <jgarzik at gmail.com> wrote:

> From a code standpoint, based off height is easy.
>
> My first internal version triggered on block 406,800 (~May 5), and each
> block increased by 20 bytes thereafter.
>
> It was changed to time, because time was the standard used in years past
> for other changes; MTP flag day is more stable than block height.
>
> It is preferred to have a single flag trigger (height or time), rather
> than the more complex trigger-on-time, increment-on-height, but any
> combination of those will work.
>
> Easy to change code back to height-based...
>
>
>
> On Fri, Dec 18, 2015 at 2:52 PM, Jorge Tim?n <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> I agree that nHeight is the simplest option and is my preference.
>> Another option is to use the median time from the previous block (thus
>> you know whether or not the next block should start the miner confirmation
>> or not). In fact, if we're going to use bip9  for 95% miner upgrade
>> confirmation, it would be nice to always pick a difficulty retarget block
>> (ie block.nHeight % DifficultyAdjustmentInterval == 0).
>> Actually I would always have an initial height in bip9, for softforks too.
>> I would also use the sign bit as the "hardfork bit" that gets activated
>> for the next diff interval after 95% is reached and a hardfork becomes
>> active (that way even SPV nodes will notice when a softfork  or hardfork
>> happens and also be able to tell which one is it).
>> I should update bip99 with all this. And if the 2 mb bump is
>> uncontroversial, maybe I can add that to the timewarp fix and th recovery
>> of the other 2 bits in block.nVersion (given that bip102 doesn't seem to
>> follow bip99's recommendations and doesn't want to give 6 full months as
>> the pre activation grace period).
>> On Dec 18, 2015 8:17 PM, "Chun Wang via bitcoin-dev" <
>> bitcoin-dev at lists.linuxfoundation.org> wrote:
>>
>>> In many BIPs we have seen, include the latest BIP202, it is the block
>>> time that determine the max block size. From from pool's point of
>>> view, it cannot issue a job with a fixed ntime due to the existence of
>>> ntime roll. It is hard to issue a job with the max block size unknown.
>>> For developers, it is also easier to implement if max block size is a
>>> function of block height instead of time. Block height is also much
>>> more simple and elegant than time.
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev at lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/20ab7c5a/attachment.html>

From jgarzik at gmail.com  Fri Dec 18 20:15:54 2015
From: jgarzik at gmail.com (Jeff Garzik)
Date: Fri, 18 Dec 2015 15:15:54 -0500
Subject: [bitcoin-dev] The increase of max block size should be
 determined by block height instead of block time
In-Reply-To: <CABm2gDoyzLErwA0g624A2aPUqSi3gXTgcmC7TTTUNDKyecDpuA@mail.gmail.com>
References: <CAFzgq-xNZmWrdwCDv3twdsqSWk-FyMuLYJjZ_bA42_5Po0mgEg@mail.gmail.com>
	<CABm2gDqJgPM1KRRSR3wSEhQ77Oq6P_VVvHwc3Yt4qnkAr7d2nA@mail.gmail.com>
	<CADm_WcYFmvu+_OXjm53DHV_q2m8z7Q9zd7QaTrs-uqfiK62CAQ@mail.gmail.com>
	<CABm2gDoyzLErwA0g624A2aPUqSi3gXTgcmC7TTTUNDKyecDpuA@mail.gmail.com>
Message-ID: <CADm_Wcah3V7yxCpt97rK89_0GY8HZm6xbCg0yqjKRWak7crt5Q@mail.gmail.com>

My preference is height activation + one step per block (i.e. also
height).  Height seems KISS.

AFAICT most of the attacks would occur around the already-heavily-watched
flag day activation event, in a height based environment, a useful
attribute.

However I would like to hear from others about possible attacks with the
various approaches, before diverging from the default community approach of
switch-based-on-time.






On Fri, Dec 18, 2015 at 3:10 PM, Jorge Tim?n <jtimon at jtimon.cc> wrote:

> Well, if it's not going to be height, I think median time of the previous
> block is better than the time of the current one, and would also solve Chun
> Wang's concerns.
> But as said I prefer to use heights that correspond to diff recalculation
> (because that's the window that bip9 will use for the later 95%
> confirmation anyway).
> On Dec 18, 2015 9:02 PM, "Jeff Garzik" <jgarzik at gmail.com> wrote:
>
>> From a code standpoint, based off height is easy.
>>
>> My first internal version triggered on block 406,800 (~May 5), and each
>> block increased by 20 bytes thereafter.
>>
>> It was changed to time, because time was the standard used in years past
>> for other changes; MTP flag day is more stable than block height.
>>
>> It is preferred to have a single flag trigger (height or time), rather
>> than the more complex trigger-on-time, increment-on-height, but any
>> combination of those will work.
>>
>> Easy to change code back to height-based...
>>
>>
>>
>> On Fri, Dec 18, 2015 at 2:52 PM, Jorge Tim?n <
>> bitcoin-dev at lists.linuxfoundation.org> wrote:
>>
>>> I agree that nHeight is the simplest option and is my preference.
>>> Another option is to use the median time from the previous block (thus
>>> you know whether or not the next block should start the miner confirmation
>>> or not). In fact, if we're going to use bip9  for 95% miner upgrade
>>> confirmation, it would be nice to always pick a difficulty retarget block
>>> (ie block.nHeight % DifficultyAdjustmentInterval == 0).
>>> Actually I would always have an initial height in bip9, for softforks
>>> too.
>>> I would also use the sign bit as the "hardfork bit" that gets activated
>>> for the next diff interval after 95% is reached and a hardfork becomes
>>> active (that way even SPV nodes will notice when a softfork  or hardfork
>>> happens and also be able to tell which one is it).
>>> I should update bip99 with all this. And if the 2 mb bump is
>>> uncontroversial, maybe I can add that to the timewarp fix and th recovery
>>> of the other 2 bits in block.nVersion (given that bip102 doesn't seem to
>>> follow bip99's recommendations and doesn't want to give 6 full months as
>>> the pre activation grace period).
>>> On Dec 18, 2015 8:17 PM, "Chun Wang via bitcoin-dev" <
>>> bitcoin-dev at lists.linuxfoundation.org> wrote:
>>>
>>>> In many BIPs we have seen, include the latest BIP202, it is the block
>>>> time that determine the max block size. From from pool's point of
>>>> view, it cannot issue a job with a fixed ntime due to the existence of
>>>> ntime roll. It is hard to issue a job with the max block size unknown.
>>>> For developers, it is also easier to implement if max block size is a
>>>> function of block height instead of time. Block height is also much
>>>> more simple and elegant than time.
>>>> _______________________________________________
>>>> bitcoin-dev mailing list
>>>> bitcoin-dev at lists.linuxfoundation.org
>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>>
>>>
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev at lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/42f35752/attachment-0001.html>

From jtimon at jtimon.cc  Fri Dec 18 20:20:13 2015
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Fri, 18 Dec 2015 21:20:13 +0100
Subject: [bitcoin-dev] The increase of max block size should be
 determined by block height instead of block time
In-Reply-To: <CADm_Wcah3V7yxCpt97rK89_0GY8HZm6xbCg0yqjKRWak7crt5Q@mail.gmail.com>
References: <CAFzgq-xNZmWrdwCDv3twdsqSWk-FyMuLYJjZ_bA42_5Po0mgEg@mail.gmail.com>
	<CABm2gDqJgPM1KRRSR3wSEhQ77Oq6P_VVvHwc3Yt4qnkAr7d2nA@mail.gmail.com>
	<CADm_WcYFmvu+_OXjm53DHV_q2m8z7Q9zd7QaTrs-uqfiK62CAQ@mail.gmail.com>
	<CABm2gDoyzLErwA0g624A2aPUqSi3gXTgcmC7TTTUNDKyecDpuA@mail.gmail.com>
	<CADm_Wcah3V7yxCpt97rK89_0GY8HZm6xbCg0yqjKRWak7crt5Q@mail.gmail.com>
Message-ID: <CABm2gDo9tjzeEEoY4A2gfbVhpC7TxgrtL1hbv=uJ6t+PajZX+A@mail.gmail.com>

I believe the attacks are the same for height or median time of the prev
block are equal, only the time of the current block has more edge cases.
On Dec 18, 2015 9:15 PM, "Jeff Garzik" <jgarzik at gmail.com> wrote:

> My preference is height activation + one step per block (i.e. also
> height).  Height seems KISS.
>
> AFAICT most of the attacks would occur around the already-heavily-watched
> flag day activation event, in a height based environment, a useful
> attribute.
>
> However I would like to hear from others about possible attacks with the
> various approaches, before diverging from the default community approach of
> switch-based-on-time.
>
>
>
>
>
>
> On Fri, Dec 18, 2015 at 3:10 PM, Jorge Tim?n <jtimon at jtimon.cc> wrote:
>
>> Well, if it's not going to be height, I think median time of the previous
>> block is better than the time of the current one, and would also solve Chun
>> Wang's concerns.
>> But as said I prefer to use heights that correspond to diff recalculation
>> (because that's the window that bip9 will use for the later 95%
>> confirmation anyway).
>> On Dec 18, 2015 9:02 PM, "Jeff Garzik" <jgarzik at gmail.com> wrote:
>>
>>> From a code standpoint, based off height is easy.
>>>
>>> My first internal version triggered on block 406,800 (~May 5), and each
>>> block increased by 20 bytes thereafter.
>>>
>>> It was changed to time, because time was the standard used in years past
>>> for other changes; MTP flag day is more stable than block height.
>>>
>>> It is preferred to have a single flag trigger (height or time), rather
>>> than the more complex trigger-on-time, increment-on-height, but any
>>> combination of those will work.
>>>
>>> Easy to change code back to height-based...
>>>
>>>
>>>
>>> On Fri, Dec 18, 2015 at 2:52 PM, Jorge Tim?n <
>>> bitcoin-dev at lists.linuxfoundation.org> wrote:
>>>
>>>> I agree that nHeight is the simplest option and is my preference.
>>>> Another option is to use the median time from the previous block (thus
>>>> you know whether or not the next block should start the miner confirmation
>>>> or not). In fact, if we're going to use bip9  for 95% miner upgrade
>>>> confirmation, it would be nice to always pick a difficulty retarget block
>>>> (ie block.nHeight % DifficultyAdjustmentInterval == 0).
>>>> Actually I would always have an initial height in bip9, for softforks
>>>> too.
>>>> I would also use the sign bit as the "hardfork bit" that gets activated
>>>> for the next diff interval after 95% is reached and a hardfork becomes
>>>> active (that way even SPV nodes will notice when a softfork  or hardfork
>>>> happens and also be able to tell which one is it).
>>>> I should update bip99 with all this. And if the 2 mb bump is
>>>> uncontroversial, maybe I can add that to the timewarp fix and th recovery
>>>> of the other 2 bits in block.nVersion (given that bip102 doesn't seem to
>>>> follow bip99's recommendations and doesn't want to give 6 full months as
>>>> the pre activation grace period).
>>>> On Dec 18, 2015 8:17 PM, "Chun Wang via bitcoin-dev" <
>>>> bitcoin-dev at lists.linuxfoundation.org> wrote:
>>>>
>>>>> In many BIPs we have seen, include the latest BIP202, it is the block
>>>>> time that determine the max block size. From from pool's point of
>>>>> view, it cannot issue a job with a fixed ntime due to the existence of
>>>>> ntime roll. It is hard to issue a job with the max block size unknown.
>>>>> For developers, it is also easier to implement if max block size is a
>>>>> function of block height instead of time. Block height is also much
>>>>> more simple and elegant than time.
>>>>> _______________________________________________
>>>>> bitcoin-dev mailing list
>>>>> bitcoin-dev at lists.linuxfoundation.org
>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>>>
>>>>
>>>> _______________________________________________
>>>> bitcoin-dev mailing list
>>>> bitcoin-dev at lists.linuxfoundation.org
>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>>
>>>>
>>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/984ddbc3/attachment.html>

From pete at petertodd.org  Fri Dec 18 20:43:35 2015
From: pete at petertodd.org (Peter Todd)
Date: Fri, 18 Dec 2015 20:43:35 +0000
Subject: [bitcoin-dev] The increase of max block size should be
	determined by block height instead of block time
In-Reply-To: <CABm2gDqJgPM1KRRSR3wSEhQ77Oq6P_VVvHwc3Yt4qnkAr7d2nA@mail.gmail.com>
References: <CAFzgq-xNZmWrdwCDv3twdsqSWk-FyMuLYJjZ_bA42_5Po0mgEg@mail.gmail.com>
	<CABm2gDqJgPM1KRRSR3wSEhQ77Oq6P_VVvHwc3Yt4qnkAr7d2nA@mail.gmail.com>
Message-ID: <99EC10C0-CA98-4AA9-B94E-FB6775BAF55B@petertodd.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512



On 18 December 2015 11:52:19 GMT-08:00, "Jorge Tim?n via bitcoin-dev" <bitcoin-dev at lists.linuxfoundation.org> wrote:
>I agree that nHeight is the simplest option and is my preference.
>Another option is to use the median time from the previous block


FWIW all these median time based schemes should be using median time past: the point is to use a time that the block creator has no direct control of, while still tying the rule to wall clock time for planning purposes.
-----BEGIN PGP SIGNATURE-----

iQE9BAEBCgAnIBxQZXRlciBUb2RkIDxwZXRlQHBldGVydG9kZC5vcmc+BQJWdG/r
AAoJEMCF8hzn9Lncz4MH/iYJv6aB9rvfvy1KuSSHAQDQ++6j7Flmk2n8f/S4jt4q
92MZnKDw09HxUJiWvwREi81wHpq4JedgK1Z/+8m3wlK+jaIyWZ7Su+Jm+EqsoOSJ
Sx6oisbyFlhVEUAdaG/XOX/K0mqh01NSvGGpoQjHAYzcG3pI03OC4G7Qg4WGeZLx
O0yb387DmK/of52JGJcei3TUx0w8Up/GdXDqerLxioH7fhGhtGCj0vyD4LugnNLQ
hka5g+hri27YltfaRxncNQ0nZT4rAfgRgRH1Qi3kHnc6ZgRcRjjb36TyrWjZ34eb
9+YDAirFwu8HGmi7lfxh9DDtVjPZCwKal7/rNeRI744=
=7f+W
-----END PGP SIGNATURE-----


From jtimon at jtimon.cc  Fri Dec 18 22:58:29 2015
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Fri, 18 Dec 2015 23:58:29 +0100
Subject: [bitcoin-dev] The increase of max block size should be
 determined by block height instead of block time
In-Reply-To: <99EC10C0-CA98-4AA9-B94E-FB6775BAF55B@petertodd.org>
References: <CAFzgq-xNZmWrdwCDv3twdsqSWk-FyMuLYJjZ_bA42_5Po0mgEg@mail.gmail.com>
	<CABm2gDqJgPM1KRRSR3wSEhQ77Oq6P_VVvHwc3Yt4qnkAr7d2nA@mail.gmail.com>
	<99EC10C0-CA98-4AA9-B94E-FB6775BAF55B@petertodd.org>
Message-ID: <CABm2gDraEJVA3+dLYLEvMYNfDCkuZj+eajLcPsMKXBwnEP+KCw@mail.gmail.com>

On Dec 18, 2015 9:43 PM, "Peter Todd" <pete at petertodd.org> wrote:
> FWIW all these median time based schemes should be using median time
past: the point is to use a time that the block creator has no direct
control of, while still tying the rule to wall clock time for planning
purposes.

Well, if after the "planned clock time" you need to wait for the next diff
retarget and then wait for 95% (bip9) I think the value of being able to
use "human friendly clock time" is very dubious (specially since median
time is different from real-world time anyway).
But yeah, not giving the creator of the current block direct control over
whether its block starts the activation process or not is achieved with
median time of the previous block just as well as nHeight does.
So even if I disagree with the value that median time brings over the
simpler height approach, let's please decide on one and always use that for
both hardforks and softforks as part of bip9 (which we would need to
modify).
An initial time threshold is not necessary for uncontroversial softforks,
but it doesn't hurt (you can always set it in the past if you want to not
use it) and in fact it simplifies bip9's implementation.
Let's please decide once and for all, update bip9 and bip99 and stop doing
something different on every hardfork patch we write.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/ae398fa6/attachment.html>

From mark at friedenbach.org  Sat Dec 19 07:50:41 2015
From: mark at friedenbach.org (Mark Friedenbach)
Date: Sat, 19 Dec 2015 15:50:41 +0800
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
	Bitcoin
In-Reply-To: <CA+c4Zoxp91rpcKFqs_FJD_o1e6QzUH0Hk+jm1r9ZVsL4so_VHA@mail.gmail.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<49257841-66C8-4EF7-980B-73DC604CA591@mattcorallo.com>
	<9869fe48a4fc53fc355a35cead73fca2@xbt.hk>
	<CAK_HAC-QmFiQGePpPH7n7qV-A4mkQdsWmgwA__mc1GBkTa6oFA@mail.gmail.com>
	<CABm2gDp+UFua=ZqzDFhZ7F6MeLbc_fBv13WYcpttSP1Lyy1ngg@mail.gmail.com>
	<CA+c4Zow4qnhQZFgaY-hOJA4LUtuM_rb1xRbMAOD7gW3i2KzB9A@mail.gmail.com>
	<20151217175541.GA10809@sapphire.erisian.com.au>
	<CA+c4Zoxp91rpcKFqs_FJD_o1e6QzUH0Hk+jm1r9ZVsL4so_VHA@mail.gmail.com>
Message-ID: <CAOG=w-tO+QCtobd=pJe_0DTNi53svKkqMY2DMO7a8x53tT0+9w@mail.gmail.com>

Not entirely correct, no. Edge cases also matter. Segwit is described as
4MB because that is the largest possible combined block size that can be
constructed. BIP 102 + segwit would allow a maximum relay of 8MB. So you
have to be confident that an 8MB relay size would be acceptable, even if a
block full of actual transactions would be closer to 3.5MB.

On Fri, Dec 18, 2015 at 6:01 PM, sickpig--- via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Anthony,
>
>
> On Thu, Dec 17, 2015 at 6:55 PM, Anthony Towns via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> On Thu, Dec 17, 2015 at 04:51:19PM +0100, sickpig--- via bitcoin-dev
>> wrote:
>> > On Thu, Dec 17, 2015 at 2:09 PM, Jorge Tim?n wrote:
>> > > Unless I'm missing something, 2 mb x4 = 8mb, so bip102 + SW is already
>> > > equivalent to the 2-4-8 "compromise" proposal [...]
>> > isn't SegWit gain ~75%? hence 2mb x 1.75 = 3.5.
>>
>> Segwit as proposed gives a 75% *discount* to witness data with the
>> same limit, so at a 1MB limit, that might give you (eg) 2.05MB made up
>> of 650kB of base block data plus 1.4MB of witness data; where 650kB +
>> 1.4MB/4 = 1MB at the 1MB limit; or 4.1MB made up of 1.3MB of base plus
>> 2.8MB of witness, for 1.3MB+2.8MB/4 = 2MB at a 2MB limit.
>>
>> > 4x is theoric gain you get in case of 2-2 multisig txs.
>>
>> With segregated witness, 2-2 multisig transactions are made up of 94B
>> of base data, plus about 214B of witness data; discounting the witness
>> data by 75% gives 94+214/4=148 bytes. That compares to about 301B for
>> a 2-2 multisig transaction with P2SH rather than segwit, and 301/148
>> gives about a 2.03x gain, not a 4x gain. A 2.05x gain is what I assumed
>> to get the numbers above.
>>
>> You get further improvements with, eg, 3-of-3 multisig, but to get
>> the full, theoretical 4x gain you'd need a fairly degenerate looking
>> transaction.
>>
>> Pay to public key hash with segwit lets you move about half the
>> transaction data into the witness, giving about a 1.6x improvement by
>> my count (eg 1.6MB = 800kB of base data plus 800kB of witness data,
>> where 800kB+800kB/4=1MB), so I think a gain of between 1.6 and 2.0 is
>> a reasonable expectation to have for the proposed segwit scheme overall.
>>
>>
> many thanks for the explanation.
>
> so it should be fair to say that BIP 102 + SW would bring a gain between
> 2*1.6 and 2*2.
>
> Just for the sake of simplicity if we take the middle of the interval we
> could say
> that BIP102 + SW will bring us a max block (virtual) size equal to 1MB * 2
> * 1.8 = 3.6
>
> Is it right?
>
>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151219/991342c9/attachment-0001.html>

From kiwigb at yahoo.com  Fri Dec 18 20:58:42 2015
From: kiwigb at yahoo.com (gb)
Date: Sat, 19 Dec 2015 09:58:42 +1300
Subject: [bitcoin-dev] The increase of max block size should be
 determined by block height instead of block time
In-Reply-To: <CADm_Wcah3V7yxCpt97rK89_0GY8HZm6xbCg0yqjKRWak7crt5Q@mail.gmail.com>
References: <CAFzgq-xNZmWrdwCDv3twdsqSWk-FyMuLYJjZ_bA42_5Po0mgEg@mail.gmail.com>
	<CABm2gDqJgPM1KRRSR3wSEhQ77Oq6P_VVvHwc3Yt4qnkAr7d2nA@mail.gmail.com>
	<CADm_WcYFmvu+_OXjm53DHV_q2m8z7Q9zd7QaTrs-uqfiK62CAQ@mail.gmail.com>
	<CABm2gDoyzLErwA0g624A2aPUqSi3gXTgcmC7TTTUNDKyecDpuA@mail.gmail.com>
	<CADm_Wcah3V7yxCpt97rK89_0GY8HZm6xbCg0yqjKRWak7crt5Q@mail.gmail.com>
Message-ID: <1450472322.15256.7.camel@yahoo.com>

On Fri, 2015-12-18 at 15:15 -0500, Jeff Garzik via bitcoin-dev wrote:
> My preference is height activation + one step per block (i.e. also
> height).  Height seems KISS.
> 
> 
Under this scheme the size of the step-per-block increase could be
decreased every 210,000 blocks (at time of reward halvings). 

So, a linear growth rate that decreases every ~4 years, ultimately
grandfathering max_block_size increases on the same block-schedule that
reward decreases.



From ctpacia at gmail.com  Sat Dec 19 01:36:21 2015
From: ctpacia at gmail.com (Chris)
Date: Fri, 18 Dec 2015 20:36:21 -0500
Subject: [bitcoin-dev] On the security of softforks
In-Reply-To: <E76D5BF9-41BF-4AF5-BBAC-06F4EF574EBE@toom.im>
References: <CAPg+sBjJcqeqGLHnPyWt23z3YoCRGozQupuMxy51J_-hdkKBSA@mail.gmail.com>
	<E76D5BF9-41BF-4AF5-BBAC-06F4EF574EBE@toom.im>
Message-ID: <5674B495.10903@gmail.com>

On Dec 18, 2015, at 10:30 AM, Pieter Wuille via bitcoin-dev 
<bitcoin-dev at lists.linuxfoundation.org 
<mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:
> 2) The risk of an old full node wallet accepting a transaction whose
> coins passed through a script that depends on the softforked rules.
>
There's that, but there's also a case where an attacker creates a 
majority chain that follows the old rules but not the new ones. 
Non-upgraded nodes would accept a transaction on what they believe to be 
the consensus chain only to find that when they try to spend those coins 
no one accepts them because they were part of an invalid chain.

This has the effect of dropping non upgraded nodes to a form of spv 
security without their consent.

This is in contrast to a hard fork where a full node operator could 
explicitly set their node to accept higher version blocks that it can't 
validate. They get the soft fork functionality back but they have at 
least consented to it rather than have it forced on them. Doing forks 
that way would also have the benefit of notifying the user they are 
accepting unvalidated coins, whereas they wont know that in a soft fork.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151218/94b6565b/attachment.html>

From kanzure at gmail.com  Sat Dec 19 15:48:09 2015
From: kanzure at gmail.com (Bryan Bishop)
Date: Sat, 19 Dec 2015 09:48:09 -0600
Subject: [bitcoin-dev] On the security of softforks
In-Reply-To: <20151218121845.GB22789@muck>
References: <E76D5BF9-41BF-4AF5-BBAC-06F4EF574EBE@toom.im>
	<em9d607452-50c0-4aa2-941e-7b637a287a70@platinum>
	<20151218121845.GB22789@muck>
Message-ID: <CABaSBay+Dqp=Te6Ckj=vmQ_oV7EyRPSPhM2M0ThDaJfYHrzv5A@mail.gmail.com>

On Fri, Dec 18, 2015 at 6:18 AM, Peter Todd via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Anyway, we should write this up as a BIP - there's been a tremendous
> amount of misinformation, even flat out lies, floating around on this
> subject.
>

Er, this sounds like something that should go into bip99. Right?

- Bryan
http://heybryan.org/
1 512 203 0507
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151219/a2ae7187/attachment.html>

From jl2012 at xbt.hk  Sat Dec 19 16:49:25 2015
From: jl2012 at xbt.hk (jl2012)
Date: Sat, 19 Dec 2015 11:49:25 -0500
Subject: [bitcoin-dev] Segregated witness softfork with moderate adoption
 has very small block size effect
Message-ID: <b19eb676c18ba451605cb02159541dd9@xbt.hk>

I have done some calculation for the effect of a SW softfork on the 
actual total block size.

Definitions:

Core block size (CBS): The block size as seen by a non-upgrading full 
node
Witness size (WS): The total size of witness in a block
Total block size (TBS): CBS + WS
Witness discount (WD): A discount factor for witness for calculation of 
VBS (1 = no discount)
Virtual block size (VBS): CBS + (WS * WD)
Witness adoption (WA): Proportion of new format transactions among all 
transactions
Prunable ratio (PR): Proportion of signature data size in a transaction

With some transformation it could be shown that:

  TBS = CBS / (1 - WA * PR) = VBS / (1 - WA * PR * (1 - WD))

sipa suggested a WD of 25%.

The PR heavily depends on the transaction script type and input-output 
ratio. For example, the PR of 1-in 2-out P2PKH and 1-in 1-out 2-of-2 
multisig P2SH are about 47% and 72% respectively. According to sipa's 
presentation, the current average PR on the blockchain is about 60%.

Assuming WD=25% and PR=60%, the MAX TBS with different MAX VBS and WA is 
listed at:

http://i.imgur.com/4bgTMRO.png

The highlight indicates whether the CBS or VBS is the limiting factor.

With moderate SW adoption at 40-60%, the total block size is 1.32-1.56MB 
when MAX VBS is 1.25MB, and 1.22-1.37MB when MAX VBS is 1.00MB.

P2SH has been introduced for 3.5 years and only about 10% of bitcoin is 
stored this way (I can't find proportion of existing P2SH address). A 
1-year adoption rate of 40% for segwit is clearly over-optimistic unless 
the tx fee becomes really high.

(btw the PR of 60% may also be over-optimistic, as using SW nested in 
P2SH will decrease the PR, and therefore TBS becomes even lower)

I am not convinced that SW softfork should be the *only* short term 
scalability solution





From pete at petertodd.org  Sat Dec 19 17:43:10 2015
From: pete at petertodd.org (Peter Todd)
Date: Sat, 19 Dec 2015 09:43:10 -0800
Subject: [bitcoin-dev] Segregated witness softfork with moderate
 adoption has very small block size effect
In-Reply-To: <b19eb676c18ba451605cb02159541dd9@xbt.hk>
References: <b19eb676c18ba451605cb02159541dd9@xbt.hk>
Message-ID: <20151219174309.GB30640@muck>

On Sat, Dec 19, 2015 at 11:49:25AM -0500, jl2012 via bitcoin-dev wrote:
> I have done some calculation for the effect of a SW softfork on the
> actual total block size.

Note how the fact that segwit needs client-side adoption to enable an
actual blocksize increase can be a good thing: it's a clear sign that
the ecosystem as a whole has opted-into a blocksize increase.

Not as good as a direct proof-of-stake vote, and somewhat coercive as a
vote as you pay lower fees, but it's an interesting side-effect.

-- 
'peter'[:-1]@petertodd.org
00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 650 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151219/4cda69f5/attachment.sig>

From onelineproof at gmail.com  Sat Dec 19 17:46:02 2015
From: onelineproof at gmail.com (Andrew)
Date: Sat, 19 Dec 2015 17:46:02 +0000
Subject: [bitcoin-dev] On the security of softforks
In-Reply-To: <E76D5BF9-41BF-4AF5-BBAC-06F4EF574EBE@toom.im>
References: <CAPg+sBjJcqeqGLHnPyWt23z3YoCRGozQupuMxy51J_-hdkKBSA@mail.gmail.com>
	<E76D5BF9-41BF-4AF5-BBAC-06F4EF574EBE@toom.im>
Message-ID: <CAL8tG=nJ=uUfAzFWwm7WUarUofXDAfJVRCQZJ8xuE7r0RtHogQ@mail.gmail.com>

On Fri, Dec 18, 2015 at 2:47 AM, Jonathan Toomim via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

>
> On Dec 18, 2015, at 10:30 AM, Pieter Wuille via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
> 1) The risk of an old full node wallet accepting a transaction that is
> invalid to the new rules.
>
> The receiver wallet chooses what address/script to accept coins on.
> They'll upgrade to the new softfork rules before creating an address
> that depends on the softfork's features.
>
> So, not a problem.
>
>
> Mallory wants to defraud Bob with a 1 BTC payment for some beer. Bob runs
> the old rules. Bob creates a p2pkh address for Mallory to use. Mallory
> takes 1 BTC, and creates an invalid SegWit transaction that Bob cannot
> properly validate and that pays into one of Mallory's wallets. Mallory then
> immediately spends the unconfirmed transaction into Bob's address. Bob sees
> what appears to be a valid transaction chain which is not actually valid.
>
> What do you mean a valid transaction chain? If Bob is fully validating
(even with old software), he should see that Mallory's signature is not on
a transaction with his address.

Do you mean Mallory creates a regular transaction as well as an
Anyone-can-spend segwit transaction that results in double spending in the
same block?

Sorry not sure what I'm missing...


> Clueless Carol is one of the 4.9% of miners who forgot to upgrade her
> mining node. Carol sees that Mallory included an enormous fee in his
> transactions, so Carol makes sure to include both transactions in her
> block.
>
> Mallory gets free beer.
>
> Anything I'm missing?
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>


-- 
PGP: B6AC 822C 451D 6304 6A28  49E9 7DB7 011C D53B 5647
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151219/7dec3138/attachment.html>

From justus at openbitcoinprivacyproject.org  Sat Dec 19 17:55:10 2015
From: justus at openbitcoinprivacyproject.org (Justus Ranvier)
Date: Sat, 19 Dec 2015 11:55:10 -0600
Subject: [bitcoin-dev] Segregated witness softfork with moderate
 adoption has very small block size effect
In-Reply-To: <b19eb676c18ba451605cb02159541dd9@xbt.hk>
References: <b19eb676c18ba451605cb02159541dd9@xbt.hk>
Message-ID: <567599FE.30202@openbitcoinprivacyproject.org>

On 12/19/2015 10:49 AM, jl2012 via bitcoin-dev wrote:
> I am not convinced that SW softfork should be the *only* short term
> scalability solution

I don't think SW is relevant at all with respect to scalability.

Fraud proofs are extremely important from a security perspective. The
network as it exists now places too much trust in miners. Creating a way
for non-full node clients to reject chains with contain invalid
transactions regardless of how much hashing power produces the invalid
chains is essential for the security of the network.

Adding a fraud proof system into blocks means that other features, like
committed UTXO sets, become less unsafe to deploy.

Solving transaction malleability is a very nice to have feature.

A scalability solution, IMHO, is "how do we buy some time to allow
continue usage growth while working on creating a situation where it
becomes safe to eliminate maximum block size as a consensus rule?"

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0xEAD9E623.asc
Type: application/pgp-keys
Size: 23337 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151219/6fc064cf/attachment-0001.bin>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 801 bytes
Desc: OpenPGP digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151219/6fc064cf/attachment-0001.sig>

From pete at petertodd.org  Sat Dec 19 18:20:38 2015
From: pete at petertodd.org (Peter Todd)
Date: Sat, 19 Dec 2015 10:20:38 -0800
Subject: [bitcoin-dev] The increase of max block size should be
 determined by block height instead of block time
In-Reply-To: <CAFzgq-xNZmWrdwCDv3twdsqSWk-FyMuLYJjZ_bA42_5Po0mgEg@mail.gmail.com>
References: <CAFzgq-xNZmWrdwCDv3twdsqSWk-FyMuLYJjZ_bA42_5Po0mgEg@mail.gmail.com>
Message-ID: <20151219182038.GA12893@muck>

On Sat, Dec 19, 2015 at 03:17:03AM +0800, Chun Wang via bitcoin-dev wrote:
> In many BIPs we have seen, include the latest BIP202, it is the block
> time that determine the max block size. From from pool's point of
> view, it cannot issue a job with a fixed ntime due to the existence of
> ntime roll. It is hard to issue a job with the max block size unknown.
> For developers, it is also easier to implement if max block size is a
> function of block height instead of time. Block height is also much
> more simple and elegant than time.

If size is calculated from the median time past, which is fixed for a
given block and has no dependency on the block header's nTime field,
does that solve your problem?

By "median time past" I mean the median time for the previous block.

-- 
'peter'[:-1]@petertodd.org
00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 650 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151219/519299d0/attachment.sig>

From pete at petertodd.org  Sat Dec 19 18:42:40 2015
From: pete at petertodd.org (Peter Todd)
Date: Sat, 19 Dec 2015 10:42:40 -0800
Subject: [bitcoin-dev] We need to fix the block withholding attack
Message-ID: <20151219184240.GB12893@muck>

At the recent Scaling Bitcoin conference in Hong Kong we had a chatham
house rules workshop session attending by representitives of a super
majority of the Bitcoin hashing power.

One of the issues raised by the pools present was block withholding
attacks, which they said are a real issue for them. In particular, pools
are receiving legitimate threats by bad actors threatening to use block
withholding attacks against them. Pools offering their services to the
general public without anti-privacy Know-Your-Customer have little
defense against such attacks, which in turn is a threat to the
decentralization of hashing power: without pools only fairly large
hashing power installations are profitable as variance is a very real
business expense. P2Pool is often brought up as a replacement for pools,
but it itself is still relatively vulnerable to block withholding, and
in any case has many other vulnerabilities and technical issues that has
prevented widespread adoption of P2Pool.

Fixing block withholding is relatively simple, but (so far) requires a
SPV-visible hardfork. (Luke-Jr's two-stage target mechanism) We should
do this hard-fork in conjunction with any blocksize increase, which will
have the desirable side effect of clearly show consent by the entire
ecosystem, SPV clients included.


Note that Ittay Eyal and Emin Gun Sirer have argued(1) that block
witholding attacks are a good thing, as in their model they can be used
by small pools against larger pools, disincentivising large pools.
However this argument is academic and not applicable to the real world,
as a much simpler defense against block withholding attacks is to use
anti-privacy KYC and the legal system combined with the variety of
withholding detection mechanisms only practical for large pools.
Equally, large hashing power installations - a dangerous thing for
decentralization - have no block withholding attack vulnerabilities.

1) http://hackingdistributed.com/2014/12/03/the-miners-dilemma/

-- 
'peter'[:-1]@petertodd.org
00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 650 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151219/8c0d100a/attachment.sig>

From pete at petertodd.org  Sat Dec 19 18:48:33 2015
From: pete at petertodd.org (Peter Todd)
Date: Sat, 19 Dec 2015 10:48:33 -0800
Subject: [bitcoin-dev] Segregated witness softfork with moderate
 adoption has very small block size effect
In-Reply-To: <1709761450550226@web28g.yandex.ru>
References: <b19eb676c18ba451605cb02159541dd9@xbt.hk>
	<20151219174309.GB30640@muck> <1709761450550226@web28g.yandex.ru>
Message-ID: <20151219184833.GC12893@muck>

On Sat, Dec 19, 2015 at 09:37:06PM +0300, Santino Napolitano wrote:
> I disagree. I think all client-side adoption of SW reliably tells you is that those implementers saw value in it greater than the cost of implementation. It's possible what they valued was the malleability fix and didn't see the limited potential circumvention of MAX_BLOCK_SIZE material to their decision.
> 
> They could just as easily attach an OP_RETURN output to all of their transactions which pushes "big blocks please" which would more directly indicate their preference for larger blocks. You could also let hand-signed letters from the heads of businesses explicitly stating their desire speak for their intentions vs. any of this nonsense. Or the media interviews, forum comments, tweets, etc...

Note that English-language measures of Bitcoin usage/activity are very
misleading, as a significant - probably super majority - of economnic
activity happens outside the English language, Western world.
Centralized forums such as twitter and reddit are easily censored and
manipulated. Finally, we can't discount the significant amount of
non-law-abiding Bitcoin economic activity that does happen, and I do not
believe we should adopt consensus-building processes that shut those
stakeholders out of the discussion.

As an aside, I have a friend of mine who made a Bitcoin related product
with non-culturally-specific appeal.  I asked where she was shipping her
product, and it turned out that a super majority went to
non-English-speaking countries. (she might be willing to go on public
record about this; I can ask)

-- 
'peter'[:-1]@petertodd.org
00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 650 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151219/4563cbc9/attachment.sig>

From dscotese at litmocracy.com  Sat Dec 19 19:04:01 2015
From: dscotese at litmocracy.com (Dave Scotese)
Date: Sat, 19 Dec 2015 11:04:01 -0800
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <CAPg+sBhsKD8jd9Y9+ngXY5tKUheO3d4P1b47eYL=Uzpat+KJ2w@mail.gmail.com>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
	<CADm_WcbrMyk-=OnQ-3UvnF_8brhn+X2NqRPbo5xUXsbcZpc0=Q@mail.gmail.com>
	<CAPg+sBjbATqf8DXGF7obw9a=371zQ_S0EgTapnUmukAVenTneQ@mail.gmail.com>
	<CA+c4Zozac8=aMrAJ1N_6SR9eBD+w0e70cEnk9CG_2oZ72AS-8g@mail.gmail.com>
	<CAPg+sBhsKD8jd9Y9+ngXY5tKUheO3d4P1b47eYL=Uzpat+KJ2w@mail.gmail.com>
Message-ID: <CAGLBAhejrg=xgjeSy4UJLt92hUz8H0=a7sO859weX3=+p+hD6Q@mail.gmail.com>

I've already publicly declared that I offer one bitcoin to "those who
suffer from a May 5, 2016 hardfork to 2MB blocks" but that's probably way
too sloppy.  Here's a better idea that transmits the economic power of
merchants and customers (well, anyone with bitcoin) to the miners on whom
they must rely for confirmations:

The bitcoin I offer is part of a fund that, when it reaches 25 BTC, will be
pledged to a miner.  Here is how that miner earns the reward:

   1. Wait until a core dev signs a release of bitcoin core in which the
   limit is double it's current level.
   2. Use the new release to mine, but use a soft limit on the blocksize to
   produce only blocks that are valid according to the old software.
   3. Wait until May 5th, 2016.
   4. Remove the soft limit on blocksize.
   5. Create a block that breaks the old limit and is valid according to
   the new signed release.
   6. Wait for the new large block to be orphaned.

Hopefully, the reward will be greater than 25 bitcoins and therefore cover
the transaction fees.  Of course, if they wait until after the halving in
step 3, then they will get twice the (new, 12.5 btc) reward if they can
arrange for the orphaning of their own block.

Any core dev could do this but I guess it would be playing with fire.  So
maybe Satoshi will do it.  He played with fire (right?) and look how that
worked out.  Come on, someone, be a hero.  Mike and Gavin tried, but I
think they went a little overboard.

Another way to do this is to identify the position in each binary where the
hard limit is stored, and write a little script that will (check the date
first, and then) alter the data at that position so that currently running
bitcoin software can be hot-patched on May 5th without the help of any core
devs (if that would work).  Obviously, the little script should be signed
by a competent programmer whom the user trusts, a slightly less stringent
requirement than being an actual core dev.

notplato

On Fri, Dec 18, 2015 at 7:48 AM, Pieter Wuille via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

>
> On Dec 18, 2015 2:13 AM, "sickpig at gmail.com" <sickpig at gmail.com> wrote:
> > 1.75 x 0.5 + 1 x 0.5 = 1.375
> >
> > after six month.
> >
> > An hard-fork on the others side would bring 1.75 since the activation,
> am I right?
>
> Yes.
>
> However, SW immediately gives a 1.75 capacity increase for anyone who
> adopts it, after the softfork, instantly. They don't need to wait for
> anyone else.
>
> A hard fork is an orthogonal improvement, which is also needed if we don't
> want to be stuck with a constant maximum ultimately.
>
> Hardforks can however only be deployed at a time when all full node
> software can reasonably have agreed to upgrade, while a softfork can be
> deployed much earlier.
>
> They are independent improvements, and we need both. I am however of the
> opinion that hard forks need a much clearer consensus and much longer
> rollout timeframes to be safe (see my thread on the security of softforks).
>
> --
> Pieter
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>


-- 
I like to provide some work at no charge to prove my value. Do you need a
techie?
I own Litmocracy <http://www.litmocracy.com> and Meme Racing
<http://www.memeracing.net> (in alpha).
I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which
now accepts Bitcoin.
I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
"He ought to find it more profitable to play by the rules" - Satoshi
Nakamoto
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151219/81c76c41/attachment.html>

From santino.napolitano at yandex.com  Sat Dec 19 18:37:06 2015
From: santino.napolitano at yandex.com (Santino Napolitano)
Date: Sat, 19 Dec 2015 21:37:06 +0300
Subject: [bitcoin-dev] Segregated witness softfork with moderate
	adoption has very small block size effect
In-Reply-To: <20151219174309.GB30640@muck>
References: <b19eb676c18ba451605cb02159541dd9@xbt.hk>
	<20151219174309.GB30640@muck>
Message-ID: <1709761450550226@web28g.yandex.ru>

I disagree. I think all client-side adoption of SW reliably tells you is that those implementers saw value in it greater than the cost of implementation. It's possible what they valued was the malleability fix and didn't see the limited potential circumvention of MAX_BLOCK_SIZE material to their decision.

They could just as easily attach an OP_RETURN output to all of their transactions which pushes "big blocks please" which would more directly indicate their preference for larger blocks. You could also let hand-signed letters from the heads of businesses explicitly stating their desire speak for their intentions vs. any of this nonsense. Or the media interviews, forum comments, tweets, etc...

19.12.2015, 20:43, "Peter Todd via bitcoin-dev" <bitcoin-dev at lists.linuxfoundation.org>:
> On Sat, Dec 19, 2015 at 11:49:25AM -0500, jl2012 via bitcoin-dev wrote:
>> ?I have done some calculation for the effect of a SW softfork on the
>> ?actual total block size.
>
> Note how the fact that segwit needs client-side adoption to enable an
> actual blocksize increase can be a good thing: it's a clear sign that
> the ecosystem as a whole has opted-into a blocksize increase.
>
> Not as good as a direct proof-of-stake vote, and somewhat coercive as a
> vote as you pay lower fees, but it's an interesting side-effect.
>
> --
> 'peter'[:-1]@petertodd.org
> 00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d
> ,
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From bob_bitcoin at mcelrath.org  Sat Dec 19 19:30:45 2015
From: bob_bitcoin at mcelrath.org (Bob McElrath)
Date: Sat, 19 Dec 2015 19:30:45 +0000
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <20151219184240.GB12893@muck>
References: <20151219184240.GB12893@muck>
Message-ID: <20151219193045.GP20063@mcelrath.org>

Peter Todd via bitcoin-dev [bitcoin-dev at lists.linuxfoundation.org] wrote:
> One of the issues raised by the pools present was block withholding
> attacks, which they said are a real issue for them. In particular, pools
> are receiving legitimate threats by bad actors threatening to use block
> withholding attacks against them.

The only possible other bad actors are other miners.  So who are the "bad actor"
miners?  It's a short list of candidates.

> P2Pool is often brought up as a replacement for pools, but it itself is still
> relatively vulnerable to block withholding, and in any case has many other
> vulnerabilities and technical issues that has prevented widespread adoption of
> P2Pool.

I've been trying to understand this source of "vulnerabilities and technical
issues" with p2pool and have received a lot of contradictory information.  Can
someone in the know summarize what the problems with p2pool are?

The economic situation where miners can be deprived of profit due to the lack of
synchronicity in block updates is a physics problem due to the size of the Earth
and will never be removed.  This is a design flaw in Bitcoin.  Therefore a
different, more comprehensive solution is called for.

My solution to this is somewhat longer term and needs more simulation but
fundamentally removes the source of contention and fixes the design flaw, while
remaining as close "in spirit" to bitcoin as possible:
    https://scalingbitcoin.org/hongkong2015/presentations/DAY2/2_breaking_the_chain_1_mcelrath.pdf
Not only does block withholding simply not work to deny other miners income due
to the absence of orphans, but I explicitly added a dis-incentive against
withholding blocks in terms of the "cohort difficulty".  Other graph-theoretic
quantities are in general possible in the reward function to better align the
incentives of miners with the correct operation of the system.  Also by lowering
the target difficulty and increasing the block (bead) rate, one lowers the
variance of miner income.

Part of the reason I ask is that there has been some interest in testing my
ideas in p2pool itself (or a new similar share pool), but I'm failing to
understand the source of all the complaints about p2pool.

--
Cheers, Bob McElrath

"For every complex problem, there is a solution that is simple, neat, and wrong."
    -- H. L. Mencken 


From jl2012 at xbt.hk  Sat Dec 19 20:03:28 2015
From: jl2012 at xbt.hk (jl2012)
Date: Sat, 19 Dec 2015 15:03:28 -0500
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <20151219184240.GB12893@muck>
References: <20151219184240.GB12893@muck>
Message-ID: <1e6039b8cc5db77ed0a75dfff7863f6d@xbt.hk>

After the meeting I find a softfork solution. It is very inefficient and 
I am leaving it here just for record.

1. In the first output of the second transaction of a block, mining pool 
will commit a random nonce with an OP_RETURN.

2. Mine as normal. When a block is found, the hash is concatenated with 
the committed random nonce and hashed.

3. The resulting hash must be smaller than 2 ^ (256 - 1/64) or the block 
is invalid. That means about 1% of blocks are discarded.

4. For each difficulty retarget, the secondary target is decreased by 2 
^ 1/64.

5. After 546096 blocks or 10 years, the secondary target becomes 2 ^ 
252. Therefore only 1 in 16 hash returned by hasher is really valid. 
This should make the detection of block withholding attack much easier.

All miners have to sacrifice 1% reward for 10 years. Confirmation will 
also be 1% slower than it should be.

If a node (full or SPV) is not updated, it becomes more vulnerable as an 
attacker could mine a chain much faster without following the new rules. 
But this is still a softfork, by definition.

---------------

ok, back to topic. Do you mean this? 
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2012-June/001506.html



Peter Todd via bitcoin-dev ? 2015-12-19 13:42 ??:
> At the recent Scaling Bitcoin conference in Hong Kong we had a chatham
> house rules workshop session attending by representitives of a super
> majority of the Bitcoin hashing power.
> 
> One of the issues raised by the pools present was block withholding
> attacks, which they said are a real issue for them. In particular, 
> pools
> are receiving legitimate threats by bad actors threatening to use block
> withholding attacks against them. Pools offering their services to the
> general public without anti-privacy Know-Your-Customer have little
> defense against such attacks, which in turn is a threat to the
> decentralization of hashing power: without pools only fairly large
> hashing power installations are profitable as variance is a very real
> business expense. P2Pool is often brought up as a replacement for 
> pools,
> but it itself is still relatively vulnerable to block withholding, and
> in any case has many other vulnerabilities and technical issues that 
> has
> prevented widespread adoption of P2Pool.
> 
> Fixing block withholding is relatively simple, but (so far) requires a
> SPV-visible hardfork. (Luke-Jr's two-stage target mechanism) We should
> do this hard-fork in conjunction with any blocksize increase, which 
> will
> have the desirable side effect of clearly show consent by the entire
> ecosystem, SPV clients included.
> 
> 
> Note that Ittay Eyal and Emin Gun Sirer have argued(1) that block
> witholding attacks are a good thing, as in their model they can be used
> by small pools against larger pools, disincentivising large pools.
> However this argument is academic and not applicable to the real world,
> as a much simpler defense against block withholding attacks is to use
> anti-privacy KYC and the legal system combined with the variety of
> withholding detection mechanisms only practical for large pools.
> Equally, large hashing power installations - a dangerous thing for
> decentralization - have no block withholding attack vulnerabilities.
> 
> 1) http://hackingdistributed.com/2014/12/03/the-miners-dilemma/
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


From dscotese at litmocracy.com  Sat Dec 19 23:03:20 2015
From: dscotese at litmocracy.com (Dave Scotese)
Date: Sat, 19 Dec 2015 15:03:20 -0800
Subject: [bitcoin-dev] Segregated Witness in the context of Scaling
	Bitcoin
In-Reply-To: <CAOG=w-tO+QCtobd=pJe_0DTNi53svKkqMY2DMO7a8x53tT0+9w@mail.gmail.com>
References: <CADm_WcYWh5EnBCzQQVc04sf-0seh2zrmc+5dH8Z-Bo78jhPnfA@mail.gmail.com>
	<49257841-66C8-4EF7-980B-73DC604CA591@mattcorallo.com>
	<9869fe48a4fc53fc355a35cead73fca2@xbt.hk>
	<CAK_HAC-QmFiQGePpPH7n7qV-A4mkQdsWmgwA__mc1GBkTa6oFA@mail.gmail.com>
	<CABm2gDp+UFua=ZqzDFhZ7F6MeLbc_fBv13WYcpttSP1Lyy1ngg@mail.gmail.com>
	<CA+c4Zow4qnhQZFgaY-hOJA4LUtuM_rb1xRbMAOD7gW3i2KzB9A@mail.gmail.com>
	<20151217175541.GA10809@sapphire.erisian.com.au>
	<CA+c4Zoxp91rpcKFqs_FJD_o1e6QzUH0Hk+jm1r9ZVsL4so_VHA@mail.gmail.com>
	<CAOG=w-tO+QCtobd=pJe_0DTNi53svKkqMY2DMO7a8x53tT0+9w@mail.gmail.com>
Message-ID: <CAGLBAhei-TobR5qkFKpdMGfj7dA8Jaiy1etLvVM7_=cymyiQkQ@mail.gmail.com>

A couple observations:

   - The consensus block limit is different than the disk space required to
   do validation.  Some participants are worried about one and some about the
   other, and sometimes they feel what amounts to an imaginary contention
   because they perceive these two different things as the same.  They are
   both addressed by scaling solutions, but to different degrees.  This is the
   most concrete I can get about my impression whenever someone writes "not
   correct."  Less concrete is my usual impression, "you're both right."

   - "Kicking the can" has value, but no one has connected the value to the
   phrase, so here it is: The more time we have to make changes, the better
   the changes will be.  Of course it's a trade-off (because we suffer through
   that extra time with the unsolved problem), but using (or thinking of)
   "kicking the can" as bad is a mistake.

   - Whether or not there is a massive campaign targeting *current
   bitcoiners* has a very strong effect on upgrade rates.

It seems that a hardfork to a 2MB limit on 5/5/16 is a tad more than one
LOC, since we want an if-then around it so it doesn't happen til the agreed
date.  But I still support it.

On Fri, Dec 18, 2015 at 11:50 PM, Mark Friedenbach via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Not entirely correct, no. Edge cases also matter. Segwit is described as
> 4MB because that is the largest possible combined block size that can be
> constructed. BIP 102 + segwit would allow a maximum relay of 8MB. So you
> have to be confident that an 8MB relay size would be acceptable, even if a
> block full of actual transactions would be closer to 3.5MB.
>
> On Fri, Dec 18, 2015 at 6:01 PM, sickpig--- via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> Anthony,
>>
>>
>> On Thu, Dec 17, 2015 at 6:55 PM, Anthony Towns via bitcoin-dev <
>> bitcoin-dev at lists.linuxfoundation.org> wrote:
>>
>>> On Thu, Dec 17, 2015 at 04:51:19PM +0100, sickpig--- via bitcoin-dev
>>> wrote:
>>> > On Thu, Dec 17, 2015 at 2:09 PM, Jorge Tim?n wrote:
>>> > > Unless I'm missing something, 2 mb x4 = 8mb, so bip102 + SW is
>>> already
>>> > > equivalent to the 2-4-8 "compromise" proposal [...]
>>> > isn't SegWit gain ~75%? hence 2mb x 1.75 = 3.5.
>>>
>>> Segwit as proposed gives a 75% *discount* to witness data with the
>>> same limit, so at a 1MB limit, that might give you (eg) 2.05MB made up
>>> of 650kB of base block data plus 1.4MB of witness data; where 650kB +
>>> 1.4MB/4 = 1MB at the 1MB limit; or 4.1MB made up of 1.3MB of base plus
>>> 2.8MB of witness, for 1.3MB+2.8MB/4 = 2MB at a 2MB limit.
>>>
>>> > 4x is theoric gain you get in case of 2-2 multisig txs.
>>>
>>> With segregated witness, 2-2 multisig transactions are made up of 94B
>>> of base data, plus about 214B of witness data; discounting the witness
>>> data by 75% gives 94+214/4=148 bytes. That compares to about 301B for
>>> a 2-2 multisig transaction with P2SH rather than segwit, and 301/148
>>> gives about a 2.03x gain, not a 4x gain. A 2.05x gain is what I assumed
>>> to get the numbers above.
>>>
>>> You get further improvements with, eg, 3-of-3 multisig, but to get
>>> the full, theoretical 4x gain you'd need a fairly degenerate looking
>>> transaction.
>>>
>>> Pay to public key hash with segwit lets you move about half the
>>> transaction data into the witness, giving about a 1.6x improvement by
>>> my count (eg 1.6MB = 800kB of base data plus 800kB of witness data,
>>> where 800kB+800kB/4=1MB), so I think a gain of between 1.6 and 2.0 is
>>> a reasonable expectation to have for the proposed segwit scheme overall.
>>>
>>>
>> many thanks for the explanation.
>>
>> so it should be fair to say that BIP 102 + SW would bring a gain between
>> 2*1.6 and 2*2.
>>
>> Just for the sake of simplicity if we take the middle of the interval we
>> could say
>> that BIP102 + SW will bring us a max block (virtual) size equal to 1MB *
>> 2 * 1.8 = 3.6
>>
>> Is it right?
>>
>>
>>
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>


-- 
I like to provide some work at no charge to prove my value. Do you need a
techie?
I own Litmocracy <http://www.litmocracy.com> and Meme Racing
<http://www.memeracing.net> (in alpha).
I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which
now accepts Bitcoin.
I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
"He ought to find it more profitable to play by the rules" - Satoshi
Nakamoto
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151219/367b6d92/attachment.html>

From martijn.meijering at mevs.nl  Thu Dec 17 17:01:30 2015
From: martijn.meijering at mevs.nl (Martijn Meijering)
Date: Thu, 17 Dec 2015 18:01:30 +0100
Subject: [bitcoin-dev] Fwd: Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <CAODYVYdGnA=L2tfGAVid0KogdytvRysoYaYaaftb-0QgT9otvw@mail.gmail.com>
References: <CAODYVYdGnA=L2tfGAVid0KogdytvRysoYaYaaftb-0QgT9otvw@mail.gmail.com>
Message-ID: <CAODYVYeYWqJi1r94j8wUbnH9=fccsktp3N6i1+gO_kfCp3=aOQ@mail.gmail.com>

Appealing moderator's decision. If my post was off-topic, then so is the
whole thread. As for content-heavy, I made a very specific compromise
proposal that I'd like to bring to the developers attention. If this isn't
the place to do that, then I don't know what is, but I'd be happy to repost
to a different forum if you can suggest one that is more appropriate.

===


It looks as if there has been movement on both sides of this debate and the
outlines of a potential medium term truce are starting to appear. Given the
enormous amount of unrest this whole debate has caused I think it would be
highly desirable if both sides could reach an honourable compromise that's
ready to be deployed next year.

I'd like to make a compromise proposal, made up of of uncontroversial
elements of other proposals.

It is intended to achieve the following goals:

- Discourage a potentially disastrous contentious hard fork and
consequently only activate with overwhelming community support.
- Provide immediate relief on both fees and growth potential once activated.
- Provide immediate reassurance that there won't be radical growth for a
number of years without another hard fork.
- Lock in a temporary modest growth path that goes meaningfully beyond BIP
102 so we have at least a few years worth of certainty for those who want
growth.
- Be no more than a kick-the-can-down-the-road solution and do not rule
anything in or out after an interim period of a number of years.
- One-off activation vote to avoid uncertainty hanging over the market
indefinitely.
- Be as simple as possible to allow for the earliest possible deployment.
- Be as uncontroversial as possible to allow for the earliest possible
deployment.
- Do not grow much beyond 2M for at least a year because of concerns
expressed by miners at Scaling Bitcoin.
- Involve both a hard fork and a soft fork.
- Have continuous, gradual growth instead of big step functions.

It's essentially a combination of a variant of 2-4-8 (necessarily a hard
fork) and a soft fork version of Segregated Witness. The advantage of the
2-4-8 hard fork is that it is very simple and can be coded and merged very
quickly, probably in January. The SW soft fork on the other hand can
probably be activated sooner. Iherefore I'd like to see the hard fork
coded, merged and deployed first, then the soft fork merged and deployed
and then the hard fork activated provided it passes its vote. In this way
SW would be sandwiched between the deployment of an as yet inactive 2-4-8
and its activation.

It does not preclude further hard forks at any time, either before or after
the proposed compromise hard fork, although it is specifically intended to
discourage *contentious* hard forks. Non-contentious hard forks that become
possible in the light of experience gained are only to be welcomed of
course.

The details are as follows:

Hard fork with gradual growth:
- +20kb each difficulty adjustment up to 8M.
- Possibly a one-off jump to 2MB, but probably not given that SW will
likely be activated first.
- 95% activation threshold hard-coded to a period of 1000 blocks before a
one-off, fixed block ("election day").
- One month grace period, with a flag date at a fixed block height
("inauguration day") that will enable the growth mechanism supposing the
threshold was met by election day.
- Pull request ready somewhere in January.
- Merged in Q1.
- Release around May.
- Election day January 1st 2017.
- Inauguration day February 1st 2017.

Soft fork Segregated Witness:
- Deployed as already suggested by the developers, but modified to be aware
of the upcoming hard fork.
- Limited to 1MB for the witness section for the first two years after
deployment to take miner concerns into account and allow time for research
into give weak blocks and IBLT research
- Witness section size set to 1x or 2x the size of the main section of the
block after two years.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151217/633b9c14/attachment-0001.html>

From joroark at vt.edu  Sun Dec 20 01:19:59 2015
From: joroark at vt.edu (Douglas Roark)
Date: Sat, 19 Dec 2015 17:19:59 -0800
Subject: [bitcoin-dev] Segregated witness softfork with moderate
 adoption has very small block size effect
In-Reply-To: <b19eb676c18ba451605cb02159541dd9@xbt.hk>
References: <b19eb676c18ba451605cb02159541dd9@xbt.hk>
Message-ID: <5676023F.1050509@vt.edu>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

On 2015/12/19 08:49, jl2012 via bitcoin-dev wrote:
> P2SH has been introduced for 3.5 years and only about 10% of
> bitcoin is stored this way (I can't find proportion of existing
> P2SH address). A 1-year adoption rate of 40% for segwit is clearly
> over-optimistic unless the tx fee becomes really high.

I don't think one can necessarily conflate P2SH and SegWit uptake. In
the case of P2SH, there's the "if it ain't broke, don't fix it"
problem. P2PKH works just fine for an awful lot of Bitcoin users. Why
should they switch over to P2SH? In the absence of a compelling
reason, they'll probably stick to a proven solution. (You can see that
line of thinking anywhere.) Even Armory, which values security and
whiz-bang features over usability, offers P2SH but keeps it off by
default.

Meanwhile, SegWit fixes multiple problems, or at least fixes some
while also sticking a bit of gum on others. True, it'll rely on wallet
uptake. I just think wallet developers will see the value in it. The
big question, of course, is when they'll enable it by default, which
is the only way SegWit will gain serious traction. My personal,
semi-educated guess is that you'll see 3-6 months of wallet
integration and testnet tweaking, then another 3-6 months of mainnet
availability if explicitly enabled by the user, and finally the switch
being thrown for all wallet users. I'm hoping for the aggressive
timeframes. I'm expecting the conservative ones.

Is 40% optimistic? Maybe, and I'd personally like to see it enabled in
concert with a minimal block size increase. I don't think 40% within a
year of deployment is out of the realm of possibility, though.

- -- 
- ---
Douglas Roark
Cryptocurrency, network security, travel, and art.
https://onename.com/droark
joroark at vt.edu
PGP key ID: 26623924
-----BEGIN PGP SIGNATURE-----
Comment: GPGTools - https://gpgtools.org

iQIcBAEBCgAGBQJWdgI/AAoJEEOBHRomYjkkZwsP/iZT+qa/Yp2xIE6ConcTrbbx
IOmz6h4O+ro/Egx/6XrukBSRybn8gsjize279WQWjR0h7O3KS4YAGuR/TKx6IrOw
cz7lZpwC08ZcZb83fUqEqz4q/Rbgp4cPU8WU1niLCYg2OCGqtTEhSSRatmO1ULXp
6KJrBCaoVBzaoqFXx6nXiJF/K1dKZsb/IuFFJZisXEmoDkvTunE82sjHZ+JgmHk9
jhhlk+JU43C7lXXkk+3KPbD+z3dMZNDYrIopaWOUXfk6yXIp8cy7MUK/z58PCm8C
V/pTk0edkMIRxu6ybJLKNNZHqhSQipyEMfG/CojVb6Qtt8zC0RIEUsYj5uDk9RQL
ITxql9RtPHQPx+uoxoCr7Zitx0448YFNpQs6S5/g81vDt7ilh5k5PgnILkMvuA7Y
F6abFvsP/ahmAkisiyDzwMmcM+xzxvJkaY9aDvKy4NNiFw8kUxkAJ2VlMeQvwVTK
2ePzyrFTOGFJRk/a1uTr0aUOxpCdI8rB1O6jcrhmLl2ENRMjN1I3Ksl79Q6h3P+F
zj3hR9CyuXrzoPxAISYF58ot32fil9nEnLcchu2mdWArYKBY2IDWVv7JiK8RCJ8b
2XymxccKsXIUHTrYJfrHg+AeRHkVuV8emyUp95A/8kb8meWbLbmpxOrt6JLEE4k9
qsl9Zkg/0IsCr+JzE6Ko
=696M
-----END PGP SIGNATURE-----

From cp368202 at ohiou.edu  Sun Dec 20 03:34:26 2015
From: cp368202 at ohiou.edu (Chris Priest)
Date: Sat, 19 Dec 2015 19:34:26 -0800
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <20151219184240.GB12893@muck>
References: <20151219184240.GB12893@muck>
Message-ID: <CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>

Block witholding attacks are only possible if you have a majority of
hashpower. If you only have 20% hashpower, you can't do this attack.
Currently, this attack is only a theoretical attack, as the ones with
all the hashpower today are not engaging in this behavior. Even if
someone who had a lot of hashpower decided to pull off this attack,
they wouldn't be able to disrupt much. Once that time comes, then I
think this problem should be solved, until then it should be a low
priority. There are more important things to work on in the meantime.

On 12/19/15, Peter Todd via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> At the recent Scaling Bitcoin conference in Hong Kong we had a chatham
> house rules workshop session attending by representitives of a super
> majority of the Bitcoin hashing power.
>
> One of the issues raised by the pools present was block withholding
> attacks, which they said are a real issue for them. In particular, pools
> are receiving legitimate threats by bad actors threatening to use block
> withholding attacks against them. Pools offering their services to the
> general public without anti-privacy Know-Your-Customer have little
> defense against such attacks, which in turn is a threat to the
> decentralization of hashing power: without pools only fairly large
> hashing power installations are profitable as variance is a very real
> business expense. P2Pool is often brought up as a replacement for pools,
> but it itself is still relatively vulnerable to block withholding, and
> in any case has many other vulnerabilities and technical issues that has
> prevented widespread adoption of P2Pool.
>
> Fixing block withholding is relatively simple, but (so far) requires a
> SPV-visible hardfork. (Luke-Jr's two-stage target mechanism) We should
> do this hard-fork in conjunction with any blocksize increase, which will
> have the desirable side effect of clearly show consent by the entire
> ecosystem, SPV clients included.
>
>
> Note that Ittay Eyal and Emin Gun Sirer have argued(1) that block
> witholding attacks are a good thing, as in their model they can be used
> by small pools against larger pools, disincentivising large pools.
> However this argument is academic and not applicable to the real world,
> as a much simpler defense against block withholding attacks is to use
> anti-privacy KYC and the legal system combined with the variety of
> withholding detection mechanisms only practical for large pools.
> Equally, large hashing power installations - a dangerous thing for
> decentralization - have no block withholding attack vulnerabilities.
>
> 1) http://hackingdistributed.com/2014/12/03/the-miners-dilemma/
>
> --
> 'peter'[:-1]@petertodd.org
> 00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d
>

From lf-lists at mattcorallo.com  Sun Dec 20 03:36:10 2015
From: lf-lists at mattcorallo.com (Matt Corallo)
Date: Sun, 20 Dec 2015 03:36:10 +0000
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
Message-ID: <4882BD35-D890-4860-9222-5C23AEB6AE89@mattcorallo.com>

Peter was referring to pool-block-withholding, not selfish mining.

On December 19, 2015 7:34:26 PM PST, Chris Priest via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
>Block witholding attacks are only possible if you have a majority of
>hashpower. If you only have 20% hashpower, you can't do this attack.
>Currently, this attack is only a theoretical attack, as the ones with
>all the hashpower today are not engaging in this behavior. Even if
>someone who had a lot of hashpower decided to pull off this attack,
>they wouldn't be able to disrupt much. Once that time comes, then I
>think this problem should be solved, until then it should be a low
>priority. There are more important things to work on in the meantime.
>
>On 12/19/15, Peter Todd via bitcoin-dev
><bitcoin-dev at lists.linuxfoundation.org> wrote:
>> At the recent Scaling Bitcoin conference in Hong Kong we had a
>chatham
>> house rules workshop session attending by representitives of a super
>> majority of the Bitcoin hashing power.
>>
>> One of the issues raised by the pools present was block withholding
>> attacks, which they said are a real issue for them. In particular,
>pools
>> are receiving legitimate threats by bad actors threatening to use
>block
>> withholding attacks against them. Pools offering their services to
>the
>> general public without anti-privacy Know-Your-Customer have little
>> defense against such attacks, which in turn is a threat to the
>> decentralization of hashing power: without pools only fairly large
>> hashing power installations are profitable as variance is a very real
>> business expense. P2Pool is often brought up as a replacement for
>pools,
>> but it itself is still relatively vulnerable to block withholding,
>and
>> in any case has many other vulnerabilities and technical issues that
>has
>> prevented widespread adoption of P2Pool.
>>
>> Fixing block withholding is relatively simple, but (so far) requires
>a
>> SPV-visible hardfork. (Luke-Jr's two-stage target mechanism) We
>should
>> do this hard-fork in conjunction with any blocksize increase, which
>will
>> have the desirable side effect of clearly show consent by the entire
>> ecosystem, SPV clients included.
>>
>>
>> Note that Ittay Eyal and Emin Gun Sirer have argued(1) that block
>> witholding attacks are a good thing, as in their model they can be
>used
>> by small pools against larger pools, disincentivising large pools.
>> However this argument is academic and not applicable to the real
>world,
>> as a much simpler defense against block withholding attacks is to use
>> anti-privacy KYC and the legal system combined with the variety of
>> withholding detection mechanisms only practical for large pools.
>> Equally, large hashing power installations - a dangerous thing for
>> decentralization - have no block withholding attack vulnerabilities.
>>
>> 1) http://hackingdistributed.com/2014/12/03/the-miners-dilemma/
>>
>> --
>> 'peter'[:-1]@petertodd.org
>> 00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d
>>
>_______________________________________________
>bitcoin-dev mailing list
>bitcoin-dev at lists.linuxfoundation.org
>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


From cp368202 at ohiou.edu  Sun Dec 20 03:37:26 2015
From: cp368202 at ohiou.edu (Chris Priest)
Date: Sat, 19 Dec 2015 19:37:26 -0800
Subject: [bitcoin-dev] Segregated witness softfork with moderate
 adoption has very small block size effect
In-Reply-To: <20151219174309.GB30640@muck>
References: <b19eb676c18ba451605cb02159541dd9@xbt.hk>
	<20151219174309.GB30640@muck>
Message-ID: <CAAcC9yvBQj=UtbcmnSpMvmwaaRd1ftfg2VLq3JuU2wmp9Vah=Q@mail.gmail.com>

By that same logic, any wallet that implemented P2SH is also voting
for an increased block size, since P2SH results in smaller
transactions, in the same way SW results in smaller transactions.

On 12/19/15, Peter Todd via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> On Sat, Dec 19, 2015 at 11:49:25AM -0500, jl2012 via bitcoin-dev wrote:
>> I have done some calculation for the effect of a SW softfork on the
>> actual total block size.
>
> Note how the fact that segwit needs client-side adoption to enable an
> actual blocksize increase can be a good thing: it's a clear sign that
> the ecosystem as a whole has opted-into a blocksize increase.
>
> Not as good as a direct proof-of-stake vote, and somewhat coercive as a
> vote as you pay lower fees, but it's an interesting side-effect.
>
> --
> 'peter'[:-1]@petertodd.org
> 00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d
>

From cp368202 at ohiou.edu  Sun Dec 20 03:43:59 2015
From: cp368202 at ohiou.edu (Chris Priest)
Date: Sat, 19 Dec 2015 19:43:59 -0800
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <4882BD35-D890-4860-9222-5C23AEB6AE89@mattcorallo.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<4882BD35-D890-4860-9222-5C23AEB6AE89@mattcorallo.com>
Message-ID: <CAAcC9yspsPs3gbumS4rTOg-P-=V=tycn2Z1nVPGGHwJ-nP+PBg@mail.gmail.com>

Then shouldn't this be something the pool deals with, not the bitcoin protocol?

On 12/19/15, Matt Corallo <lf-lists at mattcorallo.com> wrote:
> Peter was referring to pool-block-withholding, not selfish mining.
>
> On December 19, 2015 7:34:26 PM PST, Chris Priest via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org> wrote:
>>Block witholding attacks are only possible if you have a majority of
>>hashpower. If you only have 20% hashpower, you can't do this attack.
>>Currently, this attack is only a theoretical attack, as the ones with
>>all the hashpower today are not engaging in this behavior. Even if
>>someone who had a lot of hashpower decided to pull off this attack,
>>they wouldn't be able to disrupt much. Once that time comes, then I
>>think this problem should be solved, until then it should be a low
>>priority. There are more important things to work on in the meantime.
>>
>>On 12/19/15, Peter Todd via bitcoin-dev
>><bitcoin-dev at lists.linuxfoundation.org> wrote:
>>> At the recent Scaling Bitcoin conference in Hong Kong we had a
>>chatham
>>> house rules workshop session attending by representitives of a super
>>> majority of the Bitcoin hashing power.
>>>
>>> One of the issues raised by the pools present was block withholding
>>> attacks, which they said are a real issue for them. In particular,
>>pools
>>> are receiving legitimate threats by bad actors threatening to use
>>block
>>> withholding attacks against them. Pools offering their services to
>>the
>>> general public without anti-privacy Know-Your-Customer have little
>>> defense against such attacks, which in turn is a threat to the
>>> decentralization of hashing power: without pools only fairly large
>>> hashing power installations are profitable as variance is a very real
>>> business expense. P2Pool is often brought up as a replacement for
>>pools,
>>> but it itself is still relatively vulnerable to block withholding,
>>and
>>> in any case has many other vulnerabilities and technical issues that
>>has
>>> prevented widespread adoption of P2Pool.
>>>
>>> Fixing block withholding is relatively simple, but (so far) requires
>>a
>>> SPV-visible hardfork. (Luke-Jr's two-stage target mechanism) We
>>should
>>> do this hard-fork in conjunction with any blocksize increase, which
>>will
>>> have the desirable side effect of clearly show consent by the entire
>>> ecosystem, SPV clients included.
>>>
>>>
>>> Note that Ittay Eyal and Emin Gun Sirer have argued(1) that block
>>> witholding attacks are a good thing, as in their model they can be
>>used
>>> by small pools against larger pools, disincentivising large pools.
>>> However this argument is academic and not applicable to the real
>>world,
>>> as a much simpler defense against block withholding attacks is to use
>>> anti-privacy KYC and the legal system combined with the variety of
>>> withholding detection mechanisms only practical for large pools.
>>> Equally, large hashing power installations - a dangerous thing for
>>> decentralization - have no block withholding attack vulnerabilities.
>>>
>>> 1) http://hackingdistributed.com/2014/12/03/the-miners-dilemma/
>>>
>>> --
>>> 'peter'[:-1]@petertodd.org
>>> 00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d
>>>
>>_______________________________________________
>>bitcoin-dev mailing list
>>bitcoin-dev at lists.linuxfoundation.org
>>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>

From cp368202 at ohiou.edu  Sun Dec 20 03:47:12 2015
From: cp368202 at ohiou.edu (Chris Priest)
Date: Sat, 19 Dec 2015 19:47:12 -0800
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <219f125cee6ca68fd27016642e38fdf1@xbt.hk>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<219f125cee6ca68fd27016642e38fdf1@xbt.hk>
Message-ID: <CAAcC9ys_t7X0WpQ8W3577M8GLiA5sPV2F1BJ9qZbnMkE-1j3+Q@mail.gmail.com>

On 12/19/15, jl2012 <jl2012 at xbt.hk> wrote:
> Chris Priest via bitcoin-dev ? 2015-12-19 22:34 ??:
>> Block witholding attacks are only possible if you have a majority of
>> hashpower. If you only have 20% hashpower, you can't do this attack.
>> Currently, this attack is only a theoretical attack, as the ones with
>> all the hashpower today are not engaging in this behavior. Even if
>> someone who had a lot of hashpower decided to pull off this attack,
>> they wouldn't be able to disrupt much. Once that time comes, then I
>> think this problem should be solved, until then it should be a low
>> priority. There are more important things to work on in the meantime.
>>
>
> This is not true. For a pool with 5% total hash rate, an attacker only
> needs 0.5% of hash rate to sabotage 10% of their income. It's already
> enough to kill the pool
>
>

This begs the question: If this is such a devastating attack, then why
hasn't this attack brought down every pool in existence? As far as I'm
aware, there are many pools in operation despite this possibility.

From jl2012 at xbt.hk  Sun Dec 20 03:40:06 2015
From: jl2012 at xbt.hk (jl2012)
Date: Sat, 19 Dec 2015 22:40:06 -0500
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
Message-ID: <219f125cee6ca68fd27016642e38fdf1@xbt.hk>

Chris Priest via bitcoin-dev ? 2015-12-19 22:34 ??:
> Block witholding attacks are only possible if you have a majority of
> hashpower. If you only have 20% hashpower, you can't do this attack.
> Currently, this attack is only a theoretical attack, as the ones with
> all the hashpower today are not engaging in this behavior. Even if
> someone who had a lot of hashpower decided to pull off this attack,
> they wouldn't be able to disrupt much. Once that time comes, then I
> think this problem should be solved, until then it should be a low
> priority. There are more important things to work on in the meantime.
> 

This is not true. For a pool with 5% total hash rate, an attacker only
needs 0.5% of hash rate to sabotage 10% of their income. It's already
enough to kill the pool


From jl2012 at xbt.hk  Sun Dec 20 04:24:52 2015
From: jl2012 at xbt.hk (jl2012)
Date: Sat, 19 Dec 2015 23:24:52 -0500
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CAAcC9ys_t7X0WpQ8W3577M8GLiA5sPV2F1BJ9qZbnMkE-1j3+Q@mail.gmail.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<219f125cee6ca68fd27016642e38fdf1@xbt.hk>
	<CAAcC9ys_t7X0WpQ8W3577M8GLiA5sPV2F1BJ9qZbnMkE-1j3+Q@mail.gmail.com>
Message-ID: <aff8da46a69bdd7ef92ca87725866a5c@xbt.hk>

Chris Priest ? 2015-12-19 22:47 ??:
> On 12/19/15, jl2012 <jl2012 at xbt.hk> wrote:
>> Chris Priest via bitcoin-dev ? 2015-12-19 22:34 ??:
>>> Block witholding attacks are only possible if you have a majority of
>>> hashpower. If you only have 20% hashpower, you can't do this attack.
>>> Currently, this attack is only a theoretical attack, as the ones with
>>> all the hashpower today are not engaging in this behavior. Even if
>>> someone who had a lot of hashpower decided to pull off this attack,
>>> they wouldn't be able to disrupt much. Once that time comes, then I
>>> think this problem should be solved, until then it should be a low
>>> priority. There are more important things to work on in the meantime.
>>> 
>> 
>> This is not true. For a pool with 5% total hash rate, an attacker only
>> needs 0.5% of hash rate to sabotage 10% of their income. It's already
>> enough to kill the pool
>> 
>> 
> 
> This begs the question: If this is such a devastating attack, then why
> hasn't this attack brought down every pool in existence? As far as I'm
> aware, there are many pools in operation despite this possibility.

It did happen: 
https://www.reddit.com/r/Bitcoin/comments/28242v/eligius_falls_victim_to_blocksolution_withholding/

The worst thing is that the proof for such attack is probabilistic, not
deterministic.

A smarter attacker may even pretend to be many small miners, make it
even more difficult or impossible to prove who are attacking.


> Then shouldn't this be something the pool deals with, not the bitcoin 
> protocol?

The only solution is to ask for KYC registration, unless one could 
propose
a cryptographic solution that does not require a consensus fork.


From pete at petertodd.org  Sun Dec 20 04:44:51 2015
From: pete at petertodd.org (Peter Todd)
Date: Sat, 19 Dec 2015 20:44:51 -0800
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CAAcC9yspsPs3gbumS4rTOg-P-=V=tycn2Z1nVPGGHwJ-nP+PBg@mail.gmail.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<4882BD35-D890-4860-9222-5C23AEB6AE89@mattcorallo.com>
	<CAAcC9yspsPs3gbumS4rTOg-P-=V=tycn2Z1nVPGGHwJ-nP+PBg@mail.gmail.com>
Message-ID: <20151220044450.GA23942@muck>

On Sat, Dec 19, 2015 at 07:43:59PM -0800, Chris Priest via bitcoin-dev wrote:
> Then shouldn't this be something the pool deals with, not the bitcoin protocol?

There is no known way for pools - especially ones that allow anonymous
hashers - to effectively prevent block withholding attacks without
changing the Bitcoin protocol.

-- 
'peter'[:-1]@petertodd.org
00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 650 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151219/f7fbf6c9/attachment.sig>

From cp368202 at ohiou.edu  Sun Dec 20 07:39:07 2015
From: cp368202 at ohiou.edu (Chris Priest)
Date: Sat, 19 Dec 2015 23:39:07 -0800
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CAPkFh0vNECi1OmBwki+8NNAQbe6EG2FEE4RR5z=kYVLLDFHUXg@mail.gmail.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<219f125cee6ca68fd27016642e38fdf1@xbt.hk>
	<CAAcC9ys_t7X0WpQ8W3577M8GLiA5sPV2F1BJ9qZbnMkE-1j3+Q@mail.gmail.com>
	<aff8da46a69bdd7ef92ca87725866a5c@xbt.hk>
	<CAPkFh0vNECi1OmBwki+8NNAQbe6EG2FEE4RR5z=kYVLLDFHUXg@mail.gmail.com>
Message-ID: <CAAcC9ysejDQ8tyn_hhTQ_1ToKsM2f2rdhG4d1X3O5uuBj1X8NQ@mail.gmail.com>

On 12/19/15, Emin G?n Sirer <el33th4x0r at gmail.com> wrote:
>
> Chris Priest is confusing these attacks with selfish mining, and further,
> his characterization of selfish mining is incorrect. Selfish Mining is
> guaranteed to yield profits for any pool over 33% (as a result, Nick
> Szabo has dubbed this the "34% attack") and it may pay off even
> below that point if the attacker is well-positioned in the network;
> or it may not, depending on the makeup of the rest of the pools
> as well as the network characteristics (the more centralized
> and bigger the other pools are, the less likely it is to pay off). There
> was a lot of noise in the community when the SM paper came out,
> so there are tons of incorrect response narrative out there. By now,
> everyone who seems to be Bitcoin competent sees SM as a
> concern, and Ethereum has already adopted our fix. I'd have hoped
> that a poster to this list would be better informed than to repeat the
> claim that "majority will protect Bitcoin" to refute a paper whose title
> is "majority is not enough."

http://www.coindesk.com/bitcoin-mining-network-vulnerability/

just sayin'...

But anyways, I agree with you on the rest of your email. This is only
really an attack from the perspective of the mining pool. From the
user's perspective, its not an attack at all. Imagine your aunt who
has bitcoin on a SPV wallet on her iphone. Does she care that two
mining pools are attacking each other? Its has nothing to do with her,
and it has nothing to do with most users or bitcoin either. From the
bitcoin user's perspective, the mining pool landscape *should* be
constantly changing. Fixing this "attack" is promoting mining pool
statism. Existing mining pools will have an advantage over up and
coming mining pools. That is not an advantage that is best for bitcoin
from the user's perspective.

Now, on the other hand, if this technique is used so much, it results
in too many pools getting shut down such that the difficulty starts to
decrease, *then* maybe it might be time to start thinking about fixing
this issue. The difficulty dropping means the security of the network
is decreased, which *does* have an effect on every user.

From el33th4x0r at gmail.com  Sun Dec 20 05:12:49 2015
From: el33th4x0r at gmail.com (=?UTF-8?Q?Emin_G=C3=BCn_Sirer?=)
Date: Sun, 20 Dec 2015 00:12:49 -0500
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <aff8da46a69bdd7ef92ca87725866a5c@xbt.hk>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<219f125cee6ca68fd27016642e38fdf1@xbt.hk>
	<CAAcC9ys_t7X0WpQ8W3577M8GLiA5sPV2F1BJ9qZbnMkE-1j3+Q@mail.gmail.com>
	<aff8da46a69bdd7ef92ca87725866a5c@xbt.hk>
Message-ID: <CAPkFh0vNECi1OmBwki+8NNAQbe6EG2FEE4RR5z=kYVLLDFHUXg@mail.gmail.com>

There's quite a bit of confusion in this thread.

Peter is referring to block withholding attacks. Ittay Eyal (as sole
author -- I was not involved in this paper [1]) was the first
to analyze these attacks and to discover a fascinating, paradoxical
result. An attacker pool (A) can take a certain portion of its hashpower,
use it to mine on behalf of victim pool (B), furnish partial proofs of work
to B, but discard any full blocks it discovers. If A picks the amount of
attacking hashpower judiciously, it can make more money using this
attack, than it would if it were to use 100% of its hashpower for its own
mining. This last sentence should sound non-sensical to most of you,
at least, it did to me. Ittay did the math, and his paper can tell you
exactly how much of your hashpower you need to peel off and use
to attack another open pool, and you will come out ahead.

Chris Priest is confusing these attacks with selfish mining, and further,
his characterization of selfish mining is incorrect. Selfish Mining is
guaranteed to yield profits for any pool over 33% (as a result, Nick
Szabo has dubbed this the "34% attack") and it may pay off even
below that point if the attacker is well-positioned in the network;
or it may not, depending on the makeup of the rest of the pools
as well as the network characteristics (the more centralized
and bigger the other pools are, the less likely it is to pay off). There
was a lot of noise in the community when the SM paper came out,
so there are tons of incorrect response narrative out there. By now,
everyone who seems to be Bitcoin competent sees SM as a
concern, and Ethereum has already adopted our fix. I'd have hoped
that a poster to this list would be better informed than to repeat the
claim that "majority will protect Bitcoin" to refute a paper whose title
is "majority is not enough."

Back to Ittay's paradoxical discovery:

We have seen pool-block withholding attacks before; I believe Eligius
caught one case. I don't believe that any miners will deploy strong KYC
measures, and even if they did, I don't believe that these measures
will be effective, at least, as long as the attacker is somewhat savvy.
The problem with these attacks are that statistics favor the attackers.
Is someone really discarding the blocks they find, or are they just
unlucky? This is really hard to tell for small miners. Even with KYC,
one could break up one's servers, register them under different
people's names, and tunnel them through VPNs.

Keep in mind that when an open pool gets big, like GHash did and
two other pools did before them, the only thing at our disposal used
to be to yell at people about centralization until they left the big
pools and reformed into smaller groups. Not only was such yelling
kind of desperate looking, it wasn't incredibly effective, either.
We had no protocol mechanisms that put pressure on big pools to
stop signing up people. Ittay's discovery changed that: pools that
get to be very big by indiscriminately signing up miners are likely to
be infiltrated and their profitability will drop. And Peter's post is
evidence that this is, indeed, happening as predicted. This is a
good outcome, it puts pressure on the big pools to not grow.

Peter, you allude to a specific suggestion from Luke-Jr. Can you
please describe what it is?

Hope this is useful,
- egs

[1] https://www.cs.cornell.edu/~ie53/publications/btcPoolsSP15.pdf
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/5a7c575b/attachment-0001.html>

From el33th4x0r at gmail.com  Sun Dec 20 07:56:03 2015
From: el33th4x0r at gmail.com (=?UTF-8?Q?Emin_G=C3=BCn_Sirer?=)
Date: Sun, 20 Dec 2015 02:56:03 -0500
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CAAcC9ysejDQ8tyn_hhTQ_1ToKsM2f2rdhG4d1X3O5uuBj1X8NQ@mail.gmail.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<219f125cee6ca68fd27016642e38fdf1@xbt.hk>
	<CAAcC9ys_t7X0WpQ8W3577M8GLiA5sPV2F1BJ9qZbnMkE-1j3+Q@mail.gmail.com>
	<aff8da46a69bdd7ef92ca87725866a5c@xbt.hk>
	<CAPkFh0vNECi1OmBwki+8NNAQbe6EG2FEE4RR5z=kYVLLDFHUXg@mail.gmail.com>
	<CAAcC9ysejDQ8tyn_hhTQ_1ToKsM2f2rdhG4d1X3O5uuBj1X8NQ@mail.gmail.com>
Message-ID: <CAPkFh0u+pHWQfJJX5pJR5N5rhs=K0CgDaZwkf+dOS1UHMUU+bw@mail.gmail.com>

Initial reactions aren't always accurate, people's views change, and
science has its insurmountable way of convincing people. Gavin [1]
and others [2] now cite selfish mining as a concern in the block size
debate, and more importantly, the paper has been peer-reviewed,
cited, and even built-upon [3].

Let's elevate the discussion, shall we?

[1] Here's Gavin concerned about selfish mining:
http://gavinandresen.ninja/are-bigger-blocks-better-for-bigger-miners

[2] Here's Adam:
http://bitcoin-development.narkive.com/mvI8Wpjp/dynamic-limit-to-the-block-size-bip-draft-discussion

[3] This is a very nice extension of our work:
Ayelet Sapirshtein, Yonatan Sompolinsky, Aviv Zohar:
http://arxiv.org/abs/1507.06183
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/35641b59/attachment-0001.html>

From pete at petertodd.org  Sun Dec 20 11:24:54 2015
From: pete at petertodd.org (Peter Todd)
Date: Sun, 20 Dec 2015 03:24:54 -0800
Subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin
In-Reply-To: <CAAS2fgQi7aiwyOaVBiMbp6t9D58aFAmDdKPzFiscB6ouGzBK6A@mail.gmail.com>
References: <50e629572d8de852eb789d50b34da308@xbt.hk>
	<1449961269.2210.5.camel@yahoo.com>
	<CACrzPenXGQZBrx8QC+1QE2oCE3N=qmfgc_OWrowtjtLjGkZrRA@mail.gmail.com>
	<CAAS2fgQi7aiwyOaVBiMbp6t9D58aFAmDdKPzFiscB6ouGzBK6A@mail.gmail.com>
Message-ID: <20151220112454.GB16187@muck>

On Sun, Dec 13, 2015 at 02:07:36AM +0000, Gregory Maxwell via bitcoin-dev wrote:
> On Sun, Dec 13, 2015 at 1:00 AM, Vincent Truong via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org> wrote:
> > have run a node/kept their utxo before they were aware of this change and
> > then realise miners have discarded their utxo. Oops?
> 
> I believe you have misunderstood jl2012's post.  His post does not
> cause the outputs to become discarded. They are still spendable,
> but the transactions must carry a membership proof to spend them.
> They don't have to have stored the data themselves, but they must
> get it from somewhere-- including archive nodes that serve this
> purpose rather than having every full node carry all that data forever.
> 
> Please be conservative with the send button. The list loses its
> utility if every moderately complex idea is hit with reflexive
> opposition by people who don't understand it.
> 
> Peter Todd has proposed something fairly similar with "STXO
> commitments". The primary argument against this kind of approach that

That's incorrect terminology - what I proposed are "TXO commitments". I
proposed that a MMR of all prior transaction outputs's, spent and
unspent, be committed too in blocks along with a spentness flag, not
just spent transaction outputs.

That's why I often use the term (U)TXO commitments to refer to both
classes of proposals.

> I'm aware of is that the membership proofs get pretty big, and if too
> aggressive this trades bandwidth for storage, and storage is usually
> the cheaper resource. Though at least the membership proofs could be
> omitted when transmitting to a node which has signaled that it has
> kept the historical data anyways.

What I proprosed is that a consensus-critical maximum UTXO age be part
of the protocol; UTXO's younger than that age are expected to be cached.
For UTXO's older than that age, they can be dropped from the cache,
however to spend them you are required to provide the proof, and that
proof counts as blockchain space to account for the fact that they do
need to be broadcast on the network.

The proofs are relatively large, but not so much larger than a CTxIn as
to make paying for that data infeasible.

-- 
'peter'[:-1]@petertodd.org
00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 650 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/ee109d4b/attachment.sig>

From jgarzik at gmail.com  Sun Dec 20 11:34:29 2015
From: jgarzik at gmail.com (Jeff Garzik)
Date: Sun, 20 Dec 2015 06:34:29 -0500
Subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin
In-Reply-To: <20151220112454.GB16187@muck>
References: <50e629572d8de852eb789d50b34da308@xbt.hk>
	<1449961269.2210.5.camel@yahoo.com>
	<CACrzPenXGQZBrx8QC+1QE2oCE3N=qmfgc_OWrowtjtLjGkZrRA@mail.gmail.com>
	<CAAS2fgQi7aiwyOaVBiMbp6t9D58aFAmDdKPzFiscB6ouGzBK6A@mail.gmail.com>
	<20151220112454.GB16187@muck>
Message-ID: <CADm_Wca0cWRvcVaJ+p47A49yffQ1vP=u4807j7axn=mdBdsUGQ@mail.gmail.com>

On Sun, Dec 20, 2015 at 6:24 AM, Peter Todd via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> What I proprosed is that a consensus-critical maximum UTXO age be part
> of the protocol; UTXO's younger than that age are expected to be cached.
> For UTXO's older than that age, they can be dropped from the cache,
> however to spend them you are required to provide the proof, and that
> proof counts as blockchain space to account for the fact that they do
> need to be broadcast on the network.


Yes, this is almost what -has- to happen in the long term.

Ideally we should start having wallets generate those proofs now, and then
introduce the max-age as a second step as a planned hard fork a couple
years down the line.

However,
1) There is also the open question of "grandfathered" UTXOs - for those
wallets generated in 2009, buried in a landfill and then dug out 10 years
ago

2) This reverses the useful minimization attribute of HD wallets - "just
backup the seed"
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/849d17b4/attachment.html>

From tier.nolan at gmail.com  Sun Dec 20 11:38:34 2015
From: tier.nolan at gmail.com (Tier Nolan)
Date: Sun, 20 Dec 2015 11:38:34 +0000
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CAPkFh0vNECi1OmBwki+8NNAQbe6EG2FEE4RR5z=kYVLLDFHUXg@mail.gmail.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<219f125cee6ca68fd27016642e38fdf1@xbt.hk>
	<CAAcC9ys_t7X0WpQ8W3577M8GLiA5sPV2F1BJ9qZbnMkE-1j3+Q@mail.gmail.com>
	<aff8da46a69bdd7ef92ca87725866a5c@xbt.hk>
	<CAPkFh0vNECi1OmBwki+8NNAQbe6EG2FEE4RR5z=kYVLLDFHUXg@mail.gmail.com>
Message-ID: <CAE-z3OU=18VuV+9U9meg5fRxQt3MZnAnQ2jPN5QBNk+ZtSoJXw@mail.gmail.com>

On Sun, Dec 20, 2015 at 5:12 AM, Emin G?n Sirer <
bitcoin-dev at lists.linuxfoundation.org> wrote:

>  An attacker pool (A) can take a certain portion of its hashpower,
> use it to mine on behalf of victim pool (B), furnish partial proofs of work
> to B, but discard any full blocks it discovers.
>

I wonder if part of the problem here is that there is no pool identity
linked to mining pools.

If the mining protocols were altered so that miners had to indicate their
identity, then a pool couldn't forward hashing power to their victim.

If the various mining protocols were updated, they could allow checking
that the work has the domain name of the pool included.  Pools would have
to include their domain name in the block header.

A pool which provides this service is publicly saying that they will not
use the block withholding attack.  Any two pools which are doing it cannot
attack each other (since they have different domain names).  This creates
an incentive for pools to start supporting the feature.

Owners of hashing power also have an incentive to operate with pools which
offer this identity.  It means that they can ensure that they get a payout
from any blocks found.

Hosted mining is weaker, but even then, it is possible for mining hosts to
provide proof that they performed mining.  This proof would include the
identity of the mining pool.  Even if the pool was run by the host, it
would still need to have the name embedded.

Mining hosts might be able to figure out which of their customers actually
check the identity info, and then they could redirect the mining power of
those who generally don't check.  If customers randomly ask for all of the
hashing power, right back to when they joined, then this becomes expensive.

Mining power directly owned by the pool is also immune to this effect.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/feb0d395/attachment.html>

From pete at petertodd.org  Sun Dec 20 13:28:43 2015
From: pete at petertodd.org (Peter Todd)
Date: Sun, 20 Dec 2015 05:28:43 -0800
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CAPkFh0vNECi1OmBwki+8NNAQbe6EG2FEE4RR5z=kYVLLDFHUXg@mail.gmail.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<219f125cee6ca68fd27016642e38fdf1@xbt.hk>
	<CAAcC9ys_t7X0WpQ8W3577M8GLiA5sPV2F1BJ9qZbnMkE-1j3+Q@mail.gmail.com>
	<aff8da46a69bdd7ef92ca87725866a5c@xbt.hk>
	<CAPkFh0vNECi1OmBwki+8NNAQbe6EG2FEE4RR5z=kYVLLDFHUXg@mail.gmail.com>
Message-ID: <20151220132842.GA25481@muck>

On Sun, Dec 20, 2015 at 12:12:49AM -0500, Emin G?n Sirer via bitcoin-dev wrote:
> There's quite a bit of confusion in this thread.
> 
> Peter is referring to block withholding attacks. Ittay Eyal (as sole
> author -- I was not involved in this paper [1]) was the first

Ah, thanks for the correction.

> to analyze these attacks and to discover a fascinating, paradoxical
> result. An attacker pool (A) can take a certain portion of its hashpower,
> use it to mine on behalf of victim pool (B), furnish partial proofs of work
> to B, but discard any full blocks it discovers. If A picks the amount of
> attacking hashpower judiciously, it can make more money using this
> attack, than it would if it were to use 100% of its hashpower for its own
> mining. This last sentence should sound non-sensical to most of you,
> at least, it did to me. Ittay did the math, and his paper can tell you
> exactly how much of your hashpower you need to peel off and use
> to attack another open pool, and you will come out ahead.

Now to be clear, I'm not saying any of the above isn't true - it's a
fascinating result. But the hashing/mining ecosystem is significantly
more complex than just pools.

> Back to Ittay's paradoxical discovery:
> 
> We have seen pool-block withholding attacks before; I believe Eligius
> caught one case. I don't believe that any miners will deploy strong KYC
> measures, and even if they did, I don't believe that these measures
> will be effective, at least, as long as the attacker is somewhat savvy.
> The problem with these attacks are that statistics favor the attackers.
> Is someone really discarding the blocks they find, or are they just
> unlucky? This is really hard to tell for small miners. Even with KYC,
> one could break up one's servers, register them under different
> people's names, and tunnel them through VPNs.

There are a number of techniques that can be used to detect block
withholding attacks that you are not aware of. These techniques usually
have the characteristic that if known they can be avoided, so obviously
those who know about them are highly reluctant to reveal what exactly
they are. I personally know about some of them and have been asked to
keep that information secret, which I will.

In the context of KYC, this techniques would likely hold up in court,
which means that if this stuff becomes a more serious problem it's
perfectly viable for large, well-resourced, pools to prevent block
withholding attacks, in part by removing anonymity of hashing power.
This would not be a positive development for the ecosystem.

Secondly, DRM tech can also easily be used to prevent block withholding
attacks by attesting to the honest of the hashing power. This is being
discussed in the industry, and again, this isn't a positive development
for the ecosystem.

> Keep in mind that when an open pool gets big, like GHash did and
> two other pools did before them, the only thing at our disposal used
> to be to yell at people about centralization until they left the big
> pools and reformed into smaller groups. Not only was such yelling
> kind of desperate looking, it wasn't incredibly effective, either.
> We had no protocol mechanisms that put pressure on big pools to
> stop signing up people. Ittay's discovery changed that: pools that
> get to be very big by indiscriminately signing up miners are likely to
> be infiltrated and their profitability will drop. And Peter's post is
> evidence that this is, indeed, happening as predicted. This is a
> good outcome, it puts pressure on the big pools to not grow.

GHash.io was not a pure pool - they owned and operated a significant
amount of physical hashing power, and it's not at all clear that their %
of the network actually went down following that 51% debacle.

Currently a significant % of the hashing power - possibly a majority -
is in the form of large hashing installations whose owners individually,
and definitely in trusting groups, have enough hashing power to solo
mine. Eyal's results indicate those miners have incentives to attack
pools, and additionally they have the incentive of killing off pools to
make it difficult for new competition to get established, yet they
themselves are not vulnerable to that attack.

Moving to a state where new hashing power can't be brought online except
in large quantities is not a positive development for the ecosystem.

This is also way I described the suggestion that Eyal's results are a
good thing as academic - while the idea hypothetically works in a pure
open pool vs. open pool scenario, the real world is significantly more
complex than that simple model.

> Peter, you allude to a specific suggestion from Luke-Jr. Can you
> please describe what it is?

Basically you have the pool pick a secret k for each share, and commit
to H(k) in the share. Additionally the share commits to a target divider
D. The PoW validity rule is then changed from H(block header) < T, to be
H(block header) < T * D && H(H(block header) + k) < max_int / D

Because the hasher doesn't know what k is, they don't know which shares
are valid blocks and thus can't selectively discard those shares.

-- 
'peter'[:-1]@petertodd.org
00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 650 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/ea290ba1/attachment.sig>

From natanael.l at gmail.com  Sun Dec 20 08:30:37 2015
From: natanael.l at gmail.com (Natanael)
Date: Sun, 20 Dec 2015 09:30:37 +0100
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CAPkFh0u+pHWQfJJX5pJR5N5rhs=K0CgDaZwkf+dOS1UHMUU+bw@mail.gmail.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<219f125cee6ca68fd27016642e38fdf1@xbt.hk>
	<CAAcC9ys_t7X0WpQ8W3577M8GLiA5sPV2F1BJ9qZbnMkE-1j3+Q@mail.gmail.com>
	<aff8da46a69bdd7ef92ca87725866a5c@xbt.hk>
	<CAPkFh0vNECi1OmBwki+8NNAQbe6EG2FEE4RR5z=kYVLLDFHUXg@mail.gmail.com>
	<CAAcC9ysejDQ8tyn_hhTQ_1ToKsM2f2rdhG4d1X3O5uuBj1X8NQ@mail.gmail.com>
	<CAPkFh0u+pHWQfJJX5pJR5N5rhs=K0CgDaZwkf+dOS1UHMUU+bw@mail.gmail.com>
Message-ID: <CAAt2M1-MTR0+4UgpH-0W0f8PtCGAd1kjn4HmD-Jz4gaRZGuXrA@mail.gmail.com>

Wouldn't block withhold be fixed by not letting miners in pools know which
block candidates are valid before the pool knows? (Note: I haven't read any
other proposals for how to fix it, this may already be known)

As an example, by having the pool use the unique per-miner nonces sent to
each miner for effective division of labor as a kind of seed / commitment
value, where one in X block candidates will be valid, where X is the
current ratio between partial PoW blocks sent as mining proofs and the full
difficulty?

The computational work of the pool remains low (checking this isn't harder
than the partial PoW validation already performed), they pool simply looks
at which commitment value from the pool that the miner used, looks up the
correct committed value and hashes that together with the partial PoW. If
it hits the target, the block is valid.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/1fa589e5/attachment.html>

From natanael.l at gmail.com  Sun Dec 20 12:42:10 2015
From: natanael.l at gmail.com (Natanael)
Date: Sun, 20 Dec 2015 13:42:10 +0100
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CAE-z3OU=18VuV+9U9meg5fRxQt3MZnAnQ2jPN5QBNk+ZtSoJXw@mail.gmail.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<219f125cee6ca68fd27016642e38fdf1@xbt.hk>
	<CAAcC9ys_t7X0WpQ8W3577M8GLiA5sPV2F1BJ9qZbnMkE-1j3+Q@mail.gmail.com>
	<aff8da46a69bdd7ef92ca87725866a5c@xbt.hk>
	<CAPkFh0vNECi1OmBwki+8NNAQbe6EG2FEE4RR5z=kYVLLDFHUXg@mail.gmail.com>
	<CAE-z3OU=18VuV+9U9meg5fRxQt3MZnAnQ2jPN5QBNk+ZtSoJXw@mail.gmail.com>
Message-ID: <CAAt2M19QwL1AyH=pVARGa0zYKUtRM9hz8vXUzyZb05E5EhQMeA@mail.gmail.com>

Den 20 dec 2015 12:38 skrev "Tier Nolan via bitcoin-dev" <
bitcoin-dev at lists.linuxfoundation.org>:
>
> On Sun, Dec 20, 2015 at 5:12 AM, Emin G?n Sirer <
bitcoin-dev at lists.linuxfoundation.org> wrote:
>>
>>  An attacker pool (A) can take a certain portion of its hashpower,
>> use it to mine on behalf of victim pool (B), furnish partial proofs of
work
>> to B, but discard any full blocks it discovers.
>
> I wonder if part of the problem here is that there is no pool identity
linked to mining pools.
>
> If the mining protocols were altered so that miners had to indicate their
identity, then a pool couldn't forward hashing power to their victim.

Our approaches can be combined.

Each pool (or solo miner) has a public key included in their blocks that
identifies them to their miners (solo miners can use their own unique
random keys every time). This public key may be registered with DNSSEC+DANE
and the pool could point to their domain in the block template as an
identifier.

For each block the pool generates a nonce, and for each of every miner's
workers it double-hashes that nonce with their own public key and that
miner's worker ID and the previous block hash (to ensure no accidental
overlapping work is done).

The double-hash is a commitment hash, the first hash is the committed value
to be used by the pool as described below. Publishing the nonce reveals how
the hashes were derived to their miners.

Each miner puts this commitment hash in their blocks, and also the public
key of the pool separately as mentioned above.

Here's where it differs from standard mining: both the candidate block PoW
hash and the pool's commitment value above determines block validity
together.

If total difficulty is X and the ratio for full blocks to candidate blocks
shared with the pool is Y, then the candidate block PoW now has to meet X/Y
while hashing the candidate block PoW + the pool's commitment hash must
meet Y, which together makes for X/Y*Y and thus the same total difficulty.

So now miners don't know if their blocks are valid before the pool does, so
withholding isn't effective, and the public key identifiers simultaneously
stops a pool from telling honest but naive miners to attack other pools
using whatever other schemes one might come up with.

The main differences are that there's a public key identifier the miners
are told about in advance and expect to see in block templates, and that
that now the pool has to publish this commitment value together with the
block that also contains the commitment hash, and that this is verified
together with the PoW.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/f952cf21/attachment.html>

From s7r at sky-ip.org  Sun Dec 20 11:43:15 2015
From: s7r at sky-ip.org (s7r)
Date: Sun, 20 Dec 2015 13:43:15 +0200
Subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin
In-Reply-To: <CADm_Wca0cWRvcVaJ+p47A49yffQ1vP=u4807j7axn=mdBdsUGQ@mail.gmail.com>
References: <50e629572d8de852eb789d50b34da308@xbt.hk>
	<1449961269.2210.5.camel@yahoo.com>
	<CACrzPenXGQZBrx8QC+1QE2oCE3N=qmfgc_OWrowtjtLjGkZrRA@mail.gmail.com>
	<CAAS2fgQi7aiwyOaVBiMbp6t9D58aFAmDdKPzFiscB6ouGzBK6A@mail.gmail.com>
	<20151220112454.GB16187@muck>
	<CADm_Wca0cWRvcVaJ+p47A49yffQ1vP=u4807j7axn=mdBdsUGQ@mail.gmail.com>
Message-ID: <56769453.4060903@sky-ip.org>

What will be the actual effect over wallets?

Say I have the private key for a dormant UTXO older than the
consensus-critical maximum UTXO age. The UTXO is not part of the cache.
So I setup a full node and import my old private key (wallet.dat). Will
I even see the correct balance (where will it get if from, since it's
dropped from the cache), or it will show me a 0 balance?

If I can see the correct balance, where can I get the proof I need and
what ensures I'll always be able to get that proof?

On 12/20/2015 1:34 PM, Jeff Garzik via bitcoin-dev wrote:
> On Sun, Dec 20, 2015 at 6:24 AM, Peter Todd via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org
> <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:
> 
>     What I proprosed is that a consensus-critical maximum UTXO age be part
>     of the protocol; UTXO's younger than that age are expected to be cached.
>     For UTXO's older than that age, they can be dropped from the cache,
>     however to spend them you are required to provide the proof, and that
>     proof counts as blockchain space to account for the fact that they do
>     need to be broadcast on the network.
> 
> 
> Yes, this is almost what -has- to happen in the long term.
> 
> Ideally we should start having wallets generate those proofs now, and
> then introduce the max-age as a second step as a planned hard fork a
> couple years down the line.
> 
> However,
> 1) There is also the open question of "grandfathered" UTXOs - for those
> wallets generated in 2009, buried in a landfill and then dug out 10
> years ago
> 
> 2) This reverses the useful minimization attribute of HD wallets - "just
> backup the seed"
> 
> 

From joe2015 at openmailbox.org  Sun Dec 20 10:56:33 2015
From: joe2015 at openmailbox.org (joe2015 at openmailbox.org)
Date: Sun, 20 Dec 2015 18:56:33 +0800
Subject: [bitcoin-dev] Increasing the blocksize as a (generalized) softfork.
Message-ID: <1bf64a5b514d57ca37744ae5f5238149@openmailbox.org>

This is a draft.

--joe

Introduction
============

It is generally assumed that increasing the blocksize limit requires a
hardfork.  Instead we show that a increasing the limit can be achieved 
using a
"generalized" softfork.  Like standard softforks, generalized softforks 
need a
mere miner majority (>50% hashpower) rather than global consensus.

Standard Softforks
==================

After a softfork two potential chains exist:

* The new chain B3,B4,B5,... valid under the new rules and old rules.
* The old chain B3',B4',B5',... valid under the old rules only.

E.g.

                       +-- B3 --- B4 --- B5
                       |
     ... -- B1 -- B2 --+
                       |
                       +-- B3' -- B4' -- B5' -- B6'

Assuming that >50% of the hashpower follow the new rules, the old chain 
is
doomed to be orphaned:

                       +-- B3 --- B4 --- B5 --- B6 --- B7 --- B8 --- ...
                       |
     ... -- B1 -- B2 --+
                       |
                       +-- B3' -- B4' -- B5' -- B6' (orphaned)

Hardforks may result in two chains that can co-exist indefinitely:

                       +-- B3 --- B4 --- B5 --- B6 --- B7 --- B8 --- ...
                       |
     ... -- B1 -- B2 --+
                       |
                       +-- B3' -- B4' -- B5' -- B6' -- B7' -- B8' -- ...

Generalized Softforks
=====================

A *generalized* softfork introduces a transform function f(B)=B' that 
maps a
block B valid under the new rules to a block B' valid under the old 
rules.

After a generalized softfork three chains may exist:

* The new chain B3,B4,B5,... valid under the new rules only.
* The mapped chain f(B3),f(B4),f(B5),... valid under the old rules.
* The old chain B3',B4',B5',... valid under the old rules only.

E.g.

                       +-- B3 ---- B4 ---- B5
                       |
     ... -- B1 -- B2 --+-- f(B3) - f(B4) - f(B5)
                       |
                       +-- B3' --- B4' --- B5' --- B6'

This is "generalized" softfork since defining f(B)=B (identity function)
reduces to the standard softfork case above.

As with standard softforks, if the majority of the hashpower follow the 
new
rules then the old chain B3',B4',B5',... is doomed to be orphaned:

                       +-- B3 ---- B4 ---- B5 ---- B6 ---- B7 ---- ...
                       |
     ... -- B1 -- B2 --+-- f(B3) - f(B4) - f(B5) - f(B6) - f(B7) - ...
                       |
                       +-- B3' --- B4' --- B5' --- B6' (orphaned)

Example:
--------

Segregated Witness can be thought of as an example of a generalized 
softfork.
Here the new block format consists of the combined old block and witness 
data.
The function f() simply strips the witness data to reveal a valid block 
under
the old rules:

     NewBlock := OldBlock ++ Witness
     f(NewBlock) = OldBlock

An Arbitrary Block-size Increase Via a Generalized Softfork
===========================================================

Segregated Witness only allows for a modest effective blocksize increase
(although there can be other motivations for SW, but that is off-topic).

Instead we engineer a generalized softfork that allows an arbitrarily 
increase
of the blocksize limit.  The proposal consists of two parts: (a) new 
rules for
valid blocks, and (b) a transformation function f().

The new block rules are very similar to the old block rules but with 
some
small changes.  In summary the changes are:

* The MAX_BLOCK_SIZE limit is raised to some new limit
   (e.g. 8Mb, BIP101, 2-4-8, BIP202, etc., or some other limit)
* The MerkleRoot field in the header has been re-interpreted.
* The CoinBaseTx must obey some additional new rules.

As with old blocks, a block under the new rules consists of a block 
header
followed by a vector of transactions [CoinBaseTx, Tx1, .., Txn], i.e.

     NewBlock := BlockHeader ++ NumTx ++ CoinBaseTx ++ Tx1 ++ .. ++ Txn

The block header format is the same as under the old rules defined as 
follows:

     
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
     |  Ver  |                        PrevHash                            
    |
     
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
     |                           MerkleRoot                          | 
Time  |
     
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
     | Bits  | Nonce |
     +-+-+-+-+-+-+-+-+

Under the old rules MerkleRoot is the Merkle root of the hashes of all
transactions included in the block, i.e.

     MerkleRoot = merkleRoot([hash(CoinBaseTx), hash(Tx1), .., 
hash(Txn)])

Under the new rules we instead define:

     MerkleRoot = merkleRoot([hash(CoinBaseTx)])

That is, under the new rules, MerkleRoot is the Merkle root of a 
singleton
vector containing the CoinBaseTx hash only.

In order to preserve the security properties of Bitcoin we additionally
require that the CoinBaseTx somehow encodes the Merkle root of the 
remaining
transactions [Tx1, .., Txn].  For example, this could be achieved by 
requiring
a mandatory OP_RETURN output that encodes this information, e.g.

     OP_RETURN merkleRoot([hash(Tx1), .., hash(Txn)])

Alternatively the Merkle root could be encoded in the coinbase itself.  
This
ensures that new transactions cannot be added/deleted from the block 
without
altering the MerkleRoot field in the header.

Aside from these changes and the increased MAX_BLOCK_SIZE, the new block 
must
obey all the rules of the old block format, e.g. valid PoW, have valid 
block
reward, contain valid transactions, etc., etc.

In order to be a generalized softfork we also need to define a mapping 
f()
from valid new blocks to valid blocks under the old rules.  We can 
define this
as follows:

     NewBlock    := BlockHeader ++ NumTx ++ CoinBaseTx ++ Tx1 ++ .. ++ 
Txn
     f(NewBlock) := BlockHeader ++ 1 ++ CoinBaseTx

That is, function f() simply truncates the block so that it contains the
coinbase transaction only.  After truncation, the MerkleRoot field of 
the
block header is valid under the old rules.

The proposed new rules combined with the transformation f() comprise a
generalized softfork.  After the fork a new chain B3,B4,B5,... will be
generated under the new rules defined above, including an increased 
blocksize
limit.  This new chain can be mapped to a valid chain 
f(B3),f(B4),f(B5),...
under the old rules.  Assuming that >50% of the hashpower has adopted 
the new
rules, the mapped chain will orphan any competing chain under the old 
rules,
just like a standard softfork.

An interesting consequence of this design is that, since all mapped 
blocks are
empty, old clients will never see transactions confirming.  This is be a
strong incentive for users to update their clients.

Conclusion
==========

Conventional wisdoms suggests that increasing the blocksize limit 
requires a
hardfork.  We show that it can instead be achieved using a generalized
softfork.  Like with a standard softfork, a generalized softfork merely
requires a majority (>50%) of hash power rather than global consensus.
Experience has shown that the former is significantly easier to achieve.

Future Work
-----------

Investigate other kinds of hardforks that can instead be implemented as
generalized softforks, and the security implications of such...

7943a2934d0be2f96589fdef2b2e00a2a7d8c3b782546bb37625d1669accb9b1
72f018588572ca2786168cb531d10e79b81b86d3fada92298225a0f950eed3a5


From tier.nolan at gmail.com  Sun Dec 20 15:30:09 2015
From: tier.nolan at gmail.com (Tier Nolan)
Date: Sun, 20 Dec 2015 15:30:09 +0000
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CAAt2M19QwL1AyH=pVARGa0zYKUtRM9hz8vXUzyZb05E5EhQMeA@mail.gmail.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<219f125cee6ca68fd27016642e38fdf1@xbt.hk>
	<CAAcC9ys_t7X0WpQ8W3577M8GLiA5sPV2F1BJ9qZbnMkE-1j3+Q@mail.gmail.com>
	<aff8da46a69bdd7ef92ca87725866a5c@xbt.hk>
	<CAPkFh0vNECi1OmBwki+8NNAQbe6EG2FEE4RR5z=kYVLLDFHUXg@mail.gmail.com>
	<CAE-z3OU=18VuV+9U9meg5fRxQt3MZnAnQ2jPN5QBNk+ZtSoJXw@mail.gmail.com>
	<CAAt2M19QwL1AyH=pVARGa0zYKUtRM9hz8vXUzyZb05E5EhQMeA@mail.gmail.com>
Message-ID: <CAE-z3OVfhUAouWmpvYGSdXBMcYo7n=0CP=yVcSy5T0kzAtWh2Q@mail.gmail.com>

On Sun, Dec 20, 2015 at 12:42 PM, Natanael <natanael.l at gmail.com> wrote:

> If total difficulty is X and the ratio for full blocks to candidate blocks
> shared with the pool is Y, then the candidate block PoW now has to meet X/Y
> while hashing the candidate block PoW + the pool's commitment hash must
> meet Y, which together makes for X/Y*Y and thus the same total difficulty.


This gives the same total difficulty but miners are throwing away otherwise
valid blocks.

This means that it is technically a soft fork.  All new blocks are valid
according to the old rule.

In practice, it is kind of a hard fork.  If Y is 10, then all upgraded
miners are throwing away 90% of the blocks that are valid under the old
rules.

>From the perspective of non-upgraded clients, the upgraded miners operate
at a 10X disadvantage.

This means that someone with 15% of the network power has a majority of the
effective hashing power, since 15% is greater than 8.5% (85% * 0.1).

The slow roll-out helps mitigate this though.  It gives non-upgraded
clients time to react.  If there is only a 5% difference initially, then
the attacker doesn't get much benefit.

The main differences are that there's a public key identifier the miners
> are told about in advance and expect to see in block templates, and that
> that now the pool has to publish this commitment value together with the
> block that also contains the commitment hash, and that this is verified
> together with the PoW.


I don't think public keys are strictly required.  Registering them with
DNSSEC is way over the top.  They can just publish the key on their website
and then use that for their identity.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/d23c9ae8/attachment.html>

From joe2015 at openmailbox.org  Sun Dec 20 15:22:27 2015
From: joe2015 at openmailbox.org (joe2015 at openmailbox.org)
Date: Sun, 20 Dec 2015 23:22:27 +0800
Subject: [bitcoin-dev] Increasing the blocksize as a (generalized)
 softfork.
In-Reply-To: <1bf64a5b514d57ca37744ae5f5238149@openmailbox.org>
References: <1bf64a5b514d57ca37744ae5f5238149@openmailbox.org>
Message-ID: <1ff9d541f4b370faff7e2e27599bf983@openmailbox.org>

Link to better formatted version for web-users:
https://bitcointalk.org/index.php?topic=1296628.0

--joe

From tier.nolan at gmail.com  Sun Dec 20 15:50:57 2015
From: tier.nolan at gmail.com (Tier Nolan)
Date: Sun, 20 Dec 2015 15:50:57 +0000
Subject: [bitcoin-dev] Increasing the blocksize as a (generalized)
	softfork.
In-Reply-To: <1bf64a5b514d57ca37744ae5f5238149@openmailbox.org>
References: <1bf64a5b514d57ca37744ae5f5238149@openmailbox.org>
Message-ID: <CAE-z3OXxX9Xki=oco4veasE796z__x17C-KLc0jsMW8GxSrZ_Q@mail.gmail.com>

This is essentially the "nuclear option".  You are destroying the current
chain (converting it to a chain of coinbases) and using the same POW to
start the new chain.  You are also giving everyone credit in the new chain
equal to their credit in the old chain.

It would be better if the current chain wasn't destroyed.

This could be achieved by adding the hash of an extended block into the
coinbase but not requiring the coinbase to be the only transaction.

The new block is the legacy block plus the associated extended block.

Users would be allowed to move money to the extended block by spending it
to a specific output template.

<public key hash> OP_1 OP_TO_EXTENDED OP_TRUE

OP_1 is the extended block index and initially, only one level is available.

This would work like P2SH.  Users could spend the money on the extended
block chain exactly as they could on the main chain.

Money can be brought back the same way.

<public key hash> <txid1> <txid2> ... <txid-n> <N> OP_0 OP_UNLOCK OP_TRUE

The txids are for transactions that have been locked in root chain.  The
transaction is only valid if they are all fully funded.  The fee for the
transaction would be fee - (cost to fund unlocked txids).  A negative fee
tx would be invalid.

This has the advantage that it keeps the main chain operating.  People can
still send money with their un-upgraded clients.  There is also an
incentive to move funds to the extended block(s).  The new extended blocks
are more complex, but potentially have lower fees.  Nobody is forced to
change.  If the large blocks aren't needed, nobody will both to use them.

The rule could be

Now:
0) 1 MB

After change over
0) 1 MB
1) 2 MB

After 2 years
0) 1 MB
1) 2 MB
2) 4MB

After 4 years
0) 1 MB
1) 2 MB
2) 4MB
3) 8MB
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/aba78371/attachment.html>

From pete at petertodd.org  Sun Dec 20 16:35:24 2015
From: pete at petertodd.org (Peter Todd)
Date: Sun, 20 Dec 2015 16:35:24 +0000
Subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin
In-Reply-To: <CAB+qUq5P0vzZSq3VSX8__3dMP+WnmeXEgYm-MNrJOdNCYivqQA@mail.gmail.com>
References: <50e629572d8de852eb789d50b34da308@xbt.hk>
	<1449961269.2210.5.camel@yahoo.com>
	<CACrzPenXGQZBrx8QC+1QE2oCE3N=qmfgc_OWrowtjtLjGkZrRA@mail.gmail.com>
	<CAAS2fgQi7aiwyOaVBiMbp6t9D58aFAmDdKPzFiscB6ouGzBK6A@mail.gmail.com>
	<20151220112454.GB16187@muck>
	<CADm_Wca0cWRvcVaJ+p47A49yffQ1vP=u4807j7axn=mdBdsUGQ@mail.gmail.com>
	<CAB+qUq5P0vzZSq3VSX8__3dMP+WnmeXEgYm-MNrJOdNCYivqQA@mail.gmail.com>
Message-ID: <99D24C78-CAEE-4E48-A454-CA82B317D44B@petertodd.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512



On 20 December 2015 08:30:45 GMT-08:00, Chris Pacia <ctpacia at gmail.com> wrote:
>On Dec 20, 2015 6:34 AM, "Jeff Garzik via bitcoin-dev" <
>bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> 2) This reverses the useful minimization attribute of HD wallets -
>"just
>backup the seed"
>
>It would be nice if the bip37 filter matching algorithm was extended to
>serve up the proof.
>
>And if server-based wallets did the same it would preserve the "just
>backup
>the seed" functionality.

Exactly! The information will be out there - "just backup the seed" requires someone to have the exact same data needed to generate the TXO-unspent proof that my proposal requires to spend an old txout.

tl;dr: jgarzik is incorrect; theres no difference at all from the status quo.
-----BEGIN PGP SIGNATURE-----

iQE9BAEBCgAnIBxQZXRlciBUb2RkIDxwZXRlQHBldGVydG9kZC5vcmc+BQJWdtjA
AAoJEMCF8hzn9Lncz4MH/iwwa7xlbrJJiYqe7Hccr3Vm5D/qbv60eYi2btPDHFo9
mRnijzqFtt60e1AoFr9NwnCrAhhGmYkWsxLcA2oF+38H12DS9qsb9oT+pckJX34V
v06+Uap89v9VTHcxVrOEQUCx+9xQO7WkpFw/OTZJA4nmFZ8lvbgDGWE9q7mjQKof
QU1FiOM7mI6QCU8xTjRvVX5pTwvYNB7PAie/UoCfWU7/QdvsgTHRe4pq0XwJqHKy
xCr0DbfMZ2TvLFXitS5ZgTbDHURljjHlE7Qdmk9AcFNpI0PCR9YrZ5Mgb10sMygr
kX3V3uzjx2YBjKEpX9fqk6Kf/aufUqQ1PRBHF3m6bSE=
=mtj/
-----END PGP SIGNATURE-----


From el33th4x0r at gmail.com  Sun Dec 20 17:00:37 2015
From: el33th4x0r at gmail.com (=?UTF-8?Q?Emin_G=C3=BCn_Sirer?=)
Date: Sun, 20 Dec 2015 12:00:37 -0500
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <20151220132842.GA25481@muck>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<219f125cee6ca68fd27016642e38fdf1@xbt.hk>
	<CAAcC9ys_t7X0WpQ8W3577M8GLiA5sPV2F1BJ9qZbnMkE-1j3+Q@mail.gmail.com>
	<aff8da46a69bdd7ef92ca87725866a5c@xbt.hk>
	<CAPkFh0vNECi1OmBwki+8NNAQbe6EG2FEE4RR5z=kYVLLDFHUXg@mail.gmail.com>
	<20151220132842.GA25481@muck>
Message-ID: <CAPkFh0t-+WhZYVLyT_auLa87zAATNOH=CpU4S5H=n6S1wmZ-oQ@mail.gmail.com>

On Sun, Dec 20, 2015 at 8:28 AM, Peter Todd <pete at petertodd.org> wrote:

> There are a number of techniques that can be used to detect block
> withholding attacks that you are not aware of. These techniques usually
> have the characteristic that if known they can be avoided, so obviously
> those who know about them are highly reluctant to reveal what exactly
> they are. I personally know about some of them and have been asked to
> keep that information secret, which I will.
>

Indeed, there are lots of weak measures that one could employ against
an uninformed attacker. As I mentioned before, these are unlikely to be
effective against a savvy attacker, and this is a good thing.


> In the context of KYC, this techniques would likely hold up in court,
> which means that if this stuff becomes a more serious problem it's
> perfectly viable for large, well-resourced, pools to prevent block
> withholding attacks, in part by removing anonymity of hashing power.
> This would not be a positive development for the ecosystem.
>

KYC has a particular financial-regulation connotation in Bitcoin circles,
of which I'm sure you're aware, and which you're using as a spectre.
You don't mean government-regulated-KYC a la FINCEN and Bitcoin
exchanges like Coinbase, you are just referring to a pool operator
demanding to know that its customer is not coming from its competitors'
data centers.

And your prediction doesn't seem well-motivated or properly justified.
There are tons of conditionals in your prediction, starting with the premise
that every single open pool would implement some notion of identity
checking. I don't believe that will happen. Instead, we will have the bigger
pools become more suspicious of signing up new hash power, which is a
good thing. And we will have small groups of people who have some reason
for trusting each other (e.g. they know each other from IRC, conferences,
etc) band together into small pools. These are fantastic outcomes for
decentralization.

Secondly, DRM tech can also easily be used to prevent block withholding
> attacks by attesting to the honest of the hashing power. This is being
> discussed in the industry, and again, this isn't a positive development
> for the ecosystem.
>

DRM is a terrible application. Once again, I see that you're trying to use
those
three letters as a spectre as well, knowing that most people hate DRM, but
keep in mind that DRM is just an application -- it's like pointing to Adobe
Flash
to taint all browser plugins.

The tech behind DRM is called "attestation," and it provides a technical
capability not possible by any other means. In essence, attestation can
ensure that
a remote node is indeed running the code that it purports to be running.
Since
most problems in computer security and distributed systems stem from not
knowing what protocol the attacker is going to follow, attestation is the
only
technology we have that lets us step around this limitation.

It can ensure, for instance,
  - that a node purporting to be Bitcoin Core (vLatest) is indeed running an
unadulterated, latest version of Bitcoin Core
  - that a node claiming that it does not harvest IP addresses from SPV
clients indeed does not harvest IP addresses.
  - that a cloud hashing outfit that rented out X terahashes to a user did
indeed rent out X terahashes to that particular user,
  - that a miner operating on behalf of some pool P will not misbehave and
discard perfectly good blocks
and so forth. All of these would be great for the ecosystem. Just getting
rid
of the cloudhashing scams would put an end to a lot of heartache.

> Keep in mind that when an open pool gets big, like GHash did and
> > two other pools did before them, the only thing at our disposal used
> > to be to yell at people about centralization until they left the big
> > pools and reformed into smaller groups. Not only was such yelling
> > kind of desperate looking, it wasn't incredibly effective, either.
> > We had no protocol mechanisms that put pressure on big pools to
> > stop signing up people. Ittay's discovery changed that: pools that
> > get to be very big by indiscriminately signing up miners are likely to
> > be infiltrated and their profitability will drop. And Peter's post is
> > evidence that this is, indeed, happening as predicted. This is a
> > good outcome, it puts pressure on the big pools to not grow.
>
> GHash.io was not a pure pool - they owned and operated a significant
> amount of physical hashing power, and it's not at all clear that their %
> of the network actually went down following that 51% debacle.
>

Right, it's not clear at all that yelling at people has much effect. As much
fun as I had going to that meeting with GHash in London to ask them to
back down off of the 51% boundary, I am pretty sure that yelling at large
open pools will not scale. We needed better mechanisms for keeping pools
in check.

And Miner's Dilemma (MD) attacks are clearly quite effective. This is a
time when we should count our blessings, not work actively to render
them inoperable.

Currently a significant % of the hashing power - possibly a majority -
> is in the form of large hashing installations whose owners individually,
> and definitely in trusting groups, have enough hashing power to solo
> mine. Eyal's results indicate those miners have incentives to attack
> pools, and additionally they have the incentive of killing off pools to
> make it difficult for new competition to get established, yet they
> themselves are not vulnerable to that attack.
>

There are indeed solo miners out there who can attack the big open
pools. The loss of the biggest open pools would not be a bad outcome.
Pools >25% pose a danger, and the home miner doesn't need a pool
>25% for protection against variance.

> Peter, you allude to a specific suggestion from Luke-Jr. Can you
> > please describe what it is?
>
> Basically you have the pool pick a secret k for each share, and commit
> to H(k) in the share. Additionally the share commits to a target divider
> D. The PoW validity rule is then changed from H(block header) < T, to be
> H(block header) < T * D && H(H(block header) + k) < max_int / D
>

Thanks, this requires a change to the Bitcoin PoW. Good luck with that!

Once again, this suggestion would make the GHash-at-51% situation
possible again. Working extra hard to re-enable those painful days
sounds like a terrible idea.

- egs
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/9b9bc3ea/attachment-0001.html>

From ctpacia at gmail.com  Sun Dec 20 16:30:45 2015
From: ctpacia at gmail.com (Chris Pacia)
Date: Sun, 20 Dec 2015 11:30:45 -0500
Subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin
In-Reply-To: <CADm_Wca0cWRvcVaJ+p47A49yffQ1vP=u4807j7axn=mdBdsUGQ@mail.gmail.com>
References: <50e629572d8de852eb789d50b34da308@xbt.hk>
	<1449961269.2210.5.camel@yahoo.com>
	<CACrzPenXGQZBrx8QC+1QE2oCE3N=qmfgc_OWrowtjtLjGkZrRA@mail.gmail.com>
	<CAAS2fgQi7aiwyOaVBiMbp6t9D58aFAmDdKPzFiscB6ouGzBK6A@mail.gmail.com>
	<20151220112454.GB16187@muck>
	<CADm_Wca0cWRvcVaJ+p47A49yffQ1vP=u4807j7axn=mdBdsUGQ@mail.gmail.com>
Message-ID: <CAB+qUq5P0vzZSq3VSX8__3dMP+WnmeXEgYm-MNrJOdNCYivqQA@mail.gmail.com>

On Dec 20, 2015 6:34 AM, "Jeff Garzik via bitcoin-dev" <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> 2) This reverses the useful minimization attribute of HD wallets - "just
backup the seed"

It would be nice if the bip37 filter matching algorithm was extended to
serve up the proof.

And if server-based wallets did the same it would preserve the "just backup
the seed" functionality.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/5a7f7f61/attachment.html>

From joe2015 at openmailbox.org  Sun Dec 20 17:21:22 2015
From: joe2015 at openmailbox.org (joe2015 at openmailbox.org)
Date: Mon, 21 Dec 2015 01:21:22 +0800
Subject: [bitcoin-dev] Increasing the blocksize as a (generalized)
 softfork.
Message-ID: <dcf6dfa0f7fdcf96e9970e815e4c9b78@openmailbox.org>

On 2015-12-20 23:50, Tier Nolan via bitcoin-dev wrote:
> This is essentially the "nuclear option".

Remember this is proposed as an alternative to hardforks, which is also 
a "nuclear option".  Hardforks carry significant risks such as 
permanently splitting Bitcoin into two chains if global consensus is 
never reached.  A (generalized) softfork avoids this problem.

> This could be achieved by adding the hash of an extended block into
> the coinbase but not requiring the coinbase to be the only
> transaction.

I think this can also be viewed as a generalized softfork if one so 
chooses, e.g.

     NewBlock := OldBlock ++ ExtendedBlock
     f(NewBlock) = OldBlock

I do not think this is a bad idea but is more complex than my proposal, 
e.g. users having to move coins between different tiers of blocks.  
Under my proposal the Bitcoin works more or less the same except with a 
larger limit.

--joe

From kanzure at gmail.com  Sun Dec 20 18:17:30 2015
From: kanzure at gmail.com (Bryan Bishop)
Date: Sun, 20 Dec 2015 12:17:30 -0600
Subject: [bitcoin-dev] Increasing the blocksize as a (generalized)
	softfork.
In-Reply-To: <1bf64a5b514d57ca37744ae5f5238149@openmailbox.org>
References: <1bf64a5b514d57ca37744ae5f5238149@openmailbox.org>
Message-ID: <CABaSBazMYw6ou1=GDvi5XxNRgWxsQxTu9jU+PaqW9u9rAe=D-w@mail.gmail.com>

On Sun, Dec 20, 2015 at 4:56 AM, joe2015--- via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> An Arbitrary Block-size Increase Via a Generalized Softfork
>

This seems conceptually similar to "extension blocks":
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-May/008356.html
https://bitcointalk.org/index.php?topic=283746.0
http://gnusha.org/bitcoin-wizards/2015-12-20.log

"Extended blocks" are also mentioned over here too:
https://bitcointalk.org/index.php?topic=1296628.msg13307275#msg13307275

- Bryan
http://heybryan.org/
1 512 203 0507
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/124004d6/attachment.html>

From rusty at rustcorp.com.au  Sun Dec 20 04:14:25 2015
From: rusty at rustcorp.com.au (Rusty Russell)
Date: Sun, 20 Dec 2015 14:44:25 +1030
Subject: [bitcoin-dev] On the security of softforks
In-Reply-To: <E76D5BF9-41BF-4AF5-BBAC-06F4EF574EBE@toom.im>
References: <CAPg+sBjJcqeqGLHnPyWt23z3YoCRGozQupuMxy51J_-hdkKBSA@mail.gmail.com>
	<E76D5BF9-41BF-4AF5-BBAC-06F4EF574EBE@toom.im>
Message-ID: <878u4poixq.fsf@rustcorp.com.au>

Jonathan Toomim via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>
writes:
> On Dec 18, 2015, at 10:30 AM, Pieter Wuille via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> 1) The risk of an old full node wallet accepting a transaction that is
>> invalid to the new rules.
>> 
>> The receiver wallet chooses what address/script to accept coins on.
>> They'll upgrade to the new softfork rules before creating an address
>> that depends on the softfork's features.
>> 
>> So, not a problem.
>
>
> Mallory wants to defraud Bob with a 1 BTC payment for some beer. Bob
> runs the old rules. Bob creates a p2pkh address for Mallory to
> use. Mallory takes 1 BTC, and creates an invalid SegWit transaction
> that Bob cannot properly validate and that pays into one of Mallory's
> wallets. Mallory then immediately spends the unconfirmed transaction
> into Bob's address. Bob sees what appears to be a valid transaction
> chain which is not actually valid.

Pretty sure Bob's wallet will be looking for "OP_DUP OP_HASH160
<pubKeyHash> OP_EQUALVERIFY OP_CHECKSIG" scriptSig.  The SegWit-usable
outputs will (have to) look different, won't they?

Cheers,
Rusty.

From jl2012 at xbt.hk  Sun Dec 20 19:16:29 2015
From: jl2012 at xbt.hk (jl2012)
Date: Sun, 20 Dec 2015 14:16:29 -0500
Subject: [bitcoin-dev] On the security of softforks
In-Reply-To: <878u4poixq.fsf@rustcorp.com.au>
References: <CAPg+sBjJcqeqGLHnPyWt23z3YoCRGozQupuMxy51J_-hdkKBSA@mail.gmail.com>
	<E76D5BF9-41BF-4AF5-BBAC-06F4EF574EBE@toom.im>
	<878u4poixq.fsf@rustcorp.com.au>
Message-ID: <e7e3e0901347a1019db624581520e368@xbt.hk>

Rusty Russell via bitcoin-dev ? 2015-12-19 23:14 ??:
> Jonathan Toomim via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>
> writes:
>> On Dec 18, 2015, at 10:30 AM, Pieter Wuille via bitcoin-dev 
>> <bitcoin-dev at lists.linuxfoundation.org> wrote:
>> 
>>> 1) The risk of an old full node wallet accepting a transaction that 
>>> is
>>> invalid to the new rules.
>>> 
>>> The receiver wallet chooses what address/script to accept coins on.
>>> They'll upgrade to the new softfork rules before creating an address
>>> that depends on the softfork's features.
>>> 
>>> So, not a problem.
>> 
>> 
>> Mallory wants to defraud Bob with a 1 BTC payment for some beer. Bob
>> runs the old rules. Bob creates a p2pkh address for Mallory to
>> use. Mallory takes 1 BTC, and creates an invalid SegWit transaction
>> that Bob cannot properly validate and that pays into one of Mallory's
>> wallets. Mallory then immediately spends the unconfirmed transaction
>> into Bob's address. Bob sees what appears to be a valid transaction
>> chain which is not actually valid.
> 
> Pretty sure Bob's wallet will be looking for "OP_DUP OP_HASH160
> <pubKeyHash> OP_EQUALVERIFY OP_CHECKSIG" scriptSig.  The SegWit-usable
> outputs will (have to) look different, won't they?
> 
> Cheers,
> Rusty.

I think he means Mallory is paying with an invalid Segwit input, not 
output (there is no "invalid output" anyway). However, this is not a 
issue if Bob waits for a few confirmations.

From tomh at thinlink.com  Mon Dec 21 03:23:40 2015
From: tomh at thinlink.com (Tom Harding)
Date: Sun, 20 Dec 2015 19:23:40 -0800
Subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin
In-Reply-To: <CADm_Wca0cWRvcVaJ+p47A49yffQ1vP=u4807j7axn=mdBdsUGQ@mail.gmail.com>
References: <50e629572d8de852eb789d50b34da308@xbt.hk>
	<1449961269.2210.5.camel@yahoo.com>
	<CACrzPenXGQZBrx8QC+1QE2oCE3N=qmfgc_OWrowtjtLjGkZrRA@mail.gmail.com>
	<CAAS2fgQi7aiwyOaVBiMbp6t9D58aFAmDdKPzFiscB6ouGzBK6A@mail.gmail.com>
	<20151220112454.GB16187@muck>
	<CADm_Wca0cWRvcVaJ+p47A49yffQ1vP=u4807j7axn=mdBdsUGQ@mail.gmail.com>
Message-ID: <567770BC.5060407@thinlink.com>

On 12/20/2015 3:34 AM, Jeff Garzik via bitcoin-dev wrote:
> Ideally we should start having wallets generate those proofs now, and
> then introduce the max-age as a second step as a planned hard fork a
> couple years down the line.
>
> However,
> 1) There is also the open question of "grandfathered" UTXOs - for
> those wallets generated in 2009, buried in a landfill and then dug out
> 10 years ago
>
> 2) This reverses the useful minimization attribute of HD wallets -
> "just backup the seed"

Also, a change (#6550) has been merged to bitcoin core that removes
merkle branches from the wallet, and if pruning gets turned on (possible
in 0.12 with #6057), it would become quite a bit more difficult to spend
older coins under a change like this.

As a solution I would favor not removing wallet merkle branches.


From joe2015 at openmailbox.org  Mon Dec 21 03:04:31 2015
From: joe2015 at openmailbox.org (joe2015 at openmailbox.org)
Date: Mon, 21 Dec 2015 11:04:31 +0800
Subject: [bitcoin-dev] Increasing the blocksize as a (generalized)
 softfork.
In-Reply-To: <CABaSBazMYw6ou1=GDvi5XxNRgWxsQxTu9jU+PaqW9u9rAe=D-w@mail.gmail.com>
References: <1bf64a5b514d57ca37744ae5f5238149@openmailbox.org>
	<CABaSBazMYw6ou1=GDvi5XxNRgWxsQxTu9jU+PaqW9u9rAe=D-w@mail.gmail.com>
Message-ID: <8088b376ed29f3ea0af67b7567189e31@openmailbox.org>

On 2015-12-21 02:17, Bryan Bishop wrote:
> On Sun, Dec 20, 2015 at 4:56 AM, joe2015--- via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org> wrote:
> 
>> An Arbitrary Block-size Increase Via a Generalized Softfork
> 
> This seems conceptually similar to "extension blocks":
> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-May/008356.html
> [1]
> https://bitcointalk.org/index.php?topic=283746.0 [2]
> http://gnusha.org/bitcoin-wizards/2015-12-20.log [3]
> 
> "Extended blocks" are also mentioned over here too:
> https://bitcointalk.org/index.php?topic=1296628.msg13307275#msg13307275
> [4]

The main difference is that my proposal does not introduce different 
"tiers" of blocks, and does not require uses to move coins to manually 
move coins between these tiers.

Instead, my proposal uses a single flat block format that is essentially 
the same as the current block format; only bigger.

The main point is that such a change does not require a hardfork with 
global consensus, as is commonly assumed, but rather can be deployed 
like a softfork using the method described in my original post.

--joe.

From jgarzik at gmail.com  Mon Dec 21 03:34:06 2015
From: jgarzik at gmail.com (Jeff Garzik)
Date: Sun, 20 Dec 2015 22:34:06 -0500
Subject: [bitcoin-dev] Forget dormant UTXOs without confiscating bitcoin
In-Reply-To: <99D24C78-CAEE-4E48-A454-CA82B317D44B@petertodd.org>
References: <50e629572d8de852eb789d50b34da308@xbt.hk>
	<1449961269.2210.5.camel@yahoo.com>
	<CACrzPenXGQZBrx8QC+1QE2oCE3N=qmfgc_OWrowtjtLjGkZrRA@mail.gmail.com>
	<CAAS2fgQi7aiwyOaVBiMbp6t9D58aFAmDdKPzFiscB6ouGzBK6A@mail.gmail.com>
	<20151220112454.GB16187@muck>
	<CADm_Wca0cWRvcVaJ+p47A49yffQ1vP=u4807j7axn=mdBdsUGQ@mail.gmail.com>
	<CAB+qUq5P0vzZSq3VSX8__3dMP+WnmeXEgYm-MNrJOdNCYivqQA@mail.gmail.com>
	<99D24C78-CAEE-4E48-A454-CA82B317D44B@petertodd.org>
Message-ID: <CADm_Wca33DiFjDttGz2YVzNMEfGKb7GwTz0BuFpz7UOThsnRYQ@mail.gmail.com>

On Sun, Dec 20, 2015 at 11:35 AM, Peter Todd <pete at petertodd.org> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA512
>
>
>
> On 20 December 2015 08:30:45 GMT-08:00, Chris Pacia <ctpacia at gmail.com>
> wrote:
> >On Dec 20, 2015 6:34 AM, "Jeff Garzik via bitcoin-dev" <
> >bitcoin-dev at lists.linuxfoundation.org> wrote:
> >
> >> 2) This reverses the useful minimization attribute of HD wallets -
> >"just
> >backup the seed"
> >
> >It would be nice if the bip37 filter matching algorithm was extended to
> >serve up the proof.
> >
> >And if server-based wallets did the same it would preserve the "just
> >backup
> >the seed" functionality.
>
> Exactly! The information will be out there - "just backup the seed"
> requires someone to have the exact same data needed to generate the
> TXO-unspent proof that my proposal requires to spend an old txout.
>
> tl;dr: jgarzik is incorrect; theres no difference at all from the status
> quo.
>

The data stored in the legacy case makes it possible to sign and send a
transaction without any connection to a network.  The data stored in the
upgraded case, absent grandfathering, requires significant network sync at
a minimum.

The user experience and security parameters are different.

Thus, issue/recommendation #1.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/e8707e97/attachment.html>

From jgarzik at gmail.com  Mon Dec 21 03:39:52 2015
From: jgarzik at gmail.com (Jeff Garzik)
Date: Sun, 20 Dec 2015 22:39:52 -0500
Subject: [bitcoin-dev] Increasing the blocksize as a (generalized)
	softfork.
In-Reply-To: <dcf6dfa0f7fdcf96e9970e815e4c9b78@openmailbox.org>
References: <dcf6dfa0f7fdcf96e9970e815e4c9b78@openmailbox.org>
Message-ID: <CADm_WcYXU6VXG034j=mD8zkmqNLB96sjvBc6qo8Sx2NepHJk8A@mail.gmail.com>

On Sun, Dec 20, 2015 at 12:21 PM, joe2015--- via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Remember this is proposed as an alternative to hardforks, which is also a
> "nuclear option".  Hardforks carry significant risks such as permanently
> splitting Bitcoin into two chains if global consensus is never reached.  A
> (generalized) softfork avoids this problem.


Current hard fork implementations include / will include miner lock-in,
just like any soft fork.  They will not activate if global consensus is not
reached.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/264fff1d/attachment.html>

From joe2015 at openmailbox.org  Mon Dec 21 03:58:50 2015
From: joe2015 at openmailbox.org (joe2015 at openmailbox.org)
Date: Mon, 21 Dec 2015 11:58:50 +0800
Subject: [bitcoin-dev] Increasing the blocksize as a (generalized)
 softfork.
In-Reply-To: <CADm_WcYXU6VXG034j=mD8zkmqNLB96sjvBc6qo8Sx2NepHJk8A@mail.gmail.com>
References: <dcf6dfa0f7fdcf96e9970e815e4c9b78@openmailbox.org>
	<CADm_WcYXU6VXG034j=mD8zkmqNLB96sjvBc6qo8Sx2NepHJk8A@mail.gmail.com>
Message-ID: <489826397e6b36bf68391508952042f4@openmailbox.org>

On 2015-12-21 11:39, Jeff Garzik wrote:
> On Sun, Dec 20, 2015 at 12:21 PM, joe2015--- via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org> wrote:

> Current hard fork implementations include / will include miner
> lock-in, just like any soft fork.  They will not activate if global
> consensus is not reached.

That's not true at all. They activate with a miner majority (e.g. 75%, 
95%, etc.), not global consensus.  Here global really means global, i.e. 
miner, economic, all clients, etc.  In the case of a hardfork there is 
nothing stopping the miner minority from continuing the old chain.  With 
a softfork the miner minority is forced to upgrade otherwise their 
blocks will be eventually orphaned.

My proposal achieves a hardfork-like blocksize limit increase but, like 
a softfork, also forces the miner minority to upgrade.

--joe.

From jl2012 at xbt.hk  Mon Dec 21 04:23:32 2015
From: jl2012 at xbt.hk (jl2012)
Date: Sun, 20 Dec 2015 23:23:32 -0500
Subject: [bitcoin-dev] Increasing the blocksize as a (generalized)
 softfork.
In-Reply-To: <1bf64a5b514d57ca37744ae5f5238149@openmailbox.org>
References: <1bf64a5b514d57ca37744ae5f5238149@openmailbox.org>
Message-ID: <e170f3a10164019824edaafe5a04f067@xbt.hk>

I proposed something very similar 2 years ago:
https://bitcointalk.org/index.php?topic=283746.0

This is an interesting academic idea. But the way you implement it will 
immediately kill all existing full and SPV nodes (not really dead, 
rather like zombie as they can't send and receive any tx).

joe2015--- via bitcoin-dev ? 2015-12-20 05:56 ??:
> This is a draft.
> 
> --joe
> 
> Introduction
> ============
> 
> It is generally assumed that increasing the blocksize limit requires a
> hardfork.  Instead we show that a increasing the limit can be achieved 
> using a
> "generalized" softfork.  Like standard softforks, generalized softforks 
> need a
> mere miner majority (>50% hashpower) rather than global consensus.
> 


From pieter.wuille at gmail.com  Mon Dec 21 04:33:16 2015
From: pieter.wuille at gmail.com (Pieter Wuille)
Date: Mon, 21 Dec 2015 05:33:16 +0100
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAPg+sBig9O5+he0PWhTkX5iin14QLz5+eCCu6KfwU=DxntKYtg@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CAPWm=eUomq6SBC0ky0WSs5=_G942vigm4RmgYuq0O-yJ-vqC2A@mail.gmail.com>
	<CAPg+sBig9O5+he0PWhTkX5iin14QLz5+eCCu6KfwU=DxntKYtg@mail.gmail.com>
Message-ID: <CAPg+sBhQN2HDvH8dfq2VsQ0dTA9V=HgQsCJdP6B72fj1SDA4yw@mail.gmail.com>

On Tue, Dec 8, 2015 at 6:07 AM, Wladimir J. van der Laan wrote:
> On Mon, Dec 07, 2015 at 10:02:17PM +0000, Gregory Maxwell via bitcoin-dev
wrote:
>> TL;DR: I propose we work immediately towards the segwit 4MB block
>> soft-fork which increases capacity and scalability, and recent speedups
>> and incoming relay improvements make segwit a reasonable risk. BIP9
>> and segwit will also make further improvements easier and faster to
>> deploy. We?ll continue to set the stage for non-bandwidth-increase-based
>> scaling, while building additional tools that would make bandwidth
>> increases safer long term. Further work will prepare Bitcoin for further
>> increases, which will become possible when justified, while also
providing
>> the groundwork to make them justifiable.
>
> Sounds good to me.

Better late than never, let me comment on why I believe pursuing this plan
is important.

For months, the block size debate, and the apparent need for agreement on a
hardfork has distracted from needed engineering work, fed the external
impression that nothing is being done, and generally created a toxic
environment to work in. It has affected my own productivity and health, and
I do not think I am alone.

I believe that soft-fork segwit can help us out of this deadlock and get us
going again. It does not require the pervasive assumption that the entire
world will simultaneously switch to new consensus rules like a hardfork
does, while at the same time:
* Give a short-term capacity bump
* Show the world that scalability is being worked on
* Actually improve scalability (as opposed to just scale) by reducing
bandwidth/storage and indirectly improving the effectiveness of systems
like Lightning.
* Solve several unrelated problems at the same time (fraud proofs, script
extensibility, malleability, ...).

So I'd like to ask the community that we work towards this plan, as it
allows to make progress without being forced to make a possibly divisive
choice for one hardfork or another yet.

-- 
Pieter
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151221/30bc71df/attachment.html>

From justus at openbitcoinprivacyproject.org  Mon Dec 21 04:42:03 2015
From: justus at openbitcoinprivacyproject.org (Justus Ranvier)
Date: Sun, 20 Dec 2015 22:42:03 -0600
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAPg+sBhQN2HDvH8dfq2VsQ0dTA9V=HgQsCJdP6B72fj1SDA4yw@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CAPWm=eUomq6SBC0ky0WSs5=_G942vigm4RmgYuq0O-yJ-vqC2A@mail.gmail.com>
	<CAPg+sBig9O5+he0PWhTkX5iin14QLz5+eCCu6KfwU=DxntKYtg@mail.gmail.com>
	<CAPg+sBhQN2HDvH8dfq2VsQ0dTA9V=HgQsCJdP6B72fj1SDA4yw@mail.gmail.com>
Message-ID: <5677831B.1070205@openbitcoinprivacyproject.org>

On 12/20/2015 10:33 PM, Pieter Wuille via bitcoin-dev wrote:
> Solve several unrelated problems at the same time (fraud proofs, script
> extensibility, malleability, ...).

By "solve" do you mean, "actually implement", or do you mean "make
future implementation theoretically possible?"

In other words, would a deployment of SW involve the creation of new
network message for relaying fraud proofs, a specification that SPV
wallet developers can use to validate these messages and so know when to
ignore the highest (but invalid) PoW chain, and the ability to
automatically generate and broadcast these proofs in Bitcoin Core?


-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0xEAD9E623.asc
Type: application/pgp-keys
Size: 23337 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/8a874ec1/attachment-0001.bin>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 801 bytes
Desc: OpenPGP digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/8a874ec1/attachment-0001.sig>

From morcos at gmail.com  Mon Dec 21 04:44:49 2015
From: morcos at gmail.com (Alex Morcos)
Date: Sun, 20 Dec 2015 23:44:49 -0500
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAPg+sBhQN2HDvH8dfq2VsQ0dTA9V=HgQsCJdP6B72fj1SDA4yw@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CAPWm=eUomq6SBC0ky0WSs5=_G942vigm4RmgYuq0O-yJ-vqC2A@mail.gmail.com>
	<CAPg+sBig9O5+he0PWhTkX5iin14QLz5+eCCu6KfwU=DxntKYtg@mail.gmail.com>
	<CAPg+sBhQN2HDvH8dfq2VsQ0dTA9V=HgQsCJdP6B72fj1SDA4yw@mail.gmail.com>
Message-ID: <CAPWm=eX7VCdE_oZN+wT7rbUR+X7dAnWEicLp_K1mhFAnkSSnCA@mail.gmail.com>

I'm also strongly in favor of moving forward with this plan.

A couple of points:
1) There has been too much confusion in looking at segwit as an alternative
way to increase the block size and I think that is incorrect.  It should
not be drawn into the block size debate as it brings many needed
improvements and tools we'd want even if no one were worried about block
size now.
2) The full capacity increase plan Greg lays out makes it clear that we can
accomplish a tremendous amount without a contentious hard fork at this
point.
3) Let's stop arguing endlessly and actually do work that will benefit
everyone.




On Sun, Dec 20, 2015 at 11:33 PM, Pieter Wuille via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> On Tue, Dec 8, 2015 at 6:07 AM, Wladimir J. van der Laan wrote:
> > On Mon, Dec 07, 2015 at 10:02:17PM +0000, Gregory Maxwell via
> bitcoin-dev wrote:
> >> TL;DR: I propose we work immediately towards the segwit 4MB block
> >> soft-fork which increases capacity and scalability, and recent speedups
> >> and incoming relay improvements make segwit a reasonable risk. BIP9
> >> and segwit will also make further improvements easier and faster to
> >> deploy. We?ll continue to set the stage for non-bandwidth-increase-based
> >> scaling, while building additional tools that would make bandwidth
> >> increases safer long term. Further work will prepare Bitcoin for further
> >> increases, which will become possible when justified, while also
> providing
> >> the groundwork to make them justifiable.
> >
> > Sounds good to me.
>
> Better late than never, let me comment on why I believe pursuing this plan
> is important.
>
> For months, the block size debate, and the apparent need for agreement on
> a hardfork has distracted from needed engineering work, fed the external
> impression that nothing is being done, and generally created a toxic
> environment to work in. It has affected my own productivity and health, and
> I do not think I am alone.
>
> I believe that soft-fork segwit can help us out of this deadlock and get
> us going again. It does not require the pervasive assumption that the
> entire world will simultaneously switch to new consensus rules like a
> hardfork does, while at the same time:
> * Give a short-term capacity bump
> * Show the world that scalability is being worked on
> * Actually improve scalability (as opposed to just scale) by reducing
> bandwidth/storage and indirectly improving the effectiveness of systems
> like Lightning.
> * Solve several unrelated problems at the same time (fraud proofs, script
> extensibility, malleability, ...).
>
> So I'd like to ask the community that we work towards this plan, as it
> allows to make progress without being forced to make a possibly divisive
> choice for one hardfork or another yet.
>
> --
> Pieter
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151220/727153dd/attachment.html>

From joe2015 at openmailbox.org  Mon Dec 21 04:41:54 2015
From: joe2015 at openmailbox.org (joe2015 at openmailbox.org)
Date: Mon, 21 Dec 2015 12:41:54 +0800
Subject: [bitcoin-dev] Increasing the blocksize as a (generalized)
 softfork.
In-Reply-To: <e170f3a10164019824edaafe5a04f067@xbt.hk>
References: <1bf64a5b514d57ca37744ae5f5238149@openmailbox.org>
	<e170f3a10164019824edaafe5a04f067@xbt.hk>
Message-ID: <f9ad1348fb7dedca35b594782fee7e0f@openmailbox.org>

On 2015-12-21 12:23, jl2012 wrote:
> I proposed something very similar 2 years ago:
> https://bitcointalk.org/index.php?topic=283746.0

Yes there are similarities but also some important differences.  See my 
response here: 
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012085.html

In short my proposal is compatible with conventional blocksize limit 
hardfork ideas, like BIP101, BIP202, 2-4-8 etc. etc.

> This is an interesting academic idea. But the way you implement it
> will immediately kill all existing full and SPV nodes (not really
> dead, rather like zombie as they can't send and receive any tx).

That's the whole point.  After a conventional hardfork everyone needs to 
upgrade, but there is no way to force users to upgrade.  A user who is 
simply unaware of the fork, or disagrees with the fork, uses the old 
client and the currency splits.

Under this proposal old clients effectively enter "zombie" mode, forcing 
users to upgrade.

--joe


From mark at friedenbach.org  Mon Dec 21 04:50:03 2015
From: mark at friedenbach.org (Mark Friedenbach)
Date: Mon, 21 Dec 2015 12:50:03 +0800
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAPg+sBhQN2HDvH8dfq2VsQ0dTA9V=HgQsCJdP6B72fj1SDA4yw@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CAPWm=eUomq6SBC0ky0WSs5=_G942vigm4RmgYuq0O-yJ-vqC2A@mail.gmail.com>
	<CAPg+sBig9O5+he0PWhTkX5iin14QLz5+eCCu6KfwU=DxntKYtg@mail.gmail.com>
	<CAPg+sBhQN2HDvH8dfq2VsQ0dTA9V=HgQsCJdP6B72fj1SDA4yw@mail.gmail.com>
Message-ID: <CAOG=w-soDwM+iZAOAYEEh+Fb=qQ4CkY622L8mfB59yPmb8tmmQ@mail.gmail.com>

I am fully in support of the plan laid out in "Capacity increases for the
bitcoin system".

This plan provides real benefit to the ecosystem in solving a number of
longstanding problems in bitcoin. It improves the scalability of bitcoin
considerably.

Furthermore it is time that we stop bikeshedding, start implementing, and
move forward, lest we lose more developers to the toxic atmosphere this
hard-fork debacle has created.

On Mon, Dec 21, 2015 at 12:33 PM, Pieter Wuille via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> On Tue, Dec 8, 2015 at 6:07 AM, Wladimir J. van der Laan wrote:
> > On Mon, Dec 07, 2015 at 10:02:17PM +0000, Gregory Maxwell via
> bitcoin-dev wrote:
> >> TL;DR: I propose we work immediately towards the segwit 4MB block
> >> soft-fork which increases capacity and scalability, and recent speedups
> >> and incoming relay improvements make segwit a reasonable risk. BIP9
> >> and segwit will also make further improvements easier and faster to
> >> deploy. We?ll continue to set the stage for non-bandwidth-increase-based
> >> scaling, while building additional tools that would make bandwidth
> >> increases safer long term. Further work will prepare Bitcoin for further
> >> increases, which will become possible when justified, while also
> providing
> >> the groundwork to make them justifiable.
> >
> > Sounds good to me.
>
> Better late than never, let me comment on why I believe pursuing this plan
> is important.
>
> For months, the block size debate, and the apparent need for agreement on
> a hardfork has distracted from needed engineering work, fed the external
> impression that nothing is being done, and generally created a toxic
> environment to work in. It has affected my own productivity and health, and
> I do not think I am alone.
>
> I believe that soft-fork segwit can help us out of this deadlock and get
> us going again. It does not require the pervasive assumption that the
> entire world will simultaneously switch to new consensus rules like a
> hardfork does, while at the same time:
> * Give a short-term capacity bump
> * Show the world that scalability is being worked on
> * Actually improve scalability (as opposed to just scale) by reducing
> bandwidth/storage and indirectly improving the effectiveness of systems
> like Lightning.
> * Solve several unrelated problems at the same time (fraud proofs, script
> extensibility, malleability, ...).
>
> So I'd like to ask the community that we work towards this plan, as it
> allows to make progress without being forced to make a possibly divisive
> choice for one hardfork or another yet.
>
> --
> Pieter
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151221/fa4c9349/attachment.html>

From jl2012 at xbt.hk  Mon Dec 21 05:14:12 2015
From: jl2012 at xbt.hk (jl2012)
Date: Mon, 21 Dec 2015 00:14:12 -0500
Subject: [bitcoin-dev] =?utf-8?q?A_new_payment_address_format_for_segregat?=
 =?utf-8?q?ed_witness_or_not=3F?=
Message-ID: <537b176b25a681255eee5f6c4268ab6e@xbt.hk>

On the -dev IRC I asked the same question and people seem don't like it. 
I would like to further elaborate this topic and would like to consult 
merchants, exchanges, wallet devs, and users for their preference

Background:

People will be able to use segregated witness in 2 forms. They either 
put the witness program directly as the scriptPubKey, or hide the 
witness program in a P2SH address. They are referred as "native SW" and 
"SW in P2SH" respectively

Examples could be found in the draft BIP: 
https://github.com/jl2012/bips/blob/segwit/bip-segwit.mediawiki

As a tx malleability fix, native SW and SW in P2SH are equally good.

The SW in P2SH is better in terms of:
1. It allows payment from any Bitcoin reference client since version 
0.6.0.
2. Slightly better privacy by obscuration since people won't know 
whether it is a traditional P2SH or a SW tx before it is spent. I don't 
consider this is important since the type of tx will be revealed 
eventually, and is irrelevant when native SW is more popular

The SW in P2SH is worse in terms of:
1. It requires an additional push in scriptSig, which is not prunable in 
transmission, and is counted as part of the core block size
2. It requires an additional HASH160 operation than native SW
3. It provides 160bit security, while native SW provides 256bit
4. Since it is less efficient, the tx fee is likely to be higher than 
native SW (but still lower than non-SW tx)
---------------------------

The question: should we have a new payment address format for native SW?

The native SW address in my mind is basically same as existing P2PKH and 
P2SH addresses:

BASE58(address_version|witness_program|checksum) , where checksum is the 
first 4 bytes of dSHA256(address_version|witness_program)

Why not a better checksum algorithm? Reusing the existing algorithm make 
the implementation much easier and safe.

Pros for native SW address:
1. Many people and services are still using BASE58 address
2. Promote the use of native SW which allows lower fee, instead of the 
less efficient SW in P2SH
3. Not all wallets and services support payment protocol (BIP70)
4. Easy for wallets to implement
5. Even if a wallet wants to only implement SW in P2SH, they need a new 
wallet format anyway. So there is not much exta cost to introduce a new 
address format.
6. Since SW is very flexible, this is very likely to be the last address 
format to define.

Cons for native SW address:
1. Addresses are bad and should not be used anymore (some arguments 
could be found in BIP13)
2. Payment protocol is better
3. With SW in P2SH, it is not necessary to have a new address format
4. Depends on the length of the witness program, the address length 
could be a double of the existing address
5. Old wallets won't be able to pay to a new address (but no money could 
be lost this way)

------------------------------

So I'd like to suggest 2 proposals:

Proposal 1:

To define a native SW address format, while people can still use payment 
protocol or SW in P2SH if the want

Proposal 2:

No new address format is defined. If people want to pay as lowest fee as 
possible, they must use payment protocol. Otherwise, they may use SW in 
P2SH

Since this topic is more relevant to user experience, in addition to 
core devs, I would also like to consult merchants, exchanges, wallet 
devs, and users for their preferences.

From btcdrak at gmail.com  Mon Dec 21 05:21:55 2015
From: btcdrak at gmail.com (Btc Drak)
Date: Mon, 21 Dec 2015 05:21:55 +0000
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAPg+sBhQN2HDvH8dfq2VsQ0dTA9V=HgQsCJdP6B72fj1SDA4yw@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CAPWm=eUomq6SBC0ky0WSs5=_G942vigm4RmgYuq0O-yJ-vqC2A@mail.gmail.com>
	<CAPg+sBig9O5+he0PWhTkX5iin14QLz5+eCCu6KfwU=DxntKYtg@mail.gmail.com>
	<CAPg+sBhQN2HDvH8dfq2VsQ0dTA9V=HgQsCJdP6B72fj1SDA4yw@mail.gmail.com>
Message-ID: <CADJgMzuk9Q09AnmR5=77p0HfkeUWOTRSNCSkAAQN0zDq4Hsbqw@mail.gmail.com>

On Mon, Dec 21, 2015 at 4:33 AM, Pieter Wuille via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> On Tue, Dec 8, 2015 at 6:07 AM, Wladimir J. van der Laan wrote:
> > On Mon, Dec 07, 2015 at 10:02:17PM +0000, Gregory Maxwell via
> bitcoin-dev wrote:
> >> TL;DR: I propose we work immediately towards the segwit 4MB block
> >> soft-fork which increases capacity and scalability, and recent speedups
> >> and incoming relay improvements make segwit a reasonable risk. BIP9
> >> and segwit will also make further improvements easier and faster to
> >> deploy. We?ll continue to set the stage for non-bandwidth-increase-based
> >> scaling, while building additional tools that would make bandwidth
> >> increases safer long term. Further work will prepare Bitcoin for further
> >> increases, which will become possible when justified, while also
> providing
> >> the groundwork to make them justifiable.
> >
> > Sounds good to me.
>
> Better late than never, let me comment on why I believe pursuing this plan
> is important.
>
> For months, the block size debate, and the apparent need for agreement on
> a hardfork has distracted from needed engineering work, fed the external
> impression that nothing is being done, and generally created a toxic
> environment to work in. It has affected my own productivity and health, and
> I do not think I am alone.
>
> I believe that soft-fork segwit can help us out of this deadlock and get
> us going again. It does not require the pervasive assumption that the
> entire world will simultaneously switch to new consensus rules like a
> hardfork does, while at the same time:
> * Give a short-term capacity bump
> * Show the world that scalability is being worked on
> * Actually improve scalability (as opposed to just scale) by reducing
> bandwidth/storage and indirectly improving the effectiveness of systems
> like Lightning.
> * Solve several unrelated problems at the same time (fraud proofs, script
> extensibility, malleability, ...).
>
> So I'd like to ask the community that we work towards this plan, as it
> allows to make progress without being forced to make a possibly divisive
> choice for one hardfork or another yet.
>
Thank you for saying this. I also think the plan is solid and delivers
multiple benefits without being contentious. The number of wins are so
numerous, it's frankly a no-brainer.

I guess the next step for segwit is a BIP and deployment on a testnet?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151221/c11d08e4/attachment.html>

From joroark at vt.edu  Mon Dec 21 05:29:16 2015
From: joroark at vt.edu (Douglas Roark)
Date: Sun, 20 Dec 2015 21:29:16 -0800
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CAOG=w-soDwM+iZAOAYEEh+Fb=qQ4CkY622L8mfB59yPmb8tmmQ@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CAPWm=eUomq6SBC0ky0WSs5=_G942vigm4RmgYuq0O-yJ-vqC2A@mail.gmail.com>
	<CAPg+sBig9O5+he0PWhTkX5iin14QLz5+eCCu6KfwU=DxntKYtg@mail.gmail.com>
	<CAPg+sBhQN2HDvH8dfq2VsQ0dTA9V=HgQsCJdP6B72fj1SDA4yw@mail.gmail.com>
	<CAOG=w-soDwM+iZAOAYEEh+Fb=qQ4CkY622L8mfB59yPmb8tmmQ@mail.gmail.com>
Message-ID: <56778E2C.6040709@vt.edu>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

On 2015/12/20 20:50, Mark Friedenbach via bitcoin-dev wrote:
> I am fully in support of the plan laid out in "Capacity increases 
> for the bitcoin system".
> 
> This plan provides real benefit to the ecosystem in solving a 
> number of longstanding problems in bitcoin. It improves the 
> scalability of bitcoin considerably.
> 
> Furthermore it is time that we stop bikeshedding, start 
> implementing, and move forward, lest we lose more developers to
> the toxic atmosphere this hard-fork debacle has created.

Another +1 here. While I'd still like to see some sort of short-term
bump happen this year - good points have been raised about SegWit
uptake by wallet devs, for one thing - I really do think this is one
of the last pieces of the puzzle that'll make Bitcoin reasonably
stable and robust. If people have legitimate concerns, that's great,
and they should be addressed. I just worry that more navel-gazing and
bikeshedding will play into the hands of those with less than noble
intentions. That and, due to the somewhat complicated nature of
SegWit, it may take time to get skeptical miners and wallet devs on-boar
d.

While we're talking about capacity increases, I'd like to reiterate
that I do think there should be some sort of short-term bump (Jeff's
BIP 102 or his "BIP 202" variant, Dr. Back's 2/4/8 proposal ("BIP
248"), etc.), hopefully chosen by this summer so that everybody can
start to prepare. I believe the KISS theory will work best. I talked
to a couple of miners at Scaling Bitcoin. It was obvious they
generally prefer simple solutions. (For that matter, if I put my
miner's cap on, I prefer simple solutions too!) The research presented
at Scaling Bitcoin regarding block size formulas was quite interesting
and worthy of discussion. The research was also, IMO, nowhere near
ready for consensus. Work and discussions on that front should
certainly continue and push for a more permanent (final?) block size
solution. I just think that, barring some extraordinary solution that
hasn't been widely discussed yet, a permanent solution isn't feasible
right now. A temporary bump isn't ideal. It's just the only thing I've
seen that strikes me as having any real shot at consensus.

- -- 
- ---
Douglas Roark
Cryptocurrency, network security, travel, and art.
https://onename.com/droark
joroark at vt.edu
PGP key ID: 26623924
-----BEGIN PGP SIGNATURE-----
Comment: GPGTools - https://gpgtools.org

iQIcBAEBCgAGBQJWd44sAAoJEEOBHRomYjkklUkP/AqnD4+oiNNNYRGDY3m0bQSG
noUoRmWG/h86AW+2LuNYtn72UVefWJscUcmXWeOOem1KX49KdtCRWz3UZcrmfPUF
h/ilOpYpjCN69nFBhpJPp+0Jqr/PjQpoZkUQ2G1BznGIcIo3jwh7H7dQeI6PMtLB
qTbfdYEqPawb2kIhrCKVVQqsf7dLjg0Hlzvnq+xqyggZ1+k89kXSMEHJaybras7q
DFj1lOhzktzAtxquzAMcctkZM3JvFMnKUwOP6zC+ke9YlmvU0Yhu74F+30/EClLc
XGL5GMvUtvJcC0VRxDlh4pIW3m+eWjLWxvPQGe58eLE2u2Ja2MNjcuVtJdRgouLI
VSPBrUKoGOGfNfsqJH9U9jsvRuQMvT6JFS3jjxiapgi+ip1O7+Pkbq6tO55Mz7Gd
WMG71HdrLzZtjOzRmOFL5q3CkTpZp75tsXOYxn7jVcJlYJUh/jrnVMvSbPAT/VAY
yJIPtWRj+jtMKAR9m4Lx+9N4F56OC3g0M749v31luoYZkKMl7ohgkONgpKhrDRBU
uVmWH0pUIvaScsJxrUtgZdqn2AUqRowq6nM0YNDKo4go5/LyAkYYi1mICb0O0JJG
mt+3fabix6biBPHZDAvKxKX5CAPDapno2adTBx7vY36evGdhI9sWA1jw91He8Zmw
8hwnRV7R8bPdkoIfnc8e
=jJzD
-----END PGP SIGNATURE-----

From aj at erisian.com.au  Mon Dec 21 08:07:47 2015
From: aj at erisian.com.au (Anthony Towns)
Date: Mon, 21 Dec 2015 18:07:47 +1000
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <CADJgMzuk9Q09AnmR5=77p0HfkeUWOTRSNCSkAAQN0zDq4Hsbqw@mail.gmail.com>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CAPWm=eUomq6SBC0ky0WSs5=_G942vigm4RmgYuq0O-yJ-vqC2A@mail.gmail.com>
	<CAPg+sBig9O5+he0PWhTkX5iin14QLz5+eCCu6KfwU=DxntKYtg@mail.gmail.com>
	<CAPg+sBhQN2HDvH8dfq2VsQ0dTA9V=HgQsCJdP6B72fj1SDA4yw@mail.gmail.com>
	<CADJgMzuk9Q09AnmR5=77p0HfkeUWOTRSNCSkAAQN0zDq4Hsbqw@mail.gmail.com>
Message-ID: <20151221080747.GA24839@sapphire.erisian.com.au>

On Mon, Dec 21, 2015 at 05:21:55AM +0000, Btc Drak via bitcoin-dev wrote:
> On Mon, Dec 21, 2015 at 4:33 AM, Pieter Wuille via bitcoin-dev <
> > So I'd like to ask the community that we work towards this plan, as it
> > allows to make progress without being forced to make a possibly divisive
> > choice for one hardfork or another yet.
> Thank you for saying this. I also think the plan is solid and delivers
> multiple benefits without being contentious. The number of wins are so
> numerous, it's frankly a no-brainer.

+1's are off-topic, but... +1. My impression is that each of libsecp256k1,
versionbits, segregated witness, IBLT, weak blocks, and OP_CSV have
been demonstrated to be significant improvements that are implementable,
and don't introduce any new attacks or risks [0]. There's some freaking
awesome engineering that's gone into all of those.

> I guess the next step for segwit is a BIP and deployment on a testnet?

I think the following proposed features are as yet missing from Pieter's
segwit branch, and I'm guessing patches for them would be appreciated:

 - enforcing the proposed base+witness/4 < 1MB calculation
 - applying limits to sigops seen in witness signatures

I guess there might be other things that still need to be implemented
as well (and presumably bugs of course)?

I think I'm convinced that the proposed plan is the best approach (as
opposed to separate base<1MB, witness<3MB limits, or done as a hard fork,
or without committing to a merkle head for the witnesses, eg), though.

jl2012 already pointed to a draft segwit BIP in another thread, repeated
here though:

 https://github.com/jl2012/bips/blob/segwit/bip-segwit.mediawiki

Cheers,
aj (hoping that was enough content after the +1 to not get modded ;)

[0] I'm still not persuaded that even a small increase in blocksize
    doesn't introduce unacceptable risks (frankly, I'm not entirely
    persuaded the *current* limits don't have unacceptable risk) and that
    frustrates me no end. But I guess (even after six months of reading
    arguments about it!) I'm equally unpersuaded that there's actually
    more to the intense desire for more blocksize is anything other than
    fear/uncertainty/doubt mixed with a desire for transactions to be
    effectively free, rather than costing even a few cents each... So,
    personally, since the above doesn't really resolve that quandry
    for me, it doesn't really resolve the blocksize debate for me
    either. YMMV.


From jtimon at jtimon.cc  Mon Dec 21 09:56:54 2015
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Mon, 21 Dec 2015 10:56:54 +0100
Subject: [bitcoin-dev] Capacity increases for the Bitcoin system.
In-Reply-To: <20151221080747.GA24839@sapphire.erisian.com.au>
References: <CAAS2fgQyVs1fAEj+vqp8E2=FRnqsgs7VUKqALNBHNxRMDsHdVg@mail.gmail.com>
	<20151208110752.GA31180@amethyst.visucore.com>
	<CAPWm=eUomq6SBC0ky0WSs5=_G942vigm4RmgYuq0O-yJ-vqC2A@mail.gmail.com>
	<CAPg+sBig9O5+he0PWhTkX5iin14QLz5+eCCu6KfwU=DxntKYtg@mail.gmail.com>
	<CAPg+sBhQN2HDvH8dfq2VsQ0dTA9V=HgQsCJdP6B72fj1SDA4yw@mail.gmail.com>
	<CADJgMzuk9Q09AnmR5=77p0HfkeUWOTRSNCSkAAQN0zDq4Hsbqw@mail.gmail.com>
	<20151221080747.GA24839@sapphire.erisian.com.au>
Message-ID: <CABm2gDrhTMU7hBgYB3YF+pvCOzp2Bq4dAqJQRLB_fa8+Y9yRag@mail.gmail.com>

To clarify, although I have defended the deployment of segwit as a
hardfork, I have no strong opinion on whether to do that or do it as a
softfork first and then do a hardfork to move things out of the
coinbase to a better place.
I have a strong opinion against never doing the later hardfork though.
I would have supported segwit for Bitcoin even if it was only possible
as a hardfork, but there's a softfork version and that will hopefully
accelerate its deployment.
Since the plan seems to be to do a softfork first and a hardfork
moving the witness tree (and probably more things) outside of the
coinbase later, I support the plan for segwit deployment.
In fact, the plan is very exciting to me.

From jannes.faber at gmail.com  Mon Dec 21 11:39:40 2015
From: jannes.faber at gmail.com (Jannes Faber)
Date: Mon, 21 Dec 2015 12:39:40 +0100
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CAPkFh0t-+WhZYVLyT_auLa87zAATNOH=CpU4S5H=n6S1wmZ-oQ@mail.gmail.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<219f125cee6ca68fd27016642e38fdf1@xbt.hk>
	<CAAcC9ys_t7X0WpQ8W3577M8GLiA5sPV2F1BJ9qZbnMkE-1j3+Q@mail.gmail.com>
	<aff8da46a69bdd7ef92ca87725866a5c@xbt.hk>
	<CAPkFh0vNECi1OmBwki+8NNAQbe6EG2FEE4RR5z=kYVLLDFHUXg@mail.gmail.com>
	<20151220132842.GA25481@muck>
	<CAPkFh0t-+WhZYVLyT_auLa87zAATNOH=CpU4S5H=n6S1wmZ-oQ@mail.gmail.com>
Message-ID: <CABeL=0jgv3G8qx6wM+ZfwN154qhQY-GJdXnABc-iWL=YDNmhag@mail.gmail.com>

If you're saying a block withholding attack is a nice weapon to have to
dissuade large pools, isn't that easily defeated by large pools simply
masquerading as multiple small pools? As, for all we know, ghash may have
done?

If you don't know who to attack there's no point in having the weapon.
While that weapon is still dangerous in the hands of others that are
indiscriminate, like the solo miners example of Peter Todd.

Sorry if i misunderstood your point.

--
Jannes

On 20 December 2015 at 18:00, Emin G?n Sirer <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> On Sun, Dec 20, 2015 at 8:28 AM, Peter Todd <pete at petertodd.org> wrote:
>
>> There are a number of techniques that can be used to detect block
>> withholding attacks that you are not aware of. These techniques usually
>> have the characteristic that if known they can be avoided, so obviously
>> those who know about them are highly reluctant to reveal what exactly
>> they are. I personally know about some of them and have been asked to
>> keep that information secret, which I will.
>>
>
> Indeed, there are lots of weak measures that one could employ against
> an uninformed attacker. As I mentioned before, these are unlikely to be
> effective against a savvy attacker, and this is a good thing.
>
>
>> In the context of KYC, this techniques would likely hold up in court,
>> which means that if this stuff becomes a more serious problem it's
>> perfectly viable for large, well-resourced, pools to prevent block
>> withholding attacks, in part by removing anonymity of hashing power.
>> This would not be a positive development for the ecosystem.
>>
>
> KYC has a particular financial-regulation connotation in Bitcoin circles,
> of which I'm sure you're aware, and which you're using as a spectre.
> You don't mean government-regulated-KYC a la FINCEN and Bitcoin
> exchanges like Coinbase, you are just referring to a pool operator
> demanding to know that its customer is not coming from its competitors'
> data centers.
>
> And your prediction doesn't seem well-motivated or properly justified.
> There are tons of conditionals in your prediction, starting with the
> premise
> that every single open pool would implement some notion of identity
> checking. I don't believe that will happen. Instead, we will have the
> bigger
> pools become more suspicious of signing up new hash power, which is a
> good thing. And we will have small groups of people who have some reason
> for trusting each other (e.g. they know each other from IRC, conferences,
> etc) band together into small pools. These are fantastic outcomes for
> decentralization.
>
> Secondly, DRM tech can also easily be used to prevent block withholding
>> attacks by attesting to the honest of the hashing power. This is being
>> discussed in the industry, and again, this isn't a positive development
>> for the ecosystem.
>>
>
> DRM is a terrible application. Once again, I see that you're trying to use
> those
> three letters as a spectre as well, knowing that most people hate DRM, but
> keep in mind that DRM is just an application -- it's like pointing to
> Adobe Flash
> to taint all browser plugins.
>
> The tech behind DRM is called "attestation," and it provides a technical
> capability not possible by any other means. In essence, attestation can
> ensure that
> a remote node is indeed running the code that it purports to be running.
> Since
> most problems in computer security and distributed systems stem from not
> knowing what protocol the attacker is going to follow, attestation is the
> only
> technology we have that lets us step around this limitation.
>
> It can ensure, for instance,
>   - that a node purporting to be Bitcoin Core (vLatest) is indeed running
> an
> unadulterated, latest version of Bitcoin Core
>   - that a node claiming that it does not harvest IP addresses from SPV
> clients indeed does not harvest IP addresses.
>   - that a cloud hashing outfit that rented out X terahashes to a user did
> indeed rent out X terahashes to that particular user,
>   - that a miner operating on behalf of some pool P will not misbehave and
> discard perfectly good blocks
> and so forth. All of these would be great for the ecosystem. Just getting
> rid
> of the cloudhashing scams would put an end to a lot of heartache.
>
> > Keep in mind that when an open pool gets big, like GHash did and
>> > two other pools did before them, the only thing at our disposal used
>> > to be to yell at people about centralization until they left the big
>> > pools and reformed into smaller groups. Not only was such yelling
>> > kind of desperate looking, it wasn't incredibly effective, either.
>> > We had no protocol mechanisms that put pressure on big pools to
>> > stop signing up people. Ittay's discovery changed that: pools that
>> > get to be very big by indiscriminately signing up miners are likely to
>> > be infiltrated and their profitability will drop. And Peter's post is
>> > evidence that this is, indeed, happening as predicted. This is a
>> > good outcome, it puts pressure on the big pools to not grow.
>>
>> GHash.io was not a pure pool - they owned and operated a significant
>> amount of physical hashing power, and it's not at all clear that their %
>> of the network actually went down following that 51% debacle.
>>
>
> Right, it's not clear at all that yelling at people has much effect. As
> much
> fun as I had going to that meeting with GHash in London to ask them to
> back down off of the 51% boundary, I am pretty sure that yelling at large
> open pools will not scale. We needed better mechanisms for keeping pools
> in check.
>
> And Miner's Dilemma (MD) attacks are clearly quite effective. This is a
> time when we should count our blessings, not work actively to render
> them inoperable.
>
> Currently a significant % of the hashing power - possibly a majority -
>> is in the form of large hashing installations whose owners individually,
>> and definitely in trusting groups, have enough hashing power to solo
>> mine. Eyal's results indicate those miners have incentives to attack
>> pools, and additionally they have the incentive of killing off pools to
>> make it difficult for new competition to get established, yet they
>> themselves are not vulnerable to that attack.
>>
>
> There are indeed solo miners out there who can attack the big open
> pools. The loss of the biggest open pools would not be a bad outcome.
> Pools >25% pose a danger, and the home miner doesn't need a pool
> >25% for protection against variance.
>
> > Peter, you allude to a specific suggestion from Luke-Jr. Can you
>> > please describe what it is?
>>
>> Basically you have the pool pick a secret k for each share, and commit
>> to H(k) in the share. Additionally the share commits to a target divider
>> D. The PoW validity rule is then changed from H(block header) < T, to be
>> H(block header) < T * D && H(H(block header) + k) < max_int / D
>>
>
> Thanks, this requires a change to the Bitcoin PoW. Good luck with that!
>
> Once again, this suggestion would make the GHash-at-51% situation
> possible again. Working extra hard to re-enable those painful days
> sounds like a terrible idea.
>
> - egs
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151221/bf51d088/attachment.html>

From tier.nolan at gmail.com  Mon Dec 21 15:48:23 2015
From: tier.nolan at gmail.com (Tier Nolan)
Date: Mon, 21 Dec 2015 15:48:23 +0000
Subject: [bitcoin-dev] A new payment address format for segregated
 witness or not?
In-Reply-To: <537b176b25a681255eee5f6c4268ab6e@xbt.hk>
References: <537b176b25a681255eee5f6c4268ab6e@xbt.hk>
Message-ID: <CAE-z3OVV5ozYqj4ziZ8J0-8LAnFd+zhC2HO9OzkPpwZGBsEKxA@mail.gmail.com>

On Mon, Dec 21, 2015 at 5:14 AM, jl2012 via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> The SW in P2SH is worse in terms of:
> 1. It requires an additional push in scriptSig, which is not prunable in
> transmission, and is counted as part of the core block size
>

"Prunable in transmission" means that you have to include it when not
sending the witnesses?

That is a name collision with UTXO set prunable.  My initial thought when
reading that was "but scriptSigs are inherently prunable, it is
scriptPubKeys that have to be held in the UTXO database" until I saw the
"in transmission" clarification.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151221/7553ba52/attachment.html>

From laanwj at gmail.com  Tue Dec 22 15:58:45 2015
From: laanwj at gmail.com (Wladimir J. van der Laan)
Date: Tue, 22 Dec 2015 16:58:45 +0100
Subject: [bitcoin-dev] Weekly developer meetings over holidays
Message-ID: <20151222155845.GA23900@amethyst.visucore.com>


Next two weekly developer meetings would fall on:

- Thursday December 24th
- Thursday December 31th

In my timezone they're xmas eve and new year's eve respectively, so at least I
won't be there, and I'm sure they're inconvenient for most people.

So: let's have a two week hiatus, and continue January 7th.

Wladimir

From pete at petertodd.org  Wed Dec 23 01:31:19 2015
From: pete at petertodd.org (Peter Todd)
Date: Tue, 22 Dec 2015 17:31:19 -0800
Subject: [bitcoin-dev] Segregated witnesses and validationless mining
Message-ID: <20151223013119.GA31113@muck>

# Summary

1) Segregated witnesses separates transaction information about what
coins were transferred from the information proving those transfers were
legitimate.

2) In its current form, segregated witnesses makes validationless mining
easier and more profitable than the status quo, particularly as
transaction fees increase in relevance.

3) This can be easily fixed by changing the protocol to make having a
copy of the previous block's (witness) data a precondition to creating a
block.


# Background

## Why should a miner publish the blocks they find?

Suppose Alice has negligible hashing power. She finds a block. Should
she publish that block to the rest of the hashing power? Yes! If she
doesn't publish, the rest of the hashing power will build a longer chain
than her chain, and she won't be rewarded. Right?

Well, can other miners build on top of Alice's block? If she publishes
nothing at all, the answer is certainely no - block headers commit to
the previous block's hash, so without knowing at least the hash of
Alice's block other miners can't build upon it.


## Validationless mining

Suppose Bob knows the hash of Alice's new block, as well as the height
of it. This is sufficient information for Bob to create a new, valid,
block building upon Alice's block. The hash is needed because of the
prevhash field in the block header; the height is needed because the
coinbase has to contain the block height. (technically he needs to know
nTime as well to be 100% sure he's satisfying the median time rule) What
Bob is doing is validationless mining: he hasn't validated Alice's
block, and is assuming it is valid.

If Alice runs a pool her stratum or getblocktemplate interfaces give
sufficient information for Bob to figure all this out. Miners today take
advantage of this to reduce their orphan rates - the sooner you can
start mining on top of the most recently found block the more money you
earn. Pools have strong incentives to only publish work that's valid to
their hashers, so as long as the target pool doesn't know who you are,
you have high assurance that the block hash you're building upon is
real.

Of course, when this goes wrong it goes very wrong, greatly amplifying
the effect of 51% attacks and technical screwups, as seen by the July
4th 2015 chain fork, where a majority of hashing power was building on
top of an invalid block.


## Transactions

However other than coinbase transactions, validationless mined blocks
are nearly always empty: if Bob doesn't know what transactions Alice
included in her block, he doesn't know what transaction outputs are
still unspent and can't safely include transactions in his block. In
short, Bob doesn't know what the current state of the UTXO set is. This
helps limit the danger of validationless mining by making it visible to
everyone, as well as making it not as profitable due to the inability to
collect transaction fees. (among other reasons)


# Segregated witnesses and validationless mining

With segregated witnesses the information required to update the UTXO
set state is now separate from the information required to prove that
the new state is valid. We can fully expect miners to take advantage of
this to reduce latency and thus improve their profitability.

We can expect block relaying with segregated witnesses to separate block
propagation into four different parts, from fastest to propagate to
slowest:

1) Stratum/getblocktemplate - status quo between semi-trusting miners

2) Block header - bare minimum information needed to build upon a block.
Not much trust required as creating an invalid header is expensive.

3) Block w/o witness data - significant bandwidth savings, (~75%) and
allows next miner to include transactions as normal. Again, not much
trust required as creating an invalid header is expensive.

4) Witness data - proves that block is actually valid.

The problem is #4 is optional: the only case where not having the
witness data matters is when an invalid block is created, which is a
very rare event. It's also difficult to test in production, as creating
invalid blocks is extremely expensive - it would be surprising if an
anyone had ever deliberately created an invalid block meeting the
current difficulty target in the past year or two.


# The nightmare scenario - never tested code ~never works

The obvious implementation of highly optimised mining with segregated
witnesses will have the main codepath that creates blocks do no
validation at all; if the current ecosystem's validationless mining is
any indication the actual code doing this will be proprietary codebases
written on a budget with little testing, and lots of bugs. At best the
codepaths that actually do validation will be rarely, if ever, tested in
production.

Secondly, as the UTXO set can be updated without the witness data, it
would not be surprising if at least some of the wallet ecosystem skips
witness validation.

With that in mind, what happens in the event of a validation failure?
Mining could continue indefinitely on an invalid chain, producing blocks
that in isolation appear totally normal and contain apparently valid
transactions. It's easy to imagine this happening from an engineering
perspective: a simple implementation would be to have the main mining
codepaths be a separate, not-validating, process that receives "invalid
block" notifications from another process containing a validating
implementation of the Bitcoin protocol. If a bug/exploit is found that
causes that validation process to crash, what's to guarantee that the
block creation codepath will even notice? Quite likely it will continue
creating blocks unabated - the invalid block notification codepath is
never tested in production.


# Easy solution: previous witness data proof

To return segregated witnesses to the status quo, we need to at least
make having the previous block's witness data be a precondition to
creating a block with transactions; ideally we would make it a
precondition to making any valid block, although going this far may
receive pushback from miners who are currently using validationless
mining techniques.

We can require blocks to include the previous witness data, hashed with
a different hash function that the commitment in the previous block.
With witness data W, and H(W) the witness commitment in the previous
block, require the current block to include H'(W)

A possible concrete implementation would be to compute the hash of the
current block's coinbase txouts (unique per miner for obvious reasons!)
as well as the previous block hash. Then recompute the previous block's
witness data merkle tree (and optionally, transaction data merkle tree)
with that hash prepended to the serialized data for each witness.

This calculation can only be done by a trusted entity with access to all
witness data from the previous block, forcing miners to both publish
their witness data promptly, as well as at least obtain witness data
from other miners. (if not actually validate it!) This returns us to at
least the status quo, if not slightly better.

This solution is a soft-fork. As the calculation is only done once per
block, it is *not* a change to the PoW algorithm and is thus compatible
with existing miner/hasher setups. (modulo validationless mining
optimizations, which are no longer possible)


# Proofs of non-inflation vs. proofs of non-theft

Currently full nodes can easily verify both that inflation of the
currency has no occured, as well as verify that theft of coins through
invalid scriptSigs has not occured. (though as an optimisation currently
scriptSig's prior to checkpoints are not validated by default in Bitcoin
Core)

It has been proposed that with segregated witnesses old witness data
will be discarded entirely. This makes it impossible to know if miner
theft has occured in the past; as a practical matter due to the
significant amount of lost coins this also makes it possible to inflate
the currency.

How to fix this problem is an open question; it may be sufficient have
the previous witness data proof solution above require proving posession
of not just the n-1 block, but a (random?) selection of other previous
blocks as well. Adding this to the protocol could be done as soft-fork
with respect to the above previous witness data proof.

-- 
'peter'[:-1]@petertodd.org
000000000000000002c7cfc8455339de54444ac9798cad32cbfbcda77e0f2b09
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 650 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151222/6792b37a/attachment.sig>

From elombrozo at gmail.com  Wed Dec 23 15:22:30 2015
From: elombrozo at gmail.com (Eric Lombrozo)
Date: Wed, 23 Dec 2015 15:22:30 +0000
Subject: [bitcoin-dev] Segregated Witness BIPs
Message-ID: <em3b62e758-afbe-4aed-a08d-eb85b252efc4@platinum>

I've been working with jl2012 on some SEGWIT BIPs based on earlier 
discussions Pieter Wuille's implementation. We're considering submitting 
three separate BIPs:


CONSENSUS BIP: witness structures and how they're committed to blocks, 
cost metrics and limits, the scripting system (witness programs), and 
the soft fork mechanism.

PEER SERVICES BIP: relay message structures, witnesstx serialization, 
and other issues pertaining to the p2p protocol such as IBD, 
synchronization, tx and block propagation, etc...

APPLICATIONS BIP: scriptPubKey encoding formats and other wallet 
interoperability concerns.


The Consensus BIP is submitted as a draft and is pending BIP number 
assignment: https://github.com/bitcoin/bips/pull/265
The other two BIPS will be drafted soon.

---
Eric
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151223/c301011d/attachment.html>

From pete at petertodd.org  Wed Dec 23 15:41:43 2015
From: pete at petertodd.org (Peter Todd)
Date: Wed, 23 Dec 2015 07:41:43 -0800
Subject: [bitcoin-dev] Segregated witnesses and validationless mining
In-Reply-To: <20151223013119.GA31113@muck>
References: <20151223013119.GA31113@muck>
Message-ID: <20151223154143.GA2295@muck>

On Tue, Dec 22, 2015 at 05:31:19PM -0800, Peter Todd via bitcoin-dev wrote:
> # Easy solution: previous witness data proof
> 
> To return segregated witnesses to the status quo, we need to at least
> make having the previous block's witness data be a precondition to
> creating a block with transactions; ideally we would make it a
> precondition to making any valid block, although going this far may
> receive pushback from miners who are currently using validationless
> mining techniques.
> 
> We can require blocks to include the previous witness data, hashed with
> a different hash function that the commitment in the previous block.
> With witness data W, and H(W) the witness commitment in the previous
> block, require the current block to include H'(W)
> 
> A possible concrete implementation would be to compute the hash of the
> current block's coinbase txouts (unique per miner for obvious reasons!)
> as well as the previous block hash. Then recompute the previous block's
> witness data merkle tree (and optionally, transaction data merkle tree)
> with that hash prepended to the serialized data for each witness.
> 
> This calculation can only be done by a trusted entity with access to all
> witness data from the previous block, forcing miners to both publish
> their witness data promptly, as well as at least obtain witness data
> from other miners. (if not actually validate it!) This returns us to at
> least the status quo, if not slightly better.
> 
> This solution is a soft-fork. As the calculation is only done once per
> block, it is *not* a change to the PoW algorithm and is thus compatible
> with existing miner/hasher setups. (modulo validationless mining
> optimizations, which are no longer possible)

Note that this fix can be designed to retain the possibility of
validationless mining, by allowing empty blocks to be created if the
previous witness data proof is omitted. This would achieve the same goal
as Gregory Maxwell's blockchain verification flag(1) but with
significantly less ability/reason to lie about the status of that flag.

1) [bitcoin-dev] Blockchain verification flag (BIP draft),
   Gregory Maxwell, Dec 4th 2015,
   http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/011853.html

-- 
'peter'[:-1]@petertodd.org
000000000000000002c7cfc8455339de54444ac9798cad32cbfbcda77e0f2b09
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 650 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151223/f6274926/attachment.sig>

From voisine at gmail.com  Wed Dec 23 06:26:11 2015
From: voisine at gmail.com (Aaron Voisine)
Date: Tue, 22 Dec 2015 22:26:11 -0800
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
Message-ID: <CACq0ZD7PEL4ZaUndvtPOfar=MXp4hwUZfu2xQ_Fph6dOVPLV5w@mail.gmail.com>

> You present this as if the Bitcoin Core development team is in charge
> of deciding the network consensus rules, and is responsible for
> making changes to it in order to satisfy economic demand. If that is
> the case, Bitcoin has failed, in my opinion.

Pieter, what's actually happening is that the bitcoin-core release has
become a Schelling point in the consensus game:

https://en.wikipedia.org/wiki/Schelling_point

Due to the strong incentives for consensus, everyone is looking for an
obvious reference point that they think everyone else will also pick, even
though the point itself isn't critical, only that everyone agree on
whatever point is picked. Like it or not, the bitcoin-core release, and by
extension it's committers have a great degree of influence over what the
community as a whole decides to do. If core screws things up badly enough,
yes, the community will settle on some other focal point for consensus, but
the cost and risk of doing so is high, so there is indeed unavoidable moral
hazard for whoever has control over any such focus point.

Aaron Voisine
co-founder and CEO
breadwallet <http://breadwallet.com>

On Wed, Dec 16, 2015 at 10:34 AM, Pieter Wuille via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> On Wed, Dec 16, 2015 at 3:53 PM, Jeff Garzik via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org> wrote:
> > 2) If block size stays at 1M, the Bitcoin Core developer team should
> sign a
> > collective note stating their desire to transition to a new economic
> policy,
> > that of "healthy fee market" and strongly urge users to examine their fee
> > policies, wallet software, transaction volumes and other possible User
> > impacting outcomes.
>
> You present this as if the Bitcoin Core development team is in charge
> of deciding the network consensus rules, and is responsible for making
> changes to it in order to satisfy economic demand. If that is the
> case, Bitcoin has failed, in my opinion.
>
> What the Bitcoin Core team should do, in my opinion, is merge any
> consensus change that is uncontroversial. We can certainly -
> individually or not - propose solutions, and express opinions, but as
> far as maintainers of the software goes our responsibility is keeping
> the system running, and risking either a fork or establishing
> ourselves as the de-facto central bank that can make any change to the
> system would greatly undermine the system's value.
>
> Hard forking changes require that ultimately every participant in the
> system adopts the new rules. I find it immoral and dangerous to merge
> such a change without extremely widespread agreement. I am personally
> fine with a short-term small block size bump to kick the can down the
> road if that is what the ecosystem desires, but I can only agree with
> merging it in Core if I'm convinced that there is no strong opposition
> to it from others.
>
> Soft forks on the other hand only require a majority of miners to
> accept them, and everyone else can upgrade at their leisure or not at
> all. Yes, old full nodes after a soft fork are not able to fully
> validate the rules new miners enforce anymore, but they do still
> verify the rules that their operators opted to enforce. Furthermore,
> they can't be prevented. For that reason, I've proposed, and am
> working hard, on an approach that includes Segregated Witness as a
> first step. It shows the ecosystem that something is being done, it
> kicks the can down the road, it solves/issues half a dozen other
> issues at the same time, and it does not require the degree of
> certainty needed for a hardfork.
>
> --
> Pieter
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151222/2a316e8d/attachment.html>

From jl2012 at xbt.hk  Thu Dec 24 14:22:01 2015
From: jl2012 at xbt.hk (jl2012)
Date: Thu, 24 Dec 2015 09:22:01 -0500
Subject: [bitcoin-dev] Segregated Witness BIPs
In-Reply-To: <em3b62e758-afbe-4aed-a08d-eb85b252efc4@platinum>
References: <em3b62e758-afbe-4aed-a08d-eb85b252efc4@platinum>
Message-ID: <77ba13602b2d9e3c0f56a41a9799021a@xbt.hk>

The SW payment address format BIP draft is ready and is pending BIP 
number assignment:
https://github.com/bitcoin/bips/pull/267

This is the 3rd BIP for segwit. The 2nd one for Peer Services is being 
prepared by Eric Lombrozo

Eric Lombrozo via bitcoin-dev ? 2015-12-23 10:22 ??:
> I've been working with jl2012 on some SEGWIT BIPs based on earlier
> discussions Pieter Wuille's implementation. We're considering
> submitting three separate BIPs:
> 
> CONSENSUS BIP: witness structures and how they're committed to blocks,
> cost metrics and limits, the scripting system (witness programs), and
> the soft fork mechanism.
> 
> PEER SERVICES BIP: relay message structures, witnesstx serialization,
> and other issues pertaining to the p2p protocol such as IBD,
> synchronization, tx and block propagation, etc...
> 
> APPLICATIONS BIP: scriptPubKey encoding formats and other wallet
> interoperability concerns.
> 
> The Consensus BIP is submitted as a draft and is pending BIP number
> assignment: https://github.com/bitcoin/bips/pull/265 [1]
> The other two BIPS will be drafted soon.
> 
> ---
> Eric
> 
> Links:
> ------
> [1] https://github.com/bitcoin/bips/pull/265
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


From ittay.eyal at cornell.edu  Fri Dec 25 11:15:27 2015
From: ittay.eyal at cornell.edu (Ittay)
Date: Fri, 25 Dec 2015 13:15:27 +0200
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CABeL=0jgv3G8qx6wM+ZfwN154qhQY-GJdXnABc-iWL=YDNmhag@mail.gmail.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<219f125cee6ca68fd27016642e38fdf1@xbt.hk>
	<CAAcC9ys_t7X0WpQ8W3577M8GLiA5sPV2F1BJ9qZbnMkE-1j3+Q@mail.gmail.com>
	<aff8da46a69bdd7ef92ca87725866a5c@xbt.hk>
	<CAPkFh0vNECi1OmBwki+8NNAQbe6EG2FEE4RR5z=kYVLLDFHUXg@mail.gmail.com>
	<20151220132842.GA25481@muck>
	<CAPkFh0t-+WhZYVLyT_auLa87zAATNOH=CpU4S5H=n6S1wmZ-oQ@mail.gmail.com>
	<CABeL=0jgv3G8qx6wM+ZfwN154qhQY-GJdXnABc-iWL=YDNmhag@mail.gmail.com>
Message-ID: <CABT1wW=r5DPG1e6XFe7NMHrquo1FzygPCdjEJ2QQnmGbqVMH2Q@mail.gmail.com>

Treating the pool block withholding attack as a weapon has bad
connotations, and I don't think anyone directly condones such an attack.
Nevertheless, the mere possibility of the attack could drive miners away
from those overly-large open pools.

As for masquerading as multiple small pools -- that's a very good point,
with a surprising answer: it doesn't really matter. An attacker attacks all
parts of the open pool proportionally to their size, and the result is
basically identical to that of attacking a single large pool.

All that being said -- it's not great to rely on the potential of attacks
and on threats against the honest large pools out there (including GHash,
which, afaik, did nothing more wrong than being successful).

Best,
Ittay


On Mon, Dec 21, 2015 at 1:39 PM, Jannes Faber via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> If you're saying a block withholding attack is a nice weapon to have to
> dissuade large pools, isn't that easily defeated by large pools simply
> masquerading as multiple small pools? As, for all we know, ghash may have
> done?
>
> If you don't know who to attack there's no point in having the weapon.
> While that weapon is still dangerous in the hands of others that are
> indiscriminate, like the solo miners example of Peter Todd.
>
> Sorry if i misunderstood your point.
>
> --
> Jannes
>
> On 20 December 2015 at 18:00, Emin G?n Sirer <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> On Sun, Dec 20, 2015 at 8:28 AM, Peter Todd <pete at petertodd.org> wrote:
>>
>>> There are a number of techniques that can be used to detect block
>>> withholding attacks that you are not aware of. These techniques usually
>>> have the characteristic that if known they can be avoided, so obviously
>>> those who know about them are highly reluctant to reveal what exactly
>>> they are. I personally know about some of them and have been asked to
>>> keep that information secret, which I will.
>>>
>>
>> Indeed, there are lots of weak measures that one could employ against
>> an uninformed attacker. As I mentioned before, these are unlikely to be
>> effective against a savvy attacker, and this is a good thing.
>>
>>
>>> In the context of KYC, this techniques would likely hold up in court,
>>> which means that if this stuff becomes a more serious problem it's
>>> perfectly viable for large, well-resourced, pools to prevent block
>>> withholding attacks, in part by removing anonymity of hashing power.
>>> This would not be a positive development for the ecosystem.
>>>
>>
>> KYC has a particular financial-regulation connotation in Bitcoin circles,
>> of which I'm sure you're aware, and which you're using as a spectre.
>> You don't mean government-regulated-KYC a la FINCEN and Bitcoin
>> exchanges like Coinbase, you are just referring to a pool operator
>> demanding to know that its customer is not coming from its competitors'
>> data centers.
>>
>> And your prediction doesn't seem well-motivated or properly justified.
>> There are tons of conditionals in your prediction, starting with the
>> premise
>> that every single open pool would implement some notion of identity
>> checking. I don't believe that will happen. Instead, we will have the
>> bigger
>> pools become more suspicious of signing up new hash power, which is a
>> good thing. And we will have small groups of people who have some reason
>> for trusting each other (e.g. they know each other from IRC, conferences,
>> etc) band together into small pools. These are fantastic outcomes for
>> decentralization.
>>
>> Secondly, DRM tech can also easily be used to prevent block withholding
>>> attacks by attesting to the honest of the hashing power. This is being
>>> discussed in the industry, and again, this isn't a positive development
>>> for the ecosystem.
>>>
>>
>> DRM is a terrible application. Once again, I see that you're trying to
>> use those
>> three letters as a spectre as well, knowing that most people hate DRM,
>> but
>> keep in mind that DRM is just an application -- it's like pointing to
>> Adobe Flash
>> to taint all browser plugins.
>>
>> The tech behind DRM is called "attestation," and it provides a technical
>> capability not possible by any other means. In essence, attestation can
>> ensure that
>> a remote node is indeed running the code that it purports to be running.
>> Since
>> most problems in computer security and distributed systems stem from not
>> knowing what protocol the attacker is going to follow, attestation is the
>> only
>> technology we have that lets us step around this limitation.
>>
>> It can ensure, for instance,
>>   - that a node purporting to be Bitcoin Core (vLatest) is indeed running
>> an
>> unadulterated, latest version of Bitcoin Core
>>   - that a node claiming that it does not harvest IP addresses from SPV
>> clients indeed does not harvest IP addresses.
>>   - that a cloud hashing outfit that rented out X terahashes to a user
>> did
>> indeed rent out X terahashes to that particular user,
>>   - that a miner operating on behalf of some pool P will not misbehave and
>> discard perfectly good blocks
>> and so forth. All of these would be great for the ecosystem. Just getting
>> rid
>> of the cloudhashing scams would put an end to a lot of heartache.
>>
>> > Keep in mind that when an open pool gets big, like GHash did and
>>> > two other pools did before them, the only thing at our disposal used
>>> > to be to yell at people about centralization until they left the big
>>> > pools and reformed into smaller groups. Not only was such yelling
>>> > kind of desperate looking, it wasn't incredibly effective, either.
>>> > We had no protocol mechanisms that put pressure on big pools to
>>> > stop signing up people. Ittay's discovery changed that: pools that
>>> > get to be very big by indiscriminately signing up miners are likely to
>>> > be infiltrated and their profitability will drop. And Peter's post is
>>> > evidence that this is, indeed, happening as predicted. This is a
>>> > good outcome, it puts pressure on the big pools to not grow.
>>>
>>> GHash.io was not a pure pool - they owned and operated a significant
>>> amount of physical hashing power, and it's not at all clear that their %
>>> of the network actually went down following that 51% debacle.
>>>
>>
>> Right, it's not clear at all that yelling at people has much effect. As
>> much
>> fun as I had going to that meeting with GHash in London to ask them to
>> back down off of the 51% boundary, I am pretty sure that yelling at large
>> open pools will not scale. We needed better mechanisms for keeping pools
>> in check.
>>
>> And Miner's Dilemma (MD) attacks are clearly quite effective. This is a
>> time when we should count our blessings, not work actively to render
>> them inoperable.
>>
>> Currently a significant % of the hashing power - possibly a majority -
>>> is in the form of large hashing installations whose owners individually,
>>> and definitely in trusting groups, have enough hashing power to solo
>>> mine. Eyal's results indicate those miners have incentives to attack
>>> pools, and additionally they have the incentive of killing off pools to
>>> make it difficult for new competition to get established, yet they
>>> themselves are not vulnerable to that attack.
>>>
>>
>> There are indeed solo miners out there who can attack the big open
>> pools. The loss of the biggest open pools would not be a bad outcome.
>> Pools >25% pose a danger, and the home miner doesn't need a pool
>> >25% for protection against variance.
>>
>> > Peter, you allude to a specific suggestion from Luke-Jr. Can you
>>> > please describe what it is?
>>>
>>> Basically you have the pool pick a secret k for each share, and commit
>>> to H(k) in the share. Additionally the share commits to a target divider
>>> D. The PoW validity rule is then changed from H(block header) < T, to be
>>> H(block header) < T * D && H(H(block header) + k) < max_int / D
>>>
>>
>> Thanks, this requires a change to the Bitcoin PoW. Good luck with that!
>>
>> Once again, this suggestion would make the GHash-at-51% situation
>> possible again. Working extra hard to re-enable those painful days
>> sounds like a terrible idea.
>>
>> - egs
>>
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151225/54346fe1/attachment.html>

From j at toom.im  Fri Dec 25 12:00:11 2015
From: j at toom.im (Jonathan Toomim)
Date: Fri, 25 Dec 2015 04:00:11 -0800
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CABT1wW=r5DPG1e6XFe7NMHrquo1FzygPCdjEJ2QQnmGbqVMH2Q@mail.gmail.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<219f125cee6ca68fd27016642e38fdf1@xbt.hk>
	<CAAcC9ys_t7X0WpQ8W3577M8GLiA5sPV2F1BJ9qZbnMkE-1j3+Q@mail.gmail.com>
	<aff8da46a69bdd7ef92ca87725866a5c@xbt.hk>
	<CAPkFh0vNECi1OmBwki+8NNAQbe6EG2FEE4RR5z=kYVLLDFHUXg@mail.gmail.com>
	<20151220132842.GA25481@muck>
	<CAPkFh0t-+WhZYVLyT_auLa87zAATNOH=CpU4S5H=n6S1wmZ-oQ@mail.gmail.com>
	<CABeL=0jgv3G8qx6wM+ZfwN154qhQY-GJdXnABc-iWL=YDNmhag@mail.gmail.com>
	<CABT1wW=r5DPG1e6XFe7NMHrquo1FzygPCdjEJ2QQnmGbqVMH2Q@mail.gmail.com>
Message-ID: <8BA2CF44-4237-460E-8339-F22A29504AE5@toom.im>

On Dec 25, 2015, at 3:15 AM, Ittay via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:

> Treating the pool block withholding attack as a weapon has bad connotations, and I don't think anyone directly condones such an attack.

I directly condone the use of block withholding attacks whenever pools get large enough to perform selfish mining attacks. Selfish mining and large, centralized pools also have bad connotations.

It's an attack against pools, not just large pools. Solo miners are immune. As such, the presence or use of block withholding attacks makes Bitcoin more similar to Satoshi's original vision. One of the issues with mining centralization via pools is that miners have a direct financial incentive to stay relatively small, but pools do not. Investing in mining is a zero-sum game, where each miner gains revenue by making investments at the expense of existing miners. This also means that miners take revenue from themselves when they upgrade their hashrate. If a miner already has 1/5 of the network hashrate, then the marginal revenue for that miner of adding 1 TH/s is only 4/5 of the marginal revenue for a miner with 0% of the network and who adds 1 TH/s. The bigger you get, the smaller your incentive to get bigger.

This incentive applies to miners, but it does not apply to pools. Pools have an incentive to get as big as possible (except for social backlash and altruistic punishment issues). Pools are the problem. I think we should be looking for ways of making pooled mining less profitable than solo mining or p2pool-style mining. Block withholding attacks are one such tool, and maybe the only usable tool we'll get. If we have to choose between making bitcoin viable long-term and avoiding things with bad connotations, it might be better to let our hands get a little bit dirty.

I don't intend to perform any such attacks myself. I like to keep my hat a nice shiny white. However, if anyone else were to perform such an attack, I would condone it.

P.S.: Sorry, pool operators. I have nothing against you personally. I just think pools are dangerous, and I wish they didn't exist.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151225/590e0d31/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151225/590e0d31/attachment.sig>

From jannes.faber at gmail.com  Fri Dec 25 16:11:18 2015
From: jannes.faber at gmail.com (Jannes Faber)
Date: Fri, 25 Dec 2015 17:11:18 +0100
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CABT1wW=r5DPG1e6XFe7NMHrquo1FzygPCdjEJ2QQnmGbqVMH2Q@mail.gmail.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<219f125cee6ca68fd27016642e38fdf1@xbt.hk>
	<CAAcC9ys_t7X0WpQ8W3577M8GLiA5sPV2F1BJ9qZbnMkE-1j3+Q@mail.gmail.com>
	<aff8da46a69bdd7ef92ca87725866a5c@xbt.hk>
	<CAPkFh0vNECi1OmBwki+8NNAQbe6EG2FEE4RR5z=kYVLLDFHUXg@mail.gmail.com>
	<20151220132842.GA25481@muck>
	<CAPkFh0t-+WhZYVLyT_auLa87zAATNOH=CpU4S5H=n6S1wmZ-oQ@mail.gmail.com>
	<CABeL=0jgv3G8qx6wM+ZfwN154qhQY-GJdXnABc-iWL=YDNmhag@mail.gmail.com>
	<CABT1wW=r5DPG1e6XFe7NMHrquo1FzygPCdjEJ2QQnmGbqVMH2Q@mail.gmail.com>
Message-ID: <CABeL=0h1w=WCGpLAchLk1mx67oKbcTcVnhZs-K5JdXe-9OcgAg@mail.gmail.com>

On 25 Dec 2015 12:15 p.m., "Ittay" <ittay.eyal at cornell.edu> wrote:

> As for masquerading as multiple small pools -- that's a very good point,
with a surprising answer: it doesn't really matter. An attacker attacks all
parts of the open pool proportionally to their size, and the result is
basically identical to that of attacking a single large pool.

While true, that's only relevant to the indiscriminate attacker! The
vigilante attacker that wants to hurt only pools that are too large,
doesn't even know that there's a need to attack as all of them seem small.

That's what i was saying.

>
> On Mon, Dec 21, 2015 at 1:39 PM, Jannes Faber via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:
>>
>> If you're saying a block withholding attack is a nice weapon to have to
dissuade large pools, isn't that easily defeated by large pools simply
masquerading as multiple small pools? As, for all we know, ghash may have
done?
>>
>> If you don't know who to attack there's no point in having the weapon.
While that weapon is still dangerous in the hands of others that are
indiscriminate, like the solo miners example of Peter Todd.
>>
>> Sorry if i misunderstood your point.
>>
>>
>> --
>> Jannes
>>
>> On 20 December 2015 at 18:00, Emin G?n Sirer <
bitcoin-dev at lists.linuxfoundation.org> wrote:
>>>
>>> On Sun, Dec 20, 2015 at 8:28 AM, Peter Todd <pete at petertodd.org> wrote:
>>>>
>>>> There are a number of techniques that can be used to detect block
>>>> withholding attacks that you are not aware of. These techniques usually
>>>> have the characteristic that if known they can be avoided, so obviously
>>>> those who know about them are highly reluctant to reveal what exactly
>>>> they are. I personally know about some of them and have been asked to
>>>> keep that information secret, which I will.
>>>
>>>
>>> Indeed, there are lots of weak measures that one could employ against
>>> an uninformed attacker. As I mentioned before, these are unlikely to be
>>> effective against a savvy attacker, and this is a good thing.
>>>
>>>>
>>>> In the context of KYC, this techniques would likely hold up in court,
>>>> which means that if this stuff becomes a more serious problem it's
>>>> perfectly viable for large, well-resourced, pools to prevent block
>>>> withholding attacks, in part by removing anonymity of hashing power.
>>>> This would not be a positive development for the ecosystem.
>>>
>>>
>>> KYC has a particular financial-regulation connotation in Bitcoin
circles,
>>> of which I'm sure you're aware, and which you're using as a spectre.
>>> You don't mean government-regulated-KYC a la FINCEN and Bitcoin
>>> exchanges like Coinbase, you are just referring to a pool operator
>>> demanding to know that its customer is not coming from its competitors'
>>> data centers.
>>>
>>> And your prediction doesn't seem well-motivated or properly justified.
>>> There are tons of conditionals in your prediction, starting with the
premise
>>> that every single open pool would implement some notion of identity
>>> checking. I don't believe that will happen. Instead, we will have the
bigger
>>> pools become more suspicious of signing up new hash power, which is a
>>> good thing. And we will have small groups of people who have some reason
>>> for trusting each other (e.g. they know each other from IRC,
conferences,
>>> etc) band together into small pools. These are fantastic outcomes for
>>> decentralization.
>>>
>>>> Secondly, DRM tech can also easily be used to prevent block withholding
>>>> attacks by attesting to the honest of the hashing power. This is being
>>>> discussed in the industry, and again, this isn't a positive development
>>>> for the ecosystem.
>>>
>>>
>>> DRM is a terrible application. Once again, I see that you're trying to
use those
>>> three letters as a spectre as well, knowing that most people hate DRM,
but
>>> keep in mind that DRM is just an application -- it's like pointing to
Adobe Flash
>>> to taint all browser plugins.
>>>
>>> The tech behind DRM is called "attestation," and it provides a
technical
>>> capability not possible by any other means. In essence, attestation can
ensure that
>>> a remote node is indeed running the code that it purports to be
running. Since
>>> most problems in computer security and distributed systems stem from not
>>> knowing what protocol the attacker is going to follow, attestation is
the only
>>> technology we have that lets us step around this limitation.
>>>
>>> It can ensure, for instance,
>>>   - that a node purporting to be Bitcoin Core (vLatest) is indeed
running an
>>> unadulterated, latest version of Bitcoin Core
>>>   - that a node claiming that it does not harvest IP addresses from SPV
>>> clients indeed does not harvest IP addresses.
>>>   - that a cloud hashing outfit that rented out X terahashes to a user
did
>>> indeed rent out X terahashes to that particular user,
>>>   - that a miner operating on behalf of some pool P will not misbehave
and
>>> discard perfectly good blocks
>>> and so forth. All of these would be great for the ecosystem. Just
getting rid
>>> of the cloudhashing scams would put an end to a lot of heartache.
>>>
>>>> > Keep in mind that when an open pool gets big, like GHash did and
>>>> > two other pools did before them, the only thing at our disposal used
>>>> > to be to yell at people about centralization until they left the big
>>>> > pools and reformed into smaller groups. Not only was such yelling
>>>> > kind of desperate looking, it wasn't incredibly effective, either.
>>>> > We had no protocol mechanisms that put pressure on big pools to
>>>> > stop signing up people. Ittay's discovery changed that: pools that
>>>> > get to be very big by indiscriminately signing up miners are likely
to
>>>> > be infiltrated and their profitability will drop. And Peter's post is
>>>> > evidence that this is, indeed, happening as predicted. This is a
>>>> > good outcome, it puts pressure on the big pools to not grow.
>>>>
>>>> GHash.io was not a pure pool - they owned and operated a significant
>>>> amount of physical hashing power, and it's not at all clear that their
%
>>>> of the network actually went down following that 51% debacle.
>>>
>>>
>>> Right, it's not clear at all that yelling at people has much effect. As
much
>>> fun as I had going to that meeting with GHash in London to ask them to
>>> back down off of the 51% boundary, I am pretty sure that yelling at
large
>>> open pools will not scale. We needed better mechanisms for keeping pools
>>> in check.
>>>
>>> And Miner's Dilemma (MD) attacks are clearly quite effective. This is a
>>> time when we should count our blessings, not work actively to render
>>> them inoperable.
>>>
>>>> Currently a significant % of the hashing power - possibly a majority -
>>>> is in the form of large hashing installations whose owners
individually,
>>>> and definitely in trusting groups, have enough hashing power to solo
>>>> mine. Eyal's results indicate those miners have incentives to attack
>>>> pools, and additionally they have the incentive of killing off pools to
>>>> make it difficult for new competition to get established, yet they
>>>> themselves are not vulnerable to that attack.
>>>
>>>
>>> There are indeed solo miners out there who can attack the big open
>>> pools. The loss of the biggest open pools would not be a bad outcome.
>>> Pools >25% pose a danger, and the home miner doesn't need a pool
>>> >25% for protection against variance.
>>>
>>>> > Peter, you allude to a specific suggestion from Luke-Jr. Can you
>>>> > please describe what it is?
>>>>
>>>> Basically you have the pool pick a secret k for each share, and commit
>>>> to H(k) in the share. Additionally the share commits to a target
divider
>>>> D. The PoW validity rule is then changed from H(block header) < T, to
be
>>>> H(block header) < T * D && H(H(block header) + k) < max_int / D
>>>
>>>
>>> Thanks, this requires a change to the Bitcoin PoW. Good luck with that!
>>>
>>> Once again, this suggestion would make the GHash-at-51% situation
>>> possible again. Working extra hard to re-enable those painful days
>>> sounds like a terrible idea.
>>>
>>> - egs
>>>
>>>
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev at lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>>
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151225/352db21d/attachment-0001.html>

From operator at bitminter.com  Sat Dec 26 00:38:06 2015
From: operator at bitminter.com (Geir Harald Hansen)
Date: Sat, 26 Dec 2015 01:38:06 +0100
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CAPkFh0t-+WhZYVLyT_auLa87zAATNOH=CpU4S5H=n6S1wmZ-oQ@mail.gmail.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<219f125cee6ca68fd27016642e38fdf1@xbt.hk>
	<CAAcC9ys_t7X0WpQ8W3577M8GLiA5sPV2F1BJ9qZbnMkE-1j3+Q@mail.gmail.com>
	<aff8da46a69bdd7ef92ca87725866a5c@xbt.hk>
	<CAPkFh0vNECi1OmBwki+8NNAQbe6EG2FEE4RR5z=kYVLLDFHUXg@mail.gmail.com>
	<20151220132842.GA25481@muck>
	<CAPkFh0t-+WhZYVLyT_auLa87zAATNOH=CpU4S5H=n6S1wmZ-oQ@mail.gmail.com>
Message-ID: <567DE16E.8040001@bitminter.com>

On 20.12.2015 18:00, Emin G?n Sirer via bitcoin-dev wrote:
> Instead, we will have the bigger
> pools become more suspicious of signing up new hash power, which is a
> good thing.

Block withholding attacks do not differentiate between small and large
pools. When Eligius and BTCGuild got hit with this, they were far from
the biggest pools at the time.

When my pool, Bitminter, got a new large miner who found 1 block where
average luck would have had them find 3, one of the other miners claimed
they must be withholding blocks. Even if there is no logic or evidence
behind it, after one person cries wolf the others get nervous. This way
even the possibility of block withholding can keep smaller pools from
growing. It takes more hashpower to put a dent in a bigger pool, so you
will see less such panic.

> And we will have small groups of people who have some reason
> for trusting each other (e.g. they know each other from IRC, conferences,
> etc) band together into small pools. These are fantastic outcomes for
> decentralization.

Three guys with 1 TH/s, 2 TH/s and 100 GH/s meet at a conference and
decide to start a private pool? Obviously that doesn't work. Maybe three
people with huge warehouses of miners would work together if they knew
and trusted each other.

Those small miners need to mine with people they don't know to get an
acceptable variance.

If you kill off mining pools then small miners have no way to achieve
acceptable variance and they will disappear. There will only be big
warehouse miners left, the ones who are big enough to solo mine.

That's not helping decentralization.

> Right, it's not clear at all that yelling at people has much effect. As much
> fun as I had going to that meeting with GHash in London to ask them to
> back down off of the 51% boundary, I am pretty sure that yelling at large
> open pools will not scale. We needed better mechanisms for keeping pools
> in check.

I agree. It's very disappointing how most miners and pools handle this
(BTCGuild being the exception). But I do not think block withholding is
a good tool. It can easily destroy small pools, but it won't put a dent
in a pool that goes over 50%.

Block withholding is a tool big pools can use to put smaller competitors
out of business.

And even if it was effective I would not use block withholding to attack
other pools.

> And Miner's Dilemma (MD) attacks are clearly quite effective. This is a
> time when we should count our blessings, not work actively to render
> them inoperable.

Is it? Is there any example of block withholding leading to more
decentralized mining?

If I remember right, GHash being too big ended with BitFury moving some
of their hashpower out of the pool. I don't know where that hashpower
went and whether the problem was solved or merely hidden.

GHash profitability being very low for some time wasn't due to block
withholding, it was a bug that some miners abused to get paid for the
same work multiple times. This made it look like a lot of work was done
while finding few blocks.

>     Basically you have the pool pick a secret k for each share, and commit
>     to H(k) in the share. Additionally the share commits to a target divider
>     D. The PoW validity rule is then changed from H(block header) < T, to be
>     H(block header) < T * D && H(H(block header) + k) < max_int / D
> 
> 
> Thanks, this requires a change to the Bitcoin PoW. Good luck with that! 
> 
> Once again, this suggestion would make the GHash-at-51% situation 
> possible again. Working extra hard to re-enable those painful days 
> sounds like a terrible idea. 

Block withholding didn't solve the problem back then. And guess what,
those painful days are here right now. China is at 65% and block
withholding isn't solving it.

I was disappointed when GHash got too big and refused to do anything. It
was sad when their miners didn't do anything. Then they used
double-spends to scam a casino. I was shocked that noone cared. Now two
thirds of the bitcoin hashpower is within the control of a single
government. This time I expected noone would care - but I'm still
disappointed. I'm also surprised at the irrational behavior; there are
so many who go out of their way to put their own investments in danger.

For a long time now many miners and pools have been irresponsible with
the hashpower. But block withholding just makes it worse.

Regards,
Geir H. Hansen, Bitminter mining pool


From cp368202 at ohiou.edu  Sat Dec 26 03:18:13 2015
From: cp368202 at ohiou.edu (Chris Priest)
Date: Fri, 25 Dec 2015 19:18:13 -0800
Subject: [bitcoin-dev] "Hashpower liquidity" is more important than "mining
	centralization"
Message-ID: <CAAcC9yuC1ouEP4h3_9Btj3289+KeC8abPx5trzyergca-9BHYQ@mail.gmail.com>

The term "mining centralization" is very common. It come up almost in
every single discussion relating to bitcoin these days. Some people
say "mining is already centralized" and other such things. I think
this is a very bad term, and people should stop saying those things.
Let me explain:

Under normal operations, if every single miner in th network is under
one roof, nothing would happen. If there was oly one mining pool that
everyone had to use, this would have no effect on the system
whatsoever. The only time this would be a problem is if that one pool
were to censor transactions, or in any other way operate out of the
normal.

Right now, the network is in a period of peace. There are no
governments trying to coerce mining pools into censoring transaction,
or otherwise disrupting the network. For all we know, the next 500
years of bitcoin's history could be filled with complete peaceful
operations with no government interference at all.

*If* for some reason in the future a government were to decide that
they want to disrupt the bitcoin network, then all the hashpower being
under one control will be problematic, if and only if hashpower
liquidity is very low. Hashpower liquidity is the measure of how
easily hashpower can move from one pool to another. If all the mining
hardware on the network is mining one one pool and **will never or can
never switch to another pool** then the hashpower liquidity is very
low. If all the hashpower on the network can very easily move to
another pool, then hashpower liquidity is very high.

If the one single mining pool were to start censoring transactions and
there is no other pool to move to, then hashpower liquidity is very
high, and that would be very bad for bitcoin. If there was dozens of
other pools in existence, and all the mining hardware owners could
switch to another pool easiely, then the hashpower liquidity is very
high, and the censorship attack will end as soon as the hashpower
moves to other pools.

My argument is that hashpower liquidity is much more important of a
metric to think about than simply "mining centralization". The
difference between the two terms is that one term describes a
temporary condition, while the other one measures a more permanent
condition. Both terms are hard to measure in concrete terms.

Instead of saying "this change will increase mining centralization" we
should instead be thinking "will this change increase hashpower
liquidity?".

Hopefully people will understand this concept and the term "mining
centralization" will become archaic.

From benevolent at cock.li  Fri Dec 25 12:02:10 2015
From: benevolent at cock.li (benevolent at cock.li)
Date: Fri, 25 Dec 2015 12:02:10 +0000
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CABT1wW=r5DPG1e6XFe7NMHrquo1FzygPCdjEJ2QQnmGbqVMH2Q@mail.gmail.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<219f125cee6ca68fd27016642e38fdf1@xbt.hk>
	<CAAcC9ys_t7X0WpQ8W3577M8GLiA5sPV2F1BJ9qZbnMkE-1j3+Q@mail.gmail.com>
	<aff8da46a69bdd7ef92ca87725866a5c@xbt.hk>
	<CAPkFh0vNECi1OmBwki+8NNAQbe6EG2FEE4RR5z=kYVLLDFHUXg@mail.gmail.com>
	<20151220132842.GA25481@muck>
	<CAPkFh0t-+WhZYVLyT_auLa87zAATNOH=CpU4S5H=n6S1wmZ-oQ@mail.gmail.com>
	<CABeL=0jgv3G8qx6wM+ZfwN154qhQY-GJdXnABc-iWL=YDNmhag@mail.gmail.com>
	<CABT1wW=r5DPG1e6XFe7NMHrquo1FzygPCdjEJ2QQnmGbqVMH2Q@mail.gmail.com>
Message-ID: <502120bac44386921f3dcac26ce1ef18@cock.li>

On 2015-12-25 11:15, Ittay via bitcoin-dev wrote:
> 
> All that being said -- it's not great to rely on the potential of
> attacks and on threats against the honest large pools out there
> (including GHash, which, afaik, did nothing more wrong than being
> successful).

GHash.IO and double-spending against BetCoin Dice

https://bitcointalk.org/index.php?topic=327767.0


From admin at multipool.us  Sat Dec 26 08:12:13 2015
From: admin at multipool.us (Multipool Admin)
Date: Sat, 26 Dec 2015 00:12:13 -0800
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <20151220044450.GA23942@muck>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<4882BD35-D890-4860-9222-5C23AEB6AE89@mattcorallo.com>
	<CAAcC9yspsPs3gbumS4rTOg-P-=V=tycn2Z1nVPGGHwJ-nP+PBg@mail.gmail.com>
	<20151220044450.GA23942@muck>
Message-ID: <CAP3QyGJD3SaM6Bvvw66jAvVFkQhrfJfRQTxbbe8a=O1zK_P6tw@mail.gmail.com>

Any attempt to 'fix' this problem, would most likely require changes to all
mining software, why not just make mining more decentralized in general?

For example, allow anyone to submit proofs of work to Bitcoind that are
some fraction of the network difficulty and receive payment for them if
they're valid.  This would also encourage the proliferation of full nodes
since anyone could solo mine again.  Then, the next coinbase transaction
could be split among, say, the top 100 proofs of work.

Eligius already does their miner payouts like this.

If you want to fix an issue with mining, fix the selfish mining issue first
as it's a much larger and more dangerous potential issue.

I don't believe it was ever clearly established whether Eligius suffered a
block withholding attack or was just the victim of a miner with (what was,
at the time) a large amount of faulty hardware, however, from the
Bitcointalk threads at the time I believe it was assumed to be the latter.

--Adam


On Sat, Dec 19, 2015 at 8:44 PM, Peter Todd via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> On Sat, Dec 19, 2015 at 07:43:59PM -0800, Chris Priest via bitcoin-dev
> wrote:
> > Then shouldn't this be something the pool deals with, not the bitcoin
> protocol?
>
> There is no known way for pools - especially ones that allow anonymous
> hashers - to effectively prevent block withholding attacks without
> changing the Bitcoin protocol.
>
> --
> 'peter'[:-1]@petertodd.org
> 00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/d35657ba/attachment.html>

From elombrozo at gmail.com  Sat Dec 26 08:23:38 2015
From: elombrozo at gmail.com (Eric Lombrozo)
Date: Sat, 26 Dec 2015 08:23:38 +0000
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <20151220132842.GA25481@muck>
Message-ID: <ema8a70574-c28e-4c43-a1e3-5f2f4df7d3a2@platinum>

Peter Todd wrote:
  Fixing block withholding is relatively simple, but (so far) requires a
SPV-visible hardfork. (Luke-Jr's two-stage target mechanism) We should
do this hard-fork in conjunction with any blocksize increase, which will
have the desirable side effect of clearly show consent by the entire
ecosystem, SPV clients included.

I think we can generalize this and argue that it is impossible fix this 
without reducing the visible difficulty and blinding the hasher to an 
invisible difficulty. Unfortunately, changing the retargeting algo to 
compute lower visible difficulty (leaving all else the same) or 
interpreting the bits field in a way that yields a lower visible 
difficulty is a hard fork by definition - blocks that didn't meet the 
visible difficulty requirement before will now meet it.

jl2012 wrote:
>After the meeting I find a softfork solution. It is very inefficient 
>and I am leaving it here just for record.
>
>1. In the first output of the second transaction of a block, mining 
>pool will commit a random nonce with an OP_RETURN.
>
>2. Mine as normal. When a block is found, the hash is concatenated with 
>the committed random nonce and hashed.
>
>3. The resulting hash must be smaller than 2 ^ (256 - 1/64) or the 
>block is invalid. That means about 1% of blocks are discarded.
>
>4. For each difficulty retarget, the secondary target is decreased by 2 
>^ 1/64.
>
>5. After 546096 blocks or 10 years, the secondary target becomes 2 ^ 
>252. Therefore only 1 in 16 hash returned by hasher is really valid. 
>This should make the detection of block withholding attack much easier.
>
>All miners have to sacrifice 1% reward for 10 years. Confirmation will 
>also be 1% slower than it should be.
>
>If a node (full or SPV) is not updated, it becomes more vulnerable as 
>an attacker could mine a chain much faster without following the new 
>rules. But this is still a softfork, by definition.
jl2012's key discovery here is that if we add an invisible difficulty 
while keeping the retarget algo and bits semantics the same, the visible 
difficulty will decrease automatically to compensate. In other words, we 
can artificially increase the block time interval, allowing us to force 
a lower visible difficulty at the next retarget without changing the 
retarget algo nor the bits semantics. There are no other free parameters 
we can tweak, so it seems this is really the best we can do.

Unfortunately, this also means longer confirmation times, lower 
throughput, and lower miner revenue. Note, however, that confirmations 
would (on average) represent more PoW, so fewer confirmations would be 
required to achieve the same level of security.

We can compensate for lower throughput and lower miner revenue by 
increasing block size and increasing block rewards. Interestingly, it 
turns out we *can* do these things with soft forks by embedding new 
structures into blocks and nesting their hash trees into existing 
structures. Ideas such as extension blocks 
[https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-May/008356.html] 
have been proposed before...but they add significant complications to 
the protocol and require nontrivial app migration efforts. Old nodes 
would not get forked off the network but backwards compatibility would 
still be a problem as they would not be able to see at least some of the 
transactions and some of the bitcoins in blocks. But if we're willing to 
accept this, even the "sacred" 21 million asymptotic limit can be raised 
via soft fork!

So in conclusion, it *is* possible to fix this attack with a soft fork 
and without altering the basic economics...but it's almost surely a lot 
more trouble than it's worth. Luke-Jr's solution is far simpler and more 
elegant and is perhaps one of the few examples of a new feature (as 
opposed to merely a structure cleanup) that would be better to deploy as 
a hard fork since it's simple to implement and seems to stand a 
reasonable chance of near universal support...and soft fork alternatives 
are very, very ugly and significantly impact system usability...and I 
think theory tells us we can't do any better.

- Eric
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/5d48d72e/attachment-0001.html>

From elombrozo at gmail.com  Sat Dec 26 08:26:54 2015
From: elombrozo at gmail.com (Eric Lombrozo)
Date: Sat, 26 Dec 2015 08:26:54 +0000
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <ema8a70574-c28e-4c43-a1e3-5f2f4df7d3a2@platinum>
Message-ID: <em3b7f52a1-0627-432f-9c18-3c3381fdda25@platinum>

Note: my stupid email client didn't indent Peter Todd's quote correctly. 
The first paragraph is his, the second is my response.

------ Original Message ------
From: "Eric Lombrozo" <elombrozo at gmail.com>
To: "Peter Todd" <pete at petertodd.org>; "Emin G?n Sirer" 
<el33th4x0r at gmail.com>
Cc: nbvfour at gmail.com; "Bitcoin Dev" 
<bitcoin-dev at lists.linuxfoundation.org>
Sent: 12/26/2015 12:23:38 AM
Subject: Re[2]: [bitcoin-dev] We need to fix the block withholding 
attack

>Peter Todd wrote:
>  Fixing block withholding is relatively simple, but (so far) requires a
>SPV-visible hardfork. (Luke-Jr's two-stage target mechanism) We should
>do this hard-fork in conjunction with any blocksize increase, which 
>will
>have the desirable side effect of clearly show consent by the entire
>ecosystem, SPV clients included.
>
>I think we can generalize this and argue that it is impossible fix this 
>without reducing the visible difficulty and blinding the hasher to an 
>invisible difficulty. Unfortunately, changing the retargeting algo to 
>compute lower visible difficulty (leaving all else the same) or 
>interpreting the bits field in a way that yields a lower visible 
>difficulty is a hard fork by definition - blocks that didn't meet the 
>visible difficulty requirement before will now meet it.
>
>jl2012 wrote:
>>After the meeting I find a softfork solution. It is very inefficient 
>>and I am leaving it here just for record.
>>
>>1. In the first output of the second transaction of a block, mining 
>>pool will commit a random nonce with an OP_RETURN.
>>
>>2. Mine as normal. When a block is found, the hash is concatenated 
>>with the committed random nonce and hashed.
>>
>>3. The resulting hash must be smaller than 2 ^ (256 - 1/64) or the 
>>block is invalid. That means about 1% of blocks are discarded.
>>
>>4. For each difficulty retarget, the secondary target is decreased by 
>>2 ^ 1/64.
>>
>>5. After 546096 blocks or 10 years, the secondary target becomes 2 ^ 
>>252. Therefore only 1 in 16 hash returned by hasher is really valid. 
>>This should make the detection of block withholding attack much 
>>easier.
>>
>>All miners have to sacrifice 1% reward for 10 years. Confirmation will 
>>also be 1% slower than it should be.
>>
>>If a node (full or SPV) is not updated, it becomes more vulnerable as 
>>an attacker could mine a chain much faster without following the new 
>>rules. But this is still a softfork, by definition.
>jl2012's key discovery here is that if we add an invisible difficulty 
>while keeping the retarget algo and bits semantics the same, the 
>visible difficulty will decrease automatically to compensate. In other 
>words, we can artificially increase the block time interval, allowing 
>us to force a lower visible difficulty at the next retarget without 
>changing the retarget algo nor the bits semantics. There are no other 
>free parameters we can tweak, so it seems this is really the best we 
>can do.
>
>Unfortunately, this also means longer confirmation times, lower 
>throughput, and lower miner revenue. Note, however, that confirmations 
>would (on average) represent more PoW, so fewer confirmations would be 
>required to achieve the same level of security.
>
>We can compensate for lower throughput and lower miner revenue by 
>increasing block size and increasing block rewards. Interestingly, it 
>turns out we *can* do these things with soft forks by embedding new 
>structures into blocks and nesting their hash trees into existing 
>structures. Ideas such as extension blocks 
>[https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-May/008356.html] 
>have been proposed before...but they add significant complications to 
>the protocol and require nontrivial app migration efforts. Old nodes 
>would not get forked off the network but backwards compatibility would 
>still be a problem as they would not be able to see at least some of 
>the transactions and some of the bitcoins in blocks. But if we're 
>willing to accept this, even the "sacred" 21 million asymptotic limit 
>can be raised via soft fork!
>
>So in conclusion, it *is* possible to fix this attack with a soft fork 
>and without altering the basic economics...but it's almost surely a lot 
>more trouble than it's worth. Luke-Jr's solution is far simpler and 
>more elegant and is perhaps one of the few examples of a new feature 
>(as opposed to merely a structure cleanup) that would be better to 
>deploy as a hard fork since it's simple to implement and seems to stand 
>a reasonable chance of near universal support...and soft fork 
>alternatives are very, very ugly and significantly impact system 
>usability...and I think theory tells us we can't do any better.
>
>- Eric
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/381a45ab/attachment.html>

From jtimon at jtimon.cc  Sat Dec 26 15:33:53 2015
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Sat, 26 Dec 2015 16:33:53 +0100
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <ema8a70574-c28e-4c43-a1e3-5f2f4df7d3a2@platinum>
References: <20151220132842.GA25481@muck>
	<ema8a70574-c28e-4c43-a1e3-5f2f4df7d3a2@platinum>
Message-ID: <CABm2gDp4ac=E+T-FT=ZQPnW_ao2GdF1QOg8tROYoV=QKypQS1g@mail.gmail.com>

On Dec 26, 2015 9:24 AM, "Eric Lombrozo via bitcoin-dev" <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Unfortunately, this also means longer confirmation times, lower
throughput, and lower miner revenue. Note, however, that confirmations
would (on average) represent more PoW, so fewer confirmations would be
required to achieve the same level of security.
>

I'm not sure I understand this. If mining revenue per unit of time drops,
total pow per unit of time should also drop. Even if the inter-block time
is increased, it's not clear to me that the pow per block would necessarily
be higher.
What am I missing?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/071e9e45/attachment.html>

From tier.nolan at gmail.com  Sat Dec 26 16:09:18 2015
From: tier.nolan at gmail.com (Tier Nolan)
Date: Sat, 26 Dec 2015 16:09:18 +0000
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <ema8a70574-c28e-4c43-a1e3-5f2f4df7d3a2@platinum>
References: <20151220132842.GA25481@muck>
	<ema8a70574-c28e-4c43-a1e3-5f2f4df7d3a2@platinum>
Message-ID: <CAE-z3OUrCFkVQ+1th-BjkZf1YEhPjub7TA_2J-CRNmCs6Bb7Dg@mail.gmail.com>

On Sat, Dec 26, 2015 at 8:23 AM, Eric Lombrozo via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Unfortunately, this also means longer confirmation times, lower
> throughput, and lower miner revenue. Note, however, that confirmations
> would (on average) represent more PoW, so fewer confirmations would be
> required to achieve the same level of security.
>


No, the re-target compensates so that the number of blocks in the last two
weeks is 2016.  If a soft fork forces miners to throw away 25% of their
blocks, then the difficulty will drop by 75% to keep things balanced.
Throwing away 75% of blocks has the same effect on difficulty as destroying
75% of mining hardware.

The block interval will only increase until the next re-target.

Slowly increasing the fraction of blocks which are thrown away gives the
re-target algorithm time to adjust, so it is another advantage.

If the rule was instantly changed so that 95% of blocks were thrown away,
then there could be up to 40 weeks until the next retarget and that would
give 200 minute block times until the adjustment.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/c9da101f/attachment.html>

From pieter.wuille at gmail.com  Sat Dec 26 16:44:41 2015
From: pieter.wuille at gmail.com (Pieter Wuille)
Date: Sat, 26 Dec 2015 17:44:41 +0100
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <751DFAA9-9013-4C54-BC1E-5F7ECB7469CC@gmail.com>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
	<CADm_WcbrMyk-=OnQ-3UvnF_8brhn+X2NqRPbo5xUXsbcZpc0=Q@mail.gmail.com>
	<CAPg+sBjbATqf8DXGF7obw9a=371zQ_S0EgTapnUmukAVenTneQ@mail.gmail.com>
	<CA+c4Zozac8=aMrAJ1N_6SR9eBD+w0e70cEnk9CG_2oZ72AS-8g@mail.gmail.com>
	<CAPg+sBhsKD8jd9Y9+ngXY5tKUheO3d4P1b47eYL=Uzpat+KJ2w@mail.gmail.com>
	<751DFAA9-9013-4C54-BC1E-5F7ECB7469CC@gmail.com>
Message-ID: <CAPg+sBiT5=ss9e=iac6J-A=85okF0zxMeV7H4z9-Qfx3CAWHXA@mail.gmail.com>

That's exactly the point: a hard fork does not just affect miners, and
cannot just get decided by miners. All full nodes must have accepted the
new rules, or they will be forked off when the hashrate percentage triggers.

Furthermore, 75% is pretty terrible as a switchover point, as it guarantees
that old nodes will still see a 25% forked off chain temporarily.

My opinion is that the role of Bitcoin Core maintainers is judging whether
consensus for a hard fork exists, and is technically necessary and safe. We
don't need a hashpower vote to decide whether a hardfork is accepted or
not, we need to be sure that full noded will accept it, and adopt it in
time. A hashpower vote can still be used to be sure that miners _also_
agree.

Currently, a large amount of developers and others believe that the
treshhold for a hardfork is not reached, especially given the fact that we
scale in the short term, as well as make many changes that long-term
benefit scalability, with just a softfork (which does not require forking
off nodes that don't adopt the new rules, for whatever reason).

-- 
Pieter
On Dec 26, 2015 17:25, "digitsu" <jerry.d.chan at bittoku.co.jp> wrote:

> So it seems that everyone is in agreement then, since a hard fork to 2M is
> orthogonal to SW, and also that core should not be involved in dictating
> what the consensus rules should be, then why not put BIP102 into play with
> a 75% mining majority activation and let the market decide.
>
> As it?s activation only involves the miners, and its implementation
> doesn?t affect users or exchanges, then it poses no complication to SW
> which can do done in parallel.
>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/59c39d2e/attachment.html>

From jtimon at jtimon.cc  Sat Dec 26 17:20:22 2015
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Sat, 26 Dec 2015 18:20:22 +0100
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <CAPg+sBiT5=ss9e=iac6J-A=85okF0zxMeV7H4z9-Qfx3CAWHXA@mail.gmail.com>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
	<CADm_WcbrMyk-=OnQ-3UvnF_8brhn+X2NqRPbo5xUXsbcZpc0=Q@mail.gmail.com>
	<CAPg+sBjbATqf8DXGF7obw9a=371zQ_S0EgTapnUmukAVenTneQ@mail.gmail.com>
	<CA+c4Zozac8=aMrAJ1N_6SR9eBD+w0e70cEnk9CG_2oZ72AS-8g@mail.gmail.com>
	<CAPg+sBhsKD8jd9Y9+ngXY5tKUheO3d4P1b47eYL=Uzpat+KJ2w@mail.gmail.com>
	<751DFAA9-9013-4C54-BC1E-5F7ECB7469CC@gmail.com>
	<CAPg+sBiT5=ss9e=iac6J-A=85okF0zxMeV7H4z9-Qfx3CAWHXA@mail.gmail.com>
Message-ID: <CABm2gDrpi=26o57-JHPbN5EPE9WKXO2bsqLv=aYHLtAO5y4WMA@mail.gmail.com>

On Dec 26, 2015 5:45 PM, "Pieter Wuille via bitcoin-dev" <
bitcoin-dev at lists.linuxfoundation.org> wrote:
> My opinion is that the role of Bitcoin Core maintainers is judging
whether consensus for a hard fork exists, and is technically necessary and
safe. We don't need a hashpower vote to decide whether a hardfork is
accepted or not, we need to be sure that full noded will accept it, and
adopt it in time. A hashpower vote can still be used to be sure that miners
_also_ agree.

To clarify, that's the role of Bitcoin Core maintainers because they decide
what goes into Bitcoin Core, not because they decide the consensus rules of
Bitcoin. Other full node implementations (say, libbitcoin) will have to
decide on their own and Bitcoin Core mainteiners don't have any authority
over libbitcoin (or other alternative implementations). Nobody has such
authority (not even the creator of the system if he was still maintaining
Bitcoin Core).
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/567065fc/attachment.html>

From elombrozo at gmail.com  Sat Dec 26 17:38:33 2015
From: elombrozo at gmail.com (Eric Lombrozo)
Date: Sat, 26 Dec 2015 09:38:33 -0800
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CABm2gDp4ac=E+T-FT=ZQPnW_ao2GdF1QOg8tROYoV=QKypQS1g@mail.gmail.com>
References: <20151220132842.GA25481@muck>
	<ema8a70574-c28e-4c43-a1e3-5f2f4df7d3a2@platinum>
	<CABm2gDp4ac=E+T-FT=ZQPnW_ao2GdF1QOg8tROYoV=QKypQS1g@mail.gmail.com>
Message-ID: <EC9193E8-DEC6-413F-A9AC-903E26E51824@gmail.com>

For simplicity, assume total network hashpower is constant. Also, assume the soft fork activates at the beginning of a retarget period.

At the moment the soft fork activates, the effective difficulty is increased (by adding a second independent PoW check that must also be satisfied) which means more hashes on average (and proportionally more time) are required to find a block. At the end of the retarget period,  the difficulty is lowered so that if the second PoW difficulty were to be kept constant the block interval would again average 10 mins.

If we were to keep the second PoW difficulty constant, we would restore the same total PoW-to-time-unit ratio and the retarget difficulty would stabilize again so each block would once more require the same number of hashes (and same amount of time) on average as before.

But we don't keep the second PoW difficulty constant - we increase it so once again more hashes on average are required to find a block by the same proportion as before. And we keep doing this.

Now, the assumption that hashpower is constant is obviously unrealistic. If this is your bone of contention, then yes, I agree my model is overly simplistic.

My larger point was to explore the extent of what's possible with only a soft fork - and we can actually go pretty far and even compensate for these economic shifts by increasing block size and rewards. The whole thing is clearly a huge mess - and I wouldn't recommend actually doing it.



On December 26, 2015 7:33:53 AM PST, "Jorge Tim?n" <jtimon at jtimon.cc> wrote:
>On Dec 26, 2015 9:24 AM, "Eric Lombrozo via bitcoin-dev" <
>bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> Unfortunately, this also means longer confirmation times, lower
>throughput, and lower miner revenue. Note, however, that confirmations
>would (on average) represent more PoW, so fewer confirmations would be
>required to achieve the same level of security.
>>
>
>I'm not sure I understand this. If mining revenue per unit of time
>drops,
>total pow per unit of time should also drop. Even if the inter-block
>time
>is increased, it's not clear to me that the pow per block would
>necessarily
>be higher.
>What am I missing?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/91406f63/attachment-0001.html>

From jtimon at jtimon.cc  Sat Dec 26 18:01:59 2015
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Sat, 26 Dec 2015 19:01:59 +0100
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <EC9193E8-DEC6-413F-A9AC-903E26E51824@gmail.com>
References: <20151220132842.GA25481@muck>
	<ema8a70574-c28e-4c43-a1e3-5f2f4df7d3a2@platinum>
	<CABm2gDp4ac=E+T-FT=ZQPnW_ao2GdF1QOg8tROYoV=QKypQS1g@mail.gmail.com>
	<EC9193E8-DEC6-413F-A9AC-903E26E51824@gmail.com>
Message-ID: <CABm2gDrkvsShE1Vbc9KVTTeFzjJJJrGj605pRxoOAWMwpMWn1w@mail.gmail.com>

The hashpower is a function of the block reward (subsidy + fees): it's
economically irrational to have costs greater than the reward (better just
turn off your miners) and in a perfect competition (a theoretical model)
profits tend to zero. That is, the costs tend to equal revenue (block
reward).
On Dec 26, 2015 6:38 PM, "Eric Lombrozo" <elombrozo at gmail.com> wrote:

> For simplicity, assume total network hashpower is constant. Also, assume
> the soft fork activates at the beginning of a retarget period.
>
> At the moment the soft fork activates, the effective difficulty is
> increased (by adding a second independent PoW check that must also be
> satisfied) which means more hashes on average (and proportionally more
> time) are required to find a block. At the end of the retarget period, the
> difficulty is lowered so that if the second PoW difficulty were to be kept
> constant the block interval would again average 10 mins.
>
> If we were to keep the second PoW difficulty constant, we would restore
> the same total PoW-to-time-unit ratio and the retarget difficulty would
> stabilize again so each block would once more require the same number of
> hashes (and same amount of time) on average as before.
>
> But we don't keep the second PoW difficulty constant - we increase it so
> once again more hashes on average are required to find a block by the same
> proportion as before. And we keep doing this.
>
> Now, the assumption that hashpower is constant is obviously unrealistic.
> If this is your bone of contention, then yes, I agree my model is overly
> simplistic.
>
> My larger point was to explore the extent of what's possible with only a
> soft fork - and we can actually go pretty far and even compensate for these
> economic shifts by increasing block size and rewards. The whole thing is
> clearly a huge mess - and I wouldn't recommend actually doing it.
>
>
>
> On December 26, 2015 7:33:53 AM PST, "Jorge Tim?n" <jtimon at jtimon.cc>
> wrote:
>>
>>
>> On Dec 26, 2015 9:24 AM, "Eric Lombrozo via bitcoin-dev" <
>> bitcoin-dev at lists.linuxfoundation.org> wrote:
>>
>> > Unfortunately, this also means longer confirmation times, lower
>> throughput, and lower miner revenue. Note, however, that confirmations
>> would (on average) represent more PoW, so fewer confirmations would be
>> required to achieve the same level of security.
>> >
>>
>> I'm not sure I understand this. If mining revenue per unit of time drops,
>> total pow per unit of time should also drop. Even if the inter-block time
>> is increased, it's not clear to me that the pow per block would necessarily
>> be higher.
>> What am I missing?
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/f9473770/attachment.html>

From elombrozo at gmail.com  Sat Dec 26 18:30:04 2015
From: elombrozo at gmail.com (Eric Lombrozo)
Date: Sat, 26 Dec 2015 10:30:04 -0800
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CAE-z3OUrCFkVQ+1th-BjkZf1YEhPjub7TA_2J-CRNmCs6Bb7Dg@mail.gmail.com>
References: <20151220132842.GA25481@muck>
	<ema8a70574-c28e-4c43-a1e3-5f2f4df7d3a2@platinum>
	<CAE-z3OUrCFkVQ+1th-BjkZf1YEhPjub7TA_2J-CRNmCs6Bb7Dg@mail.gmail.com>
Message-ID: <99F2E7CA-42BF-432F-B371-F00DA145B817@gmail.com>

I should have stated that we're assuming the actual total hashrate remains constant. Obviously this is not what would actually happen - the rest of the post discusses ways to counter the economic forces at play pushing total hashrate down using only soft forks. The increased variance is still unaccounted for (pool operators would have to deal with this somehow)...and we still have larger block intervals even with compensation. And the practicality of deployment and usability are clearly problematic, to understate things.

It's merely an exercise seeking the theoretical limit of what's actually possible to do with a soft fork.

On December 26, 2015 8:09:18 AM PST, Tier Nolan via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
>On Sat, Dec 26, 2015 at 8:23 AM, Eric Lombrozo via bitcoin-dev <
>bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> Unfortunately, this also means longer confirmation times, lower
>> throughput, and lower miner revenue. Note, however, that
>confirmations
>> would (on average) represent more PoW, so fewer confirmations would
>be
>> required to achieve the same level of security.
>>
>
>
>No, the re-target compensates so that the number of blocks in the last
>two
>weeks is 2016.  If a soft fork forces miners to throw away 25% of their
>blocks, then the difficulty will drop by 75% to keep things balanced.
>Throwing away 75% of blocks has the same effect on difficulty as
>destroying
>75% of mining hardware.
>
>The block interval will only increase until the next re-target.
>
>Slowly increasing the fraction of blocks which are thrown away gives
>the
>re-target algorithm time to adjust, so it is another advantage.
>
>If the rule was instantly changed so that 95% of blocks were thrown
>away,
>then there could be up to 40 weeks until the next retarget and that
>would
>give 200 minute block times until the adjustment.
>
>
>------------------------------------------------------------------------
>
>_______________________________________________
>bitcoin-dev mailing list
>bitcoin-dev at lists.linuxfoundation.org
>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/c7075802/attachment.html>

From jtimon at jtimon.cc  Sat Dec 26 19:34:32 2015
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Sat, 26 Dec 2015 20:34:32 +0100
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <99F2E7CA-42BF-432F-B371-F00DA145B817@gmail.com>
References: <20151220132842.GA25481@muck>
	<ema8a70574-c28e-4c43-a1e3-5f2f4df7d3a2@platinum>
	<CAE-z3OUrCFkVQ+1th-BjkZf1YEhPjub7TA_2J-CRNmCs6Bb7Dg@mail.gmail.com>
	<99F2E7CA-42BF-432F-B371-F00DA145B817@gmail.com>
Message-ID: <CABm2gDr848up_2KcTs6HZWa7oEvKkar7AW_jn4fULjaKreBtRg@mail.gmail.com>

On Dec 26, 2015 7:30 PM, "Eric Lombrozo via bitcoin-dev" <
bitcoin-dev at lists.linuxfoundation.org> wrote:
>
> I should have stated that we're assuming the actual total hashrate
remains constant.

But that's not reasonable if you are assuming that the total reward per
unit of time drops, that's what confused me.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/130b7a07/attachment.html>

From j at toom.im  Sat Dec 26 21:22:36 2015
From: j at toom.im (Jonathan Toomim)
Date: Sat, 26 Dec 2015 13:22:36 -0800
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <ema8a70574-c28e-4c43-a1e3-5f2f4df7d3a2@platinum>
References: <ema8a70574-c28e-4c43-a1e3-5f2f4df7d3a2@platinum>
Message-ID: <CCA40088-E9CE-4EC0-9C3A-E355A6C3389F@toom.im>

Another option for how to deal with block withholding attacks: Give the miner who finds the block a bonus. This could even be part of the coinbase transaction.

Block withholding is effective because it costs the attacker 0% and costs the pool 100%. If the pool's coinbase tx was 95% to the pool, 5% (1.25 BTC) to the miner, that would make block withholding attacks much more expensive to the attacker without making a huge impact on reward variance for small miners. If your pool gets attacked by a block withholding attack, then you can respond by jacking up the bonus ratio. At some point, block withholding attacks become unfeasibly expensive to perform. This can work because the pool sacrifices a small amount of variance for its customers by increasing the bonus, but the block attacker sacrifices revenue. This should make the attacker give up before the pool does.

This system already exists in p2pool, although there the reward bonus for the block's finder is only 0.5%.

This must have been proposed before, right? Anyone know of a good analysis of the game theory math?
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/ff9f8ec9/attachment.sig>

From j at toom.im  Sat Dec 26 22:55:48 2015
From: j at toom.im (Jonathan Toomim)
Date: Sat, 26 Dec 2015 14:55:48 -0800
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
	moral hazard
In-Reply-To: <CAPg+sBiT5=ss9e=iac6J-A=85okF0zxMeV7H4z9-Qfx3CAWHXA@mail.gmail.com>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
	<CADm_WcbrMyk-=OnQ-3UvnF_8brhn+X2NqRPbo5xUXsbcZpc0=Q@mail.gmail.com>
	<CAPg+sBjbATqf8DXGF7obw9a=371zQ_S0EgTapnUmukAVenTneQ@mail.gmail.com>
	<CA+c4Zozac8=aMrAJ1N_6SR9eBD+w0e70cEnk9CG_2oZ72AS-8g@mail.gmail.com>
	<CAPg+sBhsKD8jd9Y9+ngXY5tKUheO3d4P1b47eYL=Uzpat+KJ2w@mail.gmail.com>
	<751DFAA9-9013-4C54-BC1E-5F7ECB7469CC@gmail.com>
	<CAPg+sBiT5=ss9e=iac6J-A=85okF0zxMeV7H4z9-Qfx3CAWHXA@mail.gmail.com>
Message-ID: <246AA3BE-570D-4B88-A63D-AC76CB2B0CB8@toom.im>

On Dec 26, 2015, at 8:44 AM, Pieter Wuille via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
> Furthermore, 75% is pretty terrible as a switchover point, as it guarantees that old nodes will still see a 25% forked off chain temporarily.
> 
Yes, 75% plus a grace period is better. I prefer a grace period of about 4000 to 8000 blocks (1 to 2 months).

From my discussions with miners, I think we will be able to create a hardfork proposal that reaches about 90% support among miners, or possibly higher. I'll post a summary of those discussions in the next 24 hours.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/ae50613f/attachment-0001.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/ae50613f/attachment-0001.sig>

From pieter.wuille at gmail.com  Sat Dec 26 23:01:04 2015
From: pieter.wuille at gmail.com (Pieter Wuille)
Date: Sun, 27 Dec 2015 00:01:04 +0100
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <246AA3BE-570D-4B88-A63D-AC76CB2B0CB8@toom.im>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
	<CADm_WcbrMyk-=OnQ-3UvnF_8brhn+X2NqRPbo5xUXsbcZpc0=Q@mail.gmail.com>
	<CAPg+sBjbATqf8DXGF7obw9a=371zQ_S0EgTapnUmukAVenTneQ@mail.gmail.com>
	<CA+c4Zozac8=aMrAJ1N_6SR9eBD+w0e70cEnk9CG_2oZ72AS-8g@mail.gmail.com>
	<CAPg+sBhsKD8jd9Y9+ngXY5tKUheO3d4P1b47eYL=Uzpat+KJ2w@mail.gmail.com>
	<751DFAA9-9013-4C54-BC1E-5F7ECB7469CC@gmail.com>
	<CAPg+sBiT5=ss9e=iac6J-A=85okF0zxMeV7H4z9-Qfx3CAWHXA@mail.gmail.com>
	<246AA3BE-570D-4B88-A63D-AC76CB2B0CB8@toom.im>
Message-ID: <CAPg+sBhxnxnQQ-SpWuJ-+_uxRwXkgcU07jkYdZ8BcBwVDyW-vg@mail.gmail.com>

On Dec 26, 2015 23:55, "Jonathan Toomim" <j at toom.im> wrote:
>
> On Dec 26, 2015, at 8:44 AM, Pieter Wuille via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:
>>
>> Furthermore, 75% is pretty terrible as a switchover point, as it
guarantees that old nodes will still see a 25% forked off chain temporarily.
>
> Yes, 75% plus a grace period is better. I prefer a grace period of about
4000 to 8000 blocks (1 to 2 months).

I think that's extremely short, even assuming there is no controversy about
changing the rules at all. Things like BIP65 and BIP66 already took
significantly longer than that, were uncontroversial, and only need miner
adoption. Full node adoption is even slower.

I think the shortest reasonable timeframe for an uncontroversial hardfork
is somewhere in the range between 6 and 12 months.

For a controversial one, not at all.

-- 
Pieter
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151227/9446849d/attachment.html>

From j at toom.im  Sat Dec 26 23:07:17 2015
From: j at toom.im (Jonathan Toomim)
Date: Sat, 26 Dec 2015 15:07:17 -0800
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
	moral hazard
In-Reply-To: <CAPg+sBhxnxnQQ-SpWuJ-+_uxRwXkgcU07jkYdZ8BcBwVDyW-vg@mail.gmail.com>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
	<CADm_WcbrMyk-=OnQ-3UvnF_8brhn+X2NqRPbo5xUXsbcZpc0=Q@mail.gmail.com>
	<CAPg+sBjbATqf8DXGF7obw9a=371zQ_S0EgTapnUmukAVenTneQ@mail.gmail.com>
	<CA+c4Zozac8=aMrAJ1N_6SR9eBD+w0e70cEnk9CG_2oZ72AS-8g@mail.gmail.com>
	<CAPg+sBhsKD8jd9Y9+ngXY5tKUheO3d4P1b47eYL=Uzpat+KJ2w@mail.gmail.com>
	<751DFAA9-9013-4C54-BC1E-5F7ECB7469CC@gmail.com>
	<CAPg+sBiT5=ss9e=iac6J-A=85okF0zxMeV7H4z9-Qfx3CAWHXA@mail.gmail.com>
	<246AA3BE-570D-4B88-A63D-AC76CB2B0CB8@toom.im>
	<CAPg+sBhxnxnQQ-SpWuJ-+_uxRwXkgcU07jkYdZ8BcBwVDyW-vg@mail.gmail.com>
Message-ID: <2D7C4E00-7451-45B6-94B6-07A7230FBF88@toom.im>

On Dec 26, 2015, at 3:01 PM, Pieter Wuille <pieter.wuille at gmail.com> wrote:

> I think that's extremely short, even assuming there is no controversy about changing the rules at all. Things like BIP65 and BIP66 already took significantly longer than that, were uncontroversial, and only need miner adoption. Full node adoption is even slower.
> 

BIP65 and BIP66 were uncontroversial, but also generally uninteresting. Most people don't care about OP_CLTV right now, and they won't for quite a while longer. They neglect to upgrade their full nodes because there has been no reason to.

Given that a supermajority of users and miners have been asking for a hard fork to increase the blocksize for years, I do not think that mobilizing people to upgrade their nodes is going to be hard.

When we do the hard fork, we will need to encourage people to upgrade their full nodes. We may want to request that miners not trigger the fork until some percentage of visible full nodes have upgraded.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/66d58a47/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/66d58a47/attachment.sig>

From justus at openbitcoinprivacyproject.org  Sat Dec 26 23:15:06 2015
From: justus at openbitcoinprivacyproject.org (Justus Ranvier)
Date: Sat, 26 Dec 2015 17:15:06 -0600
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <CAPg+sBhxnxnQQ-SpWuJ-+_uxRwXkgcU07jkYdZ8BcBwVDyW-vg@mail.gmail.com>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
	<CADm_WcbrMyk-=OnQ-3UvnF_8brhn+X2NqRPbo5xUXsbcZpc0=Q@mail.gmail.com>
	<CAPg+sBjbATqf8DXGF7obw9a=371zQ_S0EgTapnUmukAVenTneQ@mail.gmail.com>
	<CA+c4Zozac8=aMrAJ1N_6SR9eBD+w0e70cEnk9CG_2oZ72AS-8g@mail.gmail.com>
	<CAPg+sBhsKD8jd9Y9+ngXY5tKUheO3d4P1b47eYL=Uzpat+KJ2w@mail.gmail.com>
	<751DFAA9-9013-4C54-BC1E-5F7ECB7469CC@gmail.com>
	<CAPg+sBiT5=ss9e=iac6J-A=85okF0zxMeV7H4z9-Qfx3CAWHXA@mail.gmail.com>
	<246AA3BE-570D-4B88-A63D-AC76CB2B0CB8@toom.im>
	<CAPg+sBhxnxnQQ-SpWuJ-+_uxRwXkgcU07jkYdZ8BcBwVDyW-vg@mail.gmail.com>
Message-ID: <567F1F7A.6070700@openbitcoinprivacyproject.org>

On 12/26/2015 05:01 PM, Pieter Wuille via bitcoin-dev wrote:
> I think the shortest reasonable timeframe for an uncontroversial
> hardfork is somewhere in the range between 6 and 12 months.

This argument would hold more weight if it didn't looks like a stalling
tactic in context.

6 months ago, there was a concerted effort to being the process then,
for exactly this reason.

After 6 months of denial, stonewalling, and generally unproductive
fighting, the need for proactivity is being acknowledged with no
reference to the delay.

If the network ever ends up making a hasty forced upgrade to solve a
capacity emergency the responsibility for that difficulty will not fall
on those who did their best to prevent emergency upgrades by planning ahead.


-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0xEAD9E623.asc
Type: application/pgp-keys
Size: 23337 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/218f1c1b/attachment-0001.bin>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 801 bytes
Desc: OpenPGP digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/218f1c1b/attachment-0001.sig>

From pieter.wuille at gmail.com  Sat Dec 26 23:16:17 2015
From: pieter.wuille at gmail.com (Pieter Wuille)
Date: Sun, 27 Dec 2015 00:16:17 +0100
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <2D7C4E00-7451-45B6-94B6-07A7230FBF88@toom.im>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
	<CADm_WcbrMyk-=OnQ-3UvnF_8brhn+X2NqRPbo5xUXsbcZpc0=Q@mail.gmail.com>
	<CAPg+sBjbATqf8DXGF7obw9a=371zQ_S0EgTapnUmukAVenTneQ@mail.gmail.com>
	<CA+c4Zozac8=aMrAJ1N_6SR9eBD+w0e70cEnk9CG_2oZ72AS-8g@mail.gmail.com>
	<CAPg+sBhsKD8jd9Y9+ngXY5tKUheO3d4P1b47eYL=Uzpat+KJ2w@mail.gmail.com>
	<751DFAA9-9013-4C54-BC1E-5F7ECB7469CC@gmail.com>
	<CAPg+sBiT5=ss9e=iac6J-A=85okF0zxMeV7H4z9-Qfx3CAWHXA@mail.gmail.com>
	<246AA3BE-570D-4B88-A63D-AC76CB2B0CB8@toom.im>
	<CAPg+sBhxnxnQQ-SpWuJ-+_uxRwXkgcU07jkYdZ8BcBwVDyW-vg@mail.gmail.com>
	<2D7C4E00-7451-45B6-94B6-07A7230FBF88@toom.im>
Message-ID: <CAPg+sBi_cD-mQOxVoHrHq5CUmjdEC5afCovSh0q3wD-nGV=R3g@mail.gmail.com>

On Dec 27, 2015 00:06, "Jonathan Toomim" <j at toom.im> wrote:

> Given that a supermajority of users and miners have been asking for a
hard fork to increase the blocksize for years, I do not think that
mobilizing people to upgrade their nodes is going to be hard.
>
> When we do the hard fork, we will need to encourage people to upgrade
their full nodes. We may want to request that miners not trigger the fork
until some percentage of visible full nodes have upgraded.

I am generally not interested in a system where we rely on miners to make
that judgement call to fork off nodes that don't pay attention and/or
disagree with the change. This is not because I don't trust them, but
because I believe one of the principle values of the system is that its
consensus system should be hard to change.

I can't tell you what code to run of course, but I can decide what system I
find interesting to build. And it seems many people have signed off on
working towards a plan that does not include a hard fork being scheduled
right now: https://bitcoin.org/en/bitcoin-core/capacity-increases

Cheers,

-- 
Pieter
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151227/96ee711e/attachment.html>

From j at toom.im  Sun Dec 27 00:03:58 2015
From: j at toom.im (Jonathan Toomim)
Date: Sat, 26 Dec 2015 16:03:58 -0800
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
	moral hazard
In-Reply-To: <CAPg+sBi_cD-mQOxVoHrHq5CUmjdEC5afCovSh0q3wD-nGV=R3g@mail.gmail.com>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
	<CADm_WcbrMyk-=OnQ-3UvnF_8brhn+X2NqRPbo5xUXsbcZpc0=Q@mail.gmail.com>
	<CAPg+sBjbATqf8DXGF7obw9a=371zQ_S0EgTapnUmukAVenTneQ@mail.gmail.com>
	<CA+c4Zozac8=aMrAJ1N_6SR9eBD+w0e70cEnk9CG_2oZ72AS-8g@mail.gmail.com>
	<CAPg+sBhsKD8jd9Y9+ngXY5tKUheO3d4P1b47eYL=Uzpat+KJ2w@mail.gmail.com>
	<751DFAA9-9013-4C54-BC1E-5F7ECB7469CC@gmail.com>
	<CAPg+sBiT5=ss9e=iac6J-A=85okF0zxMeV7H4z9-Qfx3CAWHXA@mail.gmail.com>
	<246AA3BE-570D-4B88-A63D-AC76CB2B0CB8@toom.im>
	<CAPg+sBhxnxnQQ-SpWuJ-+_uxRwXkgcU07jkYdZ8BcBwVDyW-vg@mail.gmail.com>
	<2D7C4E00-7451-45B6-94B6-07A7230FBF88@toom.im>
	<CAPg+sBi_cD-mQOxVoHrHq5CUmjdEC5afCovSh0q3wD-nGV=R3g@mail.gmail.com>
Message-ID: <A9548C1C-3DA5-4631-B399-50CC301FB8A8@toom.im>

On Dec 26, 2015, at 3:16 PM, Pieter Wuille <pieter.wuille at gmail.com> wrote:

> I am generally not interested in a system where we rely on miners to make that judgement call to fork off nodes that don't pay attention and/or disagree with the change. This is not because I don't trust them, but because I believe one of the principle values of the system is that its consensus system should be hard to change.
> 
I'm not proposing that we rely on miners' assessments of full node counts. I'm simply proposing that they serve as an extra layer of safety.

For the users that don't pay attention, I don't think the miners should be the sole parties to make that judgment call. That's why I suggested the grace period. I think that 1 or 2 months more than enough time for a business or active bitcoin user to download a new version of the software. I think that 1 or 2 months after a majority of nodes and miners have upgraded is more than more than enough time. For non-active businesses and users, there is no risk from the hard fork. If you're not transacting, you can't be defrauded.

Nodes that disagree with the change are welcome to continue to run the old version and watch the small fork if they so choose. Their numbers should be small if indeed this is an uncontroversial hardfork, but they won't be zero, and that's fine. The software supports this (except for peer discovery, which can get a little bit tricky in hardfork scenarios for the minority fork). Miners have no ethical obligation to protect individuals who choose not to follow consensus.

I think that use of the Alert System https://en.bitcoin.it/wiki/Alert_system would be justified in the weeks preceding the hard fork. Maybe we can create an "Upgrade now!" message that the new version would ignore, and set it to broadcast forever to all old nodes?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/f3f84eab/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/f3f84eab/attachment.sig>

From kanzure at gmail.com  Sun Dec 27 00:13:40 2015
From: kanzure at gmail.com (Bryan Bishop)
Date: Sat, 26 Dec 2015 18:13:40 -0600
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <567F1F7A.6070700@openbitcoinprivacyproject.org>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
	<CADm_WcbrMyk-=OnQ-3UvnF_8brhn+X2NqRPbo5xUXsbcZpc0=Q@mail.gmail.com>
	<CAPg+sBjbATqf8DXGF7obw9a=371zQ_S0EgTapnUmukAVenTneQ@mail.gmail.com>
	<CA+c4Zozac8=aMrAJ1N_6SR9eBD+w0e70cEnk9CG_2oZ72AS-8g@mail.gmail.com>
	<CAPg+sBhsKD8jd9Y9+ngXY5tKUheO3d4P1b47eYL=Uzpat+KJ2w@mail.gmail.com>
	<751DFAA9-9013-4C54-BC1E-5F7ECB7469CC@gmail.com>
	<CAPg+sBiT5=ss9e=iac6J-A=85okF0zxMeV7H4z9-Qfx3CAWHXA@mail.gmail.com>
	<246AA3BE-570D-4B88-A63D-AC76CB2B0CB8@toom.im>
	<CAPg+sBhxnxnQQ-SpWuJ-+_uxRwXkgcU07jkYdZ8BcBwVDyW-vg@mail.gmail.com>
	<567F1F7A.6070700@openbitcoinprivacyproject.org>
Message-ID: <CABaSBazeNk5_GaWiQerYxgB0VJko9AX+Yq1LaUJVpZTSinGt9A@mail.gmail.com>

On Sat, Dec 26, 2015 at 5:15 PM, Justus Ranvier via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> On 12/26/2015 05:01 PM, Pieter Wuille via bitcoin-dev wrote:
> > I think the shortest reasonable timeframe for an uncontroversial
> > hardfork is somewhere in the range between 6 and 12 months.
>
> This argument would hold more weight if it didn't looks like a stalling
> tactic in context.
>

I think you'll find that there hasn't been stalling regarding an
uncontroversial hard-fork deployment. You might be confusing an
uncontroversial hard-fork decision instead with how developers have brought
up many issues about various (hard-forking) block size proposals.... I
suspect this is what you're intending to mention instead, given your
mention of "capacity emergencies" and also the subject line.


> 6 months ago, there was a concerted effort to being the process then,
> for exactly this reason.
>

The uncontroversial hard-fork proposals from 6 months ago were mostly along
the lines of jtimon's proposals, which were not about capacity. (Although,
I should say "almost entirely uncontroversial"-- obviously has been some
minor (and in my opinion, entirely solvable) disagreement regarding
prioritization of deploying a jtimon's uncontroversial hard-fork idea I
guess, seeing as how it has not yet happened.)


> After 6 months of denial, stonewalling, and generally unproductive
> fighting, the need for proactivity is being acknowledged with no
> reference to the delay.
>

There wasn't 6 months of "stonewalling" or "denial" about an
uncontroversial hard-fork proposal. There has been extensive discussion
regarding the controversial (flawed?) properties of other (block size)
proposals. But that's something else. Much of this has been rehashed ad
nauseum on this mailing list already...  thankfully I think your future
emails could be improved and made more useful if you were to read the
mailing list archives, try to employ more careful reasoning, etc. Thanks.


> If the network ever ends up making a hasty forced upgrade to solve a
> capacity emergency the responsibility for that difficulty will not fall
> on those who did their best to prevent emergency upgrades by planning
> ahead.
>

("Capacity emergency" is too ambiguous in this context because of the
competing concerns and tradeoffs regarding transaction rate capacity
exhaustion vs. p2p low-bandwidth node bandwidth exhaustion.)

- Bryan
http://heybryan.org/
1 512 203 0507
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/181a6658/attachment.html>

From justus at openbitcoinprivacyproject.org  Sun Dec 27 00:33:58 2015
From: justus at openbitcoinprivacyproject.org (Justus Ranvier)
Date: Sat, 26 Dec 2015 18:33:58 -0600
Subject: [bitcoin-dev] Block size: It's economics & user preparation &
 moral hazard
In-Reply-To: <CABaSBazeNk5_GaWiQerYxgB0VJko9AX+Yq1LaUJVpZTSinGt9A@mail.gmail.com>
References: <CADm_WcasDuBsop55ZWcTb2FvccaoREg8K032rUjgQUQhQ3g=XA@mail.gmail.com>
	<CAPg+sBi=Mw7UnxG1-0-0ZTRqxrS5+28VmowyYrGP2MAvYiu_pA@mail.gmail.com>
	<CADm_WcbrMyk-=OnQ-3UvnF_8brhn+X2NqRPbo5xUXsbcZpc0=Q@mail.gmail.com>
	<CAPg+sBjbATqf8DXGF7obw9a=371zQ_S0EgTapnUmukAVenTneQ@mail.gmail.com>
	<CA+c4Zozac8=aMrAJ1N_6SR9eBD+w0e70cEnk9CG_2oZ72AS-8g@mail.gmail.com>
	<CAPg+sBhsKD8jd9Y9+ngXY5tKUheO3d4P1b47eYL=Uzpat+KJ2w@mail.gmail.com>
	<751DFAA9-9013-4C54-BC1E-5F7ECB7469CC@gmail.com>
	<CAPg+sBiT5=ss9e=iac6J-A=85okF0zxMeV7H4z9-Qfx3CAWHXA@mail.gmail.com>
	<246AA3BE-570D-4B88-A63D-AC76CB2B0CB8@toom.im>
	<CAPg+sBhxnxnQQ-SpWuJ-+_uxRwXkgcU07jkYdZ8BcBwVDyW-vg@mail.gmail.com>
	<567F1F7A.6070700@openbitcoinprivacyproject.org>
	<CABaSBazeNk5_GaWiQerYxgB0VJko9AX+Yq1LaUJVpZTSinGt9A@mail.gmail.com>
Message-ID: <567F31F6.20300@openbitcoinprivacyproject.org>

On 12/26/2015 06:13 PM, Bryan Bishop wrote:
> I think you'll find that there hasn't been stalling regarding an
> uncontroversial hard-fork deployment. You might be confusing an
> uncontroversial hard-fork decision instead with how developers have
> brought up many issues about various (hard-forking) block size
> proposals.... I suspect this is what you're intending to mention
> instead, given your mention of "capacity emergencies" and also the
> subject line.

I think you'll find that writing in that tone makes one come across as a
complete and utter douchebag.

I suspect what you're intending to do is to use faux-polite
condescension to bait me into responding in a way to will justify my
subsequent banning from this mailing list so that the people who aren't
interested in answering certain uncomfortable questions will have a
plausible excuse for preventing them from being asked here.

> There wasn't 6 months of "stonewalling" or "denial" about an
> uncontroversial hard-fork proposal. There has been extensive discussion
> regarding the controversial (flawed?) properties of other (block size)
> proposals. But that's something else. Much of this has been rehashed ad
> nauseum on this mailing list already...  thankfully I think your future
> emails could be improved and made more useful if you were to read the
> mailing list archives, try to employ more careful reasoning, etc. Thanks.

Actually there's been 3+ years of stonewalling, deception, conflicts of
interest, and outright crimes, which have been generally ignored by
those who are desperately attempting to assume good faith.

The purpose of my email was to remind everyone that nobody is going to
get away with avoiding ownership of the consequences of their actions.

If the network experiences a painful upgrade because six months of time
that could have been used to prepare a smooth upgrade was lost, the
individuals who squandered that time own the result. They can't get
around it by demanding six additional months, as if they had nothing to
do with the six lost months.


-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0xEAD9E623.asc
Type: application/pgp-keys
Size: 23337 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/fc9ad844/attachment-0001.bin>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 801 bytes
Desc: OpenPGP digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/fc9ad844/attachment-0001.sig>

From operator at bitminter.com  Sun Dec 27 04:10:09 2015
From: operator at bitminter.com (Geir Harald Hansen)
Date: Sun, 27 Dec 2015 05:10:09 +0100
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CAP3QyGJD3SaM6Bvvw66jAvVFkQhrfJfRQTxbbe8a=O1zK_P6tw@mail.gmail.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<4882BD35-D890-4860-9222-5C23AEB6AE89@mattcorallo.com>
	<CAAcC9yspsPs3gbumS4rTOg-P-=V=tycn2Z1nVPGGHwJ-nP+PBg@mail.gmail.com>
	<20151220044450.GA23942@muck>
	<CAP3QyGJD3SaM6Bvvw66jAvVFkQhrfJfRQTxbbe8a=O1zK_P6tw@mail.gmail.com>
Message-ID: <567F64A1.8020202@bitminter.com>

Last I heard it was believed the miner had made their own mining client
and that the block withholding was a bug, not an intended feature.

On 26.12.2015 09:12, Multipool Admin via bitcoin-dev wrote:
> Any attempt to 'fix' this problem, would most likely require changes to
> all mining software, why not just make mining more decentralized in general?
> 
> For example, allow anyone to submit proofs of work to Bitcoind that are
> some fraction of the network difficulty and receive payment for them if
> they're valid.  This would also encourage the proliferation of full
> nodes since anyone could solo mine again.  Then, the next coinbase
> transaction could be split among, say, the top 100 proofs of work.
> 
> Eligius already does their miner payouts like this.
> 
> If you want to fix an issue with mining, fix the selfish mining issue
> first as it's a much larger and more dangerous potential issue.
> 
> I don't believe it was ever clearly established whether Eligius suffered
> a block withholding attack or was just the victim of a miner with (what
> was, at the time) a large amount of faulty hardware, however, from the
> Bitcointalk threads at the time I believe it was assumed to be the latter.
> 
> --Adam
> 
> 
> On Sat, Dec 19, 2015 at 8:44 PM, Peter Todd via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org
> <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:
> 
>     On Sat, Dec 19, 2015 at 07:43:59PM -0800, Chris Priest via
>     bitcoin-dev wrote:
>     > Then shouldn't this be something the pool deals with, not the bitcoin protocol?
> 
>     There is no known way for pools - especially ones that allow anonymous
>     hashers - to effectively prevent block withholding attacks without
>     changing the Bitcoin protocol.
> 
>     --
>     'peter'[:-1]@petertodd.org <http://petertodd.org>
>     00000000000000000188b6321da7feae60d74c7b0becbdab3b1a0bd57f10947d
> 
>     _______________________________________________
>     bitcoin-dev mailing list
>     bitcoin-dev at lists.linuxfoundation.org
>     <mailto:bitcoin-dev at lists.linuxfoundation.org>
>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 
> 
> 
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 


From el33th4x0r at gmail.com  Sun Dec 27 04:33:25 2015
From: el33th4x0r at gmail.com (=?UTF-8?Q?Emin_G=C3=BCn_Sirer?=)
Date: Sat, 26 Dec 2015 23:33:25 -0500
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CCA40088-E9CE-4EC0-9C3A-E355A6C3389F@toom.im>
References: <ema8a70574-c28e-4c43-a1e3-5f2f4df7d3a2@platinum>
	<CCA40088-E9CE-4EC0-9C3A-E355A6C3389F@toom.im>
Message-ID: <CAPkFh0t6uVK4Z9C+mpwNAyy+iR4YJmjS48PDAAsVAXVjfQL9cg@mail.gmail.com>

>Another option for how to deal with block withholding attacks: Give the
miner who finds the block a bonus.
...
>This must have been proposed before, right? Anyone know of a good analysis
of the game theory math?

Yes, Section 8.D. in Ittay's paper discusses this countermeasure, as well
as a few others:
    https://www.cs.cornell.edu/~ie53/publications/btcPoolsSP15.pdf

- egs
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151226/02893426/attachment.html>

From jl2012 at xbt.hk  Sun Dec 27 08:26:21 2015
From: jl2012 at xbt.hk (jl2012)
Date: Sun, 27 Dec 2015 03:26:21 -0500
Subject: [bitcoin-dev] Segregated Witness BIPs
In-Reply-To: <77ba13602b2d9e3c0f56a41a9799021a@xbt.hk>
References: <em3b62e758-afbe-4aed-a08d-eb85b252efc4@platinum>
	<77ba13602b2d9e3c0f56a41a9799021a@xbt.hk>
Message-ID: <59bc1df0151d1003a489f9ef7bf8a2f7@xbt.hk>

The SW payment address format BIP is completely rewritten to introduce 2 
types of new addresses:

https://github.com/bitcoin/bips/pull/267

jl2012 via bitcoin-dev ? 2015-12-24 09:22 ??:
> The SW payment address format BIP draft is ready and is pending BIP
> number assignment:
> https://github.com/bitcoin/bips/pull/267
> 
> This is the 3rd BIP for segwit. The 2nd one for Peer Services is being
> prepared by Eric Lombrozo
> 
> Eric Lombrozo via bitcoin-dev ? 2015-12-23 10:22 ??:
>> I've been working with jl2012 on some SEGWIT BIPs based on earlier
>> discussions Pieter Wuille's implementation. We're considering
>> submitting three separate BIPs:
>> 
>> CONSENSUS BIP: witness structures and how they're committed to blocks,
>> cost metrics and limits, the scripting system (witness programs), and
>> the soft fork mechanism.
>> 
>> PEER SERVICES BIP: relay message structures, witnesstx serialization,
>> and other issues pertaining to the p2p protocol such as IBD,
>> synchronization, tx and block propagation, etc...
>> 
>> APPLICATIONS BIP: scriptPubKey encoding formats and other wallet
>> interoperability concerns.
>> 
>> The Consensus BIP is submitted as a draft and is pending BIP number
>> assignment: https://github.com/bitcoin/bips/pull/265 [1]
>> The other two BIPS will be drafted soon.
>> 
>> ---
>> Eric
>> 
>> Links:
>> ------
>> [1] https://github.com/bitcoin/bips/pull/265
>> 
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


From j at toom.im  Mon Dec 28 00:10:36 2015
From: j at toom.im (Jonathan Toomim)
Date: Sun, 27 Dec 2015 16:10:36 -0800
Subject: [bitcoin-dev] Consensus census
Message-ID: <A6A12C69-DDC6-4743-89D6-C50730A1644B@toom.im>

I traveled around in China for a couple weeks after Hong Kong to visit with miners and confer on the blocksize increase and block propagation issues. I performed an informal survey of a few of the blocksize increase proposals that I thought would be likely to have widespread support. The results of the version 1.0 census are below.

My brother is working on a website for a version 2.0 census. You can view the beta version of it and participate in it at https://bitcoin.consider.it. If you have any requests for changes to the format, please CC him at m at toom.im.

https://docs.google.com/spreadsheets/d/1Cg9Qo9Vl5PdJYD4EiHnIGMV3G48pWmcWI3NFoKKfIzU/edit#gid=0

Or a snapshot for those behind the GFW without a VPN:
http://toom.im/files/consensus_census.pdf

HTML follows:

Miner	Hashrate	BIP103	2 MB now (BIP102)	2 MB now, 4 MB in 2 yr	2-4-8 (Adam Back)	3 MB now	3 MB now, 10 MB in 3 yr	BIP101
F2Pool	22%	N/A	Acceptable	Acceptable	Preferred	Acceptable	Acceptable	Too fast
AntPool	23%	Too slow	Acceptable	Acceptable	Acceptable	N/A	N/A	Too fast
Bitfury	18%	N/A	Acceptable	Probably/maybe	Maybe	N/A	Probably too fast	Too fast
BTCC Pool	11%	N/A	Acceptable	Acceptable	Acceptable	Acceptable	Acceptable, I think	N/A
KnCMiner	7%	N/A	Probably?	Probably?	"We like 2-4-8"	Probably?	N/A	N/A
BW.com	7%	N/A	N/A	N/A	N/A	N/A	N/A	N/A
Slush	4%	N/A	N/A	N/A	N/A	N/A	N/A	N/A
21 Inc.	3%	N/A	N/A	N/A	N/A	N/A	N/A	N/A
Eligius	1%	N/A	N/A	N/A	N/A	N/A	N/A	N/A
BitClub	1%	N/A	N/A	N/A	N/A	N/A	N/A	N/A
GHash.io	1%	N/A	N/A	N/A	N/A	N/A	N/A	N/A
Misc	2%	N/A	N/A	N/A	N/A	N/A	N/A	N/A
Certainly in favor			74%	56%	63%	33%	22%
Possibly in favor			81%	81%	81%	40%	33%	0%
Total votes counted			81%	81%	81%	40%	51%	63%
F2Pool: Blocksize increase could be phased in at block 400,000. No floating-point math. No timestamp-based forking (block height is okay). Conversation was with Wang Chun via IRC.
AntPool/Bitmain: We should get miners and devs together for few rounds of voting to decide which plan to implement. (My brother is working on a tool which may be useful for this. More info soon.) The blocksize increase should be merged into Bitcoin Core, and should not be implemented in an alternate client like BitcoinXT. A timeline of about 3 months for the fork was discussed, though I don't know if that was acceptable or preferable to Bitmain. Conversation was mostly with Micree Zhan and Kevin Pan at the Bitmain HQ. Jihan Wu was absent.
Bitfury: We should fix performance issues in bitcoind before 4 MB, and we MUST fix performance issues before 8 MB. A plan that includes 8 MB blocks in the future and assumes the performance fixes will be implemented might be acceptable to us, but we'll have to evaluate it more before coming to a conclusion. 2-4-8 "is like parachute basejumping - if you jump, and was unable to fix parachute during the 90sec drop - you will be 100% dead. plan A) [multiple hard forks] more safe." Conversation was with Alex Petrov at the conference and via email.
KnC: I only had short conversations with Sam Cole, but from what I can tell, they would be okay with just about anything reasonable.
BTCC: It would be much better to have the support of Core, but if Core doesn't include a blocksize increase soon in the master branch, we may be willing to start running a fork. Conversation was with Samson Mow and a few others at BTCC HQ.
The conversations I had with all of these entities were of an informal, non-binding nature. Positions are subject to change. BIP100 was not included in my talks because (a) coinbase voting already covers it pretty well, and (b) it is more complicated than the other proposals and currently does not seem likely to be implemented. I generally did not bring up SegWit during the conversations I had with miners, and neither did the miners, so it is also absent. (I thought that it was too early for miners to have an informed opinion of SegWit's relative merits.) I have not had any contact with BW.com or any of the smaller entities. Questions can be directed to j at toom.im.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151227/824ec2c9/attachment-0001.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151227/824ec2c9/attachment-0001.sig>

From pete at petertodd.org  Mon Dec 28 19:12:28 2015
From: pete at petertodd.org (Peter Todd)
Date: Mon, 28 Dec 2015 11:12:28 -0800
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CAP3QyGJD3SaM6Bvvw66jAvVFkQhrfJfRQTxbbe8a=O1zK_P6tw@mail.gmail.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<4882BD35-D890-4860-9222-5C23AEB6AE89@mattcorallo.com>
	<CAAcC9yspsPs3gbumS4rTOg-P-=V=tycn2Z1nVPGGHwJ-nP+PBg@mail.gmail.com>
	<20151220044450.GA23942@muck>
	<CAP3QyGJD3SaM6Bvvw66jAvVFkQhrfJfRQTxbbe8a=O1zK_P6tw@mail.gmail.com>
Message-ID: <20151228191228.GC12298@muck>

On Sat, Dec 26, 2015 at 12:12:13AM -0800, Multipool Admin wrote:
> Any attempt to 'fix' this problem, would most likely require changes to all
> mining software, why not just make mining more decentralized in general?
> 
> For example, allow anyone to submit proofs of work to Bitcoind that are
> some fraction of the network difficulty and receive payment for them if
> they're valid.  This would also encourage the proliferation of full nodes
> since anyone could solo mine again.  Then, the next coinbase transaction
> could be split among, say, the top 100 proofs of work.

That's certainly be a good place to be, but the design of Bitcoin
currently makes achieving that goal fundementally difficult. 

> Eligius already does their miner payouts like this.
> 
> If you want to fix an issue with mining, fix the selfish mining issue first
> as it's a much larger and more dangerous potential issue.

Do you specifically mean selfish mining as defined in Emin G?n
Sirer/Ittay Eyal's paper? Keep in mind that attack is only a significant
issue in a scenario - one malicious miner with >30% hashing power -
where you're already very close to the margins anyway; the difference
between a 50% attack threshold and a 30% attack threshold isn't very
significant.

Far more concerning is network propagation effects between large and
small miners. For that class of issues, if you are in an environemnt
where selfish mining is possible - a fairly flat, easily DoS/sybil
attacked network topology - the profitability difference between small
and large miners even *without* attacks going on is a hugely worrying
problem. OTOH, if you're blocksize is small enough that propagation time
is negligable to profitability, then selfish mining attacks with <30%
hashing power aren't much of a concern - they'll be naturally defeated
by anti-DoS/anti-sybil measures.

> I don't believe it was ever clearly established whether Eligius suffered a
> block withholding attack or was just the victim of a miner with (what was,
> at the time) a large amount of faulty hardware, however, from the
> Bitcointalk threads at the time I believe it was assumed to be the latter.

I think the latter was assumed as well, although ruling out of the
former is impossible.

Note though that Eligius is *not* the only pool to have had problems
with block withholding, though AFAIK Eligius is the only one who has
gone on record so far. (as I said in my original post, I'm relaying
information given to me under condition of confidentiality)

-- 
'peter'[:-1]@petertodd.org
000000000000000004a36565fb282c4bd06dda61329fda2465b0bfeaf7241dab
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 650 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151228/1e23dd8a/attachment.sig>

From el33th4x0r at gmail.com  Mon Dec 28 19:30:14 2015
From: el33th4x0r at gmail.com (=?UTF-8?Q?Emin_G=C3=BCn_Sirer?=)
Date: Mon, 28 Dec 2015 14:30:14 -0500
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <20151228191228.GC12298@muck>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<4882BD35-D890-4860-9222-5C23AEB6AE89@mattcorallo.com>
	<CAAcC9yspsPs3gbumS4rTOg-P-=V=tycn2Z1nVPGGHwJ-nP+PBg@mail.gmail.com>
	<20151220044450.GA23942@muck>
	<CAP3QyGJD3SaM6Bvvw66jAvVFkQhrfJfRQTxbbe8a=O1zK_P6tw@mail.gmail.com>
	<20151228191228.GC12298@muck>
Message-ID: <CAPkFh0vY7sSFJxAnYSWgPszY6kAjcfDBR7cPn0ptL9yc-OtFdg@mail.gmail.com>

On Mon, Dec 28, 2015 at 2:12 PM, Peter Todd via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Do you specifically mean selfish mining as defined in Emin G?n
> Sirer/Ittay Eyal's paper? Keep in mind that attack is only a significant
> issue in a scenario - one malicious miner with >30% hashing power -
> where you're already very close to the margins anyway; the difference
> between a 50% attack threshold and a 30% attack threshold isn't very
> significant.
>

This is not quite right: we know that selfish mining is a guaranteed win
at 34%. We do not know when exactly it begins to pay off. The more
consolidated and centralized the other mining pools, the less of a threat
it is below 34%; the more decentralized, the more likely it is to pay off
at lower thresholds.

Far more concerning is network propagation effects between large and
> small miners.


On a related note, the Bitcoin-NG paper took a big step towards moving
these kinds of concerns out of the realm of gut-feelings and wavy hands
into science. In particular, it introduced metrics for fairness (i.e.
differential
rate in orphans experienced by small and large miners), hash power
efficiency, as well as consensus delay.


> For that class of issues, if you are in an environemnt
> where selfish mining is possible - a fairly flat, easily DoS/sybil
> attacked network topology - the profitability difference between small
> and large miners even *without* attacks going on is a hugely worrying
> problem.


Indeed, there is a slight, quantifiable benefit to larger pools. Which is
why
we need to be diligent about not letting pools get too big.


> Note though that Eligius is *not* the only pool to have had problems
>
with block withholding, though AFAIK Eligius is the only one who has
> gone on record so far. (as I said in my original post, I'm relaying
> information given to me under condition of confidentiality)
>

I can see why they don't want to go public with this: it means that they
are less profitable than other pools.

It still looks to me like Ittay's discovery is doing exactly the right
thing:
this pool will need to be more careful when signing up new people,
curbing its otherwise steady march towards the 51% boundary.

- egs


- egs
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151228/2e490a2b/attachment.html>

From admin at multipool.us  Mon Dec 28 19:33:11 2015
From: admin at multipool.us (Multipool Admin)
Date: Mon, 28 Dec 2015 11:33:11 -0800
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <20151228191228.GC12298@muck>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<4882BD35-D890-4860-9222-5C23AEB6AE89@mattcorallo.com>
	<CAAcC9yspsPs3gbumS4rTOg-P-=V=tycn2Z1nVPGGHwJ-nP+PBg@mail.gmail.com>
	<20151220044450.GA23942@muck>
	<CAP3QyGJD3SaM6Bvvw66jAvVFkQhrfJfRQTxbbe8a=O1zK_P6tw@mail.gmail.com>
	<20151228191228.GC12298@muck>
Message-ID: <CAP3QyG+D3HCiVAkty2wXPRJw+dwcmNGV2Gn0gEdX8DyExaNSQQ@mail.gmail.com>

On Mon, Dec 28, 2015 at 11:12 AM, Peter Todd <pete at petertodd.org> wrote:

> On Sat, Dec 26, 2015 at 12:12:13AM -0800, Multipool Admin wrote:
> > Any attempt to 'fix' this problem, would most likely require changes to
> all
> > mining software, why not just make mining more decentralized in general?
> >
> > For example, allow anyone to submit proofs of work to Bitcoind that are
> > some fraction of the network difficulty and receive payment for them if
> > they're valid.  This would also encourage the proliferation of full nodes
> > since anyone could solo mine again.  Then, the next coinbase transaction
> > could be split among, say, the top 100 proofs of work.
>
> That's certainly be a good place to be, but the design of Bitcoin
> currently makes achieving that goal fundementally difficult.
>

Agreed, however I don't think it would be impossible or even really that
difficult, and would be a great way to increase decentralization while
simultaneously fixing other issues with mining.

Proofs of work would be valid if they're built on top of the current block
hash, and we could require (difficulty/N) proofs of work that are >=
(difficulty/N) to assemble a valid block.  Same as mining shares work.

The block assembler who finds the final diff/N 'share' could get a small
bonus as an incentive to complete the block as quickly as possible.  Or
alternatively, a checksum could be computed of all the current diff/N
shares in the mempool and that way only the final share would need to be
broadcasted to the entire network, and clients with the correct checksum
could assemble the block themselves without having to download the entire
block.  This would drastically decrease data usage on the network.

> Eligius already does their miner payouts like this.
> >
> > If you want to fix an issue with mining, fix the selfish mining issue
> first
> > as it's a much larger and more dangerous potential issue.
>
> Do you specifically mean selfish mining as defined in Emin G?n
> Sirer/Ittay Eyal's paper? Keep in mind that attack is only a significant
> issue in a scenario - one malicious miner with >30% hashing power -
> where you're already very close to the margins anyway; the difference
> between a 50% attack threshold and a 30% attack threshold isn't very
> significant.
>

Yes, that's what I'm talking about.


> Far more concerning is network propagation effects between large and
> small miners. For that class of issues, if you are in an environemnt
> where selfish mining is possible - a fairly flat, easily DoS/sybil
> attacked network topology - the profitability difference between small
> and large miners even *without* attacks going on is a hugely worrying
> problem. OTOH, if you're blocksize is small enough that propagation time
> is negligable to profitability, then selfish mining attacks with <30%
> hashing power aren't much of a concern - they'll be naturally defeated
> by anti-DoS/anti-sybil measures.
>

The possibility that a previously 'good' actor with 30% of the hashpower
going 'rogue' becomes more and more of a concern as the block subsidy
decreases.


> > I don't believe it was ever clearly established whether Eligius suffered
> a
> > block withholding attack or was just the victim of a miner with (what
> was,
> > at the time) a large amount of faulty hardware, however, from the
> > Bitcointalk threads at the time I believe it was assumed to be the
> latter.
>
> I think the latter was assumed as well, although ruling out of the
> former is impossible.
>
> Note though that Eligius is *not* the only pool to have had problems
> with block withholding, though AFAIK Eligius is the only one who has
> gone on record so far. (as I said in my original post, I'm relaying
> information given to me under condition of confidentiality)
>

What is the incentive not to go on record about this?

--Adam
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151228/46d76db5/attachment.html>

From admin at multipool.us  Mon Dec 28 19:35:34 2015
From: admin at multipool.us (Multipool Admin)
Date: Mon, 28 Dec 2015 11:35:34 -0800
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CAPkFh0vY7sSFJxAnYSWgPszY6kAjcfDBR7cPn0ptL9yc-OtFdg@mail.gmail.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<4882BD35-D890-4860-9222-5C23AEB6AE89@mattcorallo.com>
	<CAAcC9yspsPs3gbumS4rTOg-P-=V=tycn2Z1nVPGGHwJ-nP+PBg@mail.gmail.com>
	<20151220044450.GA23942@muck>
	<CAP3QyGJD3SaM6Bvvw66jAvVFkQhrfJfRQTxbbe8a=O1zK_P6tw@mail.gmail.com>
	<20151228191228.GC12298@muck>
	<CAPkFh0vY7sSFJxAnYSWgPszY6kAjcfDBR7cPn0ptL9yc-OtFdg@mail.gmail.com>
Message-ID: <CAP3QyGKLa_Vf+f82RbNAYz=RP0ion4sqvj19Pqjr9v5taeo4pw@mail.gmail.com>

On Mon, Dec 28, 2015 at 11:30 AM, Emin G?n Sirer <
bitcoin-dev at lists.linuxfoundation.org> wrote:

>
>
> On Mon, Dec 28, 2015 at 2:12 PM, Peter Todd via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> Do you specifically mean selfish mining as defined in Emin G?n
>> Sirer/Ittay Eyal's paper? Keep in mind that attack is only a significant
>> issue in a scenario - one malicious miner with >30% hashing power -
>> where you're already very close to the margins anyway; the difference
>> between a 50% attack threshold and a 30% attack threshold isn't very
>> significant.
>>
>
> This is not quite right: we know that selfish mining is a guaranteed win
> at 34%. We do not know when exactly it begins to pay off. The more
> consolidated and centralized the other mining pools, the less of a threat
> it is below 34%; the more decentralized, the more likely it is to pay off
> at lower thresholds.
>

Exactly.


> Far more concerning is network propagation effects between large and
>> small miners.
>
>
> On a related note, the Bitcoin-NG paper took a big step towards moving
> these kinds of concerns out of the realm of gut-feelings and wavy hands
> into science. In particular, it introduced metrics for fairness (i.e.
> differential
> rate in orphans experienced by small and large miners), hash power
> efficiency, as well as consensus delay.
>
>
>> For that class of issues, if you are in an environemnt
>> where selfish mining is possible - a fairly flat, easily DoS/sybil
>> attacked network topology - the profitability difference between small
>> and large miners even *without* attacks going on is a hugely worrying
>> problem.
>
>
> Indeed, there is a slight, quantifiable benefit to larger pools. Which is
> why
> we need to be diligent about not letting pools get too big.
>
>
>> Note though that Eligius is *not* the only pool to have had problems
>>
> with block withholding, though AFAIK Eligius is the only one who has
>> gone on record so far. (as I said in my original post, I'm relaying
>> information given to me under condition of confidentiality)
>>
>
> I can see why they don't want to go public with this: it means that they
> are less profitable than other pools.
>

This I disagree with -- if they know that they have been attacked, then
there is every reason to come forward with this information.

First of all, it offers an explanation for poor profits (this is better
than unexplained poor profits).

Second of all, if one pool can be attacked then any pool can be attacked --
this is not a reason not to mine on a particular pool.  If anything, it's a
reason to diversify hashrate among many pools.

--Adam
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151228/8c25dd48/attachment-0001.html>

From pete at petertodd.org  Mon Dec 28 20:02:21 2015
From: pete at petertodd.org (Peter Todd)
Date: Mon, 28 Dec 2015 12:02:21 -0800
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CAPkFh0t-+WhZYVLyT_auLa87zAATNOH=CpU4S5H=n6S1wmZ-oQ@mail.gmail.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<219f125cee6ca68fd27016642e38fdf1@xbt.hk>
	<CAAcC9ys_t7X0WpQ8W3577M8GLiA5sPV2F1BJ9qZbnMkE-1j3+Q@mail.gmail.com>
	<aff8da46a69bdd7ef92ca87725866a5c@xbt.hk>
	<CAPkFh0vNECi1OmBwki+8NNAQbe6EG2FEE4RR5z=kYVLLDFHUXg@mail.gmail.com>
	<20151220132842.GA25481@muck>
	<CAPkFh0t-+WhZYVLyT_auLa87zAATNOH=CpU4S5H=n6S1wmZ-oQ@mail.gmail.com>
Message-ID: <20151228200221.GD12298@muck>

On Sun, Dec 20, 2015 at 12:00:37PM -0500, Emin G?n Sirer via bitcoin-dev wrote:
> > In the context of KYC, this techniques would likely hold up in court,
> > which means that if this stuff becomes a more serious problem it's
> > perfectly viable for large, well-resourced, pools to prevent block
> > withholding attacks, in part by removing anonymity of hashing power.
> > This would not be a positive development for the ecosystem.
> >
> 
> KYC has a particular financial-regulation connotation in Bitcoin circles,
> of which I'm sure you're aware, and which you're using as a spectre.
> You don't mean government-regulated-KYC a la FINCEN and Bitcoin
> exchanges like Coinbase, you are just referring to a pool operator
> demanding to know that its customer is not coming from its competitors'
> data centers.

I mean Knowing Your Customer. The only way to know that a customer is
*not* coming from a competitor's data center is to know their identity,
which is precisely what KYC is.

In the financial world, KYC is used to refer to any time you take steps
to determine the real identity/deanonymize your customers.

> And your prediction doesn't seem well-motivated or properly justified.
> There are tons of conditionals in your prediction, starting with the premise
> that every single open pool would implement some notion of identity
> checking. I don't believe that will happen. Instead, we will have the bigger
> pools become more suspicious of signing up new hash power, which is a
> good thing. And we will have small groups of people who have some reason
> for trusting each other (e.g. they know each other from IRC, conferences,
> etc) band together into small pools. These are fantastic outcomes for
> decentralization.

That's a terrible outcomes for decentralization; we *want* people to be
able to contribute hashing power to the network even if they don't
already have personal connections with existing miners. That's how we
attract new players to the mining industry whose backgrounds are not
correlated with the backgrounds of other miners - exactly what we want
for decentralization.

Keep in mind that access to cheap power and cheap opportunities to get
rid of waste heat is naturally decentralized by physics, economics, and
politics. Basically, the cheapest power, and cheapest ways to get rid of
waste heat, is in the form of unique opportunities that don't have
economies of scale. For example, much of the Chinese hashing power takes
advantage of stranded hydroelectric plants that are located far away
from markets that would otherwise buy that power. These plants are
limited in size by the local rivers and there's no way to make them any
bigger - there's a natural diseconomy of scale involved.

Now, support if you have access to such a hydro plant - maybe a mine in
the middle of nowhere just closed and there's no-one else to sell the
power too. Right now you can buy some hashing equipment(1) and start
earning money immediately by pointing it at a pool of your choice. If
that pool fucks up, it's really easy for you to change a few lines in
your configs and point that hashing power to a different pool.

However if block withholding attacks continue and kill off open access
pools the process becomes much more arduous. Chances are you won't even
bother, and Bitcoin will end up with one less decentralized
miner.


1) If access to hashing equipment becomes a limiting factor/fails to
improve, Bitcoin itself will likely have to switch PoW functions to
succeed as a decentralized system.

> Secondly, DRM tech can also easily be used to prevent block withholding
> > attacks by attesting to the honest of the hashing power. This is being
> > discussed in the industry, and again, this isn't a positive development
> > for the ecosystem.
> >
> 
> DRM is a terrible application. Once again, I see that you're trying to use
> those
> three letters as a spectre as well, knowing that most people hate DRM, but
> keep in mind that DRM is just an application -- it's like pointing to Adobe
> Flash
> to taint all browser plugins.
> 
> The tech behind DRM is called "attestation," and it provides a technical
> capability not possible by any other means. In essence, attestation can
> ensure that
> a remote node is indeed running the code that it purports to be running.
> Since
> most problems in computer security and distributed systems stem from not
> knowing what protocol the attacker is going to follow, attestation is the
> only
> technology we have that lets us step around this limitation.
>
> It can ensure, for instance,
>   - that a node purporting to be Bitcoin Core (vLatest) is indeed running an
> unadulterated, latest version of Bitcoin Core
>   - that a node claiming that it does not harvest IP addresses from SPV
> clients indeed does not harvest IP addresses.
>   - that a cloud hashing outfit that rented out X terahashes to a user did
> indeed rent out X terahashes to that particular user,
>   - that a miner operating on behalf of some pool P will not misbehave and
> discard perfectly good blocks
> and so forth. All of these would be great for the ecosystem. Just getting
> rid
> of the cloudhashing scams would put an end to a lot of heartache.

Again, lets look at it from the perspective of someone with access to
cheap power.

With DRM tech a likely implementation is the equipment manufacturer/pool
operator sells you a locked down, tamper-resistant, box that only can
send hashing power to a specific pool. 21 for example has been
investigating this model. If such equipment is common, even though the
guy with a hydro plant in Siberia is physically and politically highly
decentralized, the control of the blocks created is highly centralized,
rendering his contribution to the network's decentralization moot.

At best we might get general purpose attestation, but implementing that
vs. locked down, single pool, boxes is more expensive and slower to
market. Even then, we'd be much more likely to get fragile and
difficult-to-reverse-engineer hashing equipment that's always going to
be easier to add backdoors too.

We're better off with an ecosystem where DRM tech like attestation isn't
needed at all.

As for cloud hashing... those scams have mostly died off as the market
has become more efficient.

> > GHash.io was not a pure pool - they owned and operated a significant
> > amount of physical hashing power, and it's not at all clear that their %
> > of the network actually went down following that 51% debacle.
> >
> 
> Right, it's not clear at all that yelling at people has much effect. As much
> fun as I had going to that meeting with GHash in London to ask them to
> back down off of the 51% boundary, I am pretty sure that yelling at large
> open pools will not scale. We needed better mechanisms for keeping pools
> in check.
> 
> And Miner's Dilemma (MD) attacks are clearly quite effective. This is a
> time when we should count our blessings, not work actively to render
> them inoperable.

What evidence do you have for them being "clearly quite effective"? Is
there any evidence that they were used against GHash.io for example?

Remember that block withholding attacks give an advantage to those with
access to large amounts of physical hashing power, like GHash.IO did at
that time.

> > Basically you have the pool pick a secret k for each share, and commit
> > to H(k) in the share. Additionally the share commits to a target divider
> > D. The PoW validity rule is then changed from H(block header) < T, to be
> > H(block header) < T * D && H(H(block header) + k) < max_int / D
> >
> 
> Thanks, this requires a change to the Bitcoin PoW. Good luck with that!

It's not a change to the PoW, just a change to the definition of block
validity; mining hardware does *not* need to change to implement
Luke-Jr's idea. Also, as mentioned elsewhere in this thread, it can be
implemented slowly as a pseudo-soft-fork.

-- 
'peter'[:-1]@petertodd.org
000000000000000001d3c4acb7446f66482fb6aceb087d7601c9e0644cf60e9a
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 650 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151228/bac9774f/attachment.sig>

From ibrightly at gmail.com  Mon Dec 28 20:26:43 2015
From: ibrightly at gmail.com (Ivan Brightly)
Date: Mon, 28 Dec 2015 15:26:43 -0500
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <20151228191228.GC12298@muck>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<4882BD35-D890-4860-9222-5C23AEB6AE89@mattcorallo.com>
	<CAAcC9yspsPs3gbumS4rTOg-P-=V=tycn2Z1nVPGGHwJ-nP+PBg@mail.gmail.com>
	<20151220044450.GA23942@muck>
	<CAP3QyGJD3SaM6Bvvw66jAvVFkQhrfJfRQTxbbe8a=O1zK_P6tw@mail.gmail.com>
	<20151228191228.GC12298@muck>
Message-ID: <CAAre=ySPjm+cyLdBY_CZkLdfXE3OFYgECEUq05AyWfY0q1KuTQ@mail.gmail.com>

>
> On Mon, Dec 28, 2015 at 2:12 PM, Peter Todd via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
> Far more concerning is network propagation effects between large and
> small miners. For that class of issues, if you are in an environemnt
> where selfish mining is possible - a fairly flat, easily DoS/sybil
> attacked network topology - the profitability difference between small
> and large miners even *without* attacks going on is a hugely worrying
> problem. OTOH, if you're blocksize is small enough that propagation time
> is negligable to profitability, then selfish mining attacks with <30%
> hashing power aren't much of a concern - they'll be naturally defeated
> by anti-DoS/anti-sybil measures.
>

Let's agree that one factor in mining profitability is bandwidth/network
reliability/stability. Why focus on that vs electricity contracts or
vertically integrated chip manufacturers? Surely, sufficient network
bandwidth is a more broadly available commodity than <$0.02/kwh
electricity, for example. I'm not sure that your stranded hydroelectric
miner is any more desirable than thousands of dorm room miners with access
to 10gbit university connections and free electricity.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151228/abe958d6/attachment.html>

From pete at petertodd.org  Tue Dec 29 05:35:59 2015
From: pete at petertodd.org (Peter Todd)
Date: Mon, 28 Dec 2015 21:35:59 -0800
Subject: [bitcoin-dev] We can trivially fix quadratic CHECKSIG with a simple
 soft-fork modifying just SignatureHash()
Message-ID: <20151229053559.GA8657@muck>

Occured to me that this hasn't been mentioned before...

We can trivially fix the quadratic CHECK(MULTI)SIG execution time issue
by soft-forking in a limitation on just SignatureHash() to only return
true if the tx size is <100KB. (or whatever limit makes sense)

This fix has the advantage over schemes that limit all txs, or try to
count sigops, of being trivial to implement, while still allowing for a
future CHECKSIG2 soft-fork that properly fixes the quadratic hashing
issue; >100KB txs would still be technically allowed, it's just that
(for now) there'd be no way for them to spend coins that are
cryptographically secured.

For example, if we had an issue with a major miner exploiting
slow-to-propagate blocks(1) to harm their competitors, this simple fix
could be deployed as a soft-fork in a matter of days, stopping the
attack quickly.

1) www.mail-archive.com/bitcoin-development at lists.sourceforge.net/msg03200.html

-- 
'peter'[:-1]@petertodd.org
0000000000000000094afcbbad10aa6c82ddd8aad102020e553d50a60b6c678f
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 650 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151228/0f5f8fd2/attachment.sig>

From jl2012 at xbt.hk  Tue Dec 29 07:47:22 2015
From: jl2012 at xbt.hk (jl2012)
Date: Tue, 29 Dec 2015 02:47:22 -0500
Subject: [bitcoin-dev] We can trivially fix quadratic CHECKSIG with a
 simple soft-fork modifying just SignatureHash()
In-Reply-To: <20151229053559.GA8657@muck>
References: <20151229053559.GA8657@muck>
Message-ID: <26ec8367f2a1cda066b19e0bff498711@xbt.hk>

Do we need to consider that someone may have a timelocked big tx, with 
private key lost?

I think we need to tell people not to do this. Related discussion:
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-November/011656.html


Peter Todd via bitcoin-dev ? 2015-12-29 00:35 ??:
> Occured to me that this hasn't been mentioned before...
> 
> We can trivially fix the quadratic CHECK(MULTI)SIG execution time issue
> by soft-forking in a limitation on just SignatureHash() to only return
> true if the tx size is <100KB. (or whatever limit makes sense)
> 
> This fix has the advantage over schemes that limit all txs, or try to
> count sigops, of being trivial to implement, while still allowing for a
> future CHECKSIG2 soft-fork that properly fixes the quadratic hashing
> issue; >100KB txs would still be technically allowed, it's just that
> (for now) there'd be no way for them to spend coins that are
> cryptographically secured.
> 
> For example, if we had an issue with a major miner exploiting
> slow-to-propagate blocks(1) to harm their competitors, this simple fix
> could be deployed as a soft-fork in a matter of days, stopping the
> attack quickly.
> 
> 1) 
> www.mail-archive.com/bitcoin-development at lists.sourceforge.net/msg03200.html
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


From j at toom.im  Tue Dec 29 12:42:55 2015
From: j at toom.im (Jonathan Toomim)
Date: Tue, 29 Dec 2015 04:42:55 -0800
Subject: [bitcoin-dev] We can trivially fix quadratic CHECKSIG with a
	simple soft-fork modifying just SignatureHash()
In-Reply-To: <26ec8367f2a1cda066b19e0bff498711@xbt.hk>
References: <20151229053559.GA8657@muck>
	<26ec8367f2a1cda066b19e0bff498711@xbt.hk>
Message-ID: <77DAE310-204C-4275-A791-4047798FCBFE@toom.im>

That sounds like a rather unlikely scenario. Unless you have a specific reason to suspect that might be the case, I think we don't need to worry about it too much. If we announce the intention to perform such a soft fork a couple of months before the soft fork becomes active, and if nobody complains about it destroying their secret stash, then I think that's fair enough and we could proceed.

On Dec 28, 2015, at 11:47 PM, jl2012 via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:

> Do we need to consider that someone may have a timelocked big tx, with private key lost?
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151229/11fa10ce/attachment.sig>

From jl2012 at xbt.hk  Tue Dec 29 12:55:28 2015
From: jl2012 at xbt.hk (jl2012)
Date: Tue, 29 Dec 2015 07:55:28 -0500
Subject: [bitcoin-dev] We can trivially fix quadratic CHECKSIG with a
 simple soft-fork modifying just SignatureHash()
In-Reply-To: <77DAE310-204C-4275-A791-4047798FCBFE@toom.im>
References: <20151229053559.GA8657@muck>
	<26ec8367f2a1cda066b19e0bff498711@xbt.hk>
	<77DAE310-204C-4275-A791-4047798FCBFE@toom.im>
Message-ID: <777b112833eb55ae99af8cacaf0e3b5a@xbt.hk>

What if someone complains? We can't even tell whether a complaint is 
legit or just trolling. That's why I think we need some general 
consensus rules which is not written in code, but as a social contract. 
Breaking those rules would be considered as a hardfork and is allowed 
only in exceptional situation.

Jonathan Toomim via bitcoin-dev ? 2015-12-29 07:42 ??:
> That sounds like a rather unlikely scenario. Unless you have a
> specific reason to suspect that might be the case, I think we don't
> need to worry about it too much. If we announce the intention to
> perform such a soft fork a couple of months before the soft fork
> becomes active, and if nobody complains about it destroying their
> secret stash, then I think that's fair enough and we could proceed.
> 
> On Dec 28, 2015, at 11:47 PM, jl2012 via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org> wrote:
> 
>> Do we need to consider that someone may have a timelocked big tx, with 
>> private key lost?
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


From j at toom.im  Tue Dec 29 13:00:45 2015
From: j at toom.im (Jonathan Toomim)
Date: Tue, 29 Dec 2015 05:00:45 -0800
Subject: [bitcoin-dev] We can trivially fix quadratic CHECKSIG with a
	simple soft-fork modifying just SignatureHash()
In-Reply-To: <777b112833eb55ae99af8cacaf0e3b5a@xbt.hk>
References: <20151229053559.GA8657@muck>
	<26ec8367f2a1cda066b19e0bff498711@xbt.hk>
	<77DAE310-204C-4275-A791-4047798FCBFE@toom.im>
	<777b112833eb55ae99af8cacaf0e3b5a@xbt.hk>
Message-ID: <39BC49BA-FE1B-41F3-A423-DB1106A2A508@toom.im>

I suggest we use short-circuit evaluation. If someone complains, we figure it out as we go, maybe depending on the nature of the complaint. If nobody complains, we get it done faster.

We're humans. We have the ability to respond to novel conditions without relying on predetermined rules and algorithms. I suggest we use that ability sometimes.

On Dec 29, 2015, at 4:55 AM, jl2012 <jl2012 at xbt.hk> wrote:

> What if someone complains? We can't even tell whether a complaint is legit or just trolling. That's why I think we need some general consensus rules which is not written in code, but as a social contract. Breaking those rules would be considered as a hardfork and is allowed only in exceptional situation.
> 
> Jonathan Toomim via bitcoin-dev ? 2015-12-29 07:42 ??:
>> That sounds like a rather unlikely scenario. Unless you have a
>> specific reason to suspect that might be the case, I think we don't
>> need to worry about it too much. If we announce the intention to
>> perform such a soft fork a couple of months before the soft fork
>> becomes active, and if nobody complains about it destroying their
>> secret stash, then I think that's fair enough and we could proceed.
>> On Dec 28, 2015, at 11:47 PM, jl2012 via bitcoin-dev
>> <bitcoin-dev at lists.linuxfoundation.org> wrote:
>>> Do we need to consider that someone may have a timelocked big tx, with private key lost?
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151229/f1c4bd08/attachment.sig>

From dscotese at litmocracy.com  Tue Dec 29 18:59:58 2015
From: dscotese at litmocracy.com (Dave Scotese)
Date: Tue, 29 Dec 2015 10:59:58 -0800
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CAAre=ySPjm+cyLdBY_CZkLdfXE3OFYgECEUq05AyWfY0q1KuTQ@mail.gmail.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<4882BD35-D890-4860-9222-5C23AEB6AE89@mattcorallo.com>
	<CAAcC9yspsPs3gbumS4rTOg-P-=V=tycn2Z1nVPGGHwJ-nP+PBg@mail.gmail.com>
	<20151220044450.GA23942@muck>
	<CAP3QyGJD3SaM6Bvvw66jAvVFkQhrfJfRQTxbbe8a=O1zK_P6tw@mail.gmail.com>
	<20151228191228.GC12298@muck>
	<CAAre=ySPjm+cyLdBY_CZkLdfXE3OFYgECEUq05AyWfY0q1KuTQ@mail.gmail.com>
Message-ID: <CAGLBAhdqKLgK09s5Mp6C4nv0k4hHBYM5c8NpgP5G7J110NseqQ@mail.gmail.com>

There have been no decent objections to altering the block-selection
mechanism (when two block solutions appear at nearly the same time) as
described at

http://bitcoin.stackexchange.com/questions/39226

Key components are:

   - Compute BitcoinDaysDestroyed using only transactions that have been in
   your mempool for some time as oBTCDD ("old BTCDD").
   - Use "nearly the same time" to mean separated in time by your guess of
   the average duration of block propagation times.
   - When two block solutions come in at nearly the same time, build on the
   one that has the most oBTCDD, rather than the one that came in first.

The goal of this change is to reduce the profitability of withholding block
solutions by severely reducing the chances that a block solved a while ago
can orphan one solved recently.  "Came in first" seems more easily gamed
than "most oBTCDD".  As I wrote there, "*old coins* is always a dwindling
resource and *global nodes willing to help cheat* is probably a growing
one."

I will write a BIP if anyone agrees it's a good idea.

On Mon, Dec 28, 2015 at 12:26 PM, Ivan Brightly via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> On Mon, Dec 28, 2015 at 2:12 PM, Peter Todd via bitcoin-dev <
>> bitcoin-dev at lists.linuxfoundation.org> wrote:
>> Far more concerning is network propagation effects between large and
>> small miners. For that class of issues, if you are in an environemnt
>> where selfish mining is possible - a fairly flat, easily DoS/sybil
>> attacked network topology - the profitability difference between small
>> and large miners even *without* attacks going on is a hugely worrying
>> problem. OTOH, if you're blocksize is small enough that propagation time
>> is negligable to profitability, then selfish mining attacks with <30%
>> hashing power aren't much of a concern - they'll be naturally defeated
>> by anti-DoS/anti-sybil measures.
>>
>
> Let's agree that one factor in mining profitability is bandwidth/network
> reliability/stability. Why focus on that vs electricity contracts or
> vertically integrated chip manufacturers? Surely, sufficient network
> bandwidth is a more broadly available commodity than <$0.02/kwh
> electricity, for example. I'm not sure that your stranded hydroelectric
> miner is any more desirable than thousands of dorm room miners with access
> to 10gbit university connections and free electricity.
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>


-- 
I like to provide some work at no charge to prove my value. Do you need a
techie?
I own Litmocracy <http://www.litmocracy.com> and Meme Racing
<http://www.memeracing.net> (in alpha).
I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which
now accepts Bitcoin.
I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
"He ought to find it more profitable to play by the rules" - Satoshi
Nakamoto
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151229/654898a2/attachment.html>

From j at toom.im  Tue Dec 29 19:08:16 2015
From: j at toom.im (Jonathan Toomim)
Date: Tue, 29 Dec 2015 11:08:16 -0800
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CAGLBAhdqKLgK09s5Mp6C4nv0k4hHBYM5c8NpgP5G7J110NseqQ@mail.gmail.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<4882BD35-D890-4860-9222-5C23AEB6AE89@mattcorallo.com>
	<CAAcC9yspsPs3gbumS4rTOg-P-=V=tycn2Z1nVPGGHwJ-nP+PBg@mail.gmail.com>
	<20151220044450.GA23942@muck>
	<CAP3QyGJD3SaM6Bvvw66jAvVFkQhrfJfRQTxbbe8a=O1zK_P6tw@mail.gmail.com>
	<20151228191228.GC12298@muck>
	<CAAre=ySPjm+cyLdBY_CZkLdfXE3OFYgECEUq05AyWfY0q1KuTQ@mail.gmail.com>
	<CAGLBAhdqKLgK09s5Mp6C4nv0k4hHBYM5c8NpgP5G7J110NseqQ@mail.gmail.com>
Message-ID: <386A5974-2368-4CA5-93BC-0DD7D5DA9D3D@toom.im>

Ultimately, a self-interested miner will chose to build on the block that leaves the most transaction fees up for grabs. (This usually means the smallest block.) It's an interesting question whether the default behavior for Core should be the rational behavior (build on the "smallest" block in terms of fees) or some other supposedly altruistic behavior (most BTCDD). This also applies to the decision of the "same time" threshold -- a selfish miner will not care if the blocks arrived at about the same time or not.

I currently do not have a strong opinion on what that behavior should be, although if the blocksize limit were increased substantially, I may prefer the selfish behavior because it ends up also being fail-safe (punishes selfish mining using large blocks or fee-stealing attempts).


On Dec 29, 2015, at 10:59 AM, Dave Scotese via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:

> There have been no decent objections to altering the block-selection mechanism (when two block solutions appear at nearly the same time) as described at
> 
> http://bitcoin.stackexchange.com/questions/39226
> 
> Key components are:
> Compute BitcoinDaysDestroyed using only transactions that have been in your mempool for some time as oBTCDD ("old BTCDD").
> Use "nearly the same time" to mean separated in time by your guess of the average duration of block propagation times.
> When two block solutions come in at nearly the same time, build on the one that has the most oBTCDD, rather than the one that came in first.
> The goal of this change is to reduce the profitability of withholding block solutions by severely reducing the chances that a block solved a while ago can orphan one solved recently.  "Came in first" seems more easily gamed than "most oBTCDD".  As I wrote there, "old coins is always a dwindling resource and global nodes willing to help cheat is probably a growing one."
> 
> I will write a BIP if anyone agrees it's a good idea.
> 
> 
> On Mon, Dec 28, 2015 at 12:26 PM, Ivan Brightly via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
> On Mon, Dec 28, 2015 at 2:12 PM, Peter Todd via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
> Far more concerning is network propagation effects between large and
> small miners. For that class of issues, if you are in an environemnt
> where selfish mining is possible - a fairly flat, easily DoS/sybil
> attacked network topology - the profitability difference between small
> and large miners even *without* attacks going on is a hugely worrying
> problem. OTOH, if you're blocksize is small enough that propagation time
> is negligable to profitability, then selfish mining attacks with <30%
> hashing power aren't much of a concern - they'll be naturally defeated
> by anti-DoS/anti-sybil measures.
> 
> Let's agree that one factor in mining profitability is bandwidth/network reliability/stability. Why focus on that vs electricity contracts or vertically integrated chip manufacturers? Surely, sufficient network bandwidth is a more broadly available commodity than <$0.02/kwh electricity, for example. I'm not sure that your stranded hydroelectric miner is any more desirable than thousands of dorm room miners with access to 10gbit university connections and free electricity.
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 
> 
> 
> 
> --
> I like to provide some work at no charge to prove my value. Do you need a techie?
> I own Litmocracy and Meme Racing (in alpha).
> I'm the webmaster for The Voluntaryist which now accepts Bitcoin.
> I also code for The Dollar Vigilante.
> "He ought to find it more profitable to play by the rules" - Satoshi Nakamoto
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151229/a62c256a/attachment-0001.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151229/a62c256a/attachment-0001.sig>

From dscotese at litmocracy.com  Tue Dec 29 21:51:29 2015
From: dscotese at litmocracy.com (Dave Scotese)
Date: Tue, 29 Dec 2015 13:51:29 -0800
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CAJfRnm6akNXAQkXtPNu_bVFA7uuDUmeQ6L9oONq06Jo7r=wMmA@mail.gmail.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<4882BD35-D890-4860-9222-5C23AEB6AE89@mattcorallo.com>
	<CAAcC9yspsPs3gbumS4rTOg-P-=V=tycn2Z1nVPGGHwJ-nP+PBg@mail.gmail.com>
	<20151220044450.GA23942@muck>
	<CAP3QyGJD3SaM6Bvvw66jAvVFkQhrfJfRQTxbbe8a=O1zK_P6tw@mail.gmail.com>
	<20151228191228.GC12298@muck>
	<CAAre=ySPjm+cyLdBY_CZkLdfXE3OFYgECEUq05AyWfY0q1KuTQ@mail.gmail.com>
	<CAGLBAhdqKLgK09s5Mp6C4nv0k4hHBYM5c8NpgP5G7J110NseqQ@mail.gmail.com>
	<CAJfRnm6akNXAQkXtPNu_bVFA7uuDUmeQ6L9oONq06Jo7r=wMmA@mail.gmail.com>
Message-ID: <CAGLBAhfaOLNKSjM+6878j_3kYfbg8ZP+MqiHxUTmVKi48YHg6Q@mail.gmail.com>

It cannot possibly be enforced.  Enforcement is not important when you're
setting defaults.  In fact, you don't want to enforce defaults, but rather
allow anyone who cares to deviate from them to do so.

The importance of default behavior is proportional to the number of folks
who mess with the defaults, and that, among miners, is pretty small as far
as I know, at least in the area of deciding how to decide which block to
build on when two show up at nearly the same time.

On Tue, Dec 29, 2015 at 11:25 AM, Allen Piscitello <
allen.piscitello at gmail.com> wrote:

> How could this possibly be enforced?
>
> On Tue, Dec 29, 2015 at 12:59 PM, Dave Scotese via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> There have been no decent objections to altering the block-selection
>> mechanism (when two block solutions appear at nearly the same time) as
>> described at
>>
>> http://bitcoin.stackexchange.com/questions/39226
>>
>> Key components are:
>>
>>    - Compute BitcoinDaysDestroyed using only transactions that have been
>>    in your mempool for some time as oBTCDD ("old BTCDD").
>>    - Use "nearly the same time" to mean separated in time by your guess
>>    of the average duration of block propagation times.
>>    - When two block solutions come in at nearly the same time, build on
>>    the one that has the most oBTCDD, rather than the one that came in first.
>>
>> The goal of this change is to reduce the profitability of withholding
>> block solutions by severely reducing the chances that a block solved a
>> while ago can orphan one solved recently.  "Came in first" seems more
>> easily gamed than "most oBTCDD".  As I wrote there, "*old coins* is
>> always a dwindling resource and *global nodes willing to help cheat* is
>> probably a growing one."
>>
>> I will write a BIP if anyone agrees it's a good idea.
>>
>> On Mon, Dec 28, 2015 at 12:26 PM, Ivan Brightly via bitcoin-dev <
>> bitcoin-dev at lists.linuxfoundation.org> wrote:
>>
>>> On Mon, Dec 28, 2015 at 2:12 PM, Peter Todd via bitcoin-dev <
>>>> bitcoin-dev at lists.linuxfoundation.org> wrote:
>>>> Far more concerning is network propagation effects between large and
>>>> small miners. For that class of issues, if you are in an environemnt
>>>> where selfish mining is possible - a fairly flat, easily DoS/sybil
>>>> attacked network topology - the profitability difference between small
>>>> and large miners even *without* attacks going on is a hugely worrying
>>>> problem. OTOH, if you're blocksize is small enough that propagation time
>>>> is negligable to profitability, then selfish mining attacks with <30%
>>>> hashing power aren't much of a concern - they'll be naturally defeated
>>>> by anti-DoS/anti-sybil measures.
>>>>
>>>
>>> Let's agree that one factor in mining profitability is bandwidth/network
>>> reliability/stability. Why focus on that vs electricity contracts or
>>> vertically integrated chip manufacturers? Surely, sufficient network
>>> bandwidth is a more broadly available commodity than <$0.02/kwh
>>> electricity, for example. I'm not sure that your stranded hydroelectric
>>> miner is any more desirable than thousands of dorm room miners with access
>>> to 10gbit university connections and free electricity.
>>>
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev at lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>>>
>>
>>
>> --
>> I like to provide some work at no charge to prove my value. Do you need a
>> techie?
>> I own Litmocracy <http://www.litmocracy.com> and Meme Racing
>> <http://www.memeracing.net> (in alpha).
>> I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com>
>> which now accepts Bitcoin.
>> I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
>> "He ought to find it more profitable to play by the rules" - Satoshi
>> Nakamoto
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>


-- 
I like to provide some work at no charge to prove my value. Do you need a
techie?
I own Litmocracy <http://www.litmocracy.com> and Meme Racing
<http://www.memeracing.net> (in alpha).
I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which
now accepts Bitcoin.
I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
"He ought to find it more profitable to play by the rules" - Satoshi
Nakamoto
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151229/35a0b842/attachment.html>

From allen.piscitello at gmail.com  Tue Dec 29 19:25:17 2015
From: allen.piscitello at gmail.com (Allen Piscitello)
Date: Tue, 29 Dec 2015 13:25:17 -0600
Subject: [bitcoin-dev] We need to fix the block withholding attack
In-Reply-To: <CAGLBAhdqKLgK09s5Mp6C4nv0k4hHBYM5c8NpgP5G7J110NseqQ@mail.gmail.com>
References: <20151219184240.GB12893@muck>
	<CAAcC9yvh2ma2dFhNDEKs7vfXyQF9L+T0YtRvOsJ15AbfVti=cw@mail.gmail.com>
	<4882BD35-D890-4860-9222-5C23AEB6AE89@mattcorallo.com>
	<CAAcC9yspsPs3gbumS4rTOg-P-=V=tycn2Z1nVPGGHwJ-nP+PBg@mail.gmail.com>
	<20151220044450.GA23942@muck>
	<CAP3QyGJD3SaM6Bvvw66jAvVFkQhrfJfRQTxbbe8a=O1zK_P6tw@mail.gmail.com>
	<20151228191228.GC12298@muck>
	<CAAre=ySPjm+cyLdBY_CZkLdfXE3OFYgECEUq05AyWfY0q1KuTQ@mail.gmail.com>
	<CAGLBAhdqKLgK09s5Mp6C4nv0k4hHBYM5c8NpgP5G7J110NseqQ@mail.gmail.com>
Message-ID: <CAJfRnm6akNXAQkXtPNu_bVFA7uuDUmeQ6L9oONq06Jo7r=wMmA@mail.gmail.com>

How could this possibly be enforced?

On Tue, Dec 29, 2015 at 12:59 PM, Dave Scotese via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> There have been no decent objections to altering the block-selection
> mechanism (when two block solutions appear at nearly the same time) as
> described at
>
> http://bitcoin.stackexchange.com/questions/39226
>
> Key components are:
>
>    - Compute BitcoinDaysDestroyed using only transactions that have been
>    in your mempool for some time as oBTCDD ("old BTCDD").
>    - Use "nearly the same time" to mean separated in time by your guess
>    of the average duration of block propagation times.
>    - When two block solutions come in at nearly the same time, build on
>    the one that has the most oBTCDD, rather than the one that came in first.
>
> The goal of this change is to reduce the profitability of withholding
> block solutions by severely reducing the chances that a block solved a
> while ago can orphan one solved recently.  "Came in first" seems more
> easily gamed than "most oBTCDD".  As I wrote there, "*old coins* is
> always a dwindling resource and *global nodes willing to help cheat* is
> probably a growing one."
>
> I will write a BIP if anyone agrees it's a good idea.
>
> On Mon, Dec 28, 2015 at 12:26 PM, Ivan Brightly via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> On Mon, Dec 28, 2015 at 2:12 PM, Peter Todd via bitcoin-dev <
>>> bitcoin-dev at lists.linuxfoundation.org> wrote:
>>> Far more concerning is network propagation effects between large and
>>> small miners. For that class of issues, if you are in an environemnt
>>> where selfish mining is possible - a fairly flat, easily DoS/sybil
>>> attacked network topology - the profitability difference between small
>>> and large miners even *without* attacks going on is a hugely worrying
>>> problem. OTOH, if you're blocksize is small enough that propagation time
>>> is negligable to profitability, then selfish mining attacks with <30%
>>> hashing power aren't much of a concern - they'll be naturally defeated
>>> by anti-DoS/anti-sybil measures.
>>>
>>
>> Let's agree that one factor in mining profitability is bandwidth/network
>> reliability/stability. Why focus on that vs electricity contracts or
>> vertically integrated chip manufacturers? Surely, sufficient network
>> bandwidth is a more broadly available commodity than <$0.02/kwh
>> electricity, for example. I'm not sure that your stranded hydroelectric
>> miner is any more desirable than thousands of dorm room miners with access
>> to 10gbit university connections and free electricity.
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>
>
> --
> I like to provide some work at no charge to prove my value. Do you need a
> techie?
> I own Litmocracy <http://www.litmocracy.com> and Meme Racing
> <http://www.memeracing.net> (in alpha).
> I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com>
> which now accepts Bitcoin.
> I also code for The Dollar Vigilante <http://dollarvigilante.com/>.
> "He ought to find it more profitable to play by the rules" - Satoshi
> Nakamoto
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151229/01725358/attachment.html>

From joe2015 at openmailbox.org  Wed Dec 30 05:46:01 2015
From: joe2015 at openmailbox.org (joe2015 at openmailbox.org)
Date: Wed, 30 Dec 2015 13:46:01 +0800
Subject: [bitcoin-dev] An implementation of BIP102 as a softfork.
Message-ID: <6fc10e581a81abb76be5cd49275ebf48@openmailbox.org>

Below is a proof-of-concept implementation of BIP102 as a softfork:

https://github.com/ZoomT/bitcoin/tree/2015_2mb_blocksize
https://github.com/jgarzik/bitcoin/compare/2015_2mb_blocksize...ZoomT:2015_2mb_blocksize?diff=split&name=2015_2mb_blocksize

BIP102 is normally a hardfork.  The softfork version (unofficial
codename BIP102s) uses the idea described here:
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012073.html

The basic idea is that post-fork blocks are constructed in such a way
they can be mapped to valid blocks under the pre-fork rules.  BIP102s
is a softfork in the sense that post-fork miners are still creating a
valid chain under the old rules, albeit indirectly.

 From the POV of non-upgraded clients, BIP102s circumvents the
block-size limit by moving transaction validation data "outside" of
the block.  This is a similar trick used by Segregated Witness and
Extension Blocks (both softfork proposals).

 From the POV of upgraded clients, the block layout is unchanged,
except:
- A larger 2MB block-size limit (=BIP102);
- The header Merkle root has a new (backwards compatible)
   interpretation;
- The coinbase encodes the Merkle root of the remaining txs.
Aside from this, blocks maintain their original format, i.e. a block
header followed by a vector of transactions.  This keeps the
implementation simple, and is distinct from SW and EB.

Since BIP102s is a softfork it means that:
- A miner majority (e.g. 75%, 95%) force miner consensus (100%).  This
   is not true for a hardfork.
- Fraud risk is significantly reduced (6-conf unlikely depending on
   activation threshold).
This should address some of the concerns with deploying a block-size
increase using a hardfork.

Notes:

- The same basic idea could be adapted to any of the other proposals
   (BIP101, 2-4-8, BIP202, etc.).
- I used Jeff Garzik's BIP102 implementation which is incomplete (?).
   The activation logic is left unchanged.
- I am not a Bitcoin dev so hopefully no embarrassing mistakes in my
   code :-(

--joe


From j at toom.im  Wed Dec 30 13:29:05 2015
From: j at toom.im (Jonathan Toomim)
Date: Wed, 30 Dec 2015 05:29:05 -0800
Subject: [bitcoin-dev] An implementation of BIP102 as a softfork.
In-Reply-To: <6fc10e581a81abb76be5cd49275ebf48@openmailbox.org>
References: <6fc10e581a81abb76be5cd49275ebf48@openmailbox.org>
Message-ID: <8E12B367-1A55-435F-9244-101C09094BDA@toom.im>

As a first impression, I think this proposal is intellectually interesting, but crufty and hackish and should never actually be deployed. Writing code for Bitcoin in a future in which we have deployed a few generalized softforks this way sounds terrifying.

Instead of this:

    CTransaction GetTransaction(CBlock block, unsigned int index) {
        return block->vtx[index];
    }

We might have this:

    CTransaction GetTransaction(CBlock block, unsigned int index) {
        if (!IsBIP102sBlock(block)) {
            return block->vtx[index];
        } else {
            if (!IsOtherGeneralizedSoftforkBlock(block)) {
                // hooray! only one generalized softfork level to deal with!
                return LookupBlock(GetGSHashFromCoinbase(block->vtx[0].vin[0].scriptSig))->vtx[index];
           } else {
               throw NotImplementedError; // I'm too lazy to write pseudocode this complicated just to argue a point
        }
    }

It might be possible to make that a bit simpler with recursion, or by doing subsequent generalized softforks in a way that doesn't have multi-levels-deep block-within-a-block-within-a-block stuff. Still: ugh.




On Dec 29, 2015, at 9:46 PM, joe2015--- via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:

> Below is a proof-of-concept implementation of BIP102 as a softfork:
> 
> https://github.com/ZoomT/bitcoin/tree/2015_2mb_blocksize
> https://github.com/jgarzik/bitcoin/compare/2015_2mb_blocksize...ZoomT:2015_2mb_blocksize?diff=split&name=2015_2mb_blocksize
> 
> BIP102 is normally a hardfork.  The softfork version (unofficial
> codename BIP102s) uses the idea described here:
> http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012073.html
> 
> The basic idea is that post-fork blocks are constructed in such a way
> they can be mapped to valid blocks under the pre-fork rules.  BIP102s
> is a softfork in the sense that post-fork miners are still creating a
> valid chain under the old rules, albeit indirectly.
> 
> From the POV of non-upgraded clients, BIP102s circumvents the
> block-size limit by moving transaction validation data "outside" of
> the block.  This is a similar trick used by Segregated Witness and
> Extension Blocks (both softfork proposals).
> 
> From the POV of upgraded clients, the block layout is unchanged,
> except:
> - A larger 2MB block-size limit (=BIP102);
> - The header Merkle root has a new (backwards compatible)
>  interpretation;
> - The coinbase encodes the Merkle root of the remaining txs.
> Aside from this, blocks maintain their original format, i.e. a block
> header followed by a vector of transactions.  This keeps the
> implementation simple, and is distinct from SW and EB.
> 
> Since BIP102s is a softfork it means that:
> - A miner majority (e.g. 75%, 95%) force miner consensus (100%).  This
>  is not true for a hardfork.
> - Fraud risk is significantly reduced (6-conf unlikely depending on
>  activation threshold).
> This should address some of the concerns with deploying a block-size
> increase using a hardfork.
> 
> Notes:
> 
> - The same basic idea could be adapted to any of the other proposals
>  (BIP101, 2-4-8, BIP202, etc.).
> - I used Jeff Garzik's BIP102 implementation which is incomplete (?).
>  The activation logic is left unchanged.
> - I am not a Bitcoin dev so hopefully no embarrassing mistakes in my
>  code :-(
> 
> --joe
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/fd516c4a/attachment.sig>

From falke.marco at gmail.com  Wed Dec 30 10:33:39 2015
From: falke.marco at gmail.com (Marco Falke)
Date: Wed, 30 Dec 2015 11:33:39 +0100
Subject: [bitcoin-dev] An implementation of BIP102 as a softfork.
In-Reply-To: <6fc10e581a81abb76be5cd49275ebf48@openmailbox.org>
References: <6fc10e581a81abb76be5cd49275ebf48@openmailbox.org>
Message-ID: <CAKJqnrGUKeUb7g4SrjnWNAcPZOuLDKB-kjP2+Jy8Rdk_MfWLyQ@mail.gmail.com>

This is an interesting approach but I don't see how this is a soft
fork. (Just because something is not a hard fork, doesn't make it a
soft fork by definition)
Softforks don't require any nodes to upgrade. [1]
Nonetheless, as I understand your approach, it requires nodes to
upgrade. Otherwise they are missing all transactions but the coinbase
transactions. Thus, they cannot update their utxoset and are easily
susceptible to double spends...

Am I missing something obvious?

-- Marco


[1] https://en.bitcoin.it/wiki/Softfork#Implications

From martijn.meijering at mevs.nl  Wed Dec 30 11:16:22 2015
From: martijn.meijering at mevs.nl (Martijn Meijering)
Date: Wed, 30 Dec 2015 12:16:22 +0100
Subject: [bitcoin-dev] An implementation of BIP102 as a softfork.
Message-ID: <CAODYVYf764XafVsbnVnYgsYZtWwKu4Q3cwzL1B=GVWUFjZ5TWg@mail.gmail.com>

That looks very interesting. But is effectively blocking old clients from
seeing transactions really safe? After all, such transactions are still
confirmed on the new chain. A person might try to send a similar
transaction several times, perhaps with increasing fees in an attempt to
get it to confirm and end up paying someone several times.

Maybe we could require the tx version number to be increased as well so
transactions sent from old clients would never confirm? Perhaps your code
already includes this idea, I need to look at it more closely.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/6a96deaf/attachment.html>

From pete at petertodd.org  Wed Dec 30 14:19:55 2015
From: pete at petertodd.org (Peter Todd)
Date: Wed, 30 Dec 2015 06:19:55 -0800
Subject: [bitcoin-dev] An implementation of BIP102 as a softfork.
In-Reply-To: <8E12B367-1A55-435F-9244-101C09094BDA@toom.im>
References: <6fc10e581a81abb76be5cd49275ebf48@openmailbox.org>
	<8E12B367-1A55-435F-9244-101C09094BDA@toom.im>
Message-ID: <20151230141955.GA15588@muck>

On Wed, Dec 30, 2015 at 05:29:05AM -0800, Jonathan Toomim via bitcoin-dev wrote:
> As a first impression, I think this proposal is intellectually interesting, but crufty and hackish and should never actually be deployed. Writing code for Bitcoin in a future in which we have deployed a few generalized softforks this way sounds terrifying.

<snip>

> It might be possible to make that a bit simpler with recursion, or by doing subsequent generalized softforks in a way that doesn't have multi-levels-deep block-within-a-block-within-a-block stuff. Still: ugh.

Your fear is misplaced: it's trivial to avoid recursion with a bit of
planning.

For instance, if Bitcoin was redesigned to incorporate the forced fork
concept, instead of block headers committing to just a merkle root,
they could instead commit to H(version + digest)

For version == 0, digest would be a merkle root of all transactions. If
the version was > 0, any digest would be allowed and the block would be
interpreted as a NOP with no effect on the UTXO set.

In the event of a major change - e.g. what would otherwise be a
hard-forking change to the way the merkle root was calculated - a
soft-fork would change the block validity rules to make version == 0
invalid, and verison == 1 blocks would interpret the digest according to
the new merkle root rules. Again, version > 1 blocks would be treated as
NOPs.

A good exercise is to apply the above to the existing Bitcoin ecosystem
as a soft-fork - it certainely can be done, and done right is
technically very simple.


Regardless of how it's done - existing Bitcoin compatible or clean sheet
redesign - you get the significant safety advantages soft-forks have
over hard-forks in nearly all situations where you'd have to do a
hard-fork. OTOH, it's kinda scary how this institutionalizes what could
be seen as 51% attacks, possibly giving miners significantly more
control over the system politically. I'm not sure I agree with that
viewpoint - miners can do this anyway - but that has made people shy
away from promoting this idea in the past. (previously it's been often
referred to as an "evil" soft-fork)

-- 
'peter'[:-1]@petertodd.org
00000000000000000831fc2554d9370aeba2701fff09980123d24a615eee7416
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 650 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/38a95928/attachment.sig>

From pete at petertodd.org  Wed Dec 30 14:28:37 2015
From: pete at petertodd.org (Peter Todd)
Date: Wed, 30 Dec 2015 06:28:37 -0800
Subject: [bitcoin-dev] An implementation of BIP102 as a softfork.
In-Reply-To: <CAODYVYf764XafVsbnVnYgsYZtWwKu4Q3cwzL1B=GVWUFjZ5TWg@mail.gmail.com>
References: <CAODYVYf764XafVsbnVnYgsYZtWwKu4Q3cwzL1B=GVWUFjZ5TWg@mail.gmail.com>
Message-ID: <20151230142836.GA19507@muck>

On Wed, Dec 30, 2015 at 12:16:22PM +0100, Martijn Meijering via bitcoin-dev wrote:
> That looks very interesting. But is effectively blocking old clients from
> seeing transactions really safe? After all, such transactions are still
> confirmed on the new chain. A person might try to send a similar
> transaction several times, perhaps with increasing fees in an attempt to
> get it to confirm and end up paying someone several times.

It's very dangerous to simply send multiple transactions in such a way
that they don't double-spend each other; you have no good way of knowing
for sure that you're seeing the longest block chain with software alone.

Competently designed software with fee-bumping wouldn't allow that
mistake to be made; the UX should make it clear that txs sent are still
pending until confirmed or clearly double-spent.

> Maybe we could require the tx version number to be increased as well so
> transactions sent from old clients would never confirm? Perhaps your code
> already includes this idea, I need to look at it more closely.

That can mess up pre-signed transations, e.g. refunds.

-- 
'peter'[:-1]@petertodd.org
00000000000000000831fc2554d9370aeba2701fff09980123d24a615eee7416
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 650 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/43ec3a9c/attachment.sig>

From pete at petertodd.org  Wed Dec 30 14:31:37 2015
From: pete at petertodd.org (Peter Todd)
Date: Wed, 30 Dec 2015 06:31:37 -0800
Subject: [bitcoin-dev] An implementation of BIP102 as a softfork.
In-Reply-To: <20151230141955.GA15588@muck>
References: <6fc10e581a81abb76be5cd49275ebf48@openmailbox.org>
	<8E12B367-1A55-435F-9244-101C09094BDA@toom.im>
	<20151230141955.GA15588@muck>
Message-ID: <20151230143137.GB19507@muck>

On Wed, Dec 30, 2015 at 06:19:55AM -0800, Peter Todd via bitcoin-dev wrote:
> On Wed, Dec 30, 2015 at 05:29:05AM -0800, Jonathan Toomim via bitcoin-dev wrote:
> > As a first impression, I think this proposal is intellectually interesting, but crufty and hackish and should never actually be deployed. Writing code for Bitcoin in a future in which we have deployed a few generalized softforks this way sounds terrifying.
> 
> <snip>
> 
> > It might be possible to make that a bit simpler with recursion, or by doing subsequent generalized softforks in a way that doesn't have multi-levels-deep block-within-a-block-within-a-block stuff. Still: ugh.
> 
> Your fear is misplaced: it's trivial to avoid recursion with a bit of
> planning.
> 
> For instance, if Bitcoin was redesigned to incorporate the forced fork

Actually, a better name is probably "forced soft-fork", making this
clear we're using the soft-fork mechanism to force everyone to upgrade.

-- 
'peter'[:-1]@petertodd.org
00000000000000000831fc2554d9370aeba2701fff09980123d24a615eee7416
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 650 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/a075a180/attachment-0001.sig>

From j at toom.im  Wed Dec 30 15:00:16 2015
From: j at toom.im (Jonathan Toomim)
Date: Wed, 30 Dec 2015 07:00:16 -0800
Subject: [bitcoin-dev] An implementation of BIP102 as a softfork.
In-Reply-To: <20151230141955.GA15588@muck>
References: <6fc10e581a81abb76be5cd49275ebf48@openmailbox.org>
	<8E12B367-1A55-435F-9244-101C09094BDA@toom.im>
	<20151230141955.GA15588@muck>
Message-ID: <D5581FB0-1F3A-4D9B-AF98-40AB7AD939F5@toom.im>


On Dec 30, 2015, at 6:19 AM, Peter Todd <pete at petertodd.org> wrote:

> Your fear is misplaced: it's trivial to avoid recursion with a bit of
> planning...

That makes some sense. I downgrade my emotions from "a future in which we have deployed a few generalized softforks this way sounds terrifying" to "the idea of a future in which we have deployed at least one generalized softfork this way gives me the heebie jeebies."
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/5c219060/attachment.sig>

From joe2015 at openmailbox.org  Wed Dec 30 16:27:50 2015
From: joe2015 at openmailbox.org (joe2015 at openmailbox.org)
Date: Thu, 31 Dec 2015 00:27:50 +0800
Subject: [bitcoin-dev] An implementation of BIP102 as a softfork.
In-Reply-To: <CAKJqnrGUKeUb7g4SrjnWNAcPZOuLDKB-kjP2+Jy8Rdk_MfWLyQ@mail.gmail.com>
References: <6fc10e581a81abb76be5cd49275ebf48@openmailbox.org>
	<CAKJqnrGUKeUb7g4SrjnWNAcPZOuLDKB-kjP2+Jy8Rdk_MfWLyQ@mail.gmail.com>
Message-ID: <814e1ba765445a4c3b7364c471299393@openmailbox.org>

On 2015-12-30 18:33, Marco Falke wrote:
> This is an interesting approach but I don't see how this is a soft
> fork. (Just because something is not a hard fork, doesn't make it a
> soft fork by definition)
> Softforks don't require any nodes to upgrade. [1]
> Nonetheless, as I understand your approach, it requires nodes to
> upgrade. Otherwise they are missing all transactions but the coinbase
> transactions. Thus, they cannot update their utxoset and are easily
> susceptible to double spends...
> 
> Am I missing something obvious?
> 
> -- Marco
> 
> 
> [1] https://en.bitcoin.it/wiki/Softfork#Implications

It just depends how you define "softfork".  In my original write-up I 
called it a "generalized" softfork, Peter suggested a "firm" fork, and 
there are some suggestions for other names.  Ultimately what you call it 
is not very important.

--joe.

From marcel at jamin.net  Wed Dec 30 13:57:08 2015
From: marcel at jamin.net (Marcel Jamin)
Date: Wed, 30 Dec 2015 14:57:08 +0100
Subject: [bitcoin-dev] An implementation of BIP102 as a softfork.
In-Reply-To: <8E12B367-1A55-435F-9244-101C09094BDA@toom.im>
References: <6fc10e581a81abb76be5cd49275ebf48@openmailbox.org>
	<8E12B367-1A55-435F-9244-101C09094BDA@toom.im>
Message-ID: <CAAUq484dsvTSKTazj=J0zaqJxNQkSVyBS9H8pGZzDNyKhw_eVQ@mail.gmail.com>

I guess the same could be said about the softfork flavoured SW
implementation. In any case, the strategy pattern helps with code structure
in situations like this.

2015-12-30 14:29 GMT+01:00 Jonathan Toomim via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org>:

> As a first impression, I think this proposal is intellectually
> interesting, but crufty and hackish and should never actually be deployed.
> Writing code for Bitcoin in a future in which we have deployed a few
> generalized softforks this way sounds terrifying.
>
> Instead of this:
>
>     CTransaction GetTransaction(CBlock block, unsigned int index) {
>         return block->vtx[index];
>     }
>
> We might have this:
>
>     CTransaction GetTransaction(CBlock block, unsigned int index) {
>         if (!IsBIP102sBlock(block)) {
>             return block->vtx[index];
>         } else {
>             if (!IsOtherGeneralizedSoftforkBlock(block)) {
>                 // hooray! only one generalized softfork level to deal
> with!
>                 return
> LookupBlock(GetGSHashFromCoinbase(block->vtx[0].vin[0].scriptSig))->vtx[index];
>            } else {
>                throw NotImplementedError; // I'm too lazy to write
> pseudocode this complicated just to argue a point
>         }
>     }
>
> It might be possible to make that a bit simpler with recursion, or by
> doing subsequent generalized softforks in a way that doesn't have
> multi-levels-deep block-within-a-block-within-a-block stuff. Still: ugh.
>
>
>
>
> On Dec 29, 2015, at 9:46 PM, joe2015--- via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
> > Below is a proof-of-concept implementation of BIP102 as a softfork:
> >
> > https://github.com/ZoomT/bitcoin/tree/2015_2mb_blocksize
> >
> https://github.com/jgarzik/bitcoin/compare/2015_2mb_blocksize...ZoomT:2015_2mb_blocksize?diff=split&name=2015_2mb_blocksize
> >
> > BIP102 is normally a hardfork.  The softfork version (unofficial
> > codename BIP102s) uses the idea described here:
> >
> http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012073.html
> >
> > The basic idea is that post-fork blocks are constructed in such a way
> > they can be mapped to valid blocks under the pre-fork rules.  BIP102s
> > is a softfork in the sense that post-fork miners are still creating a
> > valid chain under the old rules, albeit indirectly.
> >
> > From the POV of non-upgraded clients, BIP102s circumvents the
> > block-size limit by moving transaction validation data "outside" of
> > the block.  This is a similar trick used by Segregated Witness and
> > Extension Blocks (both softfork proposals).
> >
> > From the POV of upgraded clients, the block layout is unchanged,
> > except:
> > - A larger 2MB block-size limit (=BIP102);
> > - The header Merkle root has a new (backwards compatible)
> >  interpretation;
> > - The coinbase encodes the Merkle root of the remaining txs.
> > Aside from this, blocks maintain their original format, i.e. a block
> > header followed by a vector of transactions.  This keeps the
> > implementation simple, and is distinct from SW and EB.
> >
> > Since BIP102s is a softfork it means that:
> > - A miner majority (e.g. 75%, 95%) force miner consensus (100%).  This
> >  is not true for a hardfork.
> > - Fraud risk is significantly reduced (6-conf unlikely depending on
> >  activation threshold).
> > This should address some of the concerns with deploying a block-size
> > increase using a hardfork.
> >
> > Notes:
> >
> > - The same basic idea could be adapted to any of the other proposals
> >  (BIP101, 2-4-8, BIP202, etc.).
> > - I used Jeff Garzik's BIP102 implementation which is incomplete (?).
> >  The activation logic is left unchanged.
> > - I am not a Bitcoin dev so hopefully no embarrassing mistakes in my
> >  code :-(
> >
> > --joe
> >
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev at lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/5c2838ea/attachment.html>

From tomas at tomasvdw.nl  Wed Dec 30 16:35:17 2015
From: tomas at tomasvdw.nl (Tomas)
Date: Wed, 30 Dec 2015 17:35:17 +0100
Subject: [bitcoin-dev]  [BIP Draft] Decentralized Improvement Proposals
Message-ID: <1451493317.3215816.479282618.4F666D71@webmail.messagingengine.com>

In an attempt to reduce developer centralization, and to reduce the risk
of forks introduced by implementation other than bitcoin-core, I have
drafted a BIP to support changes to the protocol from different
implementations.

The BIP can be found at:
https://github.com/tomasvdw/bips/blob/master/decentralized-improvement-proposals.mediawiki

I believe this BIP could mitigate the risk of forks, and decentralize
the development of the protocol.

If you consider the proposal worthy of discussion, please assign a
BIP-number.

Regards,
Tomas van der Wansem

From luke at dashjr.org  Wed Dec 30 17:10:25 2015
From: luke at dashjr.org (Luke Dashjr)
Date: Wed, 30 Dec 2015 17:10:25 +0000
Subject: [bitcoin-dev] [BIP Draft] Decentralized Improvement Proposals
In-Reply-To: <1451493317.3215816.479282618.4F666D71@webmail.messagingengine.com>
References: <1451493317.3215816.479282618.4F666D71@webmail.messagingengine.com>
Message-ID: <201512301710.27154.luke@dashjr.org>

On Wednesday, December 30, 2015 4:35:17 PM Tomas via bitcoin-dev wrote:
> In an attempt to reduce developer centralization, and to reduce the risk
> of forks introduced by implementation other than bitcoin-core, I have
> drafted a BIP to support changes to the protocol from different
> implementations.

The premises in Motivation are false. BIPs are required to have a reference 
implementation, but that implementation need not necessarily be for Bitcoin 
Core specifically.

The specification itself looks like an inefficient and bloaty reinvention of 
version bits.

Luke

From digitsu at gmail.com  Wed Dec 30 17:58:44 2015
From: digitsu at gmail.com (David Chan)
Date: Thu, 31 Dec 2015 02:58:44 +0900
Subject: [bitcoin-dev] Generalized soft forks
Message-ID: <0DD6A08C-897A-49D6-BDDE-00C6B94DB281@gmail.com>

Please forgive the perhaps pedantic question but in the referred document
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012073.html
It talks about how a soft fork with >50% support will doom the other fork to being orphaned eventually but that hard forks could persist forever.  I fail to see why the same logic that one side having the majority will eventually win wouldn't also apply to hard forks.  

Additionally it seems to me that a notable difference between a generalized soft fork as described and a hard fork majority is in the process by which they force the other fork to be orphaned. In a hard fork an unupgraded node would know you were in a forking situation due to your node getting a lot of blocks from the other fork and having to reject them (because they are invalid) whereas in a generalized soft fork you wouldn't know there was a fork going on so there would be less of an impetus to upgrade.  Of course the downside of the hard fork is that the losing side would potentially lose money in the orphaned chain, but presumably this discussion of generalized soft forks is with regards to non-mining nodes so it shouldn't come into consideration. 

In fact if an non-upgraded miner were to start mining on top of that block which they cannot actually fully validate essentially this condones mining without verification (and trusting that others which have upgraded nodes to have validated the txns for you) as this situation can continue for a prolonged period of time does this not hurt network security ?



>> On 2015/12/31, at 1:27, joe2015--- via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
>> 
>> On 2015-12-30 18:33, Marco Falke wrote:
>> This is an interesting approach but I don't see how this is a soft
>> fork. (Just because something is not a hard fork, doesn't make it a
>> soft fork by definition)
>> Softforks don't require any nodes to upgrade. [1]
>> Nonetheless, as I understand your approach, it requires nodes to
>> upgrade. Otherwise they are missing all transactions but the coinbase
>> transactions. Thus, they cannot update their utxoset and are easily
>> susceptible to double spends...
>> Am I missing something obvious?
>> -- Marco
>> [1] https://en.bitcoin.it/wiki/Softfork#Implications
> 
> It just depends how you define "softfork".  In my original write-up I called it a "generalized" softfork, Peter suggested a "firm" fork, and there are some suggestions for other names.  Ultimately what you call it is not very important.
> 
> --joe.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151231/4526c922/attachment-0001.html>

From tomas at tomasvdw.nl  Wed Dec 30 18:22:59 2015
From: tomas at tomasvdw.nl (Tomas)
Date: Wed, 30 Dec 2015 19:22:59 +0100
Subject: [bitcoin-dev] [BIP Draft] Decentralized Improvement Proposals
In-Reply-To: <201512301710.27154.luke@dashjr.org>
References: <1451493317.3215816.479282618.4F666D71@webmail.messagingengine.com>
	<201512301710.27154.luke@dashjr.org>
Message-ID: <1451499779.3919416.479357794.2C21BFA1@webmail.messagingengine.com>


> The specification itself looks like an inefficient and bloaty reinvention
> of 
> version bits.

The actual assignment of version bits isn't clear from the
specification. Are you saying that any implementation that wants to
propose a change is encouraged to pick a free version bit and use it?

Furthermore, my proposal addresses the danger of forward-incompatible
changes; a hard-fork can no longer occur as every implementation will
agree on the active the set of rules even if it has not implemented
them. This seems to be lacking in the version bits proposal.

Tomas

From bob_bitcoin at mcelrath.org  Wed Dec 30 19:00:43 2015
From: bob_bitcoin at mcelrath.org (Bob McElrath)
Date: Wed, 30 Dec 2015 19:00:43 +0000
Subject: [bitcoin-dev] Increasing the blocksize as a (generalized)
 softfork.
In-Reply-To: <f9ad1348fb7dedca35b594782fee7e0f@openmailbox.org>
References: <1bf64a5b514d57ca37744ae5f5238149@openmailbox.org>
	<e170f3a10164019824edaafe5a04f067@xbt.hk>
	<f9ad1348fb7dedca35b594782fee7e0f@openmailbox.org>
Message-ID: <20151230190043.GJ18200@mcelrath.org>

joe2015--- via bitcoin-dev [bitcoin-dev at lists.linuxfoundation.org] wrote:
> That's the whole point.  After a conventional hardfork everyone
> needs to upgrade, but there is no way to force users to upgrade.  A
> user who is simply unaware of the fork, or disagrees with the fork,
> uses the old client and the currency splits.
> 
> Under this proposal old clients effectively enter "zombie" mode,
> forcing users to upgrade.

This is a very complex way to enter zombie mode.

A simpler way is to track valid PoW chains by examining only the header, that
are rejected for other reasons.

Once a chain is seen to be 6 or more blocks ahead of my chain tip, we should
enter "zombie mode" and refuse to mine or relay, and alert the operator, because
we don't know what we're doing and we're out of date.  This way doesn't require
any modifications to block structure at all.

--
Cheers, Bob McElrath

"For every complex problem, there is a solution that is simple, neat, and wrong."
    -- H. L. Mencken 


From el33th4x0r at gmail.com  Wed Dec 30 20:08:36 2015
From: el33th4x0r at gmail.com (=?UTF-8?Q?Emin_G=C3=BCn_Sirer?=)
Date: Wed, 30 Dec 2015 15:08:36 -0500
Subject: [bitcoin-dev] How to preserve the value of coins after a fork.
Message-ID: <CAPkFh0tj4cXYuk8=8QJOP5z4qea6bv_sELhkfHO6nU16mMnnZA@mail.gmail.com>

Ittay Eyal and I just put together a writeup that we're informally calling
Bitcoin-United for preserving the value of coins following a permanent fork:


http://hackingdistributed.com/2015/12/30/technique-to-unite-bitcoin-factions/

Half of the core idea is to eliminate double-spends (where someone spends a
UTXO on chain A and the same UTXO on chain B, at separate merchants) by
placing transactions from A on chain B, and by taking the intersection of
transactions on chain A and chain B when considering whether a payment has
been received.

The other half of the core idea is to enable minting of new coins and
collection of mining fees on both chains, while preserving the 21M maximum.
This is achieved by creating a one-to-one correspondence between coins on
one chain with coins on the other.

Given the level of the audience here, I'm keeping the description quite
terse. Much more detail and discussion is at the link above, as well as the
assumptions that need to hold for Bitcoin-United.

The high bit is that, with a few modest assumptions, it is possible to
create a cohesive coin in the aftermath of a fork, even if the core devs
are split, and even if one of the forks is (in the worst case) completely
non-cooperative. Bitcoin-United is a trick to create a cohesive coin even
when there is no consensus at the lowest level.

Bitcoin-United opens up a lot of new, mostly game-theoretic questions: what
happens to native clients who prefer A or B? What will happen to the value
of native-A or native-B coins? And so on.

We're actively working on these questions and more, but we wanted to share
the Bitcoin-United idea, mainly to receive feedback, and partly to provide
some hope about future consensus to the community. It turns out that it is
possible to craft consensus at the network level even when there isn't one
at the developer level.

Happy New Year, and may 2016 be united,
- egs & ittay
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/4b322fe8/attachment.html>

From pete at petertodd.org  Wed Dec 30 20:16:12 2015
From: pete at petertodd.org (Peter Todd)
Date: Wed, 30 Dec 2015 20:16:12 +0000
Subject: [bitcoin-dev] How to preserve the value of coins after a fork.
In-Reply-To: <CAPkFh0tj4cXYuk8=8QJOP5z4qea6bv_sELhkfHO6nU16mMnnZA@mail.gmail.com>
References: <CAPkFh0tj4cXYuk8=8QJOP5z4qea6bv_sELhkfHO6nU16mMnnZA@mail.gmail.com>
Message-ID: <6741032F-757F-4D9E-A722-3A62271B6BFD@petertodd.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

Note how transaction malleability can quickly sabotage naive notions of this idea.

Equally, if this looks like it might ever be implemented, rather than using a hard fork, using a forced soft-fork to deploy changes becomes attractive.


On 30 December 2015 12:08:36 GMT-08:00, "Emin G?n Sirer via bitcoin-dev" <bitcoin-dev at lists.linuxfoundation.org> wrote:
>Ittay Eyal and I just put together a writeup that we're informally
>calling
>Bitcoin-United for preserving the value of coins following a permanent
>fork:
>
>
>http://hackingdistributed.com/2015/12/30/technique-to-unite-bitcoin-factions/
>
>Half of the core idea is to eliminate double-spends (where someone
>spends a
>UTXO on chain A and the same UTXO on chain B, at separate merchants) by
>placing transactions from A on chain B, and by taking the intersection
>of
>transactions on chain A and chain B when considering whether a payment
>has
>been received.
>
>The other half of the core idea is to enable minting of new coins and
>collection of mining fees on both chains, while preserving the 21M
>maximum.
>This is achieved by creating a one-to-one correspondence between coins
>on
>one chain with coins on the other.
>
>Given the level of the audience here, I'm keeping the description quite
>terse. Much more detail and discussion is at the link above, as well as
>the
>assumptions that need to hold for Bitcoin-United.
>
>The high bit is that, with a few modest assumptions, it is possible to
>create a cohesive coin in the aftermath of a fork, even if the core
>devs
>are split, and even if one of the forks is (in the worst case)
>completely
>non-cooperative. Bitcoin-United is a trick to create a cohesive coin
>even
>when there is no consensus at the lowest level.
>
>Bitcoin-United opens up a lot of new, mostly game-theoretic questions:
>what
>happens to native clients who prefer A or B? What will happen to the
>value
>of native-A or native-B coins? And so on.
>
>We're actively working on these questions and more, but we wanted to
>share
>the Bitcoin-United idea, mainly to receive feedback, and partly to
>provide
>some hope about future consensus to the community. It turns out that it
>is
>possible to craft consensus at the network level even when there isn't
>one
>at the developer level.
>
>Happy New Year, and may 2016 be united,
>- egs & ittay
>
>
>------------------------------------------------------------------------
>
>_______________________________________________
>bitcoin-dev mailing list
>bitcoin-dev at lists.linuxfoundation.org
>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

- --
Sent from my Android device with K-9 Mail. Please excuse my brevity.
-----BEGIN PGP SIGNATURE-----

iQE9BAEBCgAnIBxQZXRlciBUb2RkIDxwZXRlQHBldGVydG9kZC5vcmc+BQJWhDuA
AAoJEMCF8hzn9Lncz4MIAIObFNbRRJ5g52H8yprqAjX76Lt7vw+cwCnICNzHra5h
iuTWxgbwED5fki2Q96ZzYAyUf7ju7rI45qBl8YuuVUlyxJgE6oV6h2oJoxGQNGz0
WvrOjWMkmARNs0FM4GMsKQWcmIMgZxWnWTMOXv0EDBLySsm8WFRu9H4drGBB+Fmb
wFRyi0XVDiXxsVUoNj6pCdcpekdnuq+V87IoweoxigfqgWIM31Vb9QK8Y/7vWO2b
0lu0CvVdqvw5Npx55LWLF1tY8jbw6BYvgXwZGtUazKO+x8i3Qt6+tRm07+UXvkoR
3erxzhnoZa3F66ufz+ImY7l0E/AyRE5ox+1W68hO6sk=
=d0+L
-----END PGP SIGNATURE-----


From el33th4x0r at gmail.com  Wed Dec 30 20:22:43 2015
From: el33th4x0r at gmail.com (=?UTF-8?Q?Emin_G=C3=BCn_Sirer?=)
Date: Wed, 30 Dec 2015 15:22:43 -0500
Subject: [bitcoin-dev] How to preserve the value of coins after a fork.
In-Reply-To: <6741032F-757F-4D9E-A722-3A62271B6BFD@petertodd.org>
References: <CAPkFh0tj4cXYuk8=8QJOP5z4qea6bv_sELhkfHO6nU16mMnnZA@mail.gmail.com>
	<6741032F-757F-4D9E-A722-3A62271B6BFD@petertodd.org>
Message-ID: <CAPkFh0usGPFQ4Tpn4v9FNmAL=Z8vZYm-VU50aFDnTEWMrAr5eA@mail.gmail.com>

On Wed, Dec 30, 2015 at 3:16 PM, Peter Todd <pete at petertodd.org> wrote:

> Note how transaction malleability can quickly sabotage naive notions of
> this idea.
>

Bitcoin-United relies on a notion of transaction equivalence that doesn't
involve the transaction hash at all, so it should be immune to malleability
issues and compatible with segwit.

>From the post, two transactions are equal if they "consume the same inputs
and result in the same outputs, not counting the miner fee. Simple
pay-to-pubkey-hash and pay-to-script-hash transactions are straightforward.
Multikey transactions are evaluated for equivalency by their inputs and
outputs, so it is allowable for a 2-out-of-3 payment to be signed by one
set of two keys on Dum and another set of two keys on Dee, as long as the
transaction consumes the same coins and produces the same outputs. Not that
we'll ever encounter such a case, but making this point helps pedagogically
with getting across the notion of transaction equivalence. What counts are
the consumed inputs and the destination and amounts of the outputs."

But you're right, if a naive implementation were to just use the
transaction hash, the result would be a mess.

- egs
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/1672f2d0/attachment.html>

From pete at petertodd.org  Wed Dec 30 20:32:43 2015
From: pete at petertodd.org (Peter Todd)
Date: Wed, 30 Dec 2015 20:32:43 +0000
Subject: [bitcoin-dev] How to preserve the value of coins after a fork.
In-Reply-To: <CAPkFh0usGPFQ4Tpn4v9FNmAL=Z8vZYm-VU50aFDnTEWMrAr5eA@mail.gmail.com>
References: <CAPkFh0tj4cXYuk8=8QJOP5z4qea6bv_sELhkfHO6nU16mMnnZA@mail.gmail.com>
	<6741032F-757F-4D9E-A722-3A62271B6BFD@petertodd.org>
	<CAPkFh0usGPFQ4Tpn4v9FNmAL=Z8vZYm-VU50aFDnTEWMrAr5eA@mail.gmail.com>
Message-ID: <E89CCB0E-8EA8-46E0-90C2-C2F80B51C894@petertodd.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512



On 30 December 2015 12:22:43 GMT-08:00, "Emin G?n Sirer" <el33th4x0r at gmail.com> wrote:
>On Wed, Dec 30, 2015 at 3:16 PM, Peter Todd <pete at petertodd.org> wrote:
>
>> Note how transaction malleability can quickly sabotage naive notions
>of
>> this idea.
>>
>
>Bitcoin-United relies on a notion of transaction equivalence that
>doesn't
>involve the transaction hash at all, so it should be immune to
>malleability
>issues and compatible with segwit.
>
>From the post, two transactions are equal if they "consume the same
>inputs
>and result in the same outputs, not counting the miner fee. Simple
>pay-to-pubkey-hash and pay-to-script-hash transactions are
>straightforward.
>Multikey transactions are evaluated for equivalency by their inputs and
>outputs, so it is allowable for a 2-out-of-3 payment to be signed by
>one
>set of two keys on Dum and another set of two keys on Dee, as long as
>the
>transaction consumes the same coins and produces the same outputs. Not
>that
>we'll ever encounter such a case, but making this point helps
>pedagogically
>with getting across the notion of transaction equivalence. What counts
>are
>the consumed inputs and the destination and amounts of the outputs."

You seem to not be familiar with how multisig transactions on Bitcoin work - 99.9% of the time theyre hidden behind p2sh and there is no way to know what keys are involved. Equally, multisig is just one of many complex scripts possible.

Look into what a segwit transaction hashes - that's a better notion of non-malleable transaction. But even then lots of transactions are malleable, and its easy to trigger those cases intentionally by third parties.

Most likely any Bitcoin United scheme would quickly diverge and fail; much simpler and more predictable to achieve convincing consensus, e.g. via proof of stake voting, or Adam Bank's extension blocks suggestions. (or of course, not trying to force controversial forks in the first place)

-----BEGIN PGP SIGNATURE-----

iQE9BAEBCgAnIBxQZXRlciBUb2RkIDxwZXRlQHBldGVydG9kZC5vcmc+BQJWhD9N
AAoJEMCF8hzn9Lncz4MH/0JPGVc2JLtD5q0I2w0vqmBqsoSzSueCtnKa2K1Ea10g
w9I4uhK7+cgfCLbofJznVHMChXu0uCxtWwqSj++uJx238TEupcu951gUhFfuPOeH
Egye8jmDkDFiB1P40kUSVk9N64Zt3kWLk4xSsfjawVHz/WWpM24Fn8k/bmI7JiLl
nmVwoBdRsTKffM/1dr8ix4U8YPSmJ7W+jAByNHUpSgc1R73YylqNT95pF8QD35df
dQwSK9DIc+2N4CKnp22xLvYeCivFjeS2Fm4kbcKQwMjcqlJ1mWghP/c8q/lzhaGN
Ac15/pgeHp8dPP8c81zkN9ps14rrnXoHnrzjiY+TwKY=
=FfK1
-----END PGP SIGNATURE-----


From adam at cypherspace.org  Wed Dec 30 23:05:57 2015
From: adam at cypherspace.org (Adam Back)
Date: Wed, 30 Dec 2015 23:05:57 +0000
Subject: [bitcoin-dev] fork types (Re: An implementation of BIP102 as a
	softfork.)
Message-ID: <CALqxMTF_PundsEawyYcinJHyvJJTN-iK0MjWfzpD7QDN448P1Q@mail.gmail.com>

> I guess the same could be said about the softfork flavoured SW implementation

No, segregated witness
https://bitcoin.org/en/bitcoin-core/capacity-increases-faq is a
soft-fork maybe loosely similar to P2SH - particularly it is backwards
and forwards compatible by design.

These firm forks have the advantage over hard forks that there is no
left-over weak chain that is at risk of losing money (because it
becomes a consensus rule that old transactions are blocked).

There is also another type of fork a firm hard fork that can do the
same but for format changes that are not possible with a soft-fork.

Extension blocks show a more general backwards and forwards compatible
soft-fork is also possible.
Segregated witness is simpler.

Adam

On 30 December 2015 at 13:57, Marcel Jamin via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> I guess the same could be said about the softfork flavoured SW
> implementation. In any case, the strategy pattern helps with code structure
> in situations like this.
>
> 2015-12-30 14:29 GMT+01:00 Jonathan Toomim via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org>:

From kanzure at gmail.com  Wed Dec 30 23:10:05 2015
From: kanzure at gmail.com (Bryan Bishop)
Date: Wed, 30 Dec 2015 17:10:05 -0600
Subject: [bitcoin-dev] fork types (Re: An implementation of BIP102 as a
	softfork.)
In-Reply-To: <CALqxMTF_PundsEawyYcinJHyvJJTN-iK0MjWfzpD7QDN448P1Q@mail.gmail.com>
References: <CALqxMTF_PundsEawyYcinJHyvJJTN-iK0MjWfzpD7QDN448P1Q@mail.gmail.com>
Message-ID: <CABaSBawj0pXZ0owZemL9CUkRwgP3J2MJKVzGk+GmSEMUCsQ_DQ@mail.gmail.com>

On Wed, Dec 30, 2015 at 5:05 PM, Adam Back via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> There is also another type of fork a firm hard fork that can do the
> same but for format changes that are not possible with a soft-fork.
>

I was drafting an email for a new thread with some links about this topic,
instead I'll just send this as a reply now that we are writing down fork
types...

auxiliary blocks and evil soft-forks or forced soft-forks:
https://bitcointalk.org/index.php?topic=283746.0
https://bitcointalk.org/index.php?topic=874313.0

soft-fork block size increase using extension blocks:
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-May/008356.html

generalized soft-forks:
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012073.html

bip102 forced soft-fork:
http://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012153.html

extension blocks were also discussed in this interview:
http://diyhpl.us/wiki/transcripts/bitcoin-sidechains-unchained-epicenter-adam3us-gmaxwell/
.... also there was something about a "soft-hard fork".

some discussion from today re: origin of the term evil fork, evil
soft-fork, forced soft-fork:
https://www.reddit.com/r/Bitcoin/comments/3yrsxt/bitcoindev_an_implementation_of_bip102_as_a/cyg2g7q

some much older discussion about extension blocks and sidechains:
http://gnusha.org/bitcoin-wizards/2015-01-01.log

some discussion about "generalized soft-forks" and extension blocks and
evil soft-forks:
http://gnusha.org/bitcoin-wizards/2015-12-20.log

some discussion about evil forks and evil soft-forks and extension blocks:
http://gnusha.org/bitcoin-wizards/2015-12-30.log

segwit soft-fork makes use of a similar idea:
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/011865.html
https://bitcoin.org/en/bitcoin-core/capacity-increases-faq
http://diyhpl.us/wiki/transcripts/scalingbitcoin/hong-kong/segregated-witness-and-its-impact-on-scalability/

Note: I am taking the term "forced soft-fork" from petertodd; it's pretty
much the same thing as "evil fork" in every way but intent.

This is an x-post from
https://bitcointalk.org/index.php?topic=1296628.msg13400092#msg13400092

- Bryan
http://heybryan.org/
1 512 203 0507
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/7041b5c4/attachment.html>

From nickodell at gmail.com  Wed Dec 30 23:13:56 2015
From: nickodell at gmail.com (Nick ODell)
Date: Wed, 30 Dec 2015 16:13:56 -0700
Subject: [bitcoin-dev] How to preserve the value of coins after a fork.
In-Reply-To: <E89CCB0E-8EA8-46E0-90C2-C2F80B51C894@petertodd.org>
References: <CAPkFh0tj4cXYuk8=8QJOP5z4qea6bv_sELhkfHO6nU16mMnnZA@mail.gmail.com>
	<6741032F-757F-4D9E-A722-3A62271B6BFD@petertodd.org>
	<CAPkFh0usGPFQ4Tpn4v9FNmAL=Z8vZYm-VU50aFDnTEWMrAr5eA@mail.gmail.com>
	<E89CCB0E-8EA8-46E0-90C2-C2F80B51C894@petertodd.org>
Message-ID: <CANN4kmfYwvWm=iE9f5iqp=tNwXLZbuhNHzoGuoqyHL-uU7ZKAA@mail.gmail.com>

Emin,

I have two technical criticisms of your proposal, and one economic criticism.

>Unified miners would make sure that they mine transactions on Dum first, then on Dee. Recall that Dee miners can always just take Dum transactions and stick them in their blocks.
This seems to contradict a later section that says that users can use
Dee natively, without paying fees necessary to get a transaction into
Dum. You can't have this both ways - either you can get a transaction
into Dee without getting it into Dum first, or you can't.

>Such an attack would be quite visible, and it would leave Dum vulnerable. Unified clients could counter this launching a 51% counterattack on Dum.
What if some other group that wants to hurt both Dum and Dee were to
make a false-flag attack against Dee? Mutually assured destruction
doesn't work if you can't accurately attribute attacks.

>This would create some gentle pressure to explicitly unify the two chains (by merging Dee and Dum at some compromise and doing away with Unified)
I don't think a compromise would be reachable at that point - suppose
one had a market cap of 1.2 billion, and the other had a market cap of
0.8 billion. How would the coins on the unified chain be distributed?
You could give each chain an equal number of coins, but that's not
fair to the people holding the more valuable coins. You could give
each chain a number of coins proportional to the market cap, but that
invites price manipulation. In any case, if you had a way of reaching
compromise, why not use it instead of creating two chains?


Overall, I think this proposal is a bad idea.


>You seem to not be familiar with how multisig transactions on Bitcoin work - 99.9% of the time theyre hidden behind p2sh and there is no way to know what keys are involved. Equally, multisig is just one of many complex scripts possible.
That doesn't end up mattering, though, as I understand his proposal.
The unified client would just see that both validly spend an output
with a scriptPubKey of OP_HASH160 0xabcdef... OP_EQUAL.

On Wed, Dec 30, 2015 at 1:32 PM, Peter Todd via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA512
>
>
>
> On 30 December 2015 12:22:43 GMT-08:00, "Emin G?n Sirer" <el33th4x0r at gmail.com> wrote:
>>On Wed, Dec 30, 2015 at 3:16 PM, Peter Todd <pete at petertodd.org> wrote:
>>
>>> Note how transaction malleability can quickly sabotage naive notions
>>of
>>> this idea.
>>>
>>
>>Bitcoin-United relies on a notion of transaction equivalence that
>>doesn't
>>involve the transaction hash at all, so it should be immune to
>>malleability
>>issues and compatible with segwit.
>>
> >From the post, two transactions are equal if they "consume the same
>>inputs
>>and result in the same outputs, not counting the miner fee. Simple
>>pay-to-pubkey-hash and pay-to-script-hash transactions are
>>straightforward.
>>Multikey transactions are evaluated for equivalency by their inputs and
>>outputs, so it is allowable for a 2-out-of-3 payment to be signed by
>>one
>>set of two keys on Dum and another set of two keys on Dee, as long as
>>the
>>transaction consumes the same coins and produces the same outputs. Not
>>that
>>we'll ever encounter such a case, but making this point helps
>>pedagogically
>>with getting across the notion of transaction equivalence. What counts
>>are
>>the consumed inputs and the destination and amounts of the outputs."
>
> You seem to not be familiar with how multisig transactions on Bitcoin work - 99.9% of the time theyre hidden behind p2sh and there is no way to know what keys are involved. Equally, multisig is just one of many complex scripts possible.
>
> Look into what a segwit transaction hashes - that's a better notion of non-malleable transaction. But even then lots of transactions are malleable, and its easy to trigger those cases intentionally by third parties.
>
> Most likely any Bitcoin United scheme would quickly diverge and fail; much simpler and more predictable to achieve convincing consensus, e.g. via proof of stake voting, or Adam Bank's extension blocks suggestions. (or of course, not trying to force controversial forks in the first place)
>
> -----BEGIN PGP SIGNATURE-----
>
> iQE9BAEBCgAnIBxQZXRlciBUb2RkIDxwZXRlQHBldGVydG9kZC5vcmc+BQJWhD9N
> AAoJEMCF8hzn9Lncz4MH/0JPGVc2JLtD5q0I2w0vqmBqsoSzSueCtnKa2K1Ea10g
> w9I4uhK7+cgfCLbofJznVHMChXu0uCxtWwqSj++uJx238TEupcu951gUhFfuPOeH
> Egye8jmDkDFiB1P40kUSVk9N64Zt3kWLk4xSsfjawVHz/WWpM24Fn8k/bmI7JiLl
> nmVwoBdRsTKffM/1dr8ix4U8YPSmJ7W+jAByNHUpSgc1R73YylqNT95pF8QD35df
> dQwSK9DIc+2N4CKnp22xLvYeCivFjeS2Fm4kbcKQwMjcqlJ1mWghP/c8q/lzhaGN
> Ac15/pgeHp8dPP8c81zkN9ps14rrnXoHnrzjiY+TwKY=
> =FfK1
> -----END PGP SIGNATURE-----
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From luke at dashjr.org  Wed Dec 30 23:47:16 2015
From: luke at dashjr.org (Luke Dashjr)
Date: Wed, 30 Dec 2015 23:47:16 +0000
Subject: [bitcoin-dev] [BIP Draft] Decentralized Improvement Proposals
In-Reply-To: <1451499779.3919416.479357794.2C21BFA1@webmail.messagingengine.com>
References: <1451493317.3215816.479282618.4F666D71@webmail.messagingengine.com>
	<201512301710.27154.luke@dashjr.org>
	<1451499779.3919416.479357794.2C21BFA1@webmail.messagingengine.com>
Message-ID: <201512302347.17609.luke@dashjr.org>

On Wednesday, December 30, 2015 6:22:59 PM Tomas wrote:
> > The specification itself looks like an inefficient and bloaty reinvention
> > of version bits.
> 
> The actual assignment of version bits isn't clear from the
> specification. Are you saying that any implementation that wants to
> propose a change is encouraged to pick a free version bit and use it?

That should probably be clarified in the BIP, I agree. Perhaps it ought to be 
assigned the same as BIP numbers themselves, by the BIP editor? (Although as a 
limited resource, maybe that's not the best solution.)

> Furthermore, my proposal addresses the danger of forward-incompatible
> changes; a hard-fork can no longer occur as every implementation will
> agree on the active the set of rules even if it has not implemented
> them. This seems to be lacking in the version bits proposal.

I don't think that's possible. First of all, a hardfork can always occur, 
since this is something done by the economy and not (even possibly opposed to) 
miners. Furthermore, consider the change affecting how further rule changes 
are made, such as a PoW algorithm change.

Luke

From j at toom.im  Wed Dec 30 23:49:35 2015
From: j at toom.im (Jonathan Toomim)
Date: Wed, 30 Dec 2015 15:49:35 -0800
Subject: [bitcoin-dev] Increasing the blocksize as a (generalized)
	softfork.
In-Reply-To: <20151230190043.GJ18200@mcelrath.org>
References: <1bf64a5b514d57ca37744ae5f5238149@openmailbox.org>
	<e170f3a10164019824edaafe5a04f067@xbt.hk>
	<f9ad1348fb7dedca35b594782fee7e0f@openmailbox.org>
	<20151230190043.GJ18200@mcelrath.org>
Message-ID: <16BFC301-58C1-49F9-B2E5-A2C09C82A8CA@toom.im>

On Dec 30, 2015, at 11:00 AM, Bob McElrath via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:

> joe2015--- via bitcoin-dev [bitcoin-dev at lists.linuxfoundation.org] wrote:
>> That's the whole point.  After a conventional hardfork everyone
>> needs to upgrade, but there is no way to force users to upgrade.  A
>> user who is simply unaware of the fork, or disagrees with the fork,
>> uses the old client and the currency splits.
>> 
>> Under this proposal old clients effectively enter "zombie" mode,
>> forcing users to upgrade.
> 
> This is a very complex way to enter zombie mode.


Another way you could make non-upgraded nodes enter zombie mode is to explicitly 51% attack the minority fork.

All soft forks are controlled, coordinated, developer-sanctioned 51% attacks against nodes that do not upgrade. The generalized softfork technique is a method of performing a soft fork that completely eliminates any usefulness to non-upgraded nodes while merge-mining another block structure to provide functionality to the nodes who have upgraded and know where to look for the new data.

Soft forks are "safe" forks because you can trust the miners to censor blocks and transactions that do not conform to the new consensus rules. Since we've been relying on the trustworthiness of miners during soft forks in the past (and it only failed us once!), why not

The generalized softfork method has the advantage of being merge-mined, so miners don't have to lose any revenue while performing this 51% attack against non-upgraded nodes. But then you're stuck with all of your transactions in a merge-mined/commitment-based data structure, which is a bit awkward and ugly. But you could avoid all of that code ugliness by just convincing the miners to donate some hashrate (say, 5.1% if the IsSupermajority threshold is 95%, or you could make it dynamic to save some money) to ensuring that the minority fork never has any transactions in the chain. That way, you can replace the everlasting code ugliness with a little bit of temporary sociopolitical ugliness. Fortunately, angry people are easier to ignore than ugly code. /s

Maybe we could call this a softly enforced hard fork? It's basically a combined hard fork for the supermajority and a soft fork to make the minority chain useless.

I don't personally think that these 51% attacks are useful or necessary. This is one of the main reasons why I don't like soft forks. I find them distasteful, and think that leaving minorities free to practice their own religions and blockchain rules is a good thing. But I could see how this could address some of the objections that others have raised about the dangers of hardforks, so I'm putting it out there.

> Once a chain is seen to be 6 or more blocks ahead of my chain tip, we should
> enter "zombie mode" and refuse to mine or relay

I like this method. However, it does have the problem of being voluntary. If nodes don't upgrade to a version that has the latent zombie gene long before a fork, then it does nothing.


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/5ebf647b/attachment-0001.sig>

From j at toom.im  Wed Dec 30 23:56:43 2015
From: j at toom.im (Jonathan Toomim)
Date: Wed, 30 Dec 2015 15:56:43 -0800
Subject: [bitcoin-dev] Increasing the blocksize as a (generalized)
	softfork.
In-Reply-To: <16BFC301-58C1-49F9-B2E5-A2C09C82A8CA@toom.im>
References: <1bf64a5b514d57ca37744ae5f5238149@openmailbox.org>
	<e170f3a10164019824edaafe5a04f067@xbt.hk>
	<f9ad1348fb7dedca35b594782fee7e0f@openmailbox.org>
	<20151230190043.GJ18200@mcelrath.org>
	<16BFC301-58C1-49F9-B2E5-A2C09C82A8CA@toom.im>
Message-ID: <51D6E17C-4466-44D6-9BC5-618BE4A2332C@toom.im>


On Dec 30, 2015, at 3:49 PM, Jonathan Toomim <j at toom.im> wrote:

> Since we've been relying on the trustworthiness of miners during soft forks in the past (and it only failed us once!), why not

make it explicit?

(Sorry for the premature send.)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/8d85c1b5/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/8d85c1b5/attachment.sig>

From bob_bitcoin at mcelrath.org  Thu Dec 31 00:04:42 2015
From: bob_bitcoin at mcelrath.org (Bob McElrath)
Date: Thu, 31 Dec 2015 00:04:42 +0000
Subject: [bitcoin-dev] Increasing the blocksize as a (generalized)
 softfork.
In-Reply-To: <16BFC301-58C1-49F9-B2E5-A2C09C82A8CA@toom.im>
References: <1bf64a5b514d57ca37744ae5f5238149@openmailbox.org>
	<e170f3a10164019824edaafe5a04f067@xbt.hk>
	<f9ad1348fb7dedca35b594782fee7e0f@openmailbox.org>
	<20151230190043.GJ18200@mcelrath.org>
	<16BFC301-58C1-49F9-B2E5-A2C09C82A8CA@toom.im>
Message-ID: <20151231000442.GK18200@mcelrath.org>

Jonathan Toomim [j at toom.im] wrote:
> 
> The generalized softfork method has the advantage of being merge-mined

That's an over-generalization.  There are two kinds of soft-forks WRT mining,
those which:

1. involve new validation rules by data-hiding from non-upgraded modes
    (e.g. extension blocks, generalized softfork)
2. involve NO new validation logic (e.g. P2SH)

Miners which are not validating transactions *should* be deprived of revenue,
because their role is transaction validation, not simply brute forcing sha256d.

So I'm very strongly against this "generalized softfork" idea -- I also don't
see how upgraded nodes and non-upgraded nodes can possibly end up with the same
UTXO set.

> > Once a chain is seen to be 6 or more blocks ahead of my chain tip, we should
> > enter "zombie mode" and refuse to mine or relay
> 
> I like this method. However, it does have the problem of being voluntary. If
> nodes don't upgrade to a version that has the latent zombie gene long before a
> fork, then it does nothing.

Which is why it should be put into core long before forks.  ;-)

--
Cheers, Bob McElrath

"For every complex problem, there is a solution that is simple, neat, and wrong."
    -- H. L. Mencken 


From joe2015 at openmailbox.org  Thu Dec 31 04:39:25 2015
From: joe2015 at openmailbox.org (joe2015 at openmailbox.org)
Date: Thu, 31 Dec 2015 12:39:25 +0800
Subject: [bitcoin-dev] Increasing the blocksize as a (generalized)
 softfork.
In-Reply-To: <20151231000442.GK18200@mcelrath.org>
References: <1bf64a5b514d57ca37744ae5f5238149@openmailbox.org>
	<e170f3a10164019824edaafe5a04f067@xbt.hk>
	<f9ad1348fb7dedca35b594782fee7e0f@openmailbox.org>
	<20151230190043.GJ18200@mcelrath.org>
	<16BFC301-58C1-49F9-B2E5-A2C09C82A8CA@toom.im>
	<20151231000442.GK18200@mcelrath.org>
Message-ID: <5a479e307f84c6e8547287489cd134d1@openmailbox.org>

> So I'm very strongly against this "generalized softfork" idea -- I also 
> don't
> see how upgraded nodes and non-upgraded nodes can possibly end up with 
> the same
> UTXO set.

The only way for non-upgraded nodes to get the correct UTXO set is to 
upgrade.

It is important to keep in mind this was proposed as an alternative to a 
hardfork.  With a hardfork the UTXOs also diverge as upgraded and 
non-upgraded clients follow different chains.

--joe.

From joe2015 at openmailbox.org  Thu Dec 31 11:32:20 2015
From: joe2015 at openmailbox.org (joe2015 at openmailbox.org)
Date: Thu, 31 Dec 2015 19:32:20 +0800
Subject: [bitcoin-dev] Increasing the blocksize as a (generalized)
 softfork.
In-Reply-To: <AD5B1866-BB14-4752-9E0E-88651D8FEE97@gmail.com>
References: <1bf64a5b514d57ca37744ae5f5238149@openmailbox.org>
	<e170f3a10164019824edaafe5a04f067@xbt.hk>
	<f9ad1348fb7dedca35b594782fee7e0f@openmailbox.org>
	<20151230190043.GJ18200@mcelrath.org>
	<16BFC301-58C1-49F9-B2E5-A2C09C82A8CA@toom.im>
	<20151231000442.GK18200@mcelrath.org>
	<5a479e307f84c6e8547287489cd134d1@openmailbox.org>
	<AD5B1866-BB14-4752-9E0E-88651D8FEE97@gmail.com>
Message-ID: <493d1863b74fb81e8890b6e75c1199cd@openmailbox.org>

On 2015-12-31 18:39, David Chan wrote:
> The UTXO sets may diverge but they actually will be strict
> subsets/supersets of each other as no transaction would be invalid on
> one fork vs another unless the hard fork lasts longer than 100 blocks.

The UTXO sets can also diverge thanks to double spends, i.e. A->B is 
confirmed on the old chain and A->C is confirmed on the new.

--joe.

From digitsu at gmail.com  Thu Dec 31 10:39:41 2015
From: digitsu at gmail.com (David Chan)
Date: Thu, 31 Dec 2015 19:39:41 +0900
Subject: [bitcoin-dev] Increasing the blocksize as a (generalized)
	softfork.
In-Reply-To: <5a479e307f84c6e8547287489cd134d1@openmailbox.org>
References: <1bf64a5b514d57ca37744ae5f5238149@openmailbox.org>
	<e170f3a10164019824edaafe5a04f067@xbt.hk>
	<f9ad1348fb7dedca35b594782fee7e0f@openmailbox.org>
	<20151230190043.GJ18200@mcelrath.org>
	<16BFC301-58C1-49F9-B2E5-A2C09C82A8CA@toom.im>
	<20151231000442.GK18200@mcelrath.org>
	<5a479e307f84c6e8547287489cd134d1@openmailbox.org>
Message-ID: <AD5B1866-BB14-4752-9E0E-88651D8FEE97@gmail.com>

The UTXO sets may diverge but they actually will be strict subsets/supersets of each other as no transaction would be invalid on one fork vs another unless the hard fork lasts longer than 100 blocks. 
This is of course specific to a block limit change hard fork. 



On 2015/12/31, at 13:39, joe2015--- via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:

>> So I'm very strongly against this "generalized softfork" idea -- I also don't
>> see how upgraded nodes and non-upgraded nodes can possibly end up with the same
>> UTXO set.
> 
> The only way for non-upgraded nodes to get the correct UTXO set is to upgrade.
> 
> It is important to keep in mind this was proposed as an alternative to a hardfork.  With a hardfork the UTXOs also diverge as upgraded and non-upgraded clients follow different chains.
> 
> --joe.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From marcopon at gmail.com  Wed Dec 30 16:42:47 2015
From: marcopon at gmail.com (Marco Pontello)
Date: Wed, 30 Dec 2015 17:42:47 +0100
Subject: [bitcoin-dev] BIP numbers
Message-ID: <CAE0pACJf=aQFFTwRyWn+8SxS2P-v5FmG77kbC35rq_0p42CDEw@mail.gmail.com>

Sorry to ask again but... what's up with the BIP number assignments?
I thought that it was just more or less a formality, to avoid conflicts and
BIP spamming. And that would be perfectly fine.
But since I see that it's a process that can take months (just looking at
the PR request list), it seems that something different is going on. Maybe
it's considered something that give an aura of officiality of sorts? But
that would make little sense, since that should come eventually with
subsequents steps (like adding a BIP to the main repo, and eventual
approvation).

Having # 333 assigned to a BIP, should just mean that's easy to refer to a
particular BIP.
That seems something that could be done quick and easily.

What I'm missing? Probably some historic context?
Thanks!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151230/e6b4259e/attachment.html>

From pete at petertodd.org  Thu Dec 31 23:14:40 2015
From: pete at petertodd.org (Peter Todd)
Date: Thu, 31 Dec 2015 15:14:40 -0800
Subject: [bitcoin-dev] BIP numbers
In-Reply-To: <CAE0pACJf=aQFFTwRyWn+8SxS2P-v5FmG77kbC35rq_0p42CDEw@mail.gmail.com>
References: <CAE0pACJf=aQFFTwRyWn+8SxS2P-v5FmG77kbC35rq_0p42CDEw@mail.gmail.com>
Message-ID: <20151231231440.GA5112@muck>

On Wed, Dec 30, 2015 at 05:42:47PM +0100, Marco Pontello via bitcoin-dev wrote:
> Sorry to ask again but... what's up with the BIP number assignments?
> I thought that it was just more or less a formality, to avoid conflicts and
> BIP spamming. And that would be perfectly fine.
> But since I see that it's a process that can take months (just looking at
> the PR request list), it seems that something different is going on. Maybe
> it's considered something that give an aura of officiality of sorts? But
> that would make little sense, since that should come eventually with
> subsequents steps (like adding a BIP to the main repo, and eventual
> approvation).
> 
> Having # 333 assigned to a BIP, should just mean that's easy to refer to a
> particular BIP.
> That seems something that could be done quick and easily.
> 
> What I'm missing? Probably some historic context?

You ever noticed how actually getting a BIP # assigned is the *last*
thing the better known Bitcoin Core devs do? For instance, look at the
segregated witness draft BIPs.

I think we have problem with peoples' understanding of the Bitcoin
consensus protocol development process being backwards: first write your
protocol specification - the code - and then write the human readable
reference explaining it - the BIP.

Equally, without people actually using that protocol, who cares about
the BIP?


Personally if I were assigning BIP numbers I'd be inclined to say "fuck
it" and only assign BIP numbers to BIPs after they've had significant
adoption... It'd might just cause a lot less headache than the current
system.

-- 
'peter'[:-1]@petertodd.org
000000000000000006808135a221edd19be6b5b966c4621c41004d3d719d18b7
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 650 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151231/4b62cfd2/attachment.sig>

From adrian.macneil at gmail.com  Thu Dec 31 23:30:02 2015
From: adrian.macneil at gmail.com (Adrian Macneil)
Date: Thu, 31 Dec 2015 23:30:02 +0000
Subject: [bitcoin-dev] BIP numbers
In-Reply-To: <20151231231440.GA5112@muck>
References: <CAE0pACJf=aQFFTwRyWn+8SxS2P-v5FmG77kbC35rq_0p42CDEw@mail.gmail.com>
	<20151231231440.GA5112@muck>
Message-ID: <CAFbKzyNBWju8wsgLVtv4Es9jaOkK1X75OnketGfO7rBq7e5_7Q@mail.gmail.com>

I'm not sure if anyone has suggested this in the past, but a novel approach
would be to simply let anyone open a pull request and use the PR # as the
BIP #. This would avoid conflicts, and avoid the chore of having someone
manually assign them.

Downside would be that some numbers will never get used (for example if PRs
are opened to update existing BIPs), but this doesn't seem to be a huge
problem since already many numbers are going unused.

This process can still be independent from approving/merging the BIP into
master, if it meets quality standards.
On Thu, Dec 31, 2015 at 3:14 PM Peter Todd via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> On Wed, Dec 30, 2015 at 05:42:47PM +0100, Marco Pontello via bitcoin-dev
> wrote:
> > Sorry to ask again but... what's up with the BIP number assignments?
> > I thought that it was just more or less a formality, to avoid conflicts
> and
> > BIP spamming. And that would be perfectly fine.
> > But since I see that it's a process that can take months (just looking at
> > the PR request list), it seems that something different is going on.
> Maybe
> > it's considered something that give an aura of officiality of sorts? But
> > that would make little sense, since that should come eventually with
> > subsequents steps (like adding a BIP to the main repo, and eventual
> > approvation).
> >
> > Having # 333 assigned to a BIP, should just mean that's easy to refer to
> a
> > particular BIP.
> > That seems something that could be done quick and easily.
> >
> > What I'm missing? Probably some historic context?
>
> You ever noticed how actually getting a BIP # assigned is the *last*
> thing the better known Bitcoin Core devs do? For instance, look at the
> segregated witness draft BIPs.
>
> I think we have problem with peoples' understanding of the Bitcoin
> consensus protocol development process being backwards: first write your
> protocol specification - the code - and then write the human readable
> reference explaining it - the BIP.
>
> Equally, without people actually using that protocol, who cares about
> the BIP?
>
>
> Personally if I were assigning BIP numbers I'd be inclined to say "fuck
> it" and only assign BIP numbers to BIPs after they've had significant
> adoption... It'd might just cause a lot less headache than the current
> system.
>
> --
> 'peter'[:-1]@petertodd.org
> 000000000000000006808135a221edd19be6b5b966c4621c41004d3d719d18b7
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151231/5214ee14/attachment.html>

From pete at petertodd.org  Thu Dec 31 23:48:48 2015
From: pete at petertodd.org (Peter Todd)
Date: Thu, 31 Dec 2015 15:48:48 -0800
Subject: [bitcoin-dev] Segregated witnesses and validationless mining
In-Reply-To: <20151223013119.GA31113@muck>
References: <20151223013119.GA31113@muck>
Message-ID: <20151231234847.GB5112@muck>

On Tue, Dec 22, 2015 at 05:31:19PM -0800, Peter Todd via bitcoin-dev wrote:
> # Summary

Updates from IRC discussion:

1) There was some debate about what exactly should be required from the
current block to calculate the previous block posession proof. For
instance, requiring the coinbase outputs potentially restricts some
mining setups; requiring a commitment to the current block's
(non-coinbase) transaction outputs restricts tx selection outsourcing
schemes.

However, it appears that we can allow the nonce to be picked
arbitrarily. Equally, if the nonce is arbitrary, then a future soft-fork
can be add commitments to current block contents. Thus the previous
block proof can be simple H(<nonce> + <prev-block-contents>)


2) Pieter Wuille brought up fraud proofs in relation to previous block
content proofs - specifically how the simplest H(<nonce> +
<prev-block-contents>) construction requires a large fraud proof to
prove incorrect. This followed a bunch of debate over what exactly fraud
proofs would be - a proof that some data is fraudulent, or a unmet
challenge that some data is correct?

Regardless, if the posession proof is structured as a merkle tree, then
fraud can be easily proven with a merkle path. In that model we'd take
the previous block contents and rehash it in its entirety with the
nonce. The fraud proof then becomes two merkle paths - one in the
original block with the original hash, and the second with the same
data, and same structure, but with the nonce mixed into the hashing
algorithm.


Todo: writeup the difference between the fraud proof model, and the
validity challenge model, to provide background to making this decision.


Incidentally, based the positive response to fixing this issue w/
segregated witnesses - my main objection to the plan - I've signed the
Bitcoin Core capacity increases statement:

https://github.com/bitcoin-dot-org/bitcoin.org/pull/1165#issuecomment-168263005

-- 
'peter'[:-1]@petertodd.org
000000000000000006808135a221edd19be6b5b966c4621c41004d3d719d18b7
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 650 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20151231/1f2e6e9d/attachment-0001.sig>

