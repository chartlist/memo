From peter at coinkite.com  Wed Sep  1 13:39:30 2021
From: peter at coinkite.com (Peter D. Gray)
Date: Wed, 1 Sep 2021 09:39:30 -0400
Subject: [bitcoin-dev] Proposal for a few IANA mime-types related to
 Bitcoin
In-Reply-To: <775f2a59-7cd7-6234-23d2-1780e2c4da58@achow101.com>
References: <mailman.9346.1630015566.1160.bitcoin-dev@lists.linuxfoundation.org>
 <20210831182741.GV91472@coinkite.com>
 <775f2a59-7cd7-6234-23d2-1780e2c4da58@achow101.com>
Message-ID: <20210901133930.GB91472@coinkite.com>

> ... I tried doing this with "application/bitcoin-psbt" back in
> 2019 but it was not accepted...

Thanks for this background.

Based on your experience, we should probably ignore the IANA then,
and just declare a few useful "mime types" (note the quotes) in a
new BIP. We can then agree inside the Bitcoin community on their
usage and meaning.

Anyone want to write that BIP and shepherd it? I can support you
but I'd rather write code.

---
@DocHEX  ||  Coinkite  ||  PGP: A3A31BAD 5A2A5B10

On Tue, Aug 31, 2021 at 07:46:55PM +0000, Andrew Chow wrote:
> Hi Peter,
> 
> It would be nice to have mime types registered for Bitcoin things, but
> I'm not sure that it will be possible, at least not in the way that we
> would like. I tried doing this with "application/bitcoin-psbt" back in
> 2019 but it was not accepted. From that attempt, here is what I have
> learned:
> 
> There are only a few accepted top level types, so we would not be able
> to use "bitcoin" as the top level (unless you want to submit an RFC to
> add a "bitcoin" top level). Of the available top level types,
> "application" is the most appropriate for Bitcoin.
> 
> Next is the tree that the mime type should be in. The best would be the
> Standards tree, but it has some requirements that Bitcoin doesn't really
> meet. In order to be in the standards tree, the registration must be
> either associated with an IETF specification (so a RFC) or registered by
> a recognized standards related organization. Unfortunately the closest
> thing to a standards organization that Bitcoin has is the BIPs process,
> and that is not a really a standards organization nor is it recognized
> by IANA. So in order to register the mimetypes as Standards tree types,
> we would need to write an RFC, but this could be an independent
> submission (https://www.rfc-editor.org/about/independent/) rather than
> IETF-stream submission. I did not continue to pursue this because I
> didn't have the time.
> 
> Another alternative would be to use the Vendor tree, but that would
> prefix the mimetype with "vnd." so it would end up being something like
> "application/vnd.bitcoin.psbt". I did not think this was an reasonable
> so I did not continue to pursue this avenue.
> 
> 
> Andrew Chow
> 
> On 8/31/21 2:27 PM, Peter D. Gray via bitcoin-dev wrote:
> > Hi list!
> >
> > I am proposing to register the following MIME (RFC 2046) media types with the IANA:
> >
> >
> > bitcoin/psbt
> >
> >      - aka. a BIP-174 file, in binary
> >      - does not make any claims about signed/unsigned status; lets leave that to the file
> >
> > bitcoin/txn
> >
> >      - aka. wire-ready fully-signed transaction in binary
> >
> > bitcoin/uri
> >
> >      - aka [BIP-21](https://github.com/bitcoin/bips/blob/master/bip-0021.mediawiki)
> >      - could be just a bare bech32 or base58 payment address
> >      - but can also encode amount, comments in URL args
> >      - potentially interesting as a response to 402 - Payment required
> >
> >
> > Other thoughts
> >
> > - some mime-types are proposed in BIP-71 but those are unrelated to above, and never
> >    seem to have been registered
> >
> > - for those who like to encode their binary as base64 or hex, that can be indicated
> >    as "encoding=hex" or "encoding=base64" in the optional parameters, just like
> >    "text/plain; encoding=utf-8" does. However, the default must be binary.
> >
> > - although the above are useful for web servers, they are also useful elsewhere and I
> >    intend to use them in NFC (NDEF records) where a shorter length is critical.
> >
> > - I have no idea how easily IANA will accept these proposals.
> >
> > - current approved mime types: https://www.iana.org/assignments/media-types/media-types.xhtml
> >
> > Thoughts?
> >
> > ---
> > @DocHEX  ||  Coinkite  ||  PGP: A3A31BAD 5A2A5B10
> >
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev at lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 
> 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 488 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210901/c6f95528/attachment.sig>

From zachgrw at gmail.com  Wed Sep  1 15:15:30 2021
From: zachgrw at gmail.com (Zac Greenwood)
Date: Wed, 1 Sep 2021 17:15:30 +0200
Subject: [bitcoin-dev] Exploring: limiting transaction output amount as
 a function of total input value
In-Reply-To: <A8O_5wbTwq48sO02LAHT_t0RN0GVY0yrygt2DiSPrCavXCYu_0LsZWKf3jEQUyBbboW7zQnXyVSHkNJV7CP-VliKOYXjmmnfQIF2Q6E4pl8=@protonmail.com>
References: <CAGpPWDZ0aos5qHw2=popCpjuH7OXC0geEj8i3dwDTfP0j=or4w@mail.gmail.com>
 <T9dyi_J_ZLwT8hXaxOBlCEhdoB9hsBcvFZX3r1JrtxunUTnFe7wefV6hi3Itw8z84drqAn64ZCJhfSfz7Aw0cqx4Aa8DtN1irvE-d4JPoeE=@protonmail.com>
 <CAJ4-pED7sAe+yNqxv_1HSaku=kuTQYTU3nm2o6vUVCEnhkxxFA@mail.gmail.com>
 <1qkQ1p1rAZApZhMKVFwQV6gfLyZxMYIUPrhcjtXNU4z0DBRwslPSbi76GnNnllpvPPfqt1bH3EyzJNhfK0Uxum7zJ_dh3H0DXqUpf2nmHyk=@protonmail.com>
 <CAJ4-pECSE=by2ag4QmqrXbX_R0sk6HOL1KH0z2h=+915jaEE6Q@mail.gmail.com>
 <wdlRrp4fZxH79mzVpTQWp99V9uI8jLvRU40mwdJ8lZ5A2mGMsxUK1TmZEZTV7O4_eUnNyq3feEGv5BUN-ecPSlbL-EYR6ZyLxk9ErsFiPlE=@protonmail.com>
 <CAJ4-pECwGfrrB15oS0t-+vsjn11sC=9Bz6JGsGsicUorCjuYpA@mail.gmail.com>
 <RrFU9zB125CEEua75KWb6IfANybTVN9AGfvjxE65Ysa1zOIgM-h48HmdBcfynW7HEd6kOaA5G-FhAVbmrq5DJXJJYHNArJDORGJklnBCU_I=@protonmail.com>
 <CAJ4-pEC_NTScPFg822Va+Sw1_yC87tWBAL4y7y-m7PSx+JcccQ@mail.gmail.com>
 <A8O_5wbTwq48sO02LAHT_t0RN0GVY0yrygt2DiSPrCavXCYu_0LsZWKf3jEQUyBbboW7zQnXyVSHkNJV7CP-VliKOYXjmmnfQIF2Q6E4pl8=@protonmail.com>
Message-ID: <CAJ4-pECwa6UTWy+xPp8xr8fRA9CZt=aCCA8tMPODdWsPkX+wNw@mail.gmail.com>

Hi ZmnSCPxj,

The rate-limiting algorithm would be relatively straightforward. I
documented the rate-limiting part of the algorithm below, perhaps they can
evoke new ideas of how to make this MAST-able or otherwise implement this
in a privacy preserving way.

Something like the following:

=> Create an output at block height [h0] with the following properties:

Serving as input at any block height, the maximum amount is limited to
[limit] sats;  // This rule introduces [limit] and is permanent and always
copied over to a change output
Serving as input at a block height < [h0 + window], the maximum amount is
limited to [limit - 0] sats;  // [limit - 0] to emphasize that nothing was
spent yet and no window has started.

=> A transaction occurs at block height [h1], spending [h1_spent].
The payment output created at [h1] is not encumbered and of value
[h1_spent]; // Note, this is the first encumbered transaction so [h1] is
the first block of the first window

The change output created at block height [h1] must be encumbered as
follows:
Serving as input at any block height, the maximum amount is limited to
[limit] sats;  // Permanent rule repeats
Serving as input at a block height < [h1 + window], the maximum amount is
limited to [limit - h1_spent]  // Second permanent rule reduces spendable
amount until height [h1 + window] by [h1_spent]

=> A second transaction occurs at block height [h2], spending [h2_spent].
The payment output created at [h2] is not encumbered and of value
[h2_spent]; // Second transaction, so a second window starts at [h2]

The change output created at block height [h2] must be encumbered as
follows:
Serving as input at any block height, the maximum amount is limited to
[limit] sats;  // Permanent rule repeats
Serving as input at a block height < [h1 + window], the max amount is
limited to [limit - h1_spent - h2_spent] // Reduce spendable amount between
[h1] and [h1 + window] by an additional [h2_spent]
Serving as input in range [h1 + window] <= block height < [h2 + window],
the max amount is limited to [limit - h2_spent]  // First payment no longer
inside this window so [h1_spent] no longer subtracted

... and so on. A rule that pertains to a block height < the current block
height can be abandoned, keeping the number of rules equal to the number of
transactions that exist within the oldest still active window.

Zac


On Tue, Aug 31, 2021 at 4:22 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:

> Good morning Zac,
>
> > Hi ZmnSCPxj,
> >
> > Thank you for your helpful response. We're on the same page concerning
> privacy so I'll focus on that. I understand from your mail that privacy
> would be reduced by this proposal because:
> >
> > * It requires the introduction of a new type of transaction that is
> different from a "standard" transaction (would that be P2TR in the
> future?), reducing the anonymity set for everyone;
> > * The payment and change output will be identifiable because the change
> output must be marked encumbered on-chain;
> > * The specifics of how the output is encumbered must be visible on-chain
> as well reducing privacy even further.
> >
> > I don't have the technical skills to judge whether these issues can
> somehow be resolved. In functional terms, the output should be spendable in
> a way that does not reveal that the output is encumbered, and produce a
> change output that cannot be distinguished from a non-change output while
> still being encumbered. Perhaps some clever MAST-fu could somehow help?
>
> I believe some of the covenant efforts may indeed have such clever MAST-fu
> integrated into them, which is why I pointed you to them --- the people
> developing these (aj I think? RubenSomsen?) might be able to accommodate
> this or some subset of the desired feature in a sufficiently clever
> covenant scheme.
>
> There are a number of such proposals, though, so I cannot really point you
> to one that seems likely to have a lot of traction.
>
> Regards,
> ZmnSCPxj
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210901/cf80c2eb/attachment.html>

From billy.tetrud at gmail.com  Thu Sep  2 06:46:55 2021
From: billy.tetrud at gmail.com (Billy Tetrud)
Date: Wed, 1 Sep 2021 23:46:55 -0700
Subject: [bitcoin-dev] Braidpool: Proposal for a decentralised mining
	pool
In-Reply-To: <0aff157f62ea4abba71df4f87eb54880-kohli@ctemplar.com>
References: <0aff157f62ea4abba71df4f87eb54880-kohli@ctemplar.com>
Message-ID: <CAGpPWDZNGrLnfVg9aTBax8esEEM=mMTjiC1VrdAr9hyAwTNXzg@mail.gmail.com>

How would you compare this to Stratum v2?

On Sun, Aug 29, 2021 at 1:02 AM pool2win via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> We have been working on a peer to peer mining pool that overcomes the
> problems faced by P2Pool and enables building a futures market for
> hashrate.
>
> The proposal can be found here:
> https://github.com/pool2win/braidpool/raw/main/proposal/proposal.pdf
>
> The key features of the pool are:
>
> 1. Lower variance for smaller miners, even when large miners join
>   the pool.
> 2. Miners build their own blocks, just like in P2Pool.
> 3. Payouts require a constant size blockspace, independent of the
>   number of miners in the pool.
> 4. Provide building blocks for enabling a futures market of hash
>   rates.
>
> Braidpool: Decentralised Mining Pool for Bitcoin
>
> Abstract. Bitcoin P2Pool's usage has steadily declined over the years,
> negatively impacting bitcoin's decentralisation. The variance in
> earnings for miners increases with total hashrate participating in
> P2Pool, and payouts require a linearly increasing block space with the
> number of miners participating in the pool. We present a solution that
> uses a DAG of shares replicated at all miners. The DAG is then used to
> compute rewards for miners. Rewards are paid out using one-way payment
> channels by an anonymous hub communicating with the miners using Tor's
> hidden services. Using the payment channels construction, neither the
> hub nor the miners can cheat.
>
> Full proposal at
> https://github.com/pool2win/braidpool/raw/main/proposal/proposal.pdf
>
> Details on trading hashrate are here:
>
> https://pool2win.github.io/braidpool/2021/08/18/deliver-hashrate-to-market-makers.html
>
> @pool2win
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210901/6bc8a1b5/attachment-0001.html>

From orlovsky at protonmail.com  Thu Sep  2 10:52:13 2021
From: orlovsky at protonmail.com (Dr Maxim Orlovsky)
Date: Thu, 02 Sep 2021 10:52:13 +0000
Subject: [bitcoin-dev] Proposal for a few IANA mime-types related to
	Bitcoin
In-Reply-To: <20210901133930.GB91472@coinkite.com>
References: <mailman.9346.1630015566.1160.bitcoin-dev@lists.linuxfoundation.org>
 <20210831182741.GV91472@coinkite.com>
 <775f2a59-7cd7-6234-23d2-1780e2c4da58@achow101.com>
 <20210901133930.GB91472@coinkite.com>
Message-ID: <tlMZf9bYKjQH7myNPelTptqiiJsJ_F2MVE3cYEBvabqshphle5ycwdBfyYrsWvn9KZZTKc_4VzPclI2AZGZT6THfmIz_hUnHD9IgyNyQtpo=@protonmail.com>

Hi Peter,

Yep, I think it is a good idea just to do new BIP and ignore IANA opinion on the matter. I can write it since I was going to propose pretty much the same thing some time ago. I will share the draft in this mail list in a week.

Kind regards,
Maxim Orlovsky
LNP/BP Standards Association
http://lnp-bp.org
github.com/dr-orlovsky

-------- Original Message --------
On 1 Sep 2021, 15:39, Peter D. Gray via bitcoin-dev wrote:

>> ... I tried doing this with "application/bitcoin-psbt" back in
>> 2019 but it was not accepted...
>
> Thanks for this background.
>
> Based on your experience, we should probably ignore the IANA then,
> and just declare a few useful "mime types" (note the quotes) in a
> new BIP. We can then agree inside the Bitcoin community on their
> usage and meaning.
>
> Anyone want to write that BIP and shepherd it? I can support you
> but I'd rather write code.
>
> ---
> @DocHEX || Coinkite || PGP: A3A31BAD 5A2A5B10
>
> On Tue, Aug 31, 2021 at 07:46:55PM +0000, Andrew Chow wrote:
>> Hi Peter,
>>
>> It would be nice to have mime types registered for Bitcoin things, but
>> I'm not sure that it will be possible, at least not in the way that we
>> would like. I tried doing this with "application/bitcoin-psbt" back in
>> 2019 but it was not accepted. From that attempt, here is what I have
>> learned:
>>
>> There are only a few accepted top level types, so we would not be able
>> to use "bitcoin" as the top level (unless you want to submit an RFC to
>> add a "bitcoin" top level). Of the available top level types,
>> "application" is the most appropriate for Bitcoin.
>>
>> Next is the tree that the mime type should be in. The best would be the
>> Standards tree, but it has some requirements that Bitcoin doesn't really
>> meet. In order to be in the standards tree, the registration must be
>> either associated with an IETF specification (so a RFC) or registered by
>> a recognized standards related organization. Unfortunately the closest
>> thing to a standards organization that Bitcoin has is the BIPs process,
>> and that is not a really a standards organization nor is it recognized
>> by IANA. So in order to register the mimetypes as Standards tree types,
>> we would need to write an RFC, but this could be an independent
>> submission (https://www.rfc-editor.org/about/independent/) rather than
>> IETF-stream submission. I did not continue to pursue this because I
>> didn't have the time.
>>
>> Another alternative would be to use the Vendor tree, but that would
>> prefix the mimetype with "vnd." so it would end up being something like
>> "application/vnd.bitcoin.psbt". I did not think this was an reasonable
>> so I did not continue to pursue this avenue.
>>
>>
>> Andrew Chow
>>
>> On 8/31/21 2:27 PM, Peter D. Gray via bitcoin-dev wrote:
>> > Hi list!
>> >
>> > I am proposing to register the following MIME (RFC 2046) media types with the IANA:
>> >
>> >
>> > bitcoin/psbt
>> >
>> > - aka. a BIP-174 file, in binary
>> > - does not make any claims about signed/unsigned status; lets leave that to the file
>> >
>> > bitcoin/txn
>> >
>> > - aka. wire-ready fully-signed transaction in binary
>> >
>> > bitcoin/uri
>> >
>> > - aka [BIP-21](https://github.com/bitcoin/bips/blob/master/bip-0021.mediawiki)
>> > - could be just a bare bech32 or base58 payment address
>> > - but can also encode amount, comments in URL args
>> > - potentially interesting as a response to 402 - Payment required
>> >
>> >
>> > Other thoughts
>> >
>> > - some mime-types are proposed in BIP-71 but those are unrelated to above, and never
>> > seem to have been registered
>> >
>> > - for those who like to encode their binary as base64 or hex, that can be indicated
>> > as "encoding=hex" or "encoding=base64" in the optional parameters, just like
>> > "text/plain; encoding=utf-8" does. However, the default must be binary.
>> >
>> > - although the above are useful for web servers, they are also useful elsewhere and I
>> > intend to use them in NFC (NDEF records) where a shorter length is critical.
>> >
>> > - I have no idea how easily IANA will accept these proposals.
>> >
>> > - current approved mime types: https://www.iana.org/assignments/media-types/media-types.xhtml
>> >
>> > Thoughts?
>> >
>> > ---
>> > @DocHEX || Coinkite || PGP: A3A31BAD 5A2A5B10
>> >
>> > _______________________________________________
>> > bitcoin-dev mailing list
>> > bitcoin-dev at lists.linuxfoundation.org
>> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210902/cb571cb2/attachment.html>

From prayank at tutanota.de  Thu Sep  2 17:11:22 2021
From: prayank at tutanota.de (Prayank)
Date: Thu, 2 Sep 2021 19:11:22 +0200 (CEST)
Subject: [bitcoin-dev] Drivechain: BIP 300 and 301
Message-ID: <MibU_fn--3-2@tutanota.de>

printf("Hello, World!");

What are your thoughts on Drivechain and associated BIPs?

This article compares Liquid and Lightning:?https://blog.liquid.net/six-differences-between-liquid-and-lightning/. Two things from it that I am interested in while evaluating Drivechain:

1.Trust model
2.On-Ramps and Off-Ramps

Other things:

1.Security of Bitcoin (Layer 1)
2.Bitcoin transactions and fees expected on layer 1 because of Drivechain

Similarities and Differences between RSK and Ethereum:?https://medium.com/iovlabs-innovation-stories/similarities-and-differences-between-rsk-and-ethereum-e480655eff37

Paul Sztorc had mentioned few things about fees in this video:?https://youtu.be/oga8Pwbq9M0?t=481?I am interested to know same for LN, Liquid and Rootstock as well so asked a question on Bitcoin Stackexchange today:?https://bitcoin.stackexchange.com/questions/109466/bitcoin-transactions-associated-with-layer-2-projects

Two critiques are mentioned here:?https://www.drivechain.info/peer-review/peer-review-new/?with lot of names. I don't agree with everything mentioned on project website although any comments on technical things that can help Bitcoin and Bitcoin projects will be great.

Why discuss here and not on Twitter?

1.Twitter is not the best place for such discussions. There are some interesting threads but Its mostly used for followers, likes, retweets etc. and people can write anything for it.
2.Avoid misinformation, controversies etc.?

My personal opinion:

We should encourage sidechain projects. I don't know much about Drivechain to form a strong opinion but concept looks good which can help in making better sidechains.

----------------------------------------------------------------------------------------------------------------------


The website used in the slides of above YouTube video is misleading for few reasons:

1.Blocks mined everyday (in MB) for Bitcoin is ~150 MB. It is ~600 MB for Ethereum. Block limits for Bitcoin is ~4 MB per 10 minutes and ~500 MB for Ethereum. If full nodes will be run by few organizations on AWS we can basically do everything on chain. However the main goal isn't too make money and create an illusion to do something innovative, primary goal was/is decentralized network that allows settlement of payments.

2.Bitcoin uses UTXO model while Ethereum uses Account model. Basic difference in transactions for two is explained in an article?https://coinmetrics.io/on-data-and-certainty/. Irony is the website in the slides for screenshot is using Coinmetrics API and this misleading website is even shared by Coinmetrics team on Twitter. So in some cases you are doing more transactions, paying more fees for work which could have been done with less. Inefficiency.

3.Failed transactions paying fees on Ethereum everyday, no such transactions on Bitcoin.

4.Other improvements that affect fees: Segwit, Layer 2, Batching, UTXO consolidation, Fee estimation, Coin selection, Exchanges, Wallets etc.


-- 
Prayank

A3B1 E430 2298 178F
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210902/6638e6fe/attachment.html>

From ZmnSCPxj at protonmail.com  Thu Sep  2 21:02:55 2021
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Thu, 02 Sep 2021 21:02:55 +0000
Subject: [bitcoin-dev] Drivechain: BIP 300 and 301
In-Reply-To: <MibU_fn--3-2@tutanota.de>
References: <MibU_fn--3-2@tutanota.de>
Message-ID: <3dyAATLXbsE7WBzpVIc0s4sRkSFhR_5NN04uUgx3o9LH48IKR6EHL3V45PfpRM96yxHXdsjd7WC7MGuPlBw7MRpCkpXRsB-WI7i3-Nr13Ew=@protonmail.com>

Good morning Prayank,

Just to be clear, neither Liquid nor RSK, as of my current knowledge, are Drivechain systems.

Instead, they are both federated sidechains.
The money owned by a federated sidechain is, as far s the Bitcoin blockchain is concerned, really owned by the federation that.runs the sidechain.

Basically, a mainchain->sidechain transfer is done by paying to a federation k-of-n address and a coordination signal of some kind (details depending on federated sidechain) to create the equivalent coins on the sidechain.
A sidechain->mainchain transfer is done by requesting some coins on the sidechain to be destroyed, and then the federation will send some of its mainchain k-of-n coins into whatever address you indicate you want to use on the mainchain.

In theory, a sufficient quorum of the federation can decide to ignore the sidechain data entirely and spend the mainchain money arbitrarily, and the mainchain layer will allow this (being completely ignorant of he sidechain).

In such federated sidechains, the federation is often a fixed predetermined signing set, and changes to that federation are expected to be rare.

Federated sidechains are ultimately custodial; as noted above, the federation could in theory abscond with the funds completely, and the mainchain would not care if the sidechain federation executes its final exit strategy and you lose your funds.
One can consider federated sidechains to be a custodian with multiple personality disorder, that happens to use a blockchain to keep its individual sub-personalities coordinated with each other, but ultimately control of the money is contingent on the custodian following the dictates of the supposed owners of the coin.
>From a certain point of view, it is actually immaterial that there is a separate blockchain called the "sidechain" --- it is simply that a blockchain is used to coordinate the custodians of the coin, but in principle any other coordination mechanism can be used between them, including a plain database.


With Drivechains, custody of the sidechain funds is held by mainchain miners.
Again, this is still a custodial setup.
A potential issue here is that the mainchain miners cannot be identified (the entire point is anonymity of miners is possible), which may be of concern.

In particular, note that solely on mainchain, all that miners determine is the *ordering* and *timing* of transactions.
Let us suppose that there is a major 51% attack attempt on the Bitcoin blockchain.
We expect that such an attack will be temporary --- individuals currently not mining may find that their HODLings are under threat of the 51% attack, and may find it more economic to run miners at a loss, in order to protect their stacks rather than lose it.
Thus, we expect that a 51% attack will be temporary, as other miners will arise inevitably to take back control of transaction processing.
https://github.com/libbitcoin/libbitcoin-system/wiki/Threat-Level-Paradox

In particular, on the mainchain, 51% miners cannot reverse deep history.
If you have coins you have not moved since 2017, for example, the 51% attack is expected to take about 4 years before it can begin to threaten your ownership of those coins (hopefully, in those 4 years, you will get a clue and start mining at a loss to protect your funds from outright loss, thus helping evict the 51% attacker).
51% miners can, in practice, only prevent transfers (censorship), not force transfer of funds (confiscation).
Once the 51% attacker is evicted (and they will in general be evicted), then coins you owned that were deeply confirmed remain under your control.

With Drivechains, however, sidechain funds can be confiscated by a 51% attacker, by forcing a bogus sidechain->mainchain withdrawal.
The amount of time it takes is simply the security parameter of the Drivechain spec.
It does not matter if you were holding those funds in the sidechain for several years without moving them --- a 51% attacker that is able to keep control of the mainchain blockchain, for the Drivechain security parameter, will be capable of confiscating sidechain funds outright.
Thus, even if the 51% attacker is evicted, then your coins in the sidechain can be confiscated and no longer under your control.

Increasing the Drivechain security parameter leads to slower sidechain->mainchin withdrawals, effectively a bottleneck on how much can be transferred sidechain->mainchain.
While exchanges may exist that allow sidechain->mainchain withdrawal faster, those can only operate if the number of coins exiting the sidechain is approximately equal to coins entering the sidechain (remember, it is an *exchange*, coins are not actually moved from one to the other).
If there is a "thundering herd" problem, then exchanges will saturate and the sidechain->mainchain withdrawal mechanism has to come into play, and if the Drivechain security parameter (which secures sidechains from 51% attack confiscation)
In a "thundering herd" situation, the peg can be lost, meaning that sidechain coins become devalued relative to mainchain coins they are purportedly equivalent to.

A "thundering herd" exiting the sidechain can happen, for example, if the sidechain is primarily used to prototype a new feature, and the feature is demonstrably so desirable that Bitcoin Core actually adds it.
In that case, the better security of the mainchain becomes desirable, and the sidechain no longer has a unique feature to incentivize keeping your funds there (since mainchain has/will have that feature).
In that case, the sidechain coin value can transiently drop due to the sidechain->mainchain withdrawal bottleneck caused by the Drivechain security parameter.
And if the value can temporarily drop, well, it is not much of a peg, then.

* If the Drivechain security parameter is too low, then a short 51% attack is enough to confiscate all sidechain coins.
* If the Drivechain seucrity parameter is too large, then a coincidental large number of sidechain->mainchain exits risks triggering a thundering herd that temporarily devalues the sidechain value relative to mainchain.

Against 51% attack confiscation, Paul Sztorc I believe proposes a "nuclear option" where mainchain fullnodes are upgraded to ignore historical blocks created by the 51% attacker.
The point is that a 51% attacker takes on the risk that confiscation will simply cause everyone to evict all miners and possibly destroy Bitcoin entirely, and rational 51% attackers will not do so, since then their mining hardware becomes useless.
I believe this leads to a situation where a controversial chainsplit of a sidechain can effectively "infect" mainchain, with competing mainchain miners with different views of the sidechain censoring each other, thus removing isolation of the sidechain from the mainchain.

--

More to the point: what are sidechains **for**?

* If sidechains are for prototyping new features, then you are probably better off getting a bunch of developer friends together and creating a federation that runs the sidechain so you can tinker on new features with friends.
  * This is how SegWit was prototyped in Elements Alpha, the predecessor of Liquid.
* If sidechains are for scaling, then:
  * We already ***know*** that blockchains cannot scale.
  * Your plan for scaling is to make ***more*** blockchains?
    Which we know cannot scale, right?
  * Good luck.

Now, if we were to consider scaling...

As I pointed out above, in principle a federated sidechain simply decided to use a blockchain to coordinate the federation members.
Nothing really prevents the federation from using a different mechanims.

In addition, federations (whether signer federations like in RSK or Liquid, or miner federations like in Drivechains) have custodial risk if you put your funds in them.
The only way to avoid the custodial risk is if ***you*** were one of the signatories of the federation, and the federation was an n-of-n.

Now, let us consider a 2-of-2 federation, the smallest possible federation.
As long as *you* are one of the two signatories, you have no custodial risk in putting funds in this federation --- nothing can happen to the mainchain funds without your say-so, so the federation cannot confiscate your funds.

And again, there is no real need to use a big, inefficient data structure like a **blockchain**.
In fact, in a 2-of-2 federation, there are only two members, so a lot of the blockchain overhead can be reduced to just a bunch of fairly simple protocol messages you send to each other, no need for a heavy history-retaining append-only data structure.

Of course, only you and the other signatory in this 2-of-2 federation can safely keep funds in that federation.
You cannot pay a third party with those funds, because that third party now takes on custodial risk, you and your coutnerparty can collude to steal the funds of the third party.
However, suppose your counterparty was a member of another 2-of-2 federation, this time with the third party you want to pay.
You can use an atomic swap mechanism of some kind so that you pay your couterparty if that couterparty pays the third party.

And guess what?
That is just Lightning Network.

Regards,
ZmnSCPxj

From erik at q32.com  Thu Sep  2 20:20:21 2021
From: erik at q32.com (Erik Aronesty)
Date: Thu, 2 Sep 2021 16:20:21 -0400
Subject: [bitcoin-dev] Drivechain: BIP 300 and 301
In-Reply-To: <MibU_fn--3-2@tutanota.de>
References: <MibU_fn--3-2@tutanota.de>
Message-ID: <CAJowKgLX=9FVAWsSBdxwzopR=+81mVjZH0o1=MHJd7ebvrXsbA@mail.gmail.com>

drivechain is a cool proposal.   i don't think there's a ton of
obvious risk to the network itself (not slow, not too much work for
nodes, etc), but it seems to encourage "bad behavior", not sure the
incentives line up to prevent thefts, and not sure that won't turn
around and bite bitcoin's main chain.

of course stacks can do this even without drivechain, so not sure what
we're hiding from there

if you're talking about extensions there's lightning-compatible
mimblewimble, which is probably more important, since it gets bitcoin
to global-scale payments, while improving fungibility, and probably
can't be implemented safely via drivechain



On Thu, Sep 2, 2021 at 2:24 PM Prayank via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
>
> printf("Hello, World!");
>
> What are your thoughts on Drivechain and associated BIPs?
>
> This article compares Liquid and Lightning: https://blog.liquid.net/six-differences-between-liquid-and-lightning/. Two things from it that I am interested in while evaluating Drivechain:
>
> 1.Trust model
> 2.On-Ramps and Off-Ramps
>
> Other things:
>
> 1.Security of Bitcoin (Layer 1)
> 2.Bitcoin transactions and fees expected on layer 1 because of Drivechain
>
> Similarities and Differences between RSK and Ethereum: https://medium.com/iovlabs-innovation-stories/similarities-and-differences-between-rsk-and-ethereum-e480655eff37
>
> Paul Sztorc had mentioned few things about fees in this video: https://youtu.be/oga8Pwbq9M0?t=481 I am interested to know same for LN, Liquid and Rootstock as well so asked a question on Bitcoin Stackexchange today: https://bitcoin.stackexchange.com/questions/109466/bitcoin-transactions-associated-with-layer-2-projects
>
> Two critiques are mentioned here: https://www.drivechain.info/peer-review/peer-review-new/ with lot of names. I don't agree with everything mentioned on project website although any comments on technical things that can help Bitcoin and Bitcoin projects will be great.
>
> Why discuss here and not on Twitter?
>
> 1.Twitter is not the best place for such discussions. There are some interesting threads but Its mostly used for followers, likes, retweets etc. and people can write anything for it.
> 2.Avoid misinformation, controversies etc.
>
> My personal opinion:
>
> We should encourage sidechain projects. I don't know much about Drivechain to form a strong opinion but concept looks good which can help in making better sidechains.
>
> ----------------------------------------------------------------------------------------------------------------------
>
>
> The website used in the slides of above YouTube video is misleading for few reasons:
>
> 1.Blocks mined everyday (in MB) for Bitcoin is ~150 MB. It is ~600 MB for Ethereum. Block limits for Bitcoin is ~4 MB per 10 minutes and ~500 MB for Ethereum. If full nodes will be run by few organizations on AWS we can basically do everything on chain. However the main goal isn't too make money and create an illusion to do something innovative, primary goal was/is decentralized network that allows settlement of payments.
>
> 2.Bitcoin uses UTXO model while Ethereum uses Account model. Basic difference in transactions for two is explained in an article https://coinmetrics.io/on-data-and-certainty/. Irony is the website in the slides for screenshot is using Coinmetrics API and this misleading website is even shared by Coinmetrics team on Twitter. So in some cases you are doing more transactions, paying more fees for work which could have been done with less. Inefficiency.
>
> 3.Failed transactions paying fees on Ethereum everyday, no such transactions on Bitcoin.
>
> 4.Other improvements that affect fees: Segwit, Layer 2, Batching, UTXO consolidation, Fee estimation, Coin selection, Exchanges, Wallets etc.
>
>
> --
> Prayank
>
> A3B1 E430 2298 178F
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From ts at cronosurf.com  Fri Sep  3 05:08:42 2021
From: ts at cronosurf.com (ts)
Date: Fri, 3 Sep 2021 00:08:42 -0500
Subject: [bitcoin-dev] Human readable checksum (verification code) to
 avoid errors on BTC public addresses
In-Reply-To: <CAJna-Hhtr0v_uEE-4ET4FPNnGnPv8sW2JXkVka0XDkphy_YmSg@mail.gmail.com>
References: <f31bc6b0-f9b3-be4c-190c-fc292821b24b@cronosurf.com>
 <6f69f132-211f-9d42-8023-c3b0264af439@cronosurf.com>
 <3isqiyeCtgJdzEvbbm3ZoS6h1_4l3YjtPypqJAPto5cp2K1BebmgEdVGLGTYt2j803RnfaiIbFxjGdPIac8vHHpMmelwStYm0om_szvX7xc=@wuille.net>
 <75a02b16-0aac-4afd-1a9e-f71a8396baea@cronosurf.com>
 <CAJna-Hhtr0v_uEE-4ET4FPNnGnPv8sW2JXkVka0XDkphy_YmSg@mail.gmail.com>
Message-ID: <e81a1400-2b3a-bdeb-ccfd-6aad56c09785@cronosurf.com>

Hi Marek,

Marek Palatinus wrote on 8/31/21 3:47 AM:
> I fully agree with sipa and his reasoning that this proposal is not solving any particular 
> problem, but making it actually a bit worse.
Ok, I understand. I'm just trying to find ways to reduce the risk of sending to the wrong 
address and to make the transaction process a bit more user friendly, specially for 
inexperienced users. I am sure that it can be implemented in a way without making it "worse". 
For example, if there is the risk that the user looks ONLY at the code and not at the address, 
then the code should have enough entropy to account for it. If looking at 6 characters is 
considered to be enough, then the code should also be 6 characters long. As I mentioned in my 
following message, the code could be made from specific characters of the address instead of a 
checksum (e.g. first 4 and last 2 characters). By showing these characters to the user 
separately and in a bigger font, he will be encouraged to verify all of these characters.

> Also, do you know what I hate more than copy&pasting bitcoin addresses? Copy pasting zillion 
> random fields for SEPA/wire transfers. And I believe that a single copy pasta of a bitcoin 
> address is a much better user experience after all.

I totally agree with this :)

Cheers,
TS


> Best,
> slush
>
> On Tue, Aug 31, 2021 at 9:08 AM ts via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org 
> <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:
>
>     Pieter, thanks for your comments. Here my thoughts:
>
>     Pieter Wuille wrote on 8/29/21 9:24 AM:
>     > On Saturday, August 28th, 2021 at 5:17 PM, ts via bitcoin-dev
>     <bitcoin-dev at lists.linuxfoundation.org <mailto:bitcoin-dev at lists.linuxfoundation.org>>
>     wrote:
>     >
>     >> Following up on my original proposal, I would like to get some more feedback of the
>     community
>     >>
>     >> to see if this could be realized at some point. Also, any recommendations as to who
>     to contact
>     >>
>     >> to get things rolling?
>     >
>     > I honestly don't understand the point of what you're suggesting.
>
>     It is about creating a simple technical assistance that makes it more user friendly and
>     less
>     error prone to verify the entered address. For all types of users, including those who are
>     less tech savvy.
>
>
>     > * If you're concerned about random typos, this is something already automatically
>     protected against through the checksum (both base58check or bech32/bech32m).
>
>     I agree, but as mentioned in the original proposal, it is not about random typos (although
>     this would help for other coins without integrated checksum of course), but rather about
>     copy&paste errors (both technical or user caused).
>
>
>     > * If you're concerned about accidentally entering the wrong - but honestly created -
>     address, comparing any few characters of the address is just as good as any other. It
>     doesn't even require the presence of a checksum. Looking at the last N characters, or
>     the middle N, or anything except the first few, will do, and is just as good as an
>     "external" checksum added at the end. For randomly-generated addresses (as honest ones
>     are), each of those has exactly as much entropy.
>
>     Correct. However, I believe that ADDITIONALLY to looking at N characters, a quick check
>     of a 3
>     or 4 digit code in bigger font next to the address would make for a better user experience.
>     This gives the user the reassurance that there is definitely no error. I agree that most
>     users
>     with technical background including most of us here will routinely check the first/last N
>     characters. I usually check the first 3 + last 3 characters. But I don't think this is very
>     user friendly. More importantly, I once had the case that two addresses were very
>     similar at
>     precisely those 6 characters, and only a more close and concentrated look made me see the
>     difference. Moreover, some inexperienced users that are not aware of the consequences of
>     entering a wrong address (much worse than entering the wrong bank account in an online bank
>     transfer) might forget to look at the characters altogether.
>
>
>     > * If you're concerned about maliciously constructed addresses, which are designed to
>     look similar in specific places, an attacker can just as easily make the external
>     checksum collide (and having one might even worsen this, as now the attacker can focus
>     on exactly that, rather than needing to focus on every other character).
>
>     Not so concerned about this case, since this is a very special case that can only occur
>     under
>     certain circumstances. But taking this case also into consideration, this is why the user
>     should use the verification code ADDITIONALLY to the normal way of verifying, not
>     instead. If
>     the attacker only focuses on the verification code, he will only be successful with
>     users that
>     ONLY look at this code. But if the attacker intends to be more successful, he now needs to
>     create a valid address that is both similar in specific places AND produces the same
>     verification code, which is way more difficult to achieve.
>
>
>     > Things would be different if you'd suggest a checksum in another medium than text
>     (e.g. a visual/drawing/colorcoding one). But I don't see any added value for an
>     additional text-based checksum when addresses are already text themselves.
>
>     Yes, a visual checksum could also work. Christopher Allen proposed to use LifeHash as an
>     alternative. It would be a matter of balancing the more complex implementation and need of
>     space in the app's layout with the usability and advantages of use. One advantage of the
>     digit
>     verification code is that it can be spoken in a call or written in a message.
>
>     > This is even disregarding the difficulty of getting the ecosystem to adopt such changes.
>
>     No changes are needed, only an agreement or recommendation on which algorithm for the code
>     generation should be used. Once this is done, it is up to the developers of wallets and
>     exchanges to implement this feature as they see fit.
>
>     Greetings,
>     TS
>     _______________________________________________
>     bitcoin-dev mailing list
>     bitcoin-dev at lists.linuxfoundation.org <mailto:bitcoin-dev at lists.linuxfoundation.org>
>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>     <https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210903/4f4ab95b/attachment.html>

From prayank at tutanota.de  Fri Sep  3 09:47:45 2021
From: prayank at tutanota.de (Prayank)
Date: Fri, 3 Sep 2021 11:47:45 +0200 (CEST)
Subject: [bitcoin-dev] Drivechain: BIP 300 and 301
In-Reply-To: <3dyAATLXbsE7WBzpVIc0s4sRkSFhR_5NN04uUgx3o9LH48IKR6EHL3V45PfpRM96yxHXdsjd7WC7MGuPlBw7MRpCkpXRsB-WI7i3-Nr13Ew=@protonmail.com>
References: <MibU_fn--3-2@tutanota.de>
 <3dyAATLXbsE7WBzpVIc0s4sRkSFhR_5NN04uUgx3o9LH48IKR6EHL3V45PfpRM96yxHXdsjd7WC7MGuPlBw7MRpCkpXRsB-WI7i3-Nr13Ew=@protonmail.com>
Message-ID: <Mif2dMR--3-2@tutanota.de>

Good morning?ZmnSCPxj,

Thanks for sharing all the details. One thing that I am not sure about:

>?* We already ***know*** that blockchains cannot scale
> * Your plan for scaling is to make ***more*** blockchains?

Scaling Bitcoin can be different from scaling Bitcoin sidechains. You can experiment with lot of things on sidechains to scale which isn't true for Bitcoin. Most important thing is requirements for running a node differ. Its easy to run a node for LN, Liquid and Rootstock right now. Will it remain the same? I am not sure.

LND:?https://github.com/lightningnetwork/lnd/blob/master/docs/INSTALL.md

Liquid:?https://help.blockstream.com/hc/en-us/articles/900002026026-How-do-I-set-up-a-Liquid-node-

Rootstock:?https://developers.rsk.co/rsk/node/install/

>?More to the point: what are sidechains **for**?

Smart contracts are possible on Bitcoin but with limited functionality so lot of applications are not possible using Bitcoin (Layer1). Some of these don't even make sense on Layer 1 and create other issues like MEV however deploying them on sidechains should not affect base layer.

>?Increasing the Drivechain security parameter leads to slower sidechain->mainchin withdrawals, effectively a bottleneck on how much can be transferred sidechain->mainchain.

I think 'withdrawals' is the part which can be improved in Drivechain. Not sure about any solution at this point or trade-offs involved but making few changes can help Drivechain and Bitcoin.
I agree with everything else you explained and emails like these will be helpful for everyone trying to understand what's going on with Layer 2 on Bitcoin.

-- 
Prayank

A3B1 E430 2298 178F



Sep 3, 2021, 02:32 by ZmnSCPxj at protonmail.com:

> Good morning Prayank,
>
> Just to be clear, neither Liquid nor RSK, as of my current knowledge, are Drivechain systems.
>
> Instead, they are both federated sidechains.
> The money owned by a federated sidechain is, as far s the Bitcoin blockchain is concerned, really owned by the federation that.runs the sidechain.
>
> Basically, a mainchain->sidechain transfer is done by paying to a federation k-of-n address and a coordination signal of some kind (details depending on federated sidechain) to create the equivalent coins on the sidechain.
> A sidechain->mainchain transfer is done by requesting some coins on the sidechain to be destroyed, and then the federation will send some of its mainchain k-of-n coins into whatever address you indicate you want to use on the mainchain.
>
> In theory, a sufficient quorum of the federation can decide to ignore the sidechain data entirely and spend the mainchain money arbitrarily, and the mainchain layer will allow this (being completely ignorant of he sidechain).
>
> In such federated sidechains, the federation is often a fixed predetermined signing set, and changes to that federation are expected to be rare.
>
> Federated sidechains are ultimately custodial; as noted above, the federation could in theory abscond with the funds completely, and the mainchain would not care if the sidechain federation executes its final exit strategy and you lose your funds.
> One can consider federated sidechains to be a custodian with multiple personality disorder, that happens to use a blockchain to keep its individual sub-personalities coordinated with each other, but ultimately control of the money is contingent on the custodian following the dictates of the supposed owners of the coin.
> From a certain point of view, it is actually immaterial that there is a separate blockchain called the "sidechain" --- it is simply that a blockchain is used to coordinate the custodians of the coin, but in principle any other coordination mechanism can be used between them, including a plain database.
>
>
> With Drivechains, custody of the sidechain funds is held by mainchain miners.
> Again, this is still a custodial setup.
> A potential issue here is that the mainchain miners cannot be identified (the entire point is anonymity of miners is possible), which may be of concern.
>
> In particular, note that solely on mainchain, all that miners determine is the *ordering* and *timing* of transactions.
> Let us suppose that there is a major 51% attack attempt on the Bitcoin blockchain.
> We expect that such an attack will be temporary --- individuals currently not mining may find that their HODLings are under threat of the 51% attack, and may find it more economic to run miners at a loss, in order to protect their stacks rather than lose it.
> Thus, we expect that a 51% attack will be temporary, as other miners will arise inevitably to take back control of transaction processing.
> https://github.com/libbitcoin/libbitcoin-system/wiki/Threat-Level-Paradox
>
> In particular, on the mainchain, 51% miners cannot reverse deep history.
> If you have coins you have not moved since 2017, for example, the 51% attack is expected to take about 4 years before it can begin to threaten your ownership of those coins (hopefully, in those 4 years, you will get a clue and start mining at a loss to protect your funds from outright loss, thus helping evict the 51% attacker).
> 51% miners can, in practice, only prevent transfers (censorship), not force transfer of funds (confiscation).
> Once the 51% attacker is evicted (and they will in general be evicted), then coins you owned that were deeply confirmed remain under your control.
>
> With Drivechains, however, sidechain funds can be confiscated by a 51% attacker, by forcing a bogus sidechain->mainchain withdrawal.
> The amount of time it takes is simply the security parameter of the Drivechain spec.
> It does not matter if you were holding those funds in the sidechain for several years without moving them --- a 51% attacker that is able to keep control of the mainchain blockchain, for the Drivechain security parameter, will be capable of confiscating sidechain funds outright.
> Thus, even if the 51% attacker is evicted, then your coins in the sidechain can be confiscated and no longer under your control.
>
> Increasing the Drivechain security parameter leads to slower sidechain->mainchin withdrawals, effectively a bottleneck on how much can be transferred sidechain->mainchain.
> While exchanges may exist that allow sidechain->mainchain withdrawal faster, those can only operate if the number of coins exiting the sidechain is approximately equal to coins entering the sidechain (remember, it is an *exchange*, coins are not actually moved from one to the other).
> If there is a "thundering herd" problem, then exchanges will saturate and the sidechain->mainchain withdrawal mechanism has to come into play, and if the Drivechain security parameter (which secures sidechains from 51% attack confiscation)
> In a "thundering herd" situation, the peg can be lost, meaning that sidechain coins become devalued relative to mainchain coins they are purportedly equivalent to.
>
> A "thundering herd" exiting the sidechain can happen, for example, if the sidechain is primarily used to prototype a new feature, and the feature is demonstrably so desirable that Bitcoin Core actually adds it.
> In that case, the better security of the mainchain becomes desirable, and the sidechain no longer has a unique feature to incentivize keeping your funds there (since mainchain has/will have that feature).
> In that case, the sidechain coin value can transiently drop due to the sidechain->mainchain withdrawal bottleneck caused by the Drivechain security parameter.
> And if the value can temporarily drop, well, it is not much of a peg, then.
>
> * If the Drivechain security parameter is too low, then a short 51% attack is enough to confiscate all sidechain coins.
> * If the Drivechain seucrity parameter is too large, then a coincidental large number of sidechain->mainchain exits risks triggering a thundering herd that temporarily devalues the sidechain value relative to mainchain.
>
> Against 51% attack confiscation, Paul Sztorc I believe proposes a "nuclear option" where mainchain fullnodes are upgraded to ignore historical blocks created by the 51% attacker.
> The point is that a 51% attacker takes on the risk that confiscation will simply cause everyone to evict all miners and possibly destroy Bitcoin entirely, and rational 51% attackers will not do so, since then their mining hardware becomes useless.
> I believe this leads to a situation where a controversial chainsplit of a sidechain can effectively "infect" mainchain, with competing mainchain miners with different views of the sidechain censoring each other, thus removing isolation of the sidechain from the mainchain.
>
> --
>
> More to the point: what are sidechains **for**?
>
> * If sidechains are for prototyping new features, then you are probably better off getting a bunch of developer friends together and creating a federation that runs the sidechain so you can tinker on new features with friends.
>  * This is how SegWit was prototyped in Elements Alpha, the predecessor of Liquid.
> * If sidechains are for scaling, then:
>  * We already ***know*** that blockchains cannot scale.
>  * Your plan for scaling is to make ***more*** blockchains?
>  Which we know cannot scale, right?
>  * Good luck.
>
> Now, if we were to consider scaling...
>
> As I pointed out above, in principle a federated sidechain simply decided to use a blockchain to coordinate the federation members.
> Nothing really prevents the federation from using a different mechanims.
>
> In addition, federations (whether signer federations like in RSK or Liquid, or miner federations like in Drivechains) have custodial risk if you put your funds in them.
> The only way to avoid the custodial risk is if ***you*** were one of the signatories of the federation, and the federation was an n-of-n.
>
> Now, let us consider a 2-of-2 federation, the smallest possible federation.
> As long as *you* are one of the two signatories, you have no custodial risk in putting funds in this federation --- nothing can happen to the mainchain funds without your say-so, so the federation cannot confiscate your funds.
>
> And again, there is no real need to use a big, inefficient data structure like a **blockchain**.
> In fact, in a 2-of-2 federation, there are only two members, so a lot of the blockchain overhead can be reduced to just a bunch of fairly simple protocol messages you send to each other, no need for a heavy history-retaining append-only data structure.
>
> Of course, only you and the other signatory in this 2-of-2 federation can safely keep funds in that federation.
> You cannot pay a third party with those funds, because that third party now takes on custodial risk, you and your coutnerparty can collude to steal the funds of the third party.
> However, suppose your counterparty was a member of another 2-of-2 federation, this time with the third party you want to pay.
> You can use an atomic swap mechanism of some kind so that you pay your couterparty if that couterparty pays the third party.
>
> And guess what?
> That is just Lightning Network.
>
> Regards,
> ZmnSCPxj
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210903/2499e8b6/attachment-0001.html>

From prayank at tutanota.de  Fri Sep  3 10:07:55 2021
From: prayank at tutanota.de (Prayank)
Date: Fri, 3 Sep 2021 12:07:55 +0200 (CEST)
Subject: [bitcoin-dev] Drivechain: BIP 300 and 301
Message-ID: <Mif7Fdk--3-2@tutanota.de>

> of course stacks can do this even without drivechain, so not sure whatwe're hiding from there

Stacks is not a Bitcoin sidechain IMO. It has its own native token which isn't pegged to BTC. Premined.? It uses Bitcoin as a storage and broadcast medium for recording all blocks. Marketing with lot of misinformation. None of these things really help Bitcoin.

-- 
Prayank

A3B1 E430 2298 178F
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210903/4d1737c1/attachment.html>

From jlrubin at mit.edu  Sat Sep  4 03:32:19 2021
From: jlrubin at mit.edu (Jeremy)
Date: Fri, 3 Sep 2021 20:32:19 -0700
Subject: [bitcoin-dev] Note on Sequence Lock Upgrades Defect
Message-ID: <CAD5xwhiKU1fuhqmKsx28f1nuw9CmvbyrS=BtM4X-L+WPgWY3Wg@mail.gmail.com>

Hi Bitcoin Devs,

I recently noticed a flaw in the Sequence lock implementation with respect
to upgradability. It might be the case that this is protected against by
some transaction level policy (didn't see any in policy.cpp, but if not,
I've put up a blogpost explaining the defect and patching it
https://rubin.io/bitcoin/2021/09/03/upgradable-nops-flaw/

I've proposed patching it here https://github.com/bitcoin/bitcoin/pull/22871,
it is proper to widely survey the community before patching to ensure no
one is depending on the current semantics in any live application lest this
tightening of standardness rules engender a confiscatory effect.

Best,

Jeremy

--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210903/1d2d16a4/attachment.html>

From jlrubin at mit.edu  Sun Sep  5 03:19:57 2021
From: jlrubin at mit.edu (Jeremy)
Date: Sat, 4 Sep 2021 20:19:57 -0700
Subject: [bitcoin-dev] Note on Sequence Lock Upgrades Defect
In-Reply-To: <CAD5xwhiKU1fuhqmKsx28f1nuw9CmvbyrS=BtM4X-L+WPgWY3Wg@mail.gmail.com>
References: <CAD5xwhiKU1fuhqmKsx28f1nuw9CmvbyrS=BtM4X-L+WPgWY3Wg@mail.gmail.com>
Message-ID: <CAD5xwhjrcTMSkikYFaNmMJeAhmR2cwTEbt7G80-G9Xj3vNnbrA@mail.gmail.com>

In working on resolving this issue, one issue that has come up is what
sequence values get used by wallet implementations?

E.g., in Bitcoin Core a script test says

BIP125_SEQUENCE_NUMBER = 0xfffffffd  # Sequence number that is rbf-opt-in
(BIP 125) and csv-opt-out (BIP 68)

Are any other numbers currently expected by any wallet software to be
broadcastable with the DISABLE flag set? Does anyone use *this* number? Is
there any advantage of this number v.s. just 0? Do people commonly use
0xfffffffd? 0xfffffffe is special, but it seems the former has the
alternative of either 0 valued sequence lock (1<<22 or 0).

Are there any other sequence numbers that are not defined in a BIP that
might be used somewhere?

Cheers,

Jeremy
--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>


On Fri, Sep 3, 2021 at 8:32 PM Jeremy <jlrubin at mit.edu> wrote:

> Hi Bitcoin Devs,
>
> I recently noticed a flaw in the Sequence lock implementation with respect
> to upgradability. It might be the case that this is protected against by
> some transaction level policy (didn't see any in policy.cpp, but if not,
> I've put up a blogpost explaining the defect and patching it
> https://rubin.io/bitcoin/2021/09/03/upgradable-nops-flaw/
>
> I've proposed patching it here
> https://github.com/bitcoin/bitcoin/pull/22871, it is proper to widely
> survey the community before patching to ensure no one is depending on the
> current semantics in any live application lest this tightening of
> standardness rules engender a confiscatory effect.
>
> Best,
>
> Jeremy
>
> --
> @JeremyRubin <https://twitter.com/JeremyRubin>
> <https://twitter.com/JeremyRubin>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210904/036089ac/attachment.html>

From dave at dtrt.org  Mon Sep  6 02:35:25 2021
From: dave at dtrt.org (David A. Harding)
Date: Sun, 5 Sep 2021 16:35:25 -1000
Subject: [bitcoin-dev] Note on Sequence Lock Upgrades Defect
In-Reply-To: <CAD5xwhiKU1fuhqmKsx28f1nuw9CmvbyrS=BtM4X-L+WPgWY3Wg@mail.gmail.com>
References: <CAD5xwhiKU1fuhqmKsx28f1nuw9CmvbyrS=BtM4X-L+WPgWY3Wg@mail.gmail.com>
Message-ID: <20210906023525.nui6beegrzopwfq4@ganymede>

On Fri, Sep 03, 2021 at 08:32:19PM -0700, Jeremy via bitcoin-dev wrote:
> Hi Bitcoin Devs,
> 
> I recently noticed a flaw in the Sequence lock implementation with respect
> to upgradability. It might be the case that this is protected against by
> some transaction level policy (didn't see any in policy.cpp, but if not,
> I've put up a blogpost explaining the defect and patching it
> https://rubin.io/bitcoin/2021/09/03/upgradable-nops-flaw/

Isn't this why BIP68 requires using tx.version=2?  Wouldn't we just
deploy any new nSequence rules with tx.version>2?

-Dave
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210905/2f55d419/attachment.sig>

From jlrubin at mit.edu  Mon Sep  6 03:17:17 2021
From: jlrubin at mit.edu (Jeremy)
Date: Sun, 5 Sep 2021 20:17:17 -0700
Subject: [bitcoin-dev] Note on Sequence Lock Upgrades Defect
In-Reply-To: <20210906023525.nui6beegrzopwfq4@ganymede>
References: <CAD5xwhiKU1fuhqmKsx28f1nuw9CmvbyrS=BtM4X-L+WPgWY3Wg@mail.gmail.com>
 <20210906023525.nui6beegrzopwfq4@ganymede>
Message-ID: <CAD5xwhh=oWtjumxn5cLJ3gs69wrhHvOTAD3gtywS8_kb7MLLqA@mail.gmail.com>

BIP 68 says >= 2:
*This specification defines the meaning of sequence numbers for
transactions with an nVersion greater than or equal to 2 for which the rest
of this specification relies on.*
BIP-112 says not < 2
// Fail if the transaction's version number is not set high
// enough to trigger BIP 68 rules.
if (static_cast<uint32_t>(txTo->nVersion) < 2) return false;

A further proof that this needs fix: the flawed upgradable semantic exists
in script as well as in the transaction nSeqeunce. we can't really control
the transaction version an output will be spent with in the future, so it
would be weird/bad to change the semantic in transaction version 3.

--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>


On Sun, Sep 5, 2021 at 7:36 PM David A. Harding <dave at dtrt.org> wrote:

> On Fri, Sep 03, 2021 at 08:32:19PM -0700, Jeremy via bitcoin-dev wrote:
> > Hi Bitcoin Devs,
> >
> > I recently noticed a flaw in the Sequence lock implementation with
> respect
> > to upgradability. It might be the case that this is protected against by
> > some transaction level policy (didn't see any in policy.cpp, but if not,
> > I've put up a blogpost explaining the defect and patching it
> > https://rubin.io/bitcoin/2021/09/03/upgradable-nops-flaw/
>
> Isn't this why BIP68 requires using tx.version=2?  Wouldn't we just
> deploy any new nSequence rules with tx.version>2?
>
> -Dave
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210905/036b421e/attachment.html>

From dave at dtrt.org  Mon Sep  6 06:23:41 2021
From: dave at dtrt.org (David A. Harding)
Date: Sun, 5 Sep 2021 20:23:41 -1000
Subject: [bitcoin-dev] Braidpool: Proposal for a decentralised mining
	pool
In-Reply-To: <CAGpPWDZNGrLnfVg9aTBax8esEEM=mMTjiC1VrdAr9hyAwTNXzg@mail.gmail.com>
References: <0aff157f62ea4abba71df4f87eb54880-kohli@ctemplar.com>
 <CAGpPWDZNGrLnfVg9aTBax8esEEM=mMTjiC1VrdAr9hyAwTNXzg@mail.gmail.com>
Message-ID: <20210906062341.veujnjng5nw4ykv6@ganymede>

On Wed, Sep 01, 2021 at 11:46:55PM -0700, Billy Tetrud via bitcoin-dev wrote:
> How would you compare this to Stratum v2?

Specifically, I'd be interested in learning what advantages this has
over a centralized mining pool using BetterHash or StratumV2 with
payouts made via LN (perhaps immediately after each submitted share is
validated).

-Dave
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210905/a2d81ede/attachment.sig>

From eric at voskuil.org  Mon Sep  6 07:29:01 2021
From: eric at voskuil.org (Eric Voskuil)
Date: Mon, 6 Sep 2021 09:29:01 +0200
Subject: [bitcoin-dev] Braidpool: Proposal for a decentralised mining
	pool
In-Reply-To: <20210906062341.veujnjng5nw4ykv6@ganymede>
References: <20210906062341.veujnjng5nw4ykv6@ganymede>
Message-ID: <F8F16054-78DF-4D06-BD73-F8EBE7D902A5@voskuil.org>

It doesn?t centralize payment, which ultimately controls transaction selection (censorship).

e

> On Sep 6, 2021, at 08:25, David A. Harding via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
> 
> ?On Wed, Sep 01, 2021 at 11:46:55PM -0700, Billy Tetrud via bitcoin-dev wrote:
>> How would you compare this to Stratum v2?
> 
> Specifically, I'd be interested in learning what advantages this has
> over a centralized mining pool using BetterHash or StratumV2 with
> payouts made via LN (perhaps immediately after each submitted share is
> validated).
> 
> -Dave
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From dave at dtrt.org  Mon Sep  6 07:54:30 2021
From: dave at dtrt.org (David A. Harding)
Date: Sun, 5 Sep 2021 21:54:30 -1000
Subject: [bitcoin-dev] Braidpool: Proposal for a decentralised mining
	pool
In-Reply-To: <F8F16054-78DF-4D06-BD73-F8EBE7D902A5@voskuil.org>
References: <20210906062341.veujnjng5nw4ykv6@ganymede>
 <F8F16054-78DF-4D06-BD73-F8EBE7D902A5@voskuil.org>
Message-ID: <20210906075430.hk44gaueu3njdkl3@ganymede>

On Mon, Sep 06, 2021 at 09:29:01AM +0200, Eric Voskuil wrote:
> It doesn?t centralize payment, which ultimately controls transaction selection (censorship).

Yeah, but if you get paid after each share via LN and you can switch
pools instantly, then the worst case with centralized pools is that 
you don't get paid for one share.  If the hasher sets their share
difficulty low enough, that shouldn't be a big deal.

I'm interested in whether braidpool offers any significant benefits over
an idealized version of centralized mining with independent transaction
selection.

-Dave
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210905/26c8c9d4/attachment-0001.sig>

From darosior at protonmail.com  Mon Sep  6 06:16:44 2021
From: darosior at protonmail.com (darosior)
Date: Mon, 06 Sep 2021 06:16:44 +0000
Subject: [bitcoin-dev] Note on Sequence Lock Upgrades Defect
In-Reply-To: <CAD5xwhh=oWtjumxn5cLJ3gs69wrhHvOTAD3gtywS8_kb7MLLqA@mail.gmail.com>
References: <CAD5xwhiKU1fuhqmKsx28f1nuw9CmvbyrS=BtM4X-L+WPgWY3Wg@mail.gmail.com>
 <20210906023525.nui6beegrzopwfq4@ganymede>
 <CAD5xwhh=oWtjumxn5cLJ3gs69wrhHvOTAD3gtywS8_kb7MLLqA@mail.gmail.com>
Message-ID: <2D1cXBiA_W6B87Ck0xS3PXO5S9LStH7FhqzE03wC209Md5g_M7Yb--JDI346XXu0-3vVaLx1eZpHCpPO2ZFKf-cArettuAYYNr3lNpp619g=@protonmail.com>

Hi Jeremy,

I think it would be nice to have and suggested something similar (enforce minimality) in the context of
Miniscript a few months ago [0].

However your code:

const bool seq_is_reserved = (txin.nSequence < CTxIn::SEQUENCE_FINAL-2) && (
// when sequence is set to disabled, it is reserved for future use
((txin.nSequence & CTxIn::SEQUENCE_LOCKTIME_DISABLE_FLAG) != 0) ||
// when sequence has bits set outside of the type flag and locktime mask,
// it is reserved for future use.
((~(CTxIn::SEQUENCE_LOCKTIME_TYPE_FLAG | CTxIn::SEQUENCE_LOCKTIME_MASK) &
txin.nSequence) != 0)
);

Would effectively prevent Lightning Network commitment transactions from relaying. The protocol uses
a hack encoding the commitment transaction numbering in the part of nSequence (and nLockTime)
without consensus meaning. This both sets the LOCKTIME_DISABLE_FLAG and uses bits outside of
the mask.

[0] https://github.com/rust-bitcoin/rust-miniscript/pull/246#issue-671512626
[1] https://github.com/lightningnetwork/lightning-rfc/blob/master/03-transactions.md#commitment-transaction
??????? Original Message ???????
Le lundi 6 septembre 2021 ? 5:17 AM, Jeremy via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> a ?crit :

> BIP 68 says >= 2:This specification defines the meaning of sequence numbers for transactions with an nVersion greater than or equal to 2 for which the rest of this specification relies on.
> BIP-112 says not < 2
> // Fail if the transaction's version number is not set high
> // enough to trigger BIP 68 rules.
> if (static_cast<uint32_t>(txTo->nVersion) < 2) return false;
>
> A further proof that this needs fix: the flawed upgradable semantic exists in script as well as in the transaction nSeqeunce. we can't really control the transaction version an output will be spent with in the future, so it would be weird/bad to change the semantic in transaction version 3.
>
> --
> [@JeremyRubin](https://twitter.com/JeremyRubin)https://twitter.com/JeremyRubin
>
> On Sun, Sep 5, 2021 at 7:36 PM David A. Harding <dave at dtrt.org> wrote:
>
>> On Fri, Sep 03, 2021 at 08:32:19PM -0700, Jeremy via bitcoin-dev wrote:
>>> Hi Bitcoin Devs,
>>>
>>> I recently noticed a flaw in the Sequence lock implementation with respect
>>> to upgradability. It might be the case that this is protected against by
>>> some transaction level policy (didn't see any in policy.cpp, but if not,
>>> I've put up a blogpost explaining the defect and patching it
>>> https://rubin.io/bitcoin/2021/09/03/upgradable-nops-flaw/
>>
>> Isn't this why BIP68 requires using tx.version=2? Wouldn't we just
>> deploy any new nSequence rules with tx.version>2?
>>
>> -Dave
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210906/ff395880/attachment.html>

From eric at voskuil.org  Mon Sep  6 08:26:00 2021
From: eric at voskuil.org (Eric Voskuil)
Date: Mon, 6 Sep 2021 10:26:00 +0200
Subject: [bitcoin-dev] Braidpool: Proposal for a decentralised mining
	pool
In-Reply-To: <20210906075430.hk44gaueu3njdkl3@ganymede>
References: <20210906075430.hk44gaueu3njdkl3@ganymede>
Message-ID: <944064B6-B9CC-4325-ADA7-22B786A80A3B@voskuil.org>

Switching pools has always been possible. But the largest pool is the most profitable, and centralized pools are easily controlled. Decoupling selection without decoupling payout is an engineering change without a pooling pressure change.

e

> On Sep 6, 2021, at 10:01, David A. Harding <dave at dtrt.org> wrote:
> 
> ?On Mon, Sep 06, 2021 at 09:29:01AM +0200, Eric Voskuil wrote:
>> It doesn?t centralize payment, which ultimately controls transaction selection (censorship).
> 
> Yeah, but if you get paid after each share via LN and you can switch
> pools instantly, then the worst case with centralized pools is that
> you don't get paid for one share.  If the hasher sets their share
> difficulty low enough, that shouldn't be a big deal.
> 
> I'm interested in whether braidpool offers any significant benefits over
> an idealized version of centralized mining with independent transaction
> selection.
> 
> -Dave

From kohli at ctemplar.com  Mon Sep  6 09:03:06 2021
From: kohli at ctemplar.com (pool2win)
Date: Mon, 06 Sep 2021 09:03:06 -0000
Subject: [bitcoin-dev] Braidpool: Proposal for a decentralised
 mining	pool
In-Reply-To: <944064B6-B9CC-4325-ADA7-22B786A80A3B@voskuil.org>
References: <20210906075430.hk44gaueu3njdkl3@ganymede>
 <944064B6-B9CC-4325-ADA7-22B786A80A3B@voskuil.org>
Message-ID: <3b2fd0d3066c4f649e9a398a73ee5ded-kohli@ctemplar.com>

I see Braidpool as an improvement to P2Pool - i.e. make a peer to peer pool work at scale.

This is in contrast to Stratum v2, which brings some very good and much needed engineering improvements to centralised pools.

Specifically about transaction selection in Stratum V2, as far as I understand, the pool still controls both accepting the proposed block and also as Eric says, they still could refuse payouts. Here's a quote from the Stratum V2 docs[1]:

"The name Job ?Negotiation? Protocol is telling, as job selection is indeed a negotiation process between a miner and a pool. The miner proposes a block template, and it is up to a pool to accept or reject it."

As David says, a miner is free to hop pools, but generally pool hopping can be detrimental to a pool [2].

Further still, the immediate payouts to miners will work if they opt for PPS. But most centralised pools still use PPLNS(*) or equivalent.

I'd like to highlight an additional problem with centralised pools using PPLNS. These pools are opaque, at least to smaller miners, who can't view the shares received by the pool. Miners are forced to simply trust centralised pools to be honest and compute rewards fairly. A bug in their share tracking or reward calculation protocol could go unnoticed for a long time.

With Braidpool you get:
1. Transparent view of the shares received by the pool - thus have the ability to verify reward calculation, even with a PPLNS like scheme. This is the same advantage as P2Pool.
2. Payouts over one-way channel, so we don't consume block space for miner rewards payouts. This is different from P2Pool.
3. Using the transparent view of shares, we can build delivery of such shares to market makers providing futures contracts for hashrate. This is nigh impossible with opaque centralised pools.
4. We prepare for any attacks on centralised mining pools in the future - which we want to keep as the central aim of Braidpool. All the other advantages attract miners to Braidpool now, while preparing our defense against future attacks.

[1] Stratum V2: https://braiins.com/stratum-v2
[2] Analysis of Bitcoin Pooled Mining Reward Systems: https://arxiv.org/abs/1112.4980

(*) Starting a new PPS based pool requires a lot of funds. The probability of bankruptcy for pools providing PPS is pretty high.

---------- Original Message ----------
On Mon, September 6, 2021 at 8:01 AM,  David A. Harding via bitcoin-dev<bitcoin-dev at lists.linuxfoundation.org> wrote:
On Mon, Sep 06, 2021 at 09:29:01AM +0200, Eric Voskuil wrote:
> It doesn?t centralize payment, which ultimately controls transaction selection (censorship).

Yeah, but if you get paid after each share via LN and you can switch
pools instantly, then the worst case with centralized pools is that 
you don't get paid for one share.  If the hasher sets their share
difficulty low enough, that shouldn't be a big deal.

I'm interested in whether braidpool offers any significant benefits over
an idealized version of centralized mining with independent transaction
selection.

-Dave
 _______________________________________________
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From prayank at tutanota.de  Mon Sep  6 10:15:17 2021
From: prayank at tutanota.de (Prayank)
Date: Mon, 6 Sep 2021 12:15:17 +0200 (CEST)
Subject: [bitcoin-dev] Braidpool: Proposal for a decentralised mining
 pool
Message-ID: <MiuahdA--3-2@tutanota.de>

>?How would you compare this to Stratum v2?

Stratum v2 will help miners with encryption, broadcasting new blocks, signalling bits, choose transactions set, however the mining pools can still reject negotiations and censor payments.

Maybe Stratum v2 can be used in combination with other things like discreet log contracts:?https://mailmanlists.org/pipermail/dlc-dev/2021-May/000073.html

I think Braidpool does this in a better way.


-- 
Prayank

A3B1 E430 2298 178F
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210906/f0ab2026/attachment-0001.html>

From ZmnSCPxj at protonmail.com  Tue Sep  7 09:37:43 2021
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Tue, 07 Sep 2021 09:37:43 +0000
Subject: [bitcoin-dev] Drivechain: BIP 300 and 301
In-Reply-To: <Mif2dMR--3-2@tutanota.de>
References: <MibU_fn--3-2@tutanota.de>
 <3dyAATLXbsE7WBzpVIc0s4sRkSFhR_5NN04uUgx3o9LH48IKR6EHL3V45PfpRM96yxHXdsjd7WC7MGuPlBw7MRpCkpXRsB-WI7i3-Nr13Ew=@protonmail.com>
 <Mif2dMR--3-2@tutanota.de>
Message-ID: <fUQB1iJaia8hKE2X25B7MeBnqn78XEHzRBXO8OiZ4VIFwyk5TMBqyAgeZumeScl1jcKFS0nGW79Rkd7HqhIcB7-11Bw7jPS_ZNBGjXx-Q4s=@protonmail.com>

Good morning Prayank,


> Thanks for sharing all the details. One thing that I am not sure about:
>
> >?* We already ***know*** that blockchains cannot scale
> > * Your plan for scaling is to make ***more*** blockchains?
>
> Scaling Bitcoin can be different from scaling Bitcoin sidechains. You can experiment with lot of things on sidechains to scale which isn't true for Bitcoin.

I would classify this as "prototyping new features" (i.e. it just happens to be a feature that theoretically improves blockchain scaling, with the sidechain as a demonstration and the goal eventually to get something like it into Bitcoin blockchain proper), not really scaling-by-sidechains/shards, so I think this is a fine example of "just make a federated sidechain" solution for the prototyping bit.

Do note that the above idea is a kernel for the argument that Drivechains simply allow for miner-controlled block size increases, an argument I have seen elsewhere but have no good links for, so take it is hearsay.

> Most important thing is requirements for running a node differ. Its easy to run a node for LN, Liquid and Rootstock right now. Will it remain the same? I am not sure.
>
> LND:?https://github.com/lightningnetwork/lnd/blob/master/docs/INSTALL.md
>
> Liquid:?https://help.blockstream.com/hc/en-us/articles/900002026026-How-do-I-set-up-a-Liquid-node-
>
> Rootstock:?https://developers.rsk.co/rsk/node/install/

LN will likely remain easy to install and maintain, especially if you use C-Lightning and CLBOSS *cough*.

> >?More to the point: what are sidechains **for**?
>
> Smart contracts are possible on Bitcoin but with limited functionality so lot of applications are not possible using Bitcoin (Layer1). Some of these don't even make sense on Layer 1 and create other issues like MEV however deploying them on sidechains should not affect base layer.

Key being "should" --- as noted, part of the Drivechains security argument from Paul Sztorc is that a nuclear option can be deployed, which *possibly* means that issues in the sidechain may infect the mainchain.

Also see stuff like "smart contracts unchained": https://zmnscpxj.github.io/bitcoin/unchained.html
This allows creation of small federations which are *not* coordinated via inefficient blockchain structures.

So, really, my main point is: before going for the big heavy blockchain hammer, maybe other constructions are possible for any specific application?

>
> >?Increasing the Drivechain security parameter leads to slower sidechain->mainchin withdrawals, effectively a bottleneck on how much can be transferred sidechain->mainchain.
>
> I think 'withdrawals' is the part which can be improved in Drivechain. Not sure about any solution at this point or trade-offs involved but making few changes can help Drivechain and Bitcoin.

It is precisely due to the fact that the mainchain cannot validate the sidechain rules, that side->main transfers must be bottlenecked, so that sidechain miners have an opportunity to gainsay any theft attempts that violate the sidechain rules.
Consider a similar parameter in Lightning when exiting non-cooperatively from a channel, which allows the other side to gainsay any theft attempts, a parameter which will still exist even in Decker-Russell-Osuntokun.

This parameter existed even in the old Blockstream sidechains proposal from sipa et al.
For the old Blockstream proposal the parameter is measured in sidechain blocks, and the sidechain has its own miners instead of riding off mainchain, but ultimately there exists a parameter that restricts the rate at which side->main transfers can be performed.

At least LN does not require any changes at the base layer (at least not anymore, after SegWit).

Regards,
ZmnSCPxj


From michaelfolkson at gmail.com  Tue Sep  7 11:45:42 2021
From: michaelfolkson at gmail.com (Michael Folkson)
Date: Tue, 7 Sep 2021 12:45:42 +0100
Subject: [bitcoin-dev] BIP process meeting - Tuesday September 14th 23:00
 UTC on #bitcoin-dev Libera IRC
Message-ID: <CAFvNmHQE6En8KUbmjyj+E2-0ABOn18cVw5o9Fk27OPXq8jaVTQ@mail.gmail.com>

With a new BIP editor (Kalle Alm) in place and Taproot activation
locked in it is probably/possibly as good time as any to revisit the
BIP process and see if we can bolster it, improve it or at least
inform why certain things operate the way they do.

Hence two IRC meetings are being organized, one on Tuesday September
14th (23:00 UTC) and one on Wednesday September 29th (23:00 UTC), both
on the Libera IRC channel #bitcoin-dev.

Possible discussion topics range from the relatively mundane (should
BIP champions need to ACK basic spelling change PRs) to the possibly
contentious (what role if any do the BIPs have in informing the
community of soft fork activation parameters). At the very least it
would be good to address some common misunderstandings and subtleties
of the BIP process that many (including myself) are lacking context
on.

So if you are interested in the BIP process or certainly if you have
experienced frustrations with the BIP process in the past as a BIP
champion or BIP contributor please attend and we?ll see what progress
we can make.

There is a BIP process wishlist that some have already contributed
ideas too: https://github.com/bitcoin/bips/wiki/BIP-Process-wishlist

And of course BIP 2 outlines the current BIP process:
https://github.com/bitcoin/bips/blob/master/bip-0002.mediawiki

This is being organized in the spirit of seeking to improve a process
and a resource that we as a community all rely on so please engage in
the spirit that is intended. I will keep the mailing list informed of
anything that comes out of the meetings and the #bitcoin-dev channel
is open for discussion outside of the meetings.

-- 
Michael Folkson
Email: michaelfolkson at gmail.com
Keybase: michaelfolkson
PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3

From 0xb10c at gmail.com  Tue Sep  7 16:07:47 2021
From: 0xb10c at gmail.com (0xB10C)
Date: Tue, 7 Sep 2021 18:07:47 +0200
Subject: [bitcoin-dev] Reorgs on SigNet - Looking for feedback on approach
	and parameters
Message-ID: <83272afb-ed87-15b6-e02c-16bb1102beb4@gmail.com>

Hello,

tl;dr: We want to make reorgs on SigNet a reality and are looking for
feedback on approach and parameters.

One of the ideas for SigNet is the possibility for it to be reliably
unreliable, for example, planned chain reorganizations. These have not
been implemented yet.

My summerofbitcoin.org mentee Nikhil Bartwal and I have been looking at
implementing support for reorgs on SigNet. We are looking for feedback
on which approach and parameters to use. Please consider answering the
questions below if you or your company is interested in chain
reorganizations on SigNet.

With feedback from AJ and Kalle Alm (thanks again!), we came up with two
scenarios that could be implemented in the current SigNet miner script
[0]. Both would trigger automatically in a fixed block interval.
Scenario 1 simulates a race scenario where two chains compete for D
blocks. Scenario 2 simulates a chain rollback where the top D blocks get
replaced by a chain that outgrows the earlier branch.

AJ proposed to allow SigNet users to opt-out of reorgs in case they
explicitly want to remain unaffected. This can be done by setting a
to-be-reorged version bit flag on the blocks that won't end up in the
most work chain. Node operators could choose not to accept to-be-reorged
SigNet blocks with this flag set via a configuration argument.

The reorg-interval X very much depends on the user's needs. One could
argue that there should be, for example, three reorgs per day, each 48
blocks apart. Such a short reorg interval allows developers in all time
zones to be awake during one or two reorgs per day. Developers don't
need to wait for, for example, a week until they can test their reorgs
next. However, too frequent reorgs could hinder other SigNet users.

We propose that the reorg depth D is deterministically random between a
minimum and a maximum based on, e.g., the block hash or the nonce of the
last block before the reorg. Compared to a local randint() based
implementation, this allows reorg-handling tests and external tools to
calculate the expected reorg depth.

# Scenario 1: Race between two chains

For this scenario, at least two nodes and miner scripts need to be
running. An always-miner A continuously produces blocks and rejects
blocks with the to-be-reorged version bit flag set. And a race-miner R
that only mines D blocks at the start of each interval and then waits X
blocks. A and R both have the same hash rate. Assuming both are well
connected to the network, it's random which miner will first mine and
propagate a block. In the end, the A miner chain will always win the race.

# Scenario 2: Chain rollback

This scenario only requires one miner and Bitcoin Core node but also
works in a multiminer setup. The miners mine D blocks with the
to-be-reorged version bit flag set at the start of the interval. After
allowing the block at height X+D to propagate, they invalidate the block
at height X+1 and start mining on block X again. This time without
setting the to-be-reorged version bit flag. Non-miner nodes will reorg
to the new tip at height X+D+1, and the first-seen branch stalls.

# Questions

??? 1. How do you currently test your applications reorg handling? Do
       the two discussed scenarios (race and chain rollback) cover your
       needs? Are we missing something you'd find helpful?

??? 2. How often should reorgs happen on the default SigNet? Should
       there be multiple reorgs a day (e.g., every 48 or 72 blocks
       assuming 144 blocks per day) as your engineers need to be awake?
       Do you favor less frequent reorgs (once per week or month)? Why?

    3. How deep should the reorgs be on average? Do you want to test
       deeper reorgs (10+ blocks) too?


# Next Steps

We will likely implement Scenario 1, the race between two chains, first.
We'll set up a public test-SigNet along with a faucet, block explorer,
and a block tree visualization. If there is interest in the second
approach, chain rollbacks can be implemented too. Future work will add
the possibility to include conflicting transactions in the two branches.
After enough testing, the default SigNet can start to do periodical
reorgs, too.

Thanks,
0xB10C

[0]: https://github.com/bitcoin/bitcoin/blob/master/contrib/signet/miner 


From jlrubin at mit.edu  Tue Sep  7 16:44:17 2021
From: jlrubin at mit.edu (Jeremy)
Date: Tue, 7 Sep 2021 09:44:17 -0700
Subject: [bitcoin-dev] Reorgs on SigNet - Looking for feedback on
 approach and parameters
In-Reply-To: <83272afb-ed87-15b6-e02c-16bb1102beb4@gmail.com>
References: <83272afb-ed87-15b6-e02c-16bb1102beb4@gmail.com>
Message-ID: <CAD5xwhh7j5maMOs6TGHAAKz2_HqU0RYv9C1F_wPOMFrZftPdAQ@mail.gmail.com>

If you make the to be reorged flag 2 bits, 1 bit can mark final block and
the other can mark to be reorged.

That way the nodes opting into reorg can see the reorg and ignore the final
blocks (until a certain time? Or until it's via a reorg?), and the nodes
wanting not to see reorgs get continuous service without disruption

On Tue, Sep 7, 2021, 9:12 AM 0xB10C via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Hello,
>
> tl;dr: We want to make reorgs on SigNet a reality and are looking for
> feedback on approach and parameters.
>
> One of the ideas for SigNet is the possibility for it to be reliably
> unreliable, for example, planned chain reorganizations. These have not
> been implemented yet.
>
> My summerofbitcoin.org mentee Nikhil Bartwal and I have been looking at
> implementing support for reorgs on SigNet. We are looking for feedback
> on which approach and parameters to use. Please consider answering the
> questions below if you or your company is interested in chain
> reorganizations on SigNet.
>
> With feedback from AJ and Kalle Alm (thanks again!), we came up with two
> scenarios that could be implemented in the current SigNet miner script
> [0]. Both would trigger automatically in a fixed block interval.
> Scenario 1 simulates a race scenario where two chains compete for D
> blocks. Scenario 2 simulates a chain rollback where the top D blocks get
> replaced by a chain that outgrows the earlier branch.
>
> AJ proposed to allow SigNet users to opt-out of reorgs in case they
> explicitly want to remain unaffected. This can be done by setting a
> to-be-reorged version bit flag on the blocks that won't end up in the
> most work chain. Node operators could choose not to accept to-be-reorged
> SigNet blocks with this flag set via a configuration argument.
>
> The reorg-interval X very much depends on the user's needs. One could
> argue that there should be, for example, three reorgs per day, each 48
> blocks apart. Such a short reorg interval allows developers in all time
> zones to be awake during one or two reorgs per day. Developers don't
> need to wait for, for example, a week until they can test their reorgs
> next. However, too frequent reorgs could hinder other SigNet users.
>
> We propose that the reorg depth D is deterministically random between a
> minimum and a maximum based on, e.g., the block hash or the nonce of the
> last block before the reorg. Compared to a local randint() based
> implementation, this allows reorg-handling tests and external tools to
> calculate the expected reorg depth.
>
> # Scenario 1: Race between two chains
>
> For this scenario, at least two nodes and miner scripts need to be
> running. An always-miner A continuously produces blocks and rejects
> blocks with the to-be-reorged version bit flag set. And a race-miner R
> that only mines D blocks at the start of each interval and then waits X
> blocks. A and R both have the same hash rate. Assuming both are well
> connected to the network, it's random which miner will first mine and
> propagate a block. In the end, the A miner chain will always win the race.
>
> # Scenario 2: Chain rollback
>
> This scenario only requires one miner and Bitcoin Core node but also
> works in a multiminer setup. The miners mine D blocks with the
> to-be-reorged version bit flag set at the start of the interval. After
> allowing the block at height X+D to propagate, they invalidate the block
> at height X+1 and start mining on block X again. This time without
> setting the to-be-reorged version bit flag. Non-miner nodes will reorg
> to the new tip at height X+D+1, and the first-seen branch stalls.
>
> # Questions
>
>     1. How do you currently test your applications reorg handling? Do
>        the two discussed scenarios (race and chain rollback) cover your
>        needs? Are we missing something you'd find helpful?
>
>     2. How often should reorgs happen on the default SigNet? Should
>        there be multiple reorgs a day (e.g., every 48 or 72 blocks
>        assuming 144 blocks per day) as your engineers need to be awake?
>        Do you favor less frequent reorgs (once per week or month)? Why?
>
>     3. How deep should the reorgs be on average? Do you want to test
>        deeper reorgs (10+ blocks) too?
>
>
> # Next Steps
>
> We will likely implement Scenario 1, the race between two chains, first.
> We'll set up a public test-SigNet along with a faucet, block explorer,
> and a block tree visualization. If there is interest in the second
> approach, chain rollbacks can be implemented too. Future work will add
> the possibility to include conflicting transactions in the two branches.
> After enough testing, the default SigNet can start to do periodical
> reorgs, too.
>
> Thanks,
> 0xB10C
>
> [0]: https://github.com/bitcoin/bitcoin/blob/master/contrib/signet/miner
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210907/4f2f687d/attachment.html>

From ZmnSCPxj at protonmail.com  Tue Sep  7 23:38:42 2021
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Tue, 07 Sep 2021 23:38:42 +0000
Subject: [bitcoin-dev] Braidpool: Proposal for a decentralised mining
	pool
In-Reply-To: <MiuahdA--3-2@tutanota.de>
References: <MiuahdA--3-2@tutanota.de>
Message-ID: <ceFmn7ZHyPHN70rDuE66lnPEwjgjQ7LtZLwyFgIVUpPvPDvSZSsLHUf_yiBvXTpjdEju4UxAOnDgilZaQAMvQzYcUbOkZsYvOIpuBG7japo=@protonmail.com>

Good morning all,

A thing I just realized about Braidpool is that the payout server is still a single central point-of-failure.

Although the paper claims to use Tor hidden service to protect against DDoS attacks, its centrality still cannot protect against sheer accident.
What happens if some clumsy human (all humans are clumsy, right?) fumbles the cables in the datacenter the hub is hosted in?
What happens if the country the datacenter is in is plunged into war or anarchy, because you humans love war and chaos so much?
What happens if Zeus has a random affair (like all those other times), Hera gets angry, and they get into a domestic, and then a random thrown lightning bolt hits the datacenter the hub is in?

The paper relies on economic arguments ("such an action will end the pool and the stream of future profits for the hub"), but economic arguments tend to be a lot less powerful in a monopoly, and the hub effectively has a monopoly on all Braidpool miners.
Hashers might be willing to tolerate minor peccadilloes of the hub, simply to let the pool continue (their other choices would be even worse).

So it seems to me that it would still be nicer, if it were at all possible, to use multiple hubs.
I am uncertain how easily this can be done.

Perhaps a Lightning model can be considered.
Multiple hubs may exist which offer liquidity to the Braidpool network, hashers measure uptime and timeliness of payouts, and the winning hasher elects one of the hubs.
The hub gets paid on the coinbase, and should send payouts, minus fees, on the LN to the miners.

However, this probably complicates the design too much, and it may be more beneficial to get *something* working now.
Let not the perfect be the enemy of the good.

Regards,
ZmnSCPxj

From aj at erisian.com.au  Wed Sep  8 07:59:04 2021
From: aj at erisian.com.au (Anthony Towns)
Date: Wed, 8 Sep 2021 17:59:04 +1000
Subject: [bitcoin-dev] Reorgs on SigNet - Looking for feedback on
 approach and parameters
In-Reply-To: <83272afb-ed87-15b6-e02c-16bb1102beb4@gmail.com>
References: <83272afb-ed87-15b6-e02c-16bb1102beb4@gmail.com>
Message-ID: <20210908075903.GA21644@erisian.com.au>

On Tue, Sep 07, 2021 at 06:07:47PM +0200, 0xB10C via bitcoin-dev wrote:
> The reorg-interval X very much depends on the user's needs. One could
> argue that there should be, for example, three reorgs per day, each 48
> blocks apart.

Oh, wow, I think the last suggestion was every 100 blocks (every
~16h40m). Once every ~8h sounds very convenient.

> Such a short reorg interval allows developers in all time
> zones to be awake during one or two reorgs per day.

And also for there to reliably be reorgs when they're not awake, which
might be a useful thing to be able to handle, too :)

> Developers don't
> need to wait for, for example, a week until they can test their reorgs
> next. However, too frequent reorgs could hinder other SigNet users.

Being able to run `bitcoind -signet -signetacceptreorg=0` and never
seeing any reorgs should presumably make this not a problem?

For people who do see reorgs, having an average of 2 or 3 additional
blocks every 48 blocks is perhaps a 6% increase in storage/traffic.

> # Scenario 1: Race between two chains
> 
> For this scenario, at least two nodes and miner scripts need to be
> running. An always-miner A continuously produces blocks and rejects
> blocks with the to-be-reorged version bit flag set. And a race-miner R
> that only mines D blocks at the start of each interval and then waits X
> blocks. A and R both have the same hash rate. Assuming both are well
> connected to the network, it's random which miner will first mine and
> propagate a block. In the end, the A miner chain will always win the race.

I think this description is missing that all the blocks R mines have
the to-be-reorged flag set.

>     3. How deep should the reorgs be on average? Do you want to test
>        deeper reorgs (10+ blocks) too?

Super interested in input on this -- perhaps we should get optech to
send a survey out to their members, or so?

My feeling is:

 - 1 block reorgs: these are a regular feature on mainnet, everyone
   should cope with them; having them happen multiple times a day to
   make testing easier should be great

 - 2-3 block reorgs: good for testing the "your tx didn't get enough
   confirms to be credited to your account" case, even though it barely
   ever happens on mainnet

 - 4-6 block reorgs: likely to violate business assumptions, but
   completely technically plausible, especially if there's an attack
   against the network

 - 7-100 block reorgs: for this to happen on mainnet, it would probably
   mean there was a bug and pools/miners agree the chain has to
   be immediately reverted -- eg, someone discovers and exploits an
   inflation bug, minting themselves free bitcoins and breaking the 21M
   limit (eg, the 51 block reorg in Aug 2010); or someone discovers a
   bug that splits the chain, and the less compatible chain is reverted
   (eg, the 24 block reorg due to the bdb lock limit in Mar 2013);
   or something similar. Obviously the bug would have to have been
   discovered pretty quickly after it was exploited for the reorg to be
   under a day's worth of blocks.

 - 100-2000+ block reorgs: severe bug that wasn't found quickly, or where
   getting >50% of miners organised took more than a few hours. This will
   start breaking protocol assumptions, like pool payouts, lightning's
   relative locktimes, or liquid's peg-in confirmation requirements, and
   result in hundres of MBs of changes to the utxo set

Maybe it would be good to do reorgs of 15, 150 or 1500 blocks as a
special fire-drill event, perhaps once a month/quarter/year or so,
in some pre-announced window?

I think sticking to 1-6 block reorgs initially is a fine way to start
though.

> After enough testing, the default SigNet can start to do periodical
> reorgs, too.

FWIW, the only thing that concerns me about doing this on the default
signet is making sure that nodes that set -signetacceptreorg=0 don't
end up partitioning the p2p network due to either rejecting a higher
work chain or rejecting txs due to double-spends across the two chains.

A quick draft of code for -signetacceptreorg=0 is available at 

  https://github.com/ajtowns/bitcoin/commits/202108-signetreorg

Cheers,
aj


From kohli at ctemplar.com  Wed Sep  8 10:03:05 2021
From: kohli at ctemplar.com (pool2win)
Date: Wed, 08 Sep 2021 10:03:05 -0000
Subject: [bitcoin-dev] Braidpool: Proposal for a decentralised mining
 pool
In-Reply-To: <ceFmn7ZHyPHN70rDuE66lnPEwjgjQ7LtZLwyFgIVUpPvPDvSZSsLHUf_yiBvXTpjdEju4UxAOnDgilZaQAMvQzYcUbOkZsYvOIpuBG7japo=@protonmail.com>
References: <MiuahdA--3-2@tutanota.de>
 <ceFmn7ZHyPHN70rDuE66lnPEwjgjQ7LtZLwyFgIVUpPvPDvSZSsLHUf_yiBvXTpjdEju4UxAOnDgilZaQAMvQzYcUbOkZsYvOIpuBG7japo=@protonmail.com>
Message-ID: <edbbb44e247d4e639659e1b9b989dd84-kohli@ctemplar.com>

> A thing I just realized about Braidpool is that the payout server is still a single central point-of-failure.

> However, this probably complicates the design too much, and it may be more beneficial to get *something* working now.

You have hit the nail on the head here and Chris Belcher's original proposal for using payment channels does provide a construction for multiple hubs [1]. In the Braidpool proposal however, the focus is on a single hub to describe the plan for an MVP.

Decentralising hubs is the end goal here, and either Belcher's multiple hubs construction or a leadership election based construction along the lines you propose might be a good way forward. Belcher's idea has the added advantage that the required liquidity at each hub is reduced as more hubs join, with the cost that in case of a hubs defecting, it takes longer for miners to do cascading close on channels to all hubs. TBH, it might be a cost worth paying in the absence of better ideas. But as braidpool is built, more ideas will be appear as well.

[1] Payment Channel Payouts: An Idea for Improving P2Pool Scalability: https://bitcointalk.org/index.php?topic=2135429.0

---------- Original Message ----------
On Tue, September 7, 2021 at 11:39 PM,  ZmnSCPxj via bitcoin-dev<bitcoin-dev at lists.linuxfoundation.org> wrote:
Good morning all,

A thing I just realized about Braidpool is that the payout server is still a single central point-of-failure.

Although the paper claims to use Tor hidden service to protect against DDoS attacks, its centrality still cannot protect against sheer accident.
What happens if some clumsy human (all humans are clumsy, right?) fumbles the cables in the datacenter the hub is hosted in?
What happens if the country the datacenter is in is plunged into war or anarchy, because you humans love war and chaos so much?
What happens if Zeus has a random affair (like all those other times), Hera gets angry, and they get into a domestic, and then a random thrown lightning bolt hits the datacenter the hub is in?

The paper relies on economic arguments ("such an action will end the pool and the stream of future profits for the hub"), but economic arguments tend to be a lot less powerful in a monopoly, and the hub effectively has a monopoly on all Braidpool miners.
Hashers might be willing to tolerate minor peccadilloes of the hub, simply to let the pool continue (their other choices would be even worse).

So it seems to me that it would still be nicer, if it were at all possible, to use multiple hubs.
I am uncertain how easily this can be done.

Perhaps a Lightning model can be considered.
Multiple hubs may exist which offer liquidity to the Braidpool network, hashers measure uptime and timeliness of payouts, and the winning hasher elects one of the hubs.
The hub gets paid on the coinbase, and should send payouts, minus fees, on the LN to the miners.

However, this probably complicates the design too much, and it may be more beneficial to get *something* working now.
Let not the perfect be the enemy of the good.

Regards,
ZmnSCPxj
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From jlrubin at mit.edu  Thu Sep  9 01:04:53 2021
From: jlrubin at mit.edu (Jeremy)
Date: Wed, 8 Sep 2021 18:04:53 -0700
Subject: [bitcoin-dev] Note on Sequence Lock Upgrades Defect
In-Reply-To: <CALZpt+Fk=_3=Hb_u4OptAwHKaG6R=+6igDeLQESQ_u_QbBQePg@mail.gmail.com>
References: <CAD5xwhiKU1fuhqmKsx28f1nuw9CmvbyrS=BtM4X-L+WPgWY3Wg@mail.gmail.com>
 <CALZpt+Fk=_3=Hb_u4OptAwHKaG6R=+6igDeLQESQ_u_QbBQePg@mail.gmail.com>
Message-ID: <CAD5xwhj3N7aK-Of1+-DykH_XRbKBs32E4=uu81XmJhTx5yYZ7A@mail.gmail.com>

See the current patchset proposed:
https://github.com/bitcoin/bitcoin/pull/22871/commits

Two things are happening that are separate:

1) Fixing the semantics of arg in <arg> OP_CHECKSEQUENCEVERIFY
2) Fixing the semantics on nSequence in each tx input

There is no sense in conditioning part 1 on RBF or anything else, since
it's only loosely related to 2. I think it should be a class-2 rollout as
you describe above since it's a rule tightening.

For part 2, I think the way the patches handle it currently (which is
defining 1 byte type prefix followed by 3 bytes application data) is
sufficient for immediate deployment.

I agree with you that a class-2 rollout might be appropriate for it, but
that can be followed by removing the SEQUENCE_ROOT_TYPE::SPECIAL field
later as a class-1 rollout. However, so long as it's not being used for any
particular constants, there is no need to deallocate
SEQUENCE_ROOT_TYPE::SPECIAL tag as long as no new use case must overlap
it's range.

With respect to the SEQUENCE_ROOT_TYPE::UNCHECKED_METADATA, it is in fact
*not* mempool data, but is a special type of metadata which is required for
the counterparty to efficiently respond to a unilateral channel closure (see
bolt-3 This obscures the number of commitments made on the channel in the
case of unilateral close, yet still provides a useful index for both nodes
(who know the payment_basepoints) to quickly find a revoked commitment
transaction.)

I understand wanting to remove full-rbf, but I think that fixing the
upgradability of sequences is much less controversial among the
userbase and worth doing expediently. That part 1 is doable now -- albeit
as a class 2 -- means that it would not be unreasonable to bundle parts 1
and 2 so that we don't double burden the community with an upgrade effort.
Further, RBF can be disabled on a purely ad-hoc node-by-node policy layer,
whereas this restriction requires more community coordination/awareness.
--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>


On Wed, Sep 8, 2021 at 5:03 PM Antoine Riard <antoine.riard at gmail.com>
wrote:

> Hi Jeremy,
>
> Answering here from #22871 discussions.
>
> I agree on the general principle to not blur mempool policies signaling in
> committed transaction data. Beyond preserving upgradeability, another good
> argument is to let L2 nodes update the mempool policies signaling their
> pre-signed transactions non-interactively. If one of the transaction fields
> is assigned mempool semantics, in case of tightening policy changes, you
> will need to re-sign or bear the risks of having non-propagating
> transactions which opens the door for exploitation by a malicious
> counterparty. I think this point is kinda relevant if we have future
> cross-layer coordinated safety fixes to deal with a la CVE-2021-31876.
>
> Even further, a set of L2 counterparties would like to pick up divergent
> tx-relay/mempool policies, having the signaling fields as part of the
> signature force them to come to consensus.
>
> I think we can take the opportunity of p2p packages to introduce a new
> field to signal policy. Of course, a malicious tx-relay peer could modify
> its content to jam your transaction's propagation but in that case it is
> easier to just drop it.
>
> One issue with taking back the `nSequence` field for consensus-semantic
> sounds is depriving the application-layer from a discrete, zero-cost
> payload (e.g the LN obfuscated commitment number watermark). This might be
> controversial as we'll increase the price of such applications if they're
> still willingly to relay application specific data through the p2p network
> (e.g force them to use a costly OP_RETURN output or payer/payee
> interactions to setup a pay-to-contract)
>
> W.r.t flag day activation to smooth policy deployment, I think that's
> something we might rely on in the future though we could distinguish few
> types of policy deployments :
> 1) loosening changes (e.g full-rbf/dust threshold removal), a transaction
> which was relaying under
> the former policy should relay under the new one
> 2) tightening changes (e.g #22871), a transaction which was relaying under
> the former policy
> might not relay under the new one
> 3) new feature introduced (e.g packages), a transaction is offered a new
> mode of relay
>
> I think 1) doesn't need that level of ecosystem coordination as
> applications/second-layers should always benefit from such changes. Maybe
> with the exception of full-rbf, where we have historical 0-conf softwares,
> with (broken) security assumptions made on the opt-out RBF mechanism. Same
> with 3), better to have new features deployed gradually, a flag day
> activation day in this case won't mean that all higher stacks will jump to
> use package-relay ?
>
> Where a flag day might make sense would be for 2) ? It would create a
> higher level of commitment by the base layer software instead of a pure
> communication on the ML/GH, which might not be concretized in the announced
> release due to slow review process/feature freeze/rebase conflicts...
> Reversing the process and asking for Bitcoin applications/higher layers to
> update first might get us in the trap of never doing the change, as someone
> might have a small use-case in the corner relying on a given policy
> behavior.
>
> That said, w.r.t to the proposed policy change in #22871, I think it's
> better to deploy full-rbf first, then give a time buffer to higher
> applications to free up the `nSequence` field and finally start to
> discourage the usage. Otherwise, by introducing new discouragement waivers,
> e.g not rejecting the usage of the top 8 bits, I think we're moving away
> from the policy design principle we're trying to establish (separation of
> mempool policies signaling from consensus data)
>
> Le ven. 3 sept. 2021 ? 23:32, Jeremy via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> a ?crit :
>
>> Hi Bitcoin Devs,
>>
>> I recently noticed a flaw in the Sequence lock implementation with
>> respect to upgradability. It might be the case that this is protected
>> against by some transaction level policy (didn't see any in policy.cpp, but
>> if not, I've put up a blogpost explaining the defect and patching it
>> https://rubin.io/bitcoin/2021/09/03/upgradable-nops-flaw/
>>
>> I've proposed patching it here
>> https://github.com/bitcoin/bitcoin/pull/22871, it is proper to widely
>> survey the community before patching to ensure no one is depending on the
>> current semantics in any live application lest this tightening of
>> standardness rules engender a confiscatory effect.
>>
>> Best,
>>
>> Jeremy
>>
>> --
>> @JeremyRubin <https://twitter.com/JeremyRubin>
>> <https://twitter.com/JeremyRubin>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210908/24501572/attachment.html>

From aj at erisian.com.au  Thu Sep  9 06:41:38 2021
From: aj at erisian.com.au (Anthony Towns)
Date: Thu, 9 Sep 2021 16:41:38 +1000
Subject: [bitcoin-dev] TAPLEAF_UPDATE_VERIFY covenant opcode
Message-ID: <20210909064138.GA22496@erisian.com.au>

Hello world,

A couple of years ago I had a flight of fancy [0] imagining how it
might be possible for everyone on the planet to use bitcoin in a
mostly decentralised/untrusted way, without requiring a block size
increase. It was a bit ridiculous and probably doesn't quite hold up,
and beyond needing all the existing proposals to be implemented (taproot,
ANYPREVOUT, CTV, eltoo, channel factories), it also needed a covenant
opcode [1]. I came up with something that I thought fit well with taproot,
but couldn't quite figure out how to use it for anything other than my
ridiculous scheme, so left it at that.

But recently [2] Greg Maxwell emailed me about his own cool idea for a
covenant opcode, which turned out to basically be a reinvention of the
same idea but with more functionality, a better name and a less fanciful
use case; and with that inspiration, I think I've also now figured out
how to use it for a basic vault, so it seems worth making the idea a
bit more public.

I'll split this into two emails, this one's the handwavy overview,
the followup will go into some of the implementation complexities.



The basic idea is to think about "updating" a utxo by changing the
taproot tree.

As you might recall, a taproot address is made up from an internal public
key (P) and a merkle tree of scripts (S) combined via the formula Q=P+H(P,
S)*G to calculate the scriptPubKey (Q). When spending using a script,
you provide the path to the merkle leaf that has the script you want
to use in the control block. The BIP has an example [3] with 5 scripts
arranged as ((A,B), ((C,D), E)), so if you were spending with E, you'd
reveal a path of two hashes, one for (AB), then one for (CD), then you'd
reveal your script E and satisfy it.

So that makes it relatively easy to imagine creating a new taproot address
based on the input you're spending by doing some or all of the following:

 * Updating the internal public key (ie from P to P' = P + X)
 * Trimming the merkle path (eg, removing CD)
 * Removing the script you're currently executing (ie E)
 * Adding a new step to the end of the merkle path (eg F)

Once you've done those things, you can then calculate the new merkle
root by resolving the updated merkle path (eg, S' = MerkleRootFor(AB,
F, H_TapLeaf(E))), and then calculate a new scriptPubKey based on that
and the updated internal public key (Q' = P' + H(P', S')).

So the idea is to do just that via a new opcode "TAPLEAF_UPDATE_VERIFY"
(TLUV) that takes three inputs: one that specifies how to update the
internal public key (X), one that specifies a new step for the merkle path
(F), and one that specifies whether to remove the current script and/or
how many merkle path steps to remove. The opcode then calculates the
scriptPubKey that matches that, and verifies that the output corresponding
to the current input spends to that scriptPubKey.

That's useless without some way of verifying that the new utxo retains
the bitcoin that was in the old utxo, so also include a new opcode
IN_OUT_AMOUNT that pushes two items onto the stack: the amount from this
input's utxo, and the amount in the corresponding output, and then expect
anyone using TLUV to use maths operators to verify that funds are being
appropriately retained in the updated scriptPubKey.



Here's two examples of how you might use this functionality.

First, a basic vault. The idea is that funds are ultimately protected
by a cold wallet key (COLD) that's inconvenient to access but is as
safe from theft as possible. In order to make day to day transactions
more convenient, a hot wallet key (HOT) is also available, which is
more vulnerable to theft. The vault design thus limits the hot wallet
to withdrawing at most L satoshis every D blocks, so that if funds are
stolen, you lose at most L, and have D blocks to use your cold wallet
key to re-secure the funds and prevent further losses.

To set this up with TLUV, you construct a taproot output with COLD as
the internal public key, and a script that specifies:

 * The tx is signed via HOT
 * <D> CSV -- there's a relative time lock since the last spend
 * If the input amount is less than L + dust threshold, fine, all done,
   the vault can be emptied.
 * Otherwise, the output amount must be at least (the input amount -
   L), and do a TLUV check that the resulting sPK is unchanged

So you can spend up to "L" satoshis via the hot wallet as long as you
wait D blocks since the last spend, and can do whatever you want via a
key path spend with the cold wallet.

You could extend this to have a two phase protocol for spending, where
first you use the hot wallet to say "in D blocks, allow spending up to
L satoshis", and only after that can you use the hot wallet to actually
spend funds. In that case supply a taproot sPK with COLD as the internal
public key and two scripts, the "release" script, which specifies:

 * The tx is signed via HOT
 * Output amount is greater or equal to the input amount.
 * Use TLUV to check:
   + the output sPK has the same internal public key (ie COLD)
   + the merkle path has one element trimmed
   + the current script is included
   + a new step is added that matches either H_LOCKED or H_AVAILABLE as
     described below (depending on whether 0 or 1 was provided as
     witness info)

The other script is either "locked" (which is just "OP_RETURN") or
"available" which specifies:

 * The tx is signed via HOT
 * <D> CSV -- there's a relative time lock since the last spend (ie,
   when the "release" script above was used)
 * If the input amount is less than L, fine, all done, the vault can
   be emptied
 * Otherwise, the output amount must be at least (the input amount minus
   L), and via TLUV, check the resulting sPK keeps the internal pubkey
   unchanged, keeps the merkle path, drops the current script, and adds
   H_LOCKED as the new step.

H_LOCKED and H_AVAILABLE are just the TapLeaf hash corresponding to the
"locked" and "available" scripts.

I believe this latter setup matches the design Bryan Bishop talked about
a couple of years ago [4], with the benefit that it's fully recursive,
allows withdrawals to vary rather than be the fixed amount L (due to not
relying on pre-signed transactions), and generally seems a bit simpler
to work with.



The second scheme is allowing for a utxo to represent a group's pooled
funds. The idea being that as long as everyone's around you can use
the taproot key path to efficiently move money around within the pool,
or use a single transaction and signature for many people in the pool
to make payments. But key path spends only work if everyone's available
to sign -- what happens if someone disappears, or loses access to their
keys, or similar? For that, we want to have script paths to allow other
people to reclaim their funds even if everyone else disappears. So we
setup scripts for each participant, eg for Alice:

 * The tx is signed by Alice
 * The output value must be at least the input value minus Alice's balance
 * Must pass TLUV such that:
   + the internal public key is the old internal pubkey minus Alice's key
   + the currently executing script is dropped from the merkle path
   + no steps are otherwise removed or added

The neat part here is that if you have many participants in the pool,
the pool continues to operate normally even if someone makes use of the
escape hatch -- the remaining participants can still use the key path to
spend efficiently, and they can each unilaterally withdraw their balance
via their own script path. If everyone decides to exit, whoever is last
can spend the remaining balance directly via the key path.

Compared to having on-chain transactions using non-pooled funds, this
is more efficient and private: a single one-in, one-out transaction
suffices for any number of transfers within the pool, and there's no
on-chain information about who was sending/receiving the transfers, or
how large the transfers were; and for transfers out of the pool, there's
no on-chain indication which member of the pool is sending the funds,
and multiple members of the pool can send funds to multiple destinations
with only a single signature. The major constraint is that you need
everyone in the pool to be online in order to sign via the key path,
which provides a practical limit to how many people can reasonably be
included in a pool before there's a breakdown.

Compared to lightning (eg eltoo channel factories with multiple
participants), the drawback is that no transfer is final without an
updated state being committed on chain, however there are also benefits
including that if one member of the pool unilaterally exits, that
doesn't reveal the state of anyone remaining in the pool (eg an eltoo
factory would likely reveal the balances of everyone else's channels at
that point).

A simpler case for something like this might be for funding a joint
venture -- suppose you're joining with some other early bitcoiners to
buy land to build a citadel, so you each put 20 BTC into a pooled utxo,
ready to finalise the land purchase in a few months, but you also want
to make sure you can reclaim the funds if the deal falls through. So
you might include scripts like the above that allow you to reclaim your
balance, but add a CLTV condition preventing anyone from doing that until
the deal's deadline has passed. If the deal goes ahead, you all transfer
the funds to the vendor via the keypath; if it doesn't work out, you
hopefully return your funds via the keypath, but if things turn really
sour, you can still just directly reclaim your 20 BTC yourself via the
script path.



I think a nice thing about this particular approach to recursive covenants
at a conceptual level is that it automatically leaves the key path as an
escape mechanism -- rather than having to build a base case manually,
and have the risk that it might not work because of some bug, locking
your funds into the covenant permanently; the escape path is free, easy,
and also the optimal way of spending things when everything is working
right. (Of course, you could set the internal public key to a NUMS point
and shoot yourself in the foot that way anyway)



I think there's two limitations of this method that are worth pointing out.

First it can't tweak scripts in areas of the merkle tree that it can't
see -- I don't see a way of doing that particularly efficiently, so maybe
it's best just to leave that as something for the people responsible for
the funds to negotiate via the keypath, in which case it's automatically
both private and efficient since all the details stay off-chain, anyway

And second, it doesn't provide a way for utxos to "interact", which is
something that is interesting for automated market makers [5], but perhaps
only interesting for chains aiming to support multiple asset types,
and not bitcoin directly. On the other hand, perhaps combining it with
CTV might be enough to solve that, particularly if the hash passed to
CTV is constructed via script/CAT/etc.



(I think everything described here could be simulated with CAT and
CHECKSIGFROMSTACK (and 64bit maths operators and some way to access
the internal public key), the point of introducing dedicated opcodes
for this functionality rather than (just) having more generic opcodes
would be to make the feature easy to use correctly, and, presuming it
actually has a wide set of use cases, to make it cheap and efficient
both to use in wallets, and for nodes to validate)

Cheers,
aj

[0] https://gist.github.com/ajtowns/dc9a59cf0a200bd1f9e6fb569f76f7a0

[1] Roughly, the idea was that if you have ~9 billion people using
    bitcoin, but can only have ~1000 transactions per block, then you
    need have each utxo represent a significant number of people. That
    means that you need a way of allowing the utxo's to be efficiently
    spent, but need to introduce some level of trust since expecting
    many people to constantly be online seems unreliable, but to remain
    mostly decentralised/untrusted, you want to have some way of limiting
    how much trust you're introducing, and that's where covenants come in.

[2] Recently in covid-adjusted terms, or on the bitcoin consensus
    change scale anyway...
    https://mobile.twitter.com/ajtowns/status/1385091604357124100 

[3] https://github.com/bitcoin/bips/blob/master/bip-0341.mediawiki#Constructing_and_spending_Taproot_outputs 

[4] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-August/017231.html

[5] The idea behind an automated market maker being that you setup a
    script that says "you can withdraw x BTC if you deposit f(x) units of
    USDT, or you can withdraw g(x) units of USDT if you deposit x units
    of BTC", with f(x)/x giving the buy price, and f(x)>g(x) meaning
    you make a profit. Being able to specify a covenant that links the
    change in value to the BTC utxo (+/-x) and the change in value to
    the USDT utxo (+f(x) or -g(x)) is what you'd need to support this
    sort of use case, but TLUV doesn't provide a way to do that linkage.


From aj at erisian.com.au  Thu Sep  9 06:53:30 2021
From: aj at erisian.com.au (Anthony Towns)
Date: Thu, 9 Sep 2021 16:53:30 +1000
Subject: [bitcoin-dev] TAPLEAF_UPDATE_VERIFY covenant opcode
In-Reply-To: <20210909064138.GA22496@erisian.com.au>
References: <20210909064138.GA22496@erisian.com.au>
Message-ID: <20210909065330.GB22496@erisian.com.au>

On Thu, Sep 09, 2021 at 04:41:38PM +1000, Anthony Towns wrote:
> I'll split this into two emails, this one's the handwavy overview,
> the followup will go into some of the implementation complexities.

(This is informed by discussions with Greg, Matt Corallo, David Harding
and Jeremy Rubin; opinions and mistakes my own, of course)



First, let's talk quickly about IN_OUT_AMOUNT. I think the easiest way to
deal with it is just a single opcode that pushes two values to the stack;
however it could be two opcodes, or it could even accept a parameter
letting you specify which input (and hence which corresponding output)
you're talking about (-1 meaning the current input perhaps). 

Anyway, a big complication here is that amounts in satoshis require up
to 51 bits to represent them, but script only allows you to do 32 bit
maths. However introducing IN_OUT_AMOUNT already means using an OP_SUCCESS
opcode, which in turn allows us to arbitrarily redefine the behaviour
of other opcodes -- so we can use the presence of IN_OUT_AMOUNT in the
script to upgrade ADD, SUB, and the comparison operators to support 64
bit values. Enabling MUL, DIV and MOD might also be worthwhile.



Moving onto TLUV. My theory is that it pops three items off the stack. The
top of the stack is "C" the control integer; next is "H" the
additional path step; and finally "X" the tweak for the internal
pubkey. If "H" is the empty vector, no additional path step is
added; otherwise it must be 32 bytes. If "X" is the empty vector,
the internal pubkey is not tweaked; otherwise it must be a 32 byte
x-only pubkey.

The low bit of C indicates the parity of X; if it's 0, X has even y,
if it's 1, X has odd y.

The next bit of C indicates whether the current script is dropped from
the merkle path, if it's 0, the current script is kept, if it's 1 the
current script is dropped.

The remaining bits of C (ie C >> 2) are the number of steps in the merkle
path that are dropped. (If C is negative, behaviour is to be determined
-- either always fail, or always succeed and left for definition via
future soft-fork)

For example, suppose we have a taproot utxo that had 5 scripts
(A,B,C,D,E), calculated as per the example in BIP 341 as:

    AB = H_TapBranch(A, B)
    CD = H_TapBranch(C, D)
    CDE = H_TapBranch(CD, E)
    ABCDE = H_TapBranch(AB, CDE)

And we're spending using script E, in that case the control block includes
the script E, and the merkle path to it, namely (AB, CD).

So here's some examples of what you could do with TLUV to control how
the spending scripts can change, between the input sPK and the output sPK.

At it's simplest, if we used the script "0 0 0 TLUV", then that says we
keep the current script, keep all steps in the merkle path, don't add
any new ones, and don't change the internal public key -- that is that
we want to resulting sPK to be exactly the same as the one we're spending.

If we used the script "0 F 0 TLUV" (H=F, C=0) then we keep the current
script, keep all the steps in the merkle path (AB and CD), and add
a new step to the merkle path (F), giving us:

    EF = H_TapBranch(E, F)
    CDEF =H_TapBranch(CD, EF)
    ABCDEF = H_TapBranch(AB, CDEF)

If we used the script "0 F 2 TLUV" (H=F, C=2) then we drop the current
script, but keep all the other steps, and add a new step (effectively
replacing the current script with a new one):

    CDF = H_TapBranch(CD, F)
    ABCDF = H_TapBranch(AB, CDF)

If we used the script "0 F 4 TLUV" (H=F, C=4) then we keep the current
script, but drop the last step in the merkle path, and add a new step
(effectively replacing the *sibling* of the current script):

    EF = H_TapBranch(E, F)
    ABEF = H_TapBranch(AB, EF)

If we used the script "0 0 4 TLUV" (H=empty, C=4) then we keep the current
script, drop the last step in the merkle path, and don't add anything new
(effectively dropping the sibling), giving just:

    ABE = H_TapBranch(AB, E)



Implementing the release/locked/available vault construct would then
look something like this:

Locked script = "OP_RETURN"
Available script = "<HOT> CHECKSIGVERIFY IN_OUT_AMOUNT SWAP <X> SUB DUP 0 GREATERTHAN IF GREATERTHANOREQUAL VERIFY 0 <H_LOCKED> 2 TLUV ELSE 2DROP ENDIF <D> CSV"
Release script = "<HOT> CHECKSIGVERIFY IF <H_LOCKED> ELSE <H_AVAILABLE> ENDIF 0 SWAP 4 TLUV  INPUTAMOUNT OUTPUTAMOUNT LESSTHANOREQUAL"
HOT = 32B hot wallet pubkey
X = maximum amount spendable via hot wallet at any time
D = compulsory delay between releasing funds and being able to spend them
H_LOCKED = H_TapLeaf(locked script)
H_AVAILABLE= H_TapLeaf(available script)
Internal public key = 32B cold wallet pubkey



Moving on to the pooled scheme and actually updating the internal pubkey
is, unfortunately, where things start to come apart. In particular,
since taproot uses 32-byte x-only pubkeys (with implicit even-y) for the
scriptPubKey and the internal public key, we have to worry about what
happens if, eg, A,B,C and A+B+C all have even-y, but (A+B)=(A+B+C)-C does
not have even-y. In that case allowing C to remove herself from the pool,
might result in switching from the scriptPubKey Qabc to the scriptPubKey
Qab as follows:

     Qabc = (A+B+C) + H(A+B+C, (Sa, (Sb, Sc)))*G
     Qab = -(A+B) + H( -(A+B), (Sa, Sb)*G

That's fine so far, but what happens if B then removes himself from the
pool? You take the internal public key, which turns out to be -(A+B)
since (A+B) did not have even y, and then subtract B, but that gives you
-A-2B instead of just A. So B obtains his funds, but B's signature hasn't
been cancelled out from the internal public key, so is still required
in order to do key path spends, which is definitely not what we want.

If we ignore that caveat (eg, having TLUV consider it to be an error if
you end up an internal public key that has odd-y) then the scripts for
exiting the pool are straightforward (if your balance is BAL and your
key is KEY):

    <KEY> DUP "" 1 TLUV
    CHECKSIGVERIFY 
    IN_OUT_AMOUNT SUB <BAL> GREATERTHANOREQUAL

It seems like "just ignore it" might be feasible for modest sized pools --
just choose A, B, C, D.. so that every combination of them (A+B+C, A+D,
etc) sums to a point that happens to have even-y and have each participant
in the pool verify that prior to using the pool. If I got my maths right,
you'll need to do about (2**n) trials to find a set of lucky points,
but each unlucky set will tend to fail quickly, leading to amortized
constant time for each test, so something like 3*(2**n) work overall. So
as long as n is no more than 20 or 30, that should be reasonably feasible.

To deal with it properly, you need to have the utxo commit to the parity
of the internal public key and have some way to find out that value when
using TLUV. There are probably three plausible ways of doing this.

The straightforward way is just to commit to it in the scriptPubKey --
that is, rather than taproot's approach of setting Q = P + H(P, S)*G where
P is a 32 byte x-only pubkey, also commit to the parity of P in the H(P,
S) step, and reveal the parity of the internal public key as part of the
control block when spending via the script path, in addition to revealing
the parity of the scriptPubKey point as we do already. Since taproot is
already locked in for activation, it's too late to change this behaviour
for taproot addresses, but we could include this in a future soft-fork
that enabled entroot or similar, or we could make this the behaviour of
(eg) 33B segwit v1 addresses that begin with 0x00, or similar.

If we don't commit to the parity in the scriptPubKey, there are two other
ways to commit to it in the utxo: either by having script ensure it is
committed to it in the value, or by extending the data that's saved in
the utxo database.

To commit to it in the value, you might do something like:

    <P> <H> IN_OUT_AMOUNT 2 MOD SWAP 2 MOD TUCK EQUAL 2 MUL ADD TLUV

and change TLUV's control parameter to be: C&1 = add/subtract the point,
C&2 = require the result to be even/odd y (with C&4 and C>>3 controlling
whether the current script and how many merkle paths are dropped). The
idea being to require that, if the utxo's value in satoshis is 0 mod
2, you subtract the point, and if it's 1 mod 2, you add the point,
and that the *output* amount's value in satoshis is different (mod 2)
from the input amount's value (mod 2), exactly when the resulting point
ends up with odd y.  Combined with a rule to ensure the output amount
doesn't decrease by more than your balance, this would effectively mean
that if half the time when you withdraw your balance you'll have to pay
a 1 satoshi fee to the remaining pool members so the the parity of the
remaining value is correct, which is inelegant, but seems like workable.

The other approach sits somewhere between those two, and would involve
adding a flag to each entry in the utxo database to say whether the
internal public key had been inverted. This would only be set if the
utxo had been created via a spending script that invoked TLUV, and TLUV
would use the flag to determine whether to add/subtract the provided
point. That seems quite complicated to implement to me, particularly if
you want to allow the flag to be able to be set by future opcodes that
we haven't thought of yet.



All of this so far assumed that the hashes for any new merkle steps are
fixed when the contract is created. If "OP_CAT" or similar were enabled,
however, you could construct those hashes programmatically in script,
which might lead to some interesting behaviour. For example, you could
construct a script that says "allow anyone to add themselves to the
buy-a-citadel pool, as long as they're contributing at least 10 BTC",
which would then verify they have control of the pubkey they're adding,
and allow them to add a script that lets them pull their 10 BTC back
out via that pubkey, and participate in key path spends in the same
way as everyone else. Of course, that sort of feature probably also
naturally extends to many of the "covenants considered harmful" cases,
eg a dollar-auction-like-contract: "Alice can spend this utxo after 1000
confirmations" or "anyone who increases the balance by 0.1 BTC can swap
Alice's pubkey for their own in the sibling script to this".

An interesting thing to note is that constructing the script can sometimes
be more efficient than hardcoding it, eg, I think

    "TapLeaf" SHA256 DUP CAT [0xc0016a] CAT SHA256

is correct for calculating the hash for the "OP_RETURN" script, and at
~17 bytes should be cheaper than the ~33 bytes it would take to hardcode
the hash.

To construct a new script programmatically you almost certainly need to
use templates, eg

    SIZE 32 EQUALVERIFY [0xc02220] SWAP CAT [0xac] CAT
    "TapLeaf" SHA256 DUP CAT SWAP CAT SHA256

might take a public key off the stack and turn it into the hash for a
script that expects a signature from that pubkey. I believe you could
construct multiple scripts and combine them via

    CAT "TapBranch" SHA256 DUP CAT SWAP CAT SHA256

or similar as well.

There's a serious caveat with doing that in practice though: if you allow
people to add in arbitrary opcodes when constructing the new script,
they could choose to have that opcode be one of the "OP_SUCCESS" opcodes,
and, if they're a miner, use that to bypass the covenant constraints
entirely. So if you want to think about this, the template being filled
in probably has to be very strict, eg including the specific PUSH opcode
for the data being provided in the witness, and checking that the length
of the witness data exactly matches the PUSH opcode being used.

Cheers,
aj

From antoine.riard at gmail.com  Thu Sep  9 00:02:45 2021
From: antoine.riard at gmail.com (Antoine Riard)
Date: Wed, 8 Sep 2021 20:02:45 -0400
Subject: [bitcoin-dev] Note on Sequence Lock Upgrades Defect
In-Reply-To: <CAD5xwhiKU1fuhqmKsx28f1nuw9CmvbyrS=BtM4X-L+WPgWY3Wg@mail.gmail.com>
References: <CAD5xwhiKU1fuhqmKsx28f1nuw9CmvbyrS=BtM4X-L+WPgWY3Wg@mail.gmail.com>
Message-ID: <CALZpt+Fk=_3=Hb_u4OptAwHKaG6R=+6igDeLQESQ_u_QbBQePg@mail.gmail.com>

Hi Jeremy,

Answering here from #22871 discussions.

I agree on the general principle to not blur mempool policies signaling in
committed transaction data. Beyond preserving upgradeability, another good
argument is to let L2 nodes update the mempool policies signaling their
pre-signed transactions non-interactively. If one of the transaction fields
is assigned mempool semantics, in case of tightening policy changes, you
will need to re-sign or bear the risks of having non-propagating
transactions which opens the door for exploitation by a malicious
counterparty. I think this point is kinda relevant if we have future
cross-layer coordinated safety fixes to deal with a la CVE-2021-31876.

Even further, a set of L2 counterparties would like to pick up divergent
tx-relay/mempool policies, having the signaling fields as part of the
signature force them to come to consensus.

I think we can take the opportunity of p2p packages to introduce a new
field to signal policy. Of course, a malicious tx-relay peer could modify
its content to jam your transaction's propagation but in that case it is
easier to just drop it.

One issue with taking back the `nSequence` field for consensus-semantic
sounds is depriving the application-layer from a discrete, zero-cost
payload (e.g the LN obfuscated commitment number watermark). This might be
controversial as we'll increase the price of such applications if they're
still willingly to relay application specific data through the p2p network
(e.g force them to use a costly OP_RETURN output or payer/payee
interactions to setup a pay-to-contract)

W.r.t flag day activation to smooth policy deployment, I think that's
something we might rely on in the future though we could distinguish few
types of policy deployments :
1) loosening changes (e.g full-rbf/dust threshold removal), a transaction
which was relaying under
the former policy should relay under the new one
2) tightening changes (e.g #22871), a transaction which was relaying under
the former policy
might not relay under the new one
3) new feature introduced (e.g packages), a transaction is offered a new
mode of relay

I think 1) doesn't need that level of ecosystem coordination as
applications/second-layers should always benefit from such changes. Maybe
with the exception of full-rbf, where we have historical 0-conf softwares,
with (broken) security assumptions made on the opt-out RBF mechanism. Same
with 3), better to have new features deployed gradually, a flag day
activation day in this case won't mean that all higher stacks will jump to
use package-relay ?

Where a flag day might make sense would be for 2) ? It would create a
higher level of commitment by the base layer software instead of a pure
communication on the ML/GH, which might not be concretized in the announced
release due to slow review process/feature freeze/rebase conflicts...
Reversing the process and asking for Bitcoin applications/higher layers to
update first might get us in the trap of never doing the change, as someone
might have a small use-case in the corner relying on a given policy
behavior.

That said, w.r.t to the proposed policy change in #22871, I think it's
better to deploy full-rbf first, then give a time buffer to higher
applications to free up the `nSequence` field and finally start to
discourage the usage. Otherwise, by introducing new discouragement waivers,
e.g not rejecting the usage of the top 8 bits, I think we're moving away
from the policy design principle we're trying to establish (separation of
mempool policies signaling from consensus data)

Le ven. 3 sept. 2021 ? 23:32, Jeremy via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> a ?crit :

> Hi Bitcoin Devs,
>
> I recently noticed a flaw in the Sequence lock implementation with respect
> to upgradability. It might be the case that this is protected against by
> some transaction level policy (didn't see any in policy.cpp, but if not,
> I've put up a blogpost explaining the defect and patching it
> https://rubin.io/bitcoin/2021/09/03/upgradable-nops-flaw/
>
> I've proposed patching it here
> https://github.com/bitcoin/bitcoin/pull/22871, it is proper to widely
> survey the community before patching to ensure no one is depending on the
> current semantics in any live application lest this tightening of
> standardness rules engender a confiscatory effect.
>
> Best,
>
> Jeremy
>
> --
> @JeremyRubin <https://twitter.com/JeremyRubin>
> <https://twitter.com/JeremyRubin>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210908/e40cd7f3/attachment-0001.html>

From lf-lists at mattcorallo.com  Thu Sep  9 09:16:12 2021
From: lf-lists at mattcorallo.com (Matt Corallo)
Date: Thu, 9 Sep 2021 02:16:12 -0700
Subject: [bitcoin-dev] TAPLEAF_UPDATE_VERIFY covenant opcode
In-Reply-To: <20210909064138.GA22496@erisian.com.au>
References: <20210909064138.GA22496@erisian.com.au>
Message-ID: <2AED377C-17D3-45A0-B934-9EA5242B9487@mattcorallo.com>

Thanks for taking the time to write this up!

To wax somewhat broadly here, I?m very excited about this as a direction for bitcoin covenants. Other concrete proposals seem significantly more limited, which worries me greatly. Further, this feels very ?taproot-native? in a way that encourages utilizing taproot?s features fully while building covenants, saving fees on chain and at least partially improving privacy.

I?ve been saying we need more covenants research and proposals before we move forward with one and this is a huge step in that direction, IMO. With Taproot activating soon, I?m excited for what coming forks bring.

Matt

> On Sep 8, 2021, at 23:42, Anthony Towns via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
> 
> ?Hello world,
> 
> A couple of years ago I had a flight of fancy [0] imagining how it
> might be possible for everyone on the planet to use bitcoin in a
> mostly decentralised/untrusted way, without requiring a block size
> increase. It was a bit ridiculous and probably doesn't quite hold up,
> and beyond needing all the existing proposals to be implemented (taproot,
> ANYPREVOUT, CTV, eltoo, channel factories), it also needed a covenant
> opcode [1]. I came up with something that I thought fit well with taproot,
> but couldn't quite figure out how to use it for anything other than my
> ridiculous scheme, so left it at that.
> 
> But recently [2] Greg Maxwell emailed me about his own cool idea for a
> covenant opcode, which turned out to basically be a reinvention of the
> same idea but with more functionality, a better name and a less fanciful
> use case; and with that inspiration, I think I've also now figured out
> how to use it for a basic vault, so it seems worth making the idea a
> bit more public.
> 
> I'll split this into two emails, this one's the handwavy overview,
> the followup will go into some of the implementation complexities.
> 
> 
> 
> The basic idea is to think about "updating" a utxo by changing the
> taproot tree.
> 
> As you might recall, a taproot address is made up from an internal public
> key (P) and a merkle tree of scripts (S) combined via the formula Q=P+H(P,
> S)*G to calculate the scriptPubKey (Q). When spending using a script,
> you provide the path to the merkle leaf that has the script you want
> to use in the control block. The BIP has an example [3] with 5 scripts
> arranged as ((A,B), ((C,D), E)), so if you were spending with E, you'd
> reveal a path of two hashes, one for (AB), then one for (CD), then you'd
> reveal your script E and satisfy it.
> 
> So that makes it relatively easy to imagine creating a new taproot address
> based on the input you're spending by doing some or all of the following:
> 
> * Updating the internal public key (ie from P to P' = P + X)
> * Trimming the merkle path (eg, removing CD)
> * Removing the script you're currently executing (ie E)
> * Adding a new step to the end of the merkle path (eg F)
> 
> Once you've done those things, you can then calculate the new merkle
> root by resolving the updated merkle path (eg, S' = MerkleRootFor(AB,
> F, H_TapLeaf(E))), and then calculate a new scriptPubKey based on that
> and the updated internal public key (Q' = P' + H(P', S')).
> 
> So the idea is to do just that via a new opcode "TAPLEAF_UPDATE_VERIFY"
> (TLUV) that takes three inputs: one that specifies how to update the
> internal public key (X), one that specifies a new step for the merkle path
> (F), and one that specifies whether to remove the current script and/or
> how many merkle path steps to remove. The opcode then calculates the
> scriptPubKey that matches that, and verifies that the output corresponding
> to the current input spends to that scriptPubKey.
> 
> That's useless without some way of verifying that the new utxo retains
> the bitcoin that was in the old utxo, so also include a new opcode
> IN_OUT_AMOUNT that pushes two items onto the stack: the amount from this
> input's utxo, and the amount in the corresponding output, and then expect
> anyone using TLUV to use maths operators to verify that funds are being
> appropriately retained in the updated scriptPubKey.
> 
> 
> 
> Here's two examples of how you might use this functionality.
> 
> First, a basic vault. The idea is that funds are ultimately protected
> by a cold wallet key (COLD) that's inconvenient to access but is as
> safe from theft as possible. In order to make day to day transactions
> more convenient, a hot wallet key (HOT) is also available, which is
> more vulnerable to theft. The vault design thus limits the hot wallet
> to withdrawing at most L satoshis every D blocks, so that if funds are
> stolen, you lose at most L, and have D blocks to use your cold wallet
> key to re-secure the funds and prevent further losses.
> 
> To set this up with TLUV, you construct a taproot output with COLD as
> the internal public key, and a script that specifies:
> 
> * The tx is signed via HOT
> * <D> CSV -- there's a relative time lock since the last spend
> * If the input amount is less than L + dust threshold, fine, all done,
>   the vault can be emptied.
> * Otherwise, the output amount must be at least (the input amount -
>   L), and do a TLUV check that the resulting sPK is unchanged
> 
> So you can spend up to "L" satoshis via the hot wallet as long as you
> wait D blocks since the last spend, and can do whatever you want via a
> key path spend with the cold wallet.
> 
> You could extend this to have a two phase protocol for spending, where
> first you use the hot wallet to say "in D blocks, allow spending up to
> L satoshis", and only after that can you use the hot wallet to actually
> spend funds. In that case supply a taproot sPK with COLD as the internal
> public key and two scripts, the "release" script, which specifies:
> 
> * The tx is signed via HOT
> * Output amount is greater or equal to the input amount.
> * Use TLUV to check:
>   + the output sPK has the same internal public key (ie COLD)
>   + the merkle path has one element trimmed
>   + the current script is included
>   + a new step is added that matches either H_LOCKED or H_AVAILABLE as
>     described below (depending on whether 0 or 1 was provided as
>     witness info)
> 
> The other script is either "locked" (which is just "OP_RETURN") or
> "available" which specifies:
> 
> * The tx is signed via HOT
> * <D> CSV -- there's a relative time lock since the last spend (ie,
>   when the "release" script above was used)
> * If the input amount is less than L, fine, all done, the vault can
>   be emptied
> * Otherwise, the output amount must be at least (the input amount minus
>   L), and via TLUV, check the resulting sPK keeps the internal pubkey
>   unchanged, keeps the merkle path, drops the current script, and adds
>   H_LOCKED as the new step.
> 
> H_LOCKED and H_AVAILABLE are just the TapLeaf hash corresponding to the
> "locked" and "available" scripts.
> 
> I believe this latter setup matches the design Bryan Bishop talked about
> a couple of years ago [4], with the benefit that it's fully recursive,
> allows withdrawals to vary rather than be the fixed amount L (due to not
> relying on pre-signed transactions), and generally seems a bit simpler
> to work with.
> 
> 
> 
> The second scheme is allowing for a utxo to represent a group's pooled
> funds. The idea being that as long as everyone's around you can use
> the taproot key path to efficiently move money around within the pool,
> or use a single transaction and signature for many people in the pool
> to make payments. But key path spends only work if everyone's available
> to sign -- what happens if someone disappears, or loses access to their
> keys, or similar? For that, we want to have script paths to allow other
> people to reclaim their funds even if everyone else disappears. So we
> setup scripts for each participant, eg for Alice:
> 
> * The tx is signed by Alice
> * The output value must be at least the input value minus Alice's balance
> * Must pass TLUV such that:
>   + the internal public key is the old internal pubkey minus Alice's key
>   + the currently executing script is dropped from the merkle path
>   + no steps are otherwise removed or added
> 
> The neat part here is that if you have many participants in the pool,
> the pool continues to operate normally even if someone makes use of the
> escape hatch -- the remaining participants can still use the key path to
> spend efficiently, and they can each unilaterally withdraw their balance
> via their own script path. If everyone decides to exit, whoever is last
> can spend the remaining balance directly via the key path.
> 
> Compared to having on-chain transactions using non-pooled funds, this
> is more efficient and private: a single one-in, one-out transaction
> suffices for any number of transfers within the pool, and there's no
> on-chain information about who was sending/receiving the transfers, or
> how large the transfers were; and for transfers out of the pool, there's
> no on-chain indication which member of the pool is sending the funds,
> and multiple members of the pool can send funds to multiple destinations
> with only a single signature. The major constraint is that you need
> everyone in the pool to be online in order to sign via the key path,
> which provides a practical limit to how many people can reasonably be
> included in a pool before there's a breakdown.
> 
> Compared to lightning (eg eltoo channel factories with multiple
> participants), the drawback is that no transfer is final without an
> updated state being committed on chain, however there are also benefits
> including that if one member of the pool unilaterally exits, that
> doesn't reveal the state of anyone remaining in the pool (eg an eltoo
> factory would likely reveal the balances of everyone else's channels at
> that point).
> 
> A simpler case for something like this might be for funding a joint
> venture -- suppose you're joining with some other early bitcoiners to
> buy land to build a citadel, so you each put 20 BTC into a pooled utxo,
> ready to finalise the land purchase in a few months, but you also want
> to make sure you can reclaim the funds if the deal falls through. So
> you might include scripts like the above that allow you to reclaim your
> balance, but add a CLTV condition preventing anyone from doing that until
> the deal's deadline has passed. If the deal goes ahead, you all transfer
> the funds to the vendor via the keypath; if it doesn't work out, you
> hopefully return your funds via the keypath, but if things turn really
> sour, you can still just directly reclaim your 20 BTC yourself via the
> script path.
> 
> 
> 
> I think a nice thing about this particular approach to recursive covenants
> at a conceptual level is that it automatically leaves the key path as an
> escape mechanism -- rather than having to build a base case manually,
> and have the risk that it might not work because of some bug, locking
> your funds into the covenant permanently; the escape path is free, easy,
> and also the optimal way of spending things when everything is working
> right. (Of course, you could set the internal public key to a NUMS point
> and shoot yourself in the foot that way anyway)
> 
> 
> 
> I think there's two limitations of this method that are worth pointing out.
> 
> First it can't tweak scripts in areas of the merkle tree that it can't
> see -- I don't see a way of doing that particularly efficiently, so maybe
> it's best just to leave that as something for the people responsible for
> the funds to negotiate via the keypath, in which case it's automatically
> both private and efficient since all the details stay off-chain, anyway
> 
> And second, it doesn't provide a way for utxos to "interact", which is
> something that is interesting for automated market makers [5], but perhaps
> only interesting for chains aiming to support multiple asset types,
> and not bitcoin directly. On the other hand, perhaps combining it with
> CTV might be enough to solve that, particularly if the hash passed to
> CTV is constructed via script/CAT/etc.
> 
> 
> 
> (I think everything described here could be simulated with CAT and
> CHECKSIGFROMSTACK (and 64bit maths operators and some way to access
> the internal public key), the point of introducing dedicated opcodes
> for this functionality rather than (just) having more generic opcodes
> would be to make the feature easy to use correctly, and, presuming it
> actually has a wide set of use cases, to make it cheap and efficient
> both to use in wallets, and for nodes to validate)
> 
> Cheers,
> aj
> 
> [0] https://gist.github.com/ajtowns/dc9a59cf0a200bd1f9e6fb569f76f7a0
> 
> [1] Roughly, the idea was that if you have ~9 billion people using
>    bitcoin, but can only have ~1000 transactions per block, then you
>    need have each utxo represent a significant number of people. That
>    means that you need a way of allowing the utxo's to be efficiently
>    spent, but need to introduce some level of trust since expecting
>    many people to constantly be online seems unreliable, but to remain
>    mostly decentralised/untrusted, you want to have some way of limiting
>    how much trust you're introducing, and that's where covenants come in.
> 
> [2] Recently in covid-adjusted terms, or on the bitcoin consensus
>    change scale anyway...
>    https://mobile.twitter.com/ajtowns/status/1385091604357124100 
> 
> [3] https://github.com/bitcoin/bips/blob/master/bip-0341.mediawiki#Constructing_and_spending_Taproot_outputs 
> 
> [4] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-August/017231.html
> 
> [5] The idea behind an automated market maker being that you setup a
>    script that says "you can withdraw x BTC if you deposit f(x) units of
>    USDT, or you can withdraw g(x) units of USDT if you deposit x units
>    of BTC", with f(x)/x giving the buy price, and f(x)>g(x) meaning
>    you make a profit. Being able to specify a covenant that links the
>    change in value to the BTC utxo (+/-x) and the change in value to
>    the USDT utxo (+f(x) or -g(x)) is what you'd need to support this
>    sort of use case, but TLUV doesn't provide a way to do that linkage.
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From darosior at protonmail.com  Thu Sep  9 12:56:10 2021
From: darosior at protonmail.com (darosior)
Date: Thu, 09 Sep 2021 12:56:10 +0000
Subject: [bitcoin-dev] TAPLEAF_UPDATE_VERIFY covenant opcode
In-Reply-To: <20210909065330.GB22496@erisian.com.au>
References: <20210909064138.GA22496@erisian.com.au>
 <20210909065330.GB22496@erisian.com.au>
Message-ID: <Er7M8L4KS9ZxYK6pIPy6dCtTbjb2edbIzAjhU4CCQi3n1klj6x7C0gTw0MrL4w91wZ_aRVEFuzcFOc8U8wz_KfKchlLi3Hp2Okhu7D-CTRo=@protonmail.com>

Hi Anthony,


This post is a follow-up to your insight on Twitter [0], sent here for better
posterity, accessibility and readability than Twitter. And also to motivate this
idea by giving another concrete [1] usecase for it.

Revault [2] is a multi-party "vault" protocol. It involves 2 sets of participants
that may or may not intersect (although i expect the second one to often be a subset
of the first one). The stakeholders, analogous to the "cold keys", receive coins on
a (large) N-of-N multisig and presign an Unvault transaction which creates an Unvault
output which pays to either the (small) K-of-M multisig of the managers after a timelock
or to the N-of-N immediately (allowing for a Cancel transaction).

This allows for partial delegation of the funds, and some automated policies (can't
broadcast the Unvault outside business hours, can't unvault more than <limit> BTC a
week, etc..) that can be enforced by watchtowers. That's nice, but it would be even
nicer if we could have policies on the Spend transaction (the one created by the
managers to spend the Unvault output) itself to further restrict how the coin can move [3].

But in order to do so, you'd need the managers to disclaim the Spend transaction they
are going to use before broadcasting the Unvault and somehow commit to it at unvaulting
time. Apart from stupid hacks [4] i could not find a reasonable covenant design as a
solution to this issue.
It think TLUV fixes this.

The idea (your idea, actually) is to receive coins not to a N-of-N anymore but to a
Taproot with a branch which contains the manager multisig + a TLUV which would replace
the current branch being executed by a CSV + CTV which input hash value will be taken
from the witness stack at Unvault broadcast. Therefore chosen by the managers at spending
time, and available for the entire duration of the timelock.

So, the scripts would be something like (assuming CAT, CTV, TLUV):
V = max acceptable fees
D = "CTV <X> CSV DROP 1"
C = "<32 bytes> D"
B = "
<pk man 1> CHECKSIG <pk man 2> CHECKSIGADD ... <pk man M> CHECKSIGADD <K> EQUALVERIFY
IN_OUT_AMOUNT SUB <V> LESSTHANOREQUAL DUP VERIFY
SIZE 32 EQUALVERIFY <0xc0 | len(D) + 32 + 1 | 0x20> SWAP CAT "Tapleaf" SHA256 DUP CAT SWAP CAT SHA256 0 SWAP 2 TLUV
"
A = "<pk stk 1> CHECKSIGVERIFY <pk stk 2> CHECKSIGVERIFY ... <pk stk N> CHECKSIG"

The deposit output ScriptPubKey would be Taproot(A, B) [5].
The unvault output ScriptPubKey would be Taproot(A, C).
This also allows for a lot more flexibility (batching at the Unvault level [7], use RBF
instead of more wasteful CPFP, etc..) and creates a number of problems [6] on which
i won't expand on. But it does the most important part: it enables it.

Looking forward to more feedback on your proposal!


Thanks,
Antoine



[0] https://twitter.com/ajtowns/status/1435884659146059776?s=20
[1] we've proposed Revault a year and a half ago, have been building it since. We
    should have a first version released soon (tm).
[2] https://github.com/revault
[3] technically we do optionally offer this at the moment, but at the expense of a
    reduction of security and a pretty ugly hack: by using "anti-replay" oracles
    (cosigning servers that are only going to sign only once for a given prevout)
[4] the last bad idea to date is "have ANYPREVOUT, presign the Unvault with
    SIGHASH_SINGLE, enforce that the Unvault output is only spent with a transaction
    spending <same txid>:1 and have managers append an output to the Unvault enforcing
    a covenant just before broadcast"
[5] as a branch because i don't know how to use the keypath spend for a multisig with
    cold keys (yet).
[6] as such you'd need a sig for canceling but not for unvaulting, so it reverses the
    security model from "can't do anything til everyone signed" to "can steal until
    everyone has signed" so you'd need a TLUV for the cancel spending path as well, but
    then how to make this covenant non-replayable, flexible enough to feebump but not
    enough be vulnerable to pining, etc..
[7] Note that this means all Cancel must be confirmed to recover the funds but a single
    one needs to in order to prevent a spending.

??????? Original Message ???????

Le jeudi 9 septembre 2021 ? 8:53 AM, Anthony Towns via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> a ?crit :

> On Thu, Sep 09, 2021 at 04:41:38PM +1000, Anthony Towns wrote:
>
> > I'll split this into two emails, this one's the handwavy overview,
> >
> > the followup will go into some of the implementation complexities.
>
> (This is informed by discussions with Greg, Matt Corallo, David Harding
>
> and Jeremy Rubin; opinions and mistakes my own, of course)
>
> First, let's talk quickly about IN_OUT_AMOUNT. I think the easiest way to
>
> deal with it is just a single opcode that pushes two values to the stack;
>
> however it could be two opcodes, or it could even accept a parameter
>
> letting you specify which input (and hence which corresponding output)
>
> you're talking about (-1 meaning the current input perhaps).
>
> Anyway, a big complication here is that amounts in satoshis require up
>
> to 51 bits to represent them, but script only allows you to do 32 bit
>
> maths. However introducing IN_OUT_AMOUNT already means using an OP_SUCCESS
>
> opcode, which in turn allows us to arbitrarily redefine the behaviour
>
> of other opcodes -- so we can use the presence of IN_OUT_AMOUNT in the
>
> script to upgrade ADD, SUB, and the comparison operators to support 64
>
> bit values. Enabling MUL, DIV and MOD might also be worthwhile.
>
> Moving onto TLUV. My theory is that it pops three items off the stack. The
>
> top of the stack is "C" the control integer; next is "H" the
>
> additional path step; and finally "X" the tweak for the internal
>
> pubkey. If "H" is the empty vector, no additional path step is
>
> added; otherwise it must be 32 bytes. If "X" is the empty vector,
>
> the internal pubkey is not tweaked; otherwise it must be a 32 byte
>
> x-only pubkey.
>
> The low bit of C indicates the parity of X; if it's 0, X has even y,
>
> if it's 1, X has odd y.
>
> The next bit of C indicates whether the current script is dropped from
>
> the merkle path, if it's 0, the current script is kept, if it's 1 the
>
> current script is dropped.
>
> The remaining bits of C (ie C >> 2) are the number of steps in the merkle
>
> path that are dropped. (If C is negative, behaviour is to be determined
>
> -- either always fail, or always succeed and left for definition via
>
> future soft-fork)
>
> For example, suppose we have a taproot utxo that had 5 scripts
>
> (A,B,C,D,E), calculated as per the example in BIP 341 as:
>
> AB = H_TapBranch(A, B)
>
> CD = H_TapBranch(C, D)
>
> CDE = H_TapBranch(CD, E)
>
> ABCDE = H_TapBranch(AB, CDE)
>
> And we're spending using script E, in that case the control block includes
>
> the script E, and the merkle path to it, namely (AB, CD).
>
> So here's some examples of what you could do with TLUV to control how
>
> the spending scripts can change, between the input sPK and the output sPK.
>
> At it's simplest, if we used the script "0 0 0 TLUV", then that says we
>
> keep the current script, keep all steps in the merkle path, don't add
>
> any new ones, and don't change the internal public key -- that is that
>
> we want to resulting sPK to be exactly the same as the one we're spending.
>
> If we used the script "0 F 0 TLUV" (H=F, C=0) then we keep the current
>
> script, keep all the steps in the merkle path (AB and CD), and add
>
> a new step to the merkle path (F), giving us:
>
> EF = H_TapBranch(E, F)
>
> CDEF =H_TapBranch(CD, EF)
>
> ABCDEF = H_TapBranch(AB, CDEF)
>
> If we used the script "0 F 2 TLUV" (H=F, C=2) then we drop the current
>
> script, but keep all the other steps, and add a new step (effectively
>
> replacing the current script with a new one):
>
> CDF = H_TapBranch(CD, F)
>
> ABCDF = H_TapBranch(AB, CDF)
>
> If we used the script "0 F 4 TLUV" (H=F, C=4) then we keep the current
>
> script, but drop the last step in the merkle path, and add a new step
>
> (effectively replacing the sibling of the current script):
>
> EF = H_TapBranch(E, F)
>
> ABEF = H_TapBranch(AB, EF)
>
> If we used the script "0 0 4 TLUV" (H=empty, C=4) then we keep the current
>
> script, drop the last step in the merkle path, and don't add anything new
>
> (effectively dropping the sibling), giving just:
>
> ABE = H_TapBranch(AB, E)
>
> Implementing the release/locked/available vault construct would then
>
> look something like this:
>
> Locked script = "OP_RETURN"
>
> Available script = "<HOT> CHECKSIGVERIFY IN_OUT_AMOUNT SWAP <X> SUB DUP 0 GREATERTHAN IF GREATERTHANOREQUAL VERIFY 0 <H_LOCKED> 2 TLUV ELSE 2DROP ENDIF <D> CSV"
>
> Release script = "<HOT> CHECKSIGVERIFY IF <H_LOCKED> ELSE <H_AVAILABLE> ENDIF 0 SWAP 4 TLUV INPUTAMOUNT OUTPUTAMOUNT LESSTHANOREQUAL"
>
> HOT = 32B hot wallet pubkey
>
> X = maximum amount spendable via hot wallet at any time
>
> D = compulsory delay between releasing funds and being able to spend them
>
> H_LOCKED = H_TapLeaf(locked script)
>
> H_AVAILABLE= H_TapLeaf(available script)
>
> Internal public key = 32B cold wallet pubkey
>
> Moving on to the pooled scheme and actually updating the internal pubkey
>
> is, unfortunately, where things start to come apart. In particular,
>
> since taproot uses 32-byte x-only pubkeys (with implicit even-y) for the
>
> scriptPubKey and the internal public key, we have to worry about what
>
> happens if, eg, A,B,C and A+B+C all have even-y, but (A+B)=(A+B+C)-C does
>
> not have even-y. In that case allowing C to remove herself from the pool,
>
> might result in switching from the scriptPubKey Qabc to the scriptPubKey
>
> Qab as follows:
>
> Qabc = (A+B+C) + H(A+B+C, (Sa, (Sb, Sc)))*G
>
> Qab = -(A+B) + H( -(A+B), (Sa, Sb)*G
>
> That's fine so far, but what happens if B then removes himself from the
>
> pool? You take the internal public key, which turns out to be -(A+B)
>
> since (A+B) did not have even y, and then subtract B, but that gives you
>
> -A-2B instead of just A. So B obtains his funds, but B's signature hasn't
>
> been cancelled out from the internal public key, so is still required
>
> in order to do key path spends, which is definitely not what we want.
>
> If we ignore that caveat (eg, having TLUV consider it to be an error if
>
> you end up an internal public key that has odd-y) then the scripts for
>
> exiting the pool are straightforward (if your balance is BAL and your
>
> key is KEY):
>
> <KEY> DUP "" 1 TLUV
>
>     CHECKSIGVERIFY
>     IN_OUT_AMOUNT SUB <BAL> GREATERTHANOREQUAL
>
>
> It seems like "just ignore it" might be feasible for modest sized pools --
>
> just choose A, B, C, D.. so that every combination of them (A+B+C, A+D,
>
> etc) sums to a point that happens to have even-y and have each participant
>
> in the pool verify that prior to using the pool. If I got my maths right,
>
> you'll need to do about (2n) trials to find a set of lucky points,
>
> but each unlucky set will tend to fail quickly, leading to amortized
>
> constant time for each test, so something like 3*(2n) work overall. So
>
> as long as n is no more than 20 or 30, that should be reasonably feasible.
>
> To deal with it properly, you need to have the utxo commit to the parity
>
> of the internal public key and have some way to find out that value when
>
> using TLUV. There are probably three plausible ways of doing this.
>
> The straightforward way is just to commit to it in the scriptPubKey --
>
> that is, rather than taproot's approach of setting Q = P + H(P, S)*G where
>
> P is a 32 byte x-only pubkey, also commit to the parity of P in the H(P,
>
> S) step, and reveal the parity of the internal public key as part of the
>
> control block when spending via the script path, in addition to revealing
>
> the parity of the scriptPubKey point as we do already. Since taproot is
>
> already locked in for activation, it's too late to change this behaviour
>
> for taproot addresses, but we could include this in a future soft-fork
>
> that enabled entroot or similar, or we could make this the behaviour of
>
> (eg) 33B segwit v1 addresses that begin with 0x00, or similar.
>
> If we don't commit to the parity in the scriptPubKey, there are two other
>
> ways to commit to it in the utxo: either by having script ensure it is
>
> committed to it in the value, or by extending the data that's saved in
>
> the utxo database.
>
> To commit to it in the value, you might do something like:
>
> <P> <H> IN_OUT_AMOUNT 2 MOD SWAP 2 MOD TUCK EQUAL 2 MUL ADD TLUV
>
> and change TLUV's control parameter to be: C&1 = add/subtract the point,
>
> C&2 = require the result to be even/odd y (with C&4 and C>>3 controlling
>
> whether the current script and how many merkle paths are dropped). The
>
> idea being to require that, if the utxo's value in satoshis is 0 mod
>
> 2, you subtract the point, and if it's 1 mod 2, you add the point,
>
> and that the output amount's value in satoshis is different (mod 2)
>
> from the input amount's value (mod 2), exactly when the resulting point
>
> ends up with odd y. Combined with a rule to ensure the output amount
>
> doesn't decrease by more than your balance, this would effectively mean
>
> that if half the time when you withdraw your balance you'll have to pay
>
> a 1 satoshi fee to the remaining pool members so the the parity of the
>
> remaining value is correct, which is inelegant, but seems like workable.
>
> The other approach sits somewhere between those two, and would involve
>
> adding a flag to each entry in the utxo database to say whether the
>
> internal public key had been inverted. This would only be set if the
>
> utxo had been created via a spending script that invoked TLUV, and TLUV
>
> would use the flag to determine whether to add/subtract the provided
>
> point. That seems quite complicated to implement to me, particularly if
>
> you want to allow the flag to be able to be set by future opcodes that
>
> we haven't thought of yet.
>
> All of this so far assumed that the hashes for any new merkle steps are
>
> fixed when the contract is created. If "OP_CAT" or similar were enabled,
>
> however, you could construct those hashes programmatically in script,
>
> which might lead to some interesting behaviour. For example, you could
>
> construct a script that says "allow anyone to add themselves to the
>
> buy-a-citadel pool, as long as they're contributing at least 10 BTC",
>
> which would then verify they have control of the pubkey they're adding,
>
> and allow them to add a script that lets them pull their 10 BTC back
>
> out via that pubkey, and participate in key path spends in the same
>
> way as everyone else. Of course, that sort of feature probably also
>
> naturally extends to many of the "covenants considered harmful" cases,
>
> eg a dollar-auction-like-contract: "Alice can spend this utxo after 1000
>
> confirmations" or "anyone who increases the balance by 0.1 BTC can swap
>
> Alice's pubkey for their own in the sibling script to this".
>
> An interesting thing to note is that constructing the script can sometimes
>
> be more efficient than hardcoding it, eg, I think
>
> "TapLeaf" SHA256 DUP CAT [0xc0016a] CAT SHA256
>
> is correct for calculating the hash for the "OP_RETURN" script, and at
>
> ~17 bytes should be cheaper than the ~33 bytes it would take to hardcode
>
> the hash.
>
> To construct a new script programmatically you almost certainly need to
>
> use templates, eg
>
> SIZE 32 EQUALVERIFY [0xc02220] SWAP CAT [0xac] CAT
>
> "TapLeaf" SHA256 DUP CAT SWAP CAT SHA256
>
> might take a public key off the stack and turn it into the hash for a
>
> script that expects a signature from that pubkey. I believe you could
>
> construct multiple scripts and combine them via
>
> CAT "TapBranch" SHA256 DUP CAT SWAP CAT SHA256
>
> or similar as well.
>
> There's a serious caveat with doing that in practice though: if you allow
>
> people to add in arbitrary opcodes when constructing the new script,
>
> they could choose to have that opcode be one of the "OP_SUCCESS" opcodes,
>
> and, if they're a miner, use that to bypass the covenant constraints
>
> entirely. So if you want to think about this, the template being filled
>
> in probably has to be very strict, eg including the specific PUSH opcode
>
> for the data being provided in the witness, and checking that the length
>
> of the witness data exactly matches the PUSH opcode being used.
>
> Cheers,
>
> aj
>
> bitcoin-dev mailing list
>
> bitcoin-dev at lists.linuxfoundation.org
>
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From jlrubin at mit.edu  Thu Sep  9 15:54:25 2021
From: jlrubin at mit.edu (Jeremy)
Date: Thu, 9 Sep 2021 08:54:25 -0700
Subject: [bitcoin-dev] TAPLEAF_UPDATE_VERIFY covenant opcode
In-Reply-To: <20210909065330.GB22496@erisian.com.au>
References: <20210909064138.GA22496@erisian.com.au>
 <20210909065330.GB22496@erisian.com.au>
Message-ID: <CAD5xwhimL-J1cmOWQxbxFjGFWRKtyB_2LHS4k9L9Y5KN8HycJA@mail.gmail.com>

I like this proposal, I think it has interesting use cases! I'm quick to
charitably Matt's comment, "I?ve been saying we need more covenants
research and proposals before we move forward with one", as before we move
forward with *any.* I don't think that these efforts are rival -- different
opcodes for different nodes as they say.

I've previously done some analysis comparing Coin / Payment Pools with CTV
to TapLeafUpdate which make CTV out favorably in terms of chain load and
privacy.

On the "anyone can withdraw themselves in O(1) transactions" front, is that
if you contrast a CTV-style tree, the withdraws are O(log(n)) but E[O(1)]
for all participants, e.g. summing over the entire tree as it splits to
evict a bad actor ends up being O(N) total work over N participants, so you
do have to look at the exact transactions that come out w.r.t. script size
to determine which Payment Pool has overall less chain work to trustlessly
withdraw. This is compounded by the fact that a Taproot for N participants
uses a O(log N) witness.


Let's do out that basic math. First, let's assume we have 30 participants.
The basic script for each node would be:

TLUV: Taproot(Tweaked Key, {<KEY> DUP "" 1 TLUV
    CHECKSIGVERIFY
    IN_OUT_AMOUNT SUB <BAL> GREATERTHANOREQUAL, ...})

Under this, the first withdraw for TLUV would require in witnesses stack:
Assume average amount is 0.005BTC, so we have 4.2 B users = 18.9 bits =3
bytes

1 signature (1+64 bytes) + (1 Script = (+ 1 1 32 1 1 1 1 1 1 1 3 1 1) = 46
bytes) + (1 taproot path = 2 + 33 + log2(N)*32)
= 146+log2(N)*32.

now, because we delete the key, we need to sum this from N=0 to N=30:

>>> sum([65+46+35+math.log(N,2)*32 for N in range(1, 31)])
7826.690154943152 bytes of witness data

Each transaction should have 1 input (40 bytes), 2 outputs (2* (34+8) =
84), 4 bytes locktime, 4 bytes version, 2 byte witness flag, 1 byte in
counter 1 byte out counter  = 136 bytes (we already count witnesses above)


136 * 30 + 7827 = 11907 bytes to withdraw all trustlessly

Now for CTV:
-CTV: Taproot(MuSigKey(subparties), <H splits pool with radix 4> CTV)

*sidebar: **why radix 4? A while ago, I did the math out and a radix of 4
or 5 was optimal for bare script... assuming this result holds with
taproot.*


balance holders: 0..30
you have a base set of transactions paying out: 0..4 4..8 8..12 12..16
16..20 20..24 24..27 27..30
interior nodes covering: 0..16 16..30
root node covering: 0..30

The witness for each of these looks like:

(Taproot Script = 1+1+32+1) + (Taproot Control = 33) = 68 bytes

A transaction with two outputs should have 1 input (40 bytes), 2 outputs
(2* (34+8) = 84), 4 bytes locktime, 4 bytes version, 2 byte witness flag, 1
byte in counter 1 byte out counter  = 136 bytes + 68 bytes witness = 204
A transaction with three outputs should have 1 input (40 bytes), 3 outputs
(3* (34+8) = 126), 4 bytes locktime, 4 bytes version, 2 byte witness flag,
1 byte in counter 1 byte out counter  = 178 bytes + 68 bytes witness = 246
A transaction with 4 outputs should have 1 input (40 bytes), 4 outputs (4*
(34+8) = 126), 4 bytes locktime, 4 bytes version, 2 byte witness flag, 1
byte in counter 1 byte out counter  = 220 bytes + 68 bytes witness = 288

204 + 288*6 + 246*2 = 2424 bytes

Therefore the CTV style pool is, in this example, about 5x more efficient
in block space utilization as compared to TLUV at trustlessly withdrawing
all participants. This extra space leaves lots of headroom to e.g.
including things like OP_TRUE anchor outputs (12*10) = 120 bytes total for
CPFP; an optional script path with 2 inputs for a gas-paying input (cost is
around 32 bytes for taproot?). The design also scales beyond 30
participants, where the advantage grows further (iirc, sum i = 0 to n log i
is relatively close to n log n).

In the single withdrawal case, the cost to eject a single participant with
CTV is 204+288 = 492 bytes, compared to 65+46+35+math.log(30,2)*32+136 =
439 bytes. The cost to eject a second participant in CTV is much smaller as
it amortizes -- worst case is 288, best case is 0 (already expanded),
whereas in TLUV there is limited amortization so it would be about 438
bytes.

The protocols are identical in the cooperative case.

In terms of privacy, the CTV version is a little bit worse. At every
splitting, radix of the root nodes total value gets broadcast. So to eject
a participant, you end up leaking a bit more information. However, it might
be a reasonable assumption that if one of your counterparties is
uncooperative, they might dox you anyways. CTV trees are also superior
during updates for privacy in the cooperative case. With the TLUV pool, you
must know all tapleafs and the corresponding balances. Whereas in CTV
trees, you only need to know the balances of the nodes above you. E.g., we
can update the balances

from: [[1 Alice, 2 Bob], [3 Carol, 4 Dave]]
to: [[2.5 Alice, 0.5 Bob], [3 Carol, 4 Dave]]

without informing Carol or Dave about the updates in our subtree, just that
our slice of participants signed off on it. So even in the 1 party
uncooperative case, 3/4 the pool has update privacy against them, and the
subtrees that share a branch with the uncooperative also have this privacy
recursively to an extent.

CTV's TXID stability also lets you embed payment channels a bit easier, but
perhaps TLUV would be in an ANYPREVOUT universe w.r.t. channel protocols so
that might matter less across the designs. Nevertheless, I think it's
exciting to think about these payment pools where every node is an
updatable channel.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210909/b7a5c0bd/attachment-0001.html>

From mrosset at bufio.org  Thu Sep  9 12:54:18 2021
From: mrosset at bufio.org (Mike Rosset)
Date: Thu, 09 Sep 2021 05:54:18 -0700
Subject: [bitcoin-dev] Clarification on the use of getblocktemplate RPC
	method.
Message-ID: <87r1dxbz4s.fsf@bufio.org>


Hello all,

I recently went down the bitcoin protocol rabbit hole. I wanted to use
GNU guile scheme to experiment with bitcoin. I initially started by
creating a toy bitcoin miner but I've run into some inconsistencies with
the documentation found on
https://en.bitcoin.it/wiki/Getblocktemplate. Namely with creating the
templates merkle root.

>From my understanding a coinbase transaction should have the
transactions data concatenated before creating the merkle root. But
getblocktemplate does not have a json cointbasetxn field. So I'm not
sure how to create a coinbase transaction without that.

I have a test template response data found here.
https://raw.githubusercontent.com/mrosset/prospect/master/test-suite/data.json
and using a modified version of the merkle python reference script found
on the wiki page. see
https://github.com/mrosset/prospect/blob/master/scripts/merkle.py . I'm
able to create a merkle root with the hash
c5fff939f628a04428c080ed5bd7cd9bc0b4722b2522743049adb18213adf28a but
that's minus the coinbase transaction.

So far I'm able to replicate this hash using the test data in guile. But
I'd like to sanitize this so that I'm using a coinbase transaction and
making sure the python and guile merkle roots match.

In short how do I get the coinbase transaction without the coinbasetxn
field existing?

Mike

From luke at dashjr.org  Thu Sep  9 17:36:17 2021
From: luke at dashjr.org (Luke Dashjr)
Date: Thu, 9 Sep 2021 17:36:17 +0000
Subject: [bitcoin-dev] Clarification on the use of getblocktemplate RPC
	method.
In-Reply-To: <87r1dxbz4s.fsf@bufio.org>
References: <87r1dxbz4s.fsf@bufio.org>
Message-ID: <202109091736.17891.luke@dashjr.org>

https://github.com/bitcoin/libblkmaker/blob/master/blkmaker.c#L172

On Thursday 09 September 2021 12:54:18 Mike Rosset via bitcoin-dev wrote:
> Hello all,
>
> I recently went down the bitcoin protocol rabbit hole. I wanted to use
> GNU guile scheme to experiment with bitcoin. I initially started by
> creating a toy bitcoin miner but I've run into some inconsistencies with
> the documentation found on
> https://en.bitcoin.it/wiki/Getblocktemplate. Namely with creating the
> templates merkle root.
>
> From my understanding a coinbase transaction should have the
> transactions data concatenated before creating the merkle root. But
> getblocktemplate does not have a json cointbasetxn field. So I'm not
> sure how to create a coinbase transaction without that.
>
> I have a test template response data found here.
> https://raw.githubusercontent.com/mrosset/prospect/master/test-suite/data.j
>son and using a modified version of the merkle python reference script found
> on the wiki page. see
> https://github.com/mrosset/prospect/blob/master/scripts/merkle.py . I'm
> able to create a merkle root with the hash
> c5fff939f628a04428c080ed5bd7cd9bc0b4722b2522743049adb18213adf28a but
> that's minus the coinbase transaction.
>
> So far I'm able to replicate this hash using the test data in guile. But
> I'd like to sanitize this so that I'm using a coinbase transaction and
> making sure the python and guile merkle roots match.
>
> In short how do I get the coinbase transaction without the coinbasetxn
> field existing?
>
> Mike
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


From jlrubin at mit.edu  Thu Sep  9 19:26:37 2021
From: jlrubin at mit.edu (Jeremy)
Date: Thu, 9 Sep 2021 12:26:37 -0700
Subject: [bitcoin-dev] TAPLEAF_UPDATE_VERIFY covenant opcode
In-Reply-To: <20210909065330.GB22496@erisian.com.au>
References: <20210909064138.GA22496@erisian.com.au>
 <20210909065330.GB22496@erisian.com.au>
Message-ID: <CAD5xwhgFZPkQ2GbtphQ1DdzMDyeUEt0fKbA5g3jVoGRDVW4kKQ@mail.gmail.com>

I'm a bit skeptical of the safety of the control byte. Have you considered
the following issues?



> The low bit of C indicates the parity of X; if it's 0, X has even y,
> if it's 1, X has odd y.
>
> The next bit of C indicates whether the current script is dropped from
> the merkle path, if it's 0, the current script is kept, if it's 1 the
> current script is dropped.
>
> The remaining bits of C (ie C >> 2) are the number of steps in the merkle
> path that are dropped. (If C is negative, behaviour is to be determined
> -- either always fail, or always succeed and left for definition via
> future soft-fork)
>
> For example, suppose we have a taproot utxo that had 5 scripts
> (A,B,C,D,E), calculated as per the example in BIP 341 as:
>
>     AB = H_TapBranch(A, B)
>     CD = H_TapBranch(C, D)
>     CDE = H_TapBranch(CD, E)
>     ABCDE = H_TapBranch(AB, CDE)
>
> And we're spending using script E, in that case the control block includes
> the script E, and the merkle path to it, namely (AB, CD).
>
> So here's some examples of what you could do with TLUV to control how
> the spending scripts can change, between the input sPK and the output sPK.
>
> At it's simplest, if we used the script "0 0 0 TLUV", then that says we
> keep the current script, keep all steps in the merkle path, don't add
> any new ones, and don't change the internal public key -- that is that
> we want to resulting sPK to be exactly the same as the one we're spending.
>
> If we used the script "0 F 0 TLUV" (H=F, C=0) then we keep the current
> script, keep all the steps in the merkle path (AB and CD), and add
> a new step to the merkle path (F), giving us:
>
>     EF = H_TapBranch(E, F)
>     CDEF =H_TapBranch(CD, EF)
>     ABCDEF = H_TapBranch(AB, CDEF)
>
> If we used the script "0 F 2 TLUV" (H=F, C=2) then we drop the current
> script, but keep all the other steps, and add a new step (effectively
> replacing the current script with a new one):
>
>     CDF = H_TapBranch(CD, F)
>     ABCDF = H_TapBranch(AB, CDF)
>

If we recursively apply this rule, would it not be possible to repeatedly
apply it and end up burning out path E beyond the 128 Taproot depth limit?

Suppose we protect against this by checking that after adding F the depth
is not more than 128 for E.

The E path that adds F could also be burned for future use once the depth
is hit, and if adding F is necessary for correctness, then we're burned
anyways.

I don't see a way to protect against this generically.

Perhaps it's OK: E can always approve burning E?




>
> If we used the script "0 F 4 TLUV" (H=F, C=4) then we keep the current
> script, but drop the last step in the merkle path, and add a new step
> (effectively replacing the *sibling* of the current script):
>
>     EF = H_TapBranch(E, F)
>     ABEF = H_TapBranch(AB, EF)


> If we used the script "0 0 4 TLUV" (H=empty, C=4) then we keep the current
> script, drop the last step in the merkle path, and don't add anything new
> (effectively dropping the sibling), giving just:
>
>     ABE = H_TapBranch(AB, E)
>
>
>
Is C = 4 stable across all state transitions? I may be missing something,
but it seems that the location of C would not be stable across transitions.


E.g., What happens when, C and E are similar scripts and C adds some
clauses F1, F2, F3, then what does this sibling replacement do? Should a
sibling not be able to specify (e.g., by leaf version?) a NOREPLACE flag
that prevents siblings from modifying it?

What happens when E adds a bunch of F's F1 F2 F3, is C still in the same
position as when E was created?

Especially since nodes are lexicographically sorted, it seems hard to
create stable path descriptors even if you index from the root downwards.

Identifying nodes by Hash is also not acceptable because of hash cycles,
unless you want to restrict the tree structure accordingly (maybe OK
tradeoff?).
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210909/f906f558/attachment.html>

From lf-lists at mattcorallo.com  Fri Sep 10 00:50:08 2021
From: lf-lists at mattcorallo.com (Matt Corallo)
Date: Thu, 9 Sep 2021 17:50:08 -0700
Subject: [bitcoin-dev] Reorgs on SigNet - Looking for feedback on
 approach and parameters
In-Reply-To: <83272afb-ed87-15b6-e02c-16bb1102beb4@gmail.com>
References: <83272afb-ed87-15b6-e02c-16bb1102beb4@gmail.com>
Message-ID: <e11d718f-2bb7-335a-80dc-7d44244a0e98@mattcorallo.com>


On 9/7/21 09:07, 0xB10C via bitcoin-dev wrote:
> Hello,
> 
> tl;dr: We want to make reorgs on SigNet a reality and are looking for
> feedback on approach and parameters.

Awesome!

> AJ proposed to allow SigNet users to opt-out of reorgs in case they
> explicitly want to remain unaffected. This can be done by setting a
> to-be-reorged version bit flag on the blocks that won't end up in the
> most work chain. Node operators could choose not to accept to-be-reorged
> SigNet blocks with this flag set via a configuration argument.

Why bother with a version bit? This seems substantially more complicated than the original proposal 
that surfaced many times before signet launched to just have a different reorg signing key. Thus, 
users who wish to follow reorgs can use a 1-of-2 (or higher multisig) and users who wish to not 
follow reorgs would use a 1-of-1 (or higher multisig), simply marking the reorg blocks as invalid 
without touching any header bits that non-full clients will ever see.

> The reorg-interval X very much depends on the user's needs. One could
> argue that there should be, for example, three reorgs per day, each 48
> blocks apart. Such a short reorg interval allows developers in all time
> zones to be awake during one or two reorgs per day. Developers don't
> need to wait for, for example, a week until they can test their reorgs
> next. However, too frequent reorgs could hinder other SigNet users.

I see zero reason whatsoever to not simply reorg ~every block, or as often as is practical. If users 
opt in to wanting to test with reorgs, they should be able to test with reorgs, not wait a day to 
test with reorgs.

> We propose that the reorg depth D is deterministically random between a
> minimum and a maximum based on, e.g., the block hash or the nonce of the
> last block before the reorg. Compared to a local randint() based
> implementation, this allows reorg-handling tests and external tools to
> calculate the expected reorg depth.
> 
> # Scenario 1: Race between two chains
> 
> For this scenario, at least two nodes and miner scripts need to be
> running. An always-miner A continuously produces blocks and rejects
> blocks with the to-be-reorged version bit flag set. And a race-miner R
> that only mines D blocks at the start of each interval and then waits X
> blocks. A and R both have the same hash rate. Assuming both are well
> connected to the network, it's random which miner will first mine and
> propagate a block. In the end, the A miner chain will always win the race.
> 
> # Scenario 2: Chain rollback
> 
> This scenario only requires one miner and Bitcoin Core node but also
> works in a multiminer setup. The miners mine D blocks with the
> to-be-reorged version bit flag set at the start of the interval. After
> allowing the block at height X+D to propagate, they invalidate the block
> at height X+1 and start mining on block X again. This time without
> setting the to-be-reorged version bit flag. Non-miner nodes will reorg
> to the new tip at height X+D+1, and the first-seen branch stalls.

Both seem reasonable. I'm honestly not sure what software cases would be hit differently between one 
or the other, as long as reorgs happen regularly and at random depth. Nodes should presumably only 
ever be following one chain.

> # Questions
> 
>  ??? 1. How do you currently test your applications reorg handling? Do
>         the two discussed scenarios (race and chain rollback) cover your
>         needs? Are we missing something you'd find helpful?
> 
>  ??? 2. How often should reorgs happen on the default SigNet? Should
>         there be multiple reorgs a day (e.g., every 48 or 72 blocks
>         assuming 144 blocks per day) as your engineers need to be awake?
>         Do you favor less frequent reorgs (once per week or month)? Why?
> 
>      3. How deep should the reorgs be on average? Do you want to test
>         deeper reorgs (10+ blocks) too?

6 is the "standard" confirmation window for mainnet. Its arguably much too low, but for testing 
purposes we've gotta pick something, so that seems reasonable?

Matt

From aj at erisian.com.au  Fri Sep 10 07:42:19 2021
From: aj at erisian.com.au (Anthony Towns)
Date: Fri, 10 Sep 2021 17:42:19 +1000
Subject: [bitcoin-dev] TAPLEAF_UPDATE_VERIFY covenant opcode
In-Reply-To: <CAD5xwhgFZPkQ2GbtphQ1DdzMDyeUEt0fKbA5g3jVoGRDVW4kKQ@mail.gmail.com>
References: <20210909064138.GA22496@erisian.com.au>
 <20210909065330.GB22496@erisian.com.au>
 <CAD5xwhgFZPkQ2GbtphQ1DdzMDyeUEt0fKbA5g3jVoGRDVW4kKQ@mail.gmail.com>
Message-ID: <20210910074219.GA23578@erisian.com.au>

On Thu, Sep 09, 2021 at 12:26:37PM -0700, Jeremy wrote:
> I'm a bit skeptical of the safety of the control byte. Have you considered the
> following issues?

>     If we used the script "0 F 0 TLUV" (H=F, C=0) then we keep the current
>     script, keep all the steps in the merkle path (AB and CD), and add
>     a new step to the merkle path (F), giving us:
>     ? ? EF = H_TapBranch(E, F)
>     ? ? CDEF =H_TapBranch(CD, EF)
>     ? ? ABCDEF = H_TapBranch(AB, CDEF)
> 
> If we recursively apply this rule, would it not be possible to repeatedly apply
> it and end up burning out path E beyond the 128 Taproot depth limit?

Sure. Suppose you had a script X which allows adding a new script A[0..n]
as its sibling. You'd start with X and then go to (A0, X), then (A0,
(A1, X)), then (A0, (A1, (A2, X))) and by the time you added A127 TLUV
would fail because it'd be trying to add a path longer than 128 elements.

But this would be bad anyway -- you'd already have a maximally unbalanced
tree. So the fix for both these things would be to do a key path spend
and rebalance the tree. With taproot, you always want to do key path
spends if possible.

Another approach would be to have X replace itself not with (X, A) but
with (X, (X, A)) -- that way you go from:

   /\
  A  X

to 
     /\
    A /\
     X /\
      B  X
  
to 
      /\
     /  \
    A   /\
       /  \
      /    \
     /\    /\
    C  X  B  X

and can keep the tree height at O(log(n)) of the number of members.

This means the script X would need a way to reference its own hash, but
you could do that by invoking TLUV twice, once to check that your new
sPK is adding a sibling (X', B) to the current script X, and a second
time to check that you're replacing the current script with (X', (X',
B)). Executing it twice ensures that you've verified X' = X, so you can
provide X' on the stack, rather than trying to include the script's on
hash in itself.

> Perhaps it's OK: E can always approve burning E?

As long as you've got the key path, then I think that's the thing to do.

>     If we used the script "0 F 4 TLUV" (H=F, C=4) then we keep the current
>     script, but drop the last step in the merkle path, and add a new step
>     (effectively replacing the *sibling* of the current script):
>     ? ? EF = H_TapBranch(E, F)
>     ? ? ABEF = H_TapBranch(AB, EF)?
>     If we used the script "0 0 4 TLUV" (H=empty, C=4) then we keep the current
>     script, drop the last step in the merkle path, and don't add anything new
>     (effectively dropping the sibling), giving just:
>     ? ? ABE = H_TapBranch(AB, E)
> 
> Is C = 4 stable across all state transitions? I may be missing something, but
> it seems that the location of C would not be stable across transitions.

Dropping a sibling without replacing it or dropping the current script
would mean you could re-execute the same script on the new utxo, and
repeat that enough times and the only remaining ways of spending would
be that script and the key path.

> E.g., What happens when, C and E are similar scripts and C adds some clauses
> F1, F2, F3, then what does this sibling replacement do? Should a sibling not be
> able to specify (e.g., by leaf version?) a NOREPLACE flag that prevents
> siblings from modifying it?

If you want a utxo where some script paths are constant, don't construct
the utxo with script paths that can modify them.

> What happens when E adds a bunch of F's F1 F2 F3, is C still in the same
> position as when E was created?

That depends how you define "position". If you have:

    
   /\
  R  S

and

   /\
  R /\
   S  T

then I'd say that "R" has stayed in the same position, while "S" has
been lowered to allow for a new sibling "T". But the merkle path to
R will have changed (from "H(S)" to "H(H(S),H(T))"). 

> Especially since nodes are lexicographically sorted, it seems hard to create
> stable path descriptors even if you index from the root downwards.

The merkle path will always change unless you have the exact same set
of scripts, so that doesn't seem like a very interesting way to define
"position" when you're adding/removing/replacing scripts.

The "lexical ordering" is just a modification to how the hash is
calculated that makes it commutative, so that H(A,B) = H(B,A), with
the result being that the merkle path for any script in the the R,(S,T)
tree above is the same for the corresponding script in the tree:

   /\
  /\ R
 T  S

Cheers,
aj


From antoine.riard at gmail.com  Fri Sep 10 04:12:24 2021
From: antoine.riard at gmail.com (Antoine Riard)
Date: Fri, 10 Sep 2021 00:12:24 -0400
Subject: [bitcoin-dev] TAPLEAF_UPDATE_VERIFY covenant opcode
In-Reply-To: <20210909064138.GA22496@erisian.com.au>
References: <20210909064138.GA22496@erisian.com.au>
Message-ID: <CALZpt+FnnbGJC4=KO_OPiKxt0Ey9Bzh1gxP1dQSDz2aBi9WyOA@mail.gmail.com>

Hi AJ,

Thanks for finally putting the pieces together! [0]

We've been hacking with Gleb on a paper for the CoinPool protocol [1]
during the last weeks and it should be public soon, hopefully highlighting
what kind of scheme, TAPLEAF_UPDATE_VERIFY-style of covenant enable :)

Here few early feedbacks on this specific proposal,

> So that makes it relatively easy to imagine creating a new taproot address
> based on the input you're spending by doing some or all of the following:
>
>  * Updating the internal public key (ie from P to P' = P + X)
>  * Trimming the merkle path (eg, removing CD)
>  * Removing the script you're currently executing (ie E)
>  * Adding a new step to the end of the merkle path (eg F)

"Talk is cheap. Show me the code" :p

    case OP_MERKLESUB:
    {
        if (!(flags & SCRIPT_VERIFY_MERKLESUB)) {
            break;
        }

        if (stack.size() < 2) {
            return set_error(serror, SCRIPT_ERR_INVALID_STACK_OPERATION);
        }

        valtype& vchPubKey = stacktop(-1);

        if (vchPubKey.size() != 32) {
            break;
        }

        const std::vector<unsigned char>& vch = stacktop(-2);
        int nOutputPos = CScriptNum(stacktop(-2), fRequireMinimal).getint();

        if (nOutputPos < 0) {
            return set_error(serror, SCRIPT_ERR_NEGATIVE_MERKLEVOUT);
        }

        if (!checker.CheckMerkleUpdate(*execdata.m_control, nOutputPos,
vchPubKey)) {
            return set_error(serror, SCRIPT_ERR_UNSATISFIED_MERKLESUB);
        }
        break;
    }

    case OP_NOP1: case OP_NOP5:



    template <class T>
    bool GenericTransactionSignatureChecker<T>::CheckMerkleUpdate(const
std::vector<unsigned char>& control, unsigned int out_pos, const
std::vector<unsigned char>& point) const
    {
        //! The internal pubkey (x-only, so no Y coordinate parity).
        XOnlyPubKey p{uint256(std::vector<unsigned char>(control.begin() +
1, control.begin() + TAPROOT_CONTROL_BASE_SIZE))};
        //! Update the internal key by subtracting the point.
        XOnlyPubKey s{uint256(point)};
        XOnlyPubKey u;
        try {
            u = p.UpdateInternalKey(s).value();
        } catch (const std::bad_optional_access& e) {
            return false;
        }

        //! The first control node is made the new tapleaf hash.
        //! TODO: what if there is no control node ?
        uint256 updated_tapleaf_hash;
        updated_tapleaf_hash = uint256(std::vector<unsigned
char>(control.data() + TAPROOT_CONTROL_BASE_SIZE, control.data() +
TAPROOT_CONTROL_BASE_SIZE + TAPROOT_CONTROL_NODE_SIZE));

        //! The committed-to output must be in the spent transaction vout
range.
        if (out_pos >= txTo->vout.size()) return false;
        int witnessversion;
        std::vector<unsigned char> witnessprogram;
        txTo->vout[out_pos].scriptPubKey.IsWitnessProgram(witnessversion,
witnessprogram);
        //! The committed to output must be a witness v1 program at least
        if (witnessversion == 0) {
            return false;
        } else if (witnessversion == 1) {
            //! The committed-to output.
            const XOnlyPubKey q{uint256(witnessprogram)};
            //! Compute the Merkle root from the leaf and the incremented
by one path.
            const uint256 merkle_root = ComputeTaprootMerkleRoot(control,
updated_tapleaf_hash, 1);
            //! TODO modify MERKLESUB design
            bool parity_ret = q.CheckTapTweak(u, merkle_root, true);
            bool no_parity_ret = q.CheckTapTweak(u, merkle_root, false);
            if (!parity_ret && !no_parity_ret) {
                return false;
            }
        }
        return true;
    }


Here the main chunks for an "<n> <point> OP_MERKLESUB" opcode, with `n` the
output position which is checked for update and `point` the x-only pubkey
which must be subtracted from the internal key.

I think one design advantage of explicitly passing the output position as a
stack element is giving more flexibility to your contract dev. The first
output could be SIGHASH_ALL locked-down. e.g "you have to pay Alice on
output 1 while pursuing the contract semantic on output 2".

One could also imagine a list of output positions to force the taproot
update on multiple outputs ("OP_MULTIMERKLESUB"). Taking back your citadel
joint venture example, partners could decide to split the funds in 3
equivalent amounts *while* conserving the pre-negotiated script policies [2]

For the merkle branches extension, I was thinking of introducing a separate
OP_MERKLEADD, maybe to *add* a point to the internal pubkey group signer.
If you're only interested in leaf pruning, using OP_MERKLESUB only should
save you one byte of empty vector ?

We can also explore more fancy opcodes where the updated merkle branch is
pushed on the stack for deep manipulations. Or even n-dimensions
inspections if combined with your G'root [3] ?

Note, this current OP_MERKLESUB proposal doesn't deal with committing the
parity of the internal pubkey as part of the spent utxo. As you highlighted
well in your other mail, if we want to conserve the updated key-path across
a sequence of TLUV-covenanted transactions, we need either
a) to select a set of initial points, where whatever combination of
add/sub, it yields an even-y point. Or b) have the even/odd bit
re-committed at each update. Otherwise, we're not guaranteed to cancel the
point from the aggregated key.

This property is important for CoinPool. Let's say you have A+B+C+D, after
the D withdraw transaction has been confirmed on-chain, you want A+B+C to
retain the ability to use the key-path and update the off-chain state,
without forcing a script path spend to a new setup.

If we put the updated internal key parity bit in the first control byte, we
need to have a  redundant commitment somewhere else as we can't trust the
spender to not be willingly to break the key-path spend of the remaining
group of signers.

One solution I was thinking about was introducing a new tapscript version
(`TAPROOT_INTERNAL_TAPSCRIPT`) signaling that VerifyTaprootCommitment must
compute the TapTweak with a new TapTweak=(internal_pubkey || merkle_root ||
parity_bit). A malicious participant wouldn't be able to interfere with the
updated internal key as it would break its own spending taproot commitment
verification ?

> That's useless without some way of verifying that the new utxo retains
> the bitcoin that was in the old utxo, so also include a new opcode
> IN_OUT_AMOUNT that pushes two items onto the stack: the amount from this
> input's utxo, and the amount in the corresponding output, and then expect
> anyone using TLUV to use maths operators to verify that funds are being
> appropriately retained in the updated scriptPubKey.

Credit to you for the SIGHASH_GROUP design, here the code, with
SIGHASH_ANYPUBKEY/ANYAMOUNT extensions.

    if ((output_type & SIGHASH_GROUP) == SIGHASH_GROUP) {
        // Verify the output group bounds
        if (execdata.m_bundle->first == execdata.m_bundle->second ||
execdata.m_bundle->second >= tx_to.vout.size()) return false;

        // Verify the value commitment
        if (VerifyOutputsGroup(tx_to, cache.m_spent_outputs[in_pos].nValue,
execdata.m_bundle->first, execdata.m_bundle->second)) return false;



        for (unsigned int out_pos = execdata.m_bundle->first; out_pos <
execdata.m_bundle->second + 1; out_pos++) {
            bool anypubkey_flag = false;
            bool anyamount_flag = false;
            std::map<unsigned int, char>::const_iterator it;

            if ((output_type & SIGHASH_GROUP_ANYPUBKEY) ==
SIGHASH_GROUP_ANYPUBKEY) {
                it = execdata.m_anypubkeys.find(out_pos);
                if (it != execdata.m_anypubkeys.end() && it->second == 1) {
                    anypubkey_flag = true;
                }
            }

            if ((output_type & SIGHASH_GROUP_ANYAMOUNT) ==
SIGHASH_GROUP_ANYAMOUNT) {
                it = execdata.m_anyamounts.find(out_pos);
                if (it != execdata.m_anyamounts.end() && it->second == 1) {
                    anyamount_flag = true;
                }
            }

            if (!anypubkey_flag) {
                ss << tx_to.vout[out_pos].scriptPubKey;
            }
            if (!anyamount_flag) {
                ss << tx_to.vout[out_pos].nValue;
            }

        }
    }

I think it's achieving the same effect as IN_OUT_AMOUNT, at least for
CoinPool use-case. A MuSig  `contract_pubkey` can commit to the
`to_withdraw` output while allowing a wildcard for the `to_pool` output
nValue/scriptPubKey. The nValue correctness will be ensured by the
group-value-lock validation rule (`VerifyOutputsGroup`) and scriptPubkey by
OP_MERKLESUB commitment.

I think witness data size it's roughly equivalent as the annex fields must
be occupied by the output group commitment. SIGHASH_GROUP might be more
flexible than IN_OUT_AMOUNT for a range of use-cases, see my point on AMM.

> The second scheme is allowing for a utxo to represent a group's pooled
> funds. The idea being that as long as everyone's around you can use
> the taproot key path to efficiently move money around within the pool,
> or use a single transaction and signature for many people in the pool
> to make payments. But key path spends only work if everyone's available
> to sign -- what happens if someone disappears, or loses access to their
> keys, or similar? For that, we want to have script paths to allow other
> people to reclaim their funds even if everyone else disappears. So we
> setup scripts for each participant, eg for Alice:
>
>  * The tx is signed by Alice
>  * The output value must be at least the input value minus Alice's balance
>  * Must pass TLUV such that:
>    + the internal public key is the old internal pubkey minus Alice's key
>    + the currently executing script is dropped from the merkle path
>    + no steps are otherwise removed or added

Yes the security model is roughly similar to the LN one. Instead of a
counter-signed commitment transaction which can be broadcast at any point
during channel lifetime, you have a pre-signed withdraw transaction sending
to {`to_withdraw`,`to_pool`} outputs. Former is your off-chain balance, the
latter one is the pool balance, and one grieved with the updated Taproot
output. The withdraw tapscript force the point subtraction with the
following format (`<n> <withdraw_point> <OP_MERKLESUB> <33-byte
contract_pubkey> OP_CHECKSIG)

> A simpler case for something like this might be for funding a joint
> venture -- suppose you're joining with some other early bitcoiners to
> buy land to build a citadel, so you each put 20 BTC into a pooled utxo,
> ready to finalise the land purchase in a few months, but you also want
> to make sure you can reclaim the funds if the deal falls through. So
> you might include scripts like the above that allow you to reclaim your
> balance, but add a CLTV condition preventing anyone from doing that until
> the deal's deadline has passed. If the deal goes ahead, you all transfer
> the funds to the vendor via the keypath; if it doesn't work out, you
> hopefully return your funds via the keypath, but if things turn really
> sour, you can still just directly reclaim your 20 BTC yourself via the
> script path.

Yes, that kind of blockchain validation semantic extension is vaudoo-magic
if we want to enable smart corporation/scalable multi-event contracts. I
gave a presentation on advanced bitcoin contracts two years ago, mentioning
we would need covenants to solve the factorial complexity on edge-case [4]

Bitcoin ledger would fit perfectly well to host international commerce law
style of contracts, where you have a lot of usual fancy provisions (e.g
hardship, delay penalty, ...) :)

> First it can't tweak scripts in areas of the merkle tree that it can't
> see -- I don't see a way of doing that particularly efficiently, so maybe
> it's best just to leave that as something for the people responsible for
> the funds to negotiate via the keypath, in which case it's automatically
> both private and efficient since all the details stay off-chain, anyway

Yeah, in that kind of case, we might want to push the merkle root as a
stack element but still update the internal pubkey from the spent utxo ?
This new merkle_root would be the tree of tweaked scripts as you expect
them if you execute *this* tapscript. And you can still this new tree with
a tapbranch inherited from the taproot output.

(I think I could come with some use-case from lex mercatoria where if you
play out a hardship provision you want to tweak all the other provisions by
a CSV delay while conserving the rest of their policy)

> And second, it doesn't provide a way for utxos to "interact", which is
> something that is interesting for automated market makers [5], but perhaps
> only interesting for chains aiming to support multiple asset types,
> and not bitcoin directly. On the other hand, perhaps combining it with
> CTV might be enough to solve that, particularly if the hash passed to
> CTV is constructed via script/CAT/etc.

That's where SIGHASH_GROUP might be more interesting as you could generate
transaction "puzzles".

IIUC, the problem is how to have a set of ratios between x/f(x). I think it
can be simplified to just generate pairs of input btc-amount/output
usdt-amount for the whole range of strike price you want to cover.

Each transaction puzzle has 1-input/2-outputs. The first output is signed
with SIGHASH_ANYPUBKEY but committed to a USDT amount. The second output is
signed with SIGHASH_ANYAMOUNT but committed to the maker pubkey. The input
commits to the spent BTC amount but not the spent txid/scriptPubKey.
The maker generates a Taproot tree where each leaf is committing to a
different "strike price".

A taker is finalizing the puzzle by inserting its withdraw scriptPubKey for
the first output and the maker amount for the second output. The
transitivity value output group rule guarantees that a malicious taker
can't siphon the fund.

> (I think everything described here could be simulated with CAT and
> CHECKSIGFROMSTACK (and 64bit maths operators and some way to access
> the internal public key), the point of introducing dedicated opcodes
> for this functionality rather than (just) having more generic opcodes
> would be to make the feature easy to use correctly, and, presuming it
> actually has a wide set of use cases, to make it cheap and efficient
> both to use in wallets, and for nodes to validate)

Yeah, I think CHECKSIGFROMSTACK is a no-go if we want to emulate
TAPLEAF_UPDATE_VERIFY functionality. If you want to update the 100th
tapscript, I believe we'll have to throw on the stack the corresponding
merkle branch and it sounds inefficient in terms of witness space ? Though
ofc, in both cases we bear the tree traversal computational cost ?

Really really excited to see progress on more powerful covenants for
Bitcoin :)

Cheers,
Antoine

[0] For the ideas genealogy, I think Greg's OP_MERKLE_UPDATE has been
circulating for a while and we chatted with Jeremy last year about the
current limitation of the script interpreter w.r.t expressing the factorial
complexity of advanced off-chain systems. I also remember Matt's artistic
drawing of a TAPLEAF_UPDATE_VERIFY ancestor on a Chaincode whiteboard :)

[1]
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-June/017964.html

[2]
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-July/016249.html

[3] A legal construction well-spread in the real-world. Known as
"indivision" in civil law".

[4] https://github.com/ariard/talk-slides/blob/master/advanced-contracts.pdf

Le jeu. 9 sept. 2021 ? 02:42, Anthony Towns via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> a ?crit :

> Hello world,
>
> A couple of years ago I had a flight of fancy [0] imagining how it
> might be possible for everyone on the planet to use bitcoin in a
> mostly decentralised/untrusted way, without requiring a block size
> increase. It was a bit ridiculous and probably doesn't quite hold up,
> and beyond needing all the existing proposals to be implemented (taproot,
> ANYPREVOUT, CTV, eltoo, channel factories), it also needed a covenant
> opcode [1]. I came up with something that I thought fit well with taproot,
> but couldn't quite figure out how to use it for anything other than my
> ridiculous scheme, so left it at that.
>
> But recently [2] Greg Maxwell emailed me about his own cool idea for a
> covenant opcode, which turned out to basically be a reinvention of the
> same idea but with more functionality, a better name and a less fanciful
> use case; and with that inspiration, I think I've also now figured out
> how to use it for a basic vault, so it seems worth making the idea a
> bit more public.
>
> I'll split this into two emails, this one's the handwavy overview,
> the followup will go into some of the implementation complexities.
>
>
>
> The basic idea is to think about "updating" a utxo by changing the
> taproot tree.
>
> As you might recall, a taproot address is made up from an internal public
> key (P) and a merkle tree of scripts (S) combined via the formula Q=P+H(P,
> S)*G to calculate the scriptPubKey (Q). When spending using a script,
> you provide the path to the merkle leaf that has the script you want
> to use in the control block. The BIP has an example [3] with 5 scripts
> arranged as ((A,B), ((C,D), E)), so if you were spending with E, you'd
> reveal a path of two hashes, one for (AB), then one for (CD), then you'd
> reveal your script E and satisfy it.
>
> So that makes it relatively easy to imagine creating a new taproot address
> based on the input you're spending by doing some or all of the following:
>
>  * Updating the internal public key (ie from P to P' = P + X)
>  * Trimming the merkle path (eg, removing CD)
>  * Removing the script you're currently executing (ie E)
>  * Adding a new step to the end of the merkle path (eg F)
>
> Once you've done those things, you can then calculate the new merkle
> root by resolving the updated merkle path (eg, S' = MerkleRootFor(AB,
> F, H_TapLeaf(E))), and then calculate a new scriptPubKey based on that
> and the updated internal public key (Q' = P' + H(P', S')).
>
> So the idea is to do just that via a new opcode "TAPLEAF_UPDATE_VERIFY"
> (TLUV) that takes three inputs: one that specifies how to update the
> internal public key (X), one that specifies a new step for the merkle path
> (F), and one that specifies whether to remove the current script and/or
> how many merkle path steps to remove. The opcode then calculates the
> scriptPubKey that matches that, and verifies that the output corresponding
> to the current input spends to that scriptPubKey.
>
> That's useless without some way of verifying that the new utxo retains
> the bitcoin that was in the old utxo, so also include a new opcode
> IN_OUT_AMOUNT that pushes two items onto the stack: the amount from this
> input's utxo, and the amount in the corresponding output, and then expect
> anyone using TLUV to use maths operators to verify that funds are being
> appropriately retained in the updated scriptPubKey.
>
>
>
> Here's two examples of how you might use this functionality.
>
> First, a basic vault. The idea is that funds are ultimately protected
> by a cold wallet key (COLD) that's inconvenient to access but is as
> safe from theft as possible. In order to make day to day transactions
> more convenient, a hot wallet key (HOT) is also available, which is
> more vulnerable to theft. The vault design thus limits the hot wallet
> to withdrawing at most L satoshis every D blocks, so that if funds are
> stolen, you lose at most L, and have D blocks to use your cold wallet
> key to re-secure the funds and prevent further losses.
>
> To set this up with TLUV, you construct a taproot output with COLD as
> the internal public key, and a script that specifies:
>
>  * The tx is signed via HOT
>  * <D> CSV -- there's a relative time lock since the last spend
>  * If the input amount is less than L + dust threshold, fine, all done,
>    the vault can be emptied.
>  * Otherwise, the output amount must be at least (the input amount -
>    L), and do a TLUV check that the resulting sPK is unchanged
>
> So you can spend up to "L" satoshis via the hot wallet as long as you
> wait D blocks since the last spend, and can do whatever you want via a
> key path spend with the cold wallet.
>
> You could extend this to have a two phase protocol for spending, where
> first you use the hot wallet to say "in D blocks, allow spending up to
> L satoshis", and only after that can you use the hot wallet to actually
> spend funds. In that case supply a taproot sPK with COLD as the internal
> public key and two scripts, the "release" script, which specifies:
>
>  * The tx is signed via HOT
>  * Output amount is greater or equal to the input amount.
>  * Use TLUV to check:
>    + the output sPK has the same internal public key (ie COLD)
>    + the merkle path has one element trimmed
>    + the current script is included
>    + a new step is added that matches either H_LOCKED or H_AVAILABLE as
>      described below (depending on whether 0 or 1 was provided as
>      witness info)
>
> The other script is either "locked" (which is just "OP_RETURN") or
> "available" which specifies:
>
>  * The tx is signed via HOT
>  * <D> CSV -- there's a relative time lock since the last spend (ie,
>    when the "release" script above was used)
>  * If the input amount is less than L, fine, all done, the vault can
>    be emptied
>  * Otherwise, the output amount must be at least (the input amount minus
>    L), and via TLUV, check the resulting sPK keeps the internal pubkey
>    unchanged, keeps the merkle path, drops the current script, and adds
>    H_LOCKED as the new step.
>
> H_LOCKED and H_AVAILABLE are just the TapLeaf hash corresponding to the
> "locked" and "available" scripts.
>
> I believe this latter setup matches the design Bryan Bishop talked about
> a couple of years ago [4], with the benefit that it's fully recursive,
> allows withdrawals to vary rather than be the fixed amount L (due to not
> relying on pre-signed transactions), and generally seems a bit simpler
> to work with.
>
>
>
> The second scheme is allowing for a utxo to represent a group's pooled
> funds. The idea being that as long as everyone's around you can use
> the taproot key path to efficiently move money around within the pool,
> or use a single transaction and signature for many people in the pool
> to make payments. But key path spends only work if everyone's available
> to sign -- what happens if someone disappears, or loses access to their
> keys, or similar? For that, we want to have script paths to allow other
> people to reclaim their funds even if everyone else disappears. So we
> setup scripts for each participant, eg for Alice:
>
>  * The tx is signed by Alice
>  * The output value must be at least the input value minus Alice's balance
>  * Must pass TLUV such that:
>    + the internal public key is the old internal pubkey minus Alice's key
>    + the currently executing script is dropped from the merkle path
>    + no steps are otherwise removed or added
>
> The neat part here is that if you have many participants in the pool,
> the pool continues to operate normally even if someone makes use of the
> escape hatch -- the remaining participants can still use the key path to
> spend efficiently, and they can each unilaterally withdraw their balance
> via their own script path. If everyone decides to exit, whoever is last
> can spend the remaining balance directly via the key path.
>
> Compared to having on-chain transactions using non-pooled funds, this
> is more efficient and private: a single one-in, one-out transaction
> suffices for any number of transfers within the pool, and there's no
> on-chain information about who was sending/receiving the transfers, or
> how large the transfers were; and for transfers out of the pool, there's
> no on-chain indication which member of the pool is sending the funds,
> and multiple members of the pool can send funds to multiple destinations
> with only a single signature. The major constraint is that you need
> everyone in the pool to be online in order to sign via the key path,
> which provides a practical limit to how many people can reasonably be
> included in a pool before there's a breakdown.
>
> Compared to lightning (eg eltoo channel factories with multiple
> participants), the drawback is that no transfer is final without an
> updated state being committed on chain, however there are also benefits
> including that if one member of the pool unilaterally exits, that
> doesn't reveal the state of anyone remaining in the pool (eg an eltoo
> factory would likely reveal the balances of everyone else's channels at
> that point).
>
> A simpler case for something like this might be for funding a joint
> venture -- suppose you're joining with some other early bitcoiners to
> buy land to build a citadel, so you each put 20 BTC into a pooled utxo,
> ready to finalise the land purchase in a few months, but you also want
> to make sure you can reclaim the funds if the deal falls through. So
> you might include scripts like the above that allow you to reclaim your
> balance, but add a CLTV condition preventing anyone from doing that until
> the deal's deadline has passed. If the deal goes ahead, you all transfer
> the funds to the vendor via the keypath; if it doesn't work out, you
> hopefully return your funds via the keypath, but if things turn really
> sour, you can still just directly reclaim your 20 BTC yourself via the
> script path.
>
>
>
> I think a nice thing about this particular approach to recursive covenants
> at a conceptual level is that it automatically leaves the key path as an
> escape mechanism -- rather than having to build a base case manually,
> and have the risk that it might not work because of some bug, locking
> your funds into the covenant permanently; the escape path is free, easy,
> and also the optimal way of spending things when everything is working
> right. (Of course, you could set the internal public key to a NUMS point
> and shoot yourself in the foot that way anyway)
>
>
>
> I think there's two limitations of this method that are worth pointing out.
>
> First it can't tweak scripts in areas of the merkle tree that it can't
> see -- I don't see a way of doing that particularly efficiently, so maybe
> it's best just to leave that as something for the people responsible for
> the funds to negotiate via the keypath, in which case it's automatically
> both private and efficient since all the details stay off-chain, anyway
>
> And second, it doesn't provide a way for utxos to "interact", which is
> something that is interesting for automated market makers [5], but perhaps
> only interesting for chains aiming to support multiple asset types,
> and not bitcoin directly. On the other hand, perhaps combining it with
> CTV might be enough to solve that, particularly if the hash passed to
> CTV is constructed via script/CAT/etc.
>
>
>
> (I think everything described here could be simulated with CAT and
> CHECKSIGFROMSTACK (and 64bit maths operators and some way to access
> the internal public key), the point of introducing dedicated opcodes
> for this functionality rather than (just) having more generic opcodes
> would be to make the feature easy to use correctly, and, presuming it
> actually has a wide set of use cases, to make it cheap and efficient
> both to use in wallets, and for nodes to validate)
>
> Cheers,
> aj
>
> [0] https://gist.github.com/ajtowns/dc9a59cf0a200bd1f9e6fb569f76f7a0
>
> [1] Roughly, the idea was that if you have ~9 billion people using
>     bitcoin, but can only have ~1000 transactions per block, then you
>     need have each utxo represent a significant number of people. That
>     means that you need a way of allowing the utxo's to be efficiently
>     spent, but need to introduce some level of trust since expecting
>     many people to constantly be online seems unreliable, but to remain
>     mostly decentralised/untrusted, you want to have some way of limiting
>     how much trust you're introducing, and that's where covenants come in.
>
> [2] Recently in covid-adjusted terms, or on the bitcoin consensus
>     change scale anyway...
>     https://mobile.twitter.com/ajtowns/status/1385091604357124100
>
> [3]
> https://github.com/bitcoin/bips/blob/master/bip-0341.mediawiki#Constructing_and_spending_Taproot_outputs
>
> [4]
> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-August/017231.html
>
> [5] The idea behind an automated market maker being that you setup a
>     script that says "you can withdraw x BTC if you deposit f(x) units of
>     USDT, or you can withdraw g(x) units of USDT if you deposit x units
>     of BTC", with f(x)/x giving the buy price, and f(x)>g(x) meaning
>     you make a profit. Being able to specify a covenant that links the
>     change in value to the BTC utxo (+/-x) and the change in value to
>     the USDT utxo (+f(x) or -g(x)) is what you'd need to support this
>     sort of use case, but TLUV doesn't provide a way to do that linkage.
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210910/0a78717c/attachment-0001.html>

From fmerli1 at gmail.com  Fri Sep 10 09:30:31 2021
From: fmerli1 at gmail.com (Filippo Merli)
Date: Fri, 10 Sep 2021 11:30:31 +0200
Subject: [bitcoin-dev] Braidpool: Proposal for a decentralised mining
	pool
In-Reply-To: <edbbb44e247d4e639659e1b9b989dd84-kohli@ctemplar.com>
References: <MiuahdA--3-2@tutanota.de>
 <ceFmn7ZHyPHN70rDuE66lnPEwjgjQ7LtZLwyFgIVUpPvPDvSZSsLHUf_yiBvXTpjdEju4UxAOnDgilZaQAMvQzYcUbOkZsYvOIpuBG7japo=@protonmail.com>
 <edbbb44e247d4e639659e1b9b989dd84-kohli@ctemplar.com>
Message-ID: <CAO1K=nnGXasdu_M4NgCkcCFMB16sW5r-Xd462d6jfR9mBBCgSA@mail.gmail.com>

Hi!

>From the proposal it is not clear why a miner must reference other miners'
shares in his shares.
What I mean is that there is a huge incentive for a rogue miner to not
reference any share from
other miner so he won't share the reward with anyone, but it will be paid
for the share that he
create because good miners will reference his shares.
The pool will probably become unprofitable for good miners.

Another thing that I do not understand is how to resolve conflicts. For
example, using figure 1 at
page 1, a node could be receive this 2 valid states:

1. L -> a1 -> a2 -> a3 -> R
2. L -> a1* -> a2* -> R

To resolve the above fork the only two method that comes to my mind are:

1. use the one that has more work
2. use the longest one

Btw both methods present an issue IMHO.

If the longest chain is used:
When a block (L) is find, a miner (a) could easily create a lot of share
with low difficulty
(L -> a1* -> a2* -> ... -> an*), then start to mine shares with his real
hashrate (L -> a1 -> a2)
and publish them so they get referenced. If someone else finds a block he
gets the reward cause he
has been referenced. If he finds the block he just attaches the funded
block to the longest chain
(that reference no one) and publishes it without sharing the reward
(L -> a1* -> a2* -> ... -> an* -> R).

If is used the one with more work:
A miner that has published the shares (L -> a1 -> a2 -> a3) when find a
block R that alone has more
work than a1 + a2 + a3 it just publish (L -> R) and he do not share the
reward with anyone.

On Wed, Sep 8, 2021 at 1:15 PM pool2win via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> > A thing I just realized about Braidpool is that the payout server is
> still a single central point-of-failure.
>
> > However, this probably complicates the design too much, and it may be
> more beneficial to get *something* working now.
>
> You have hit the nail on the head here and Chris Belcher's original
> proposal for using payment channels does provide a construction for
> multiple hubs [1]. In the Braidpool proposal however, the focus is on a
> single hub to describe the plan for an MVP.
>
> Decentralising hubs is the end goal here, and either Belcher's multiple
> hubs construction or a leadership election based construction along the
> lines you propose might be a good way forward. Belcher's idea has the added
> advantage that the required liquidity at each hub is reduced as more hubs
> join, with the cost that in case of a hubs defecting, it takes longer for
> miners to do cascading close on channels to all hubs. TBH, it might be a
> cost worth paying in the absence of better ideas. But as braidpool is
> built, more ideas will be appear as well.
>
> [1] Payment Channel Payouts: An Idea for Improving P2Pool Scalability:
> https://bitcointalk.org/index.php?topic=2135429.0
>
> ---------- Original Message ----------
> On Tue, September 7, 2021 at 11:39 PM,  ZmnSCPxj via bitcoin-dev<
> bitcoin-dev at lists.linuxfoundation.org> wrote:
> Good morning all,
>
> A thing I just realized about Braidpool is that the payout server is still
> a single central point-of-failure.
>
> Although the paper claims to use Tor hidden service to protect against
> DDoS attacks, its centrality still cannot protect against sheer accident.
> What happens if some clumsy human (all humans are clumsy, right?) fumbles
> the cables in the datacenter the hub is hosted in?
> What happens if the country the datacenter is in is plunged into war or
> anarchy, because you humans love war and chaos so much?
> What happens if Zeus has a random affair (like all those other times),
> Hera gets angry, and they get into a domestic, and then a random thrown
> lightning bolt hits the datacenter the hub is in?
>
> The paper relies on economic arguments ("such an action will end the pool
> and the stream of future profits for the hub"), but economic arguments tend
> to be a lot less powerful in a monopoly, and the hub effectively has a
> monopoly on all Braidpool miners.
> Hashers might be willing to tolerate minor peccadilloes of the hub, simply
> to let the pool continue (their other choices would be even worse).
>
> So it seems to me that it would still be nicer, if it were at all
> possible, to use multiple hubs.
> I am uncertain how easily this can be done.
>
> Perhaps a Lightning model can be considered.
> Multiple hubs may exist which offer liquidity to the Braidpool network,
> hashers measure uptime and timeliness of payouts, and the winning hasher
> elects one of the hubs.
> The hub gets paid on the coinbase, and should send payouts, minus fees, on
> the LN to the miners.
>
> However, this probably complicates the design too much, and it may be more
> beneficial to get *something* working now.
> Let not the perfect be the enemy of the good.
>
> Regards,
> ZmnSCPxj
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210910/00bba50d/attachment.html>

From michaelfolkson at gmail.com  Fri Sep 10 13:05:40 2021
From: michaelfolkson at gmail.com (Michael Folkson)
Date: Fri, 10 Sep 2021 14:05:40 +0100
Subject: [bitcoin-dev] Reorgs on SigNet - Looking for feedback on
 approach and parameters
Message-ID: <CAFvNmHR+SkAcd_Pr50bMhpWQwCULEo3rC1cwDSRAnu6kmGYAiQ@mail.gmail.com>

> I see zero reason whatsoever to not simply reorg ~every block, or as often as is practical. If users opt in to wanting to test with reorgs, they should be able to test with reorgs, not wait a day to test with reorgs.

One of the goals of the default Signet was to make the default Signet
resemble mainnet as much as possible. (You can do whatever you want on
a custom signet you set up yourself including manufacturing a re-org
every block if you wish.) Hence I'm a bit wary of making the behavior
on the default Signet deviate significantly from what you might
experience on mainnet. Given re-orgs don't occur that often on mainnet
I can see the argument for making them more regular (every 8 hours
seems reasonable to me) on the default Signet but every block seems
excessive. It makes the default Signet into an environment for purely
testing whether your application can withstand various flavors of edge
case re-orgs. You may want to test whether your application can
withstand normal mainnet behavior (no re-orgs for long periods of
time) first before you concern yourself with re-orgs.

> Why bother with a version bit? This seems substantially more complicated than the original proposal that surfaced many times before signet launched to just have a different reorg signing key. Thus, users who wish to follow reorgs can use a 1-of-2 (or higher multisig) and users who wish to not follow reorgs would use a 1-of-1 (or higher multisig), simply marking the reorg blocks as invalid without touching any header bits that non-full clients will ever see.

If I understand this correctly this is introducing a need for users to
sign blocks when currently with the default Signet the user does not
need to concern themselves with signing blocks. That is entirely left
to the network block signers of the default Signet (who were AJ and
Kalle last time I checked). Again I don't think this additional
complexity is needed on the default Signet when you can set up your
own custom Signet if you want to test edge case scenarios that deviate
significantly from what you are likely to experience on mainnet. A
flag set via a configuration argument (the AJ, 0xB10C proposal) with
no-reorgs (or 8 hour re-orgs) as the default seems to me like it would
introduce no additional complexity to the casual (or alpha stage)
tester experience though of course it introduces implementation
complexity.

To move the default Signet in the direction of resembling mainnet even
closer would be to randomly generate batches of transactions to fill
up blocks and create a fee market. It would be great to be able to
test features like RBF and Lightning unhappy paths (justice
transactions, perhaps even pinning attacks etc) on the default Signet
in future.

-- 
Michael Folkson
Email: michaelfolkson at gmail.com
Keybase: michaelfolkson
PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3

From lf-lists at mattcorallo.com  Fri Sep 10 18:24:15 2021
From: lf-lists at mattcorallo.com (Matt Corallo)
Date: Fri, 10 Sep 2021 11:24:15 -0700
Subject: [bitcoin-dev] Reorgs on SigNet - Looking for feedback on
 approach and parameters
In-Reply-To: <CAFvNmHR+SkAcd_Pr50bMhpWQwCULEo3rC1cwDSRAnu6kmGYAiQ@mail.gmail.com>
References: <CAFvNmHR+SkAcd_Pr50bMhpWQwCULEo3rC1cwDSRAnu6kmGYAiQ@mail.gmail.com>
Message-ID: <50970c07-b447-0b49-3f2b-b8a4961761f1@mattcorallo.com>



On 9/10/21 06:05, Michael Folkson wrote:
>> I see zero reason whatsoever to not simply reorg ~every block, or as often as is practical. If users opt in to wanting to test with reorgs, they should be able to test with reorgs, not wait a day to test with reorgs.
> 
> One of the goals of the default Signet was to make the default Signet
> resemble mainnet as much as possible. (You can do whatever you want on
> a custom signet you set up yourself including manufacturing a re-org
> every block if you wish.) Hence I'm a bit wary of making the behavior
> on the default Signet deviate significantly from what you might
> experience on mainnet. Given re-orgs don't occur that often on mainnet
> I can see the argument for making them more regular (every 8 hours
> seems reasonable to me) on the default Signet but every block seems
> excessive. It makes the default Signet into an environment for purely
> testing whether your application can withstand various flavors of edge
> case re-orgs. You may want to test whether your application can
> withstand normal mainnet behavior (no re-orgs for long periods of
> time) first before you concern yourself with re-orgs.

Huh? Why would the goal be to match mainnet? The goal, as I understand it, is to allow software to 
use SigNet without modification *to make testing simpler* - keep the header format the same to let 
SPV clients function without (significant) modification, etc. The point of the whole thing is to 
make testing as easy as possible, why would we do otherwise.

Further, because one goal here is to enable clients to opt in or out of the reorg chain at will 
(presumably by just changing one config flag in bitcoin.conf), why would we worry about making it 
"similar to mainnet". If users want an experience "similar to mainnet", they can simply turn off 
reorgs and they'll see a consistent chain moving forward which never reorgs, similar to the 
practical experience of mainnet.

Once you've opted into reorgs, you almost certainly are looking to *test* reorgs - you just 
restarted Bitcoin Core with the reorg flag set, waiting around for a reorg after doing that seems 
like the experience of testnet3 today, and the whole reason why we wanted signet to begin with - 
things happen sporadically and inconsistently, making developers wait around forever. Please lets 
not replicate the "gotta wait for blocks before I can go to lunch" experience of testnet today on 
signet, I'm tired of eating lunch late.

>> Why bother with a version bit? This seems substantially more complicated than the original proposal that surfaced many times before signet launched to just have a different reorg signing key. Thus, users who wish to follow reorgs can use a 1-of-2 (or higher multisig) and users who wish to not follow reorgs would use a 1-of-1 (or higher multisig), simply marking the reorg blocks as invalid without touching any header bits that non-full clients will ever see.
> 
> If I understand this correctly this is introducing a need for users to
> sign blocks when currently with the default Signet the user does not
> need to concern themselves with signing blocks. That is entirely left
> to the network block signers of the default Signet (who were AJ and
> Kalle last time I checked). Again I don't think this additional
> complexity is needed on the default Signet when you can set up your
> own custom Signet if you want to test edge case scenarios that deviate
> significantly from what you are likely to experience on mainnet. A
> flag set via a configuration argument (the AJ, 0xB10C proposal) with
> no-reorgs (or 8 hour re-orgs) as the default seems to me like it would
> introduce no additional complexity to the casual (or alpha stage)
> tester experience though of course it introduces implementation
> complexity.
> 
> To move the default Signet in the direction of resembling mainnet even
> closer would be to randomly generate batches of transactions to fill
> up blocks and create a fee market. It would be great to be able to
> test features like RBF and Lightning unhappy paths (justice
> transactions, perhaps even pinning attacks etc) on the default Signet
> in future.

I believe my suggestion was not correctly understood. I'm not suggesting *users* sign blocks or 
otherwise do anything manually here, only that the existing block producers each generate a new key, 
and we then only sign reorgs with *those* keys. Users will be able to set a flag to indicate "I want 
to accept sigs from either sets of keys, and see reorgs" or "I only want sigs from the non-reorg 
keys, and will consider the reorg keys-signed blocks invalid"

Matt

From lf-lists at mattcorallo.com  Fri Sep 10 19:22:00 2021
From: lf-lists at mattcorallo.com (Matt Corallo)
Date: Fri, 10 Sep 2021 12:22:00 -0700
Subject: [bitcoin-dev] Reorgs on SigNet - Looking for feedback on
 approach and parameters
In-Reply-To: <CAFvNmHRKBt-KndgEtuT6da8qJAJgHSoime40J3x6Q=8tnnYpOw@mail.gmail.com>
References: <CAFvNmHR+SkAcd_Pr50bMhpWQwCULEo3rC1cwDSRAnu6kmGYAiQ@mail.gmail.com>
 <50970c07-b447-0b49-3f2b-b8a4961761f1@mattcorallo.com>
 <CAFvNmHRKBt-KndgEtuT6da8qJAJgHSoime40J3x6Q=8tnnYpOw@mail.gmail.com>
Message-ID: <966de823-557a-ad71-68b3-c9c8938e60e5@mattcorallo.com>

Fwiw, your email client is broken and does not properly quote in the plaintext copy. I believe this 
is a known gmail bug, but I'd recommend avoiding gmail's web interface for list posting :).

On 9/10/21 12:00, Michael Folkson wrote:
>> Huh? Why would the goal be to match mainnet? The goal, as I understand it, is to allow software to
> use SigNet without modification *to make testing simpler* - keep the
> header format the same to let
> SPV clients function without (significant) modification, etc. The
> point of the whole thing is to
> make testing as easy as possible, why would we do otherwise.
> 
> I guess Kalle (and AJ) can answer this question better than me but my
> understanding is that the motivation for Signet was that testnet
> deviated erratically from mainnet behavior (e.g. long delays before
> any blocks were mined followed by a multitude of blocks mined in a
> short period of time) which meant it wasn't conducive to normal
> testing of applications. Why would you want a mainnet like chain? To
> check if your application works on a mainnet like chain without
> risking any actual value before moving to mainnet. The same purpose as
> testnet but more reliably resembling mainnet behavior. You are well
> within your rights to demand more than that but my preference would be
> to push some of those demands to custom signets rather than the
> default Signet.

Huh? You haven't made an argument here as to why such a chain is easier to test with, only that we 
should "match mainnet". Testing on mainnet sucks, 99% of the time testing on mainnet involves no 
reorgs, which *doesn't* match in-the-field reality of mainnet, with occasional reorgs. Matching 
mainnet's behavior is, in fact, a terrible way to test if your application will run fine on mainnet.

My point is that the goal should be making it easier to test. I'm not entirely sure why there's 
debate here.  I *regularly* have lunch late because I'm waiting for blocks either on mainnet or 
testnet3, and would quite like to avoid that in the future. It takes *forever* to test things on 
mainnet and testnet3, matching their behavior would mean its equally impossible to test things on 
mainnet and testnet3, why is that something we should stirve for?


> Testing out proposed soft forks in advance of them being considered
> for activation would already be introducing a dimension of complexity
> that is going to be hard to manage [0]. I'm generally of the view that
> if you are going to introduce a complexity dimension, keep the other
> dimensions as vanilla as possible. Otherwise you are battling
> complexity in multiple different dimensions and it becomes hard or
> impossible to maintain it and meet your initial objectives.

Yep! Great reason to not have any probabilistic nonsense or try to match mainnet or something on 
signet, just make it deterministic, reorg once a block or twice an our or whatever and call it a day!

Matt

From dave at dtrt.org  Fri Sep 10 20:00:05 2021
From: dave at dtrt.org (David A. Harding)
Date: Fri, 10 Sep 2021 10:00:05 -1000
Subject: [bitcoin-dev] Reorgs on SigNet - Looking for feedback on
 approach and parameters
In-Reply-To: <50970c07-b447-0b49-3f2b-b8a4961761f1@mattcorallo.com>
References: <CAFvNmHR+SkAcd_Pr50bMhpWQwCULEo3rC1cwDSRAnu6kmGYAiQ@mail.gmail.com>
 <50970c07-b447-0b49-3f2b-b8a4961761f1@mattcorallo.com>
Message-ID: <20210910200005.misxood4z7qzqxdl@ganymede>

On Fri, Sep 10, 2021 at 11:24:15AM -0700, Matt Corallo via bitcoin-dev wrote:
> I'm [...] suggesting [...] that the existing block producers each
> generate a new key, and we then only sign reorgs with *those* keys.
> Users will be able to set a flag to indicate "I want to accept sigs
> from either sets of keys, and see reorgs" or "I only want sigs from
> the non-reorg keys, and will consider the reorg keys-signed blocks
> invalid"

This seems pretty useful to me.  I think we might want multiple sets of
keys:

0. No reorgs

1. Periodic reorgs of small to moderate depth for ongoing testing
without excessive disruption (e.g. the every 8 hours proposal).  I think
this probably ought to be the default-default `-signet` in Bitcoin Core
and other nodes.

2. Either frequent reorgs (e.g. every block) or a webapp that generates
reorgs on demand to further reduce testing delays.

If we can only have two, I'd suggest dropping 0.  I think it's already
the case that too few people test their software with reorgs.

-Dave
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210910/4dc3cf45/attachment.sig>

From michaelfolkson at gmail.com  Fri Sep 10 19:00:39 2021
From: michaelfolkson at gmail.com (Michael Folkson)
Date: Fri, 10 Sep 2021 20:00:39 +0100
Subject: [bitcoin-dev] Reorgs on SigNet - Looking for feedback on
 approach and parameters
In-Reply-To: <50970c07-b447-0b49-3f2b-b8a4961761f1@mattcorallo.com>
References: <CAFvNmHR+SkAcd_Pr50bMhpWQwCULEo3rC1cwDSRAnu6kmGYAiQ@mail.gmail.com>
 <50970c07-b447-0b49-3f2b-b8a4961761f1@mattcorallo.com>
Message-ID: <CAFvNmHRKBt-KndgEtuT6da8qJAJgHSoime40J3x6Q=8tnnYpOw@mail.gmail.com>

> Huh? Why would the goal be to match mainnet? The goal, as I understand it, is to allow software to
use SigNet without modification *to make testing simpler* - keep the
header format the same to let
SPV clients function without (significant) modification, etc. The
point of the whole thing is to
make testing as easy as possible, why would we do otherwise.

I guess Kalle (and AJ) can answer this question better than me but my
understanding is that the motivation for Signet was that testnet
deviated erratically from mainnet behavior (e.g. long delays before
any blocks were mined followed by a multitude of blocks mined in a
short period of time) which meant it wasn't conducive to normal
testing of applications. Why would you want a mainnet like chain? To
check if your application works on a mainnet like chain without
risking any actual value before moving to mainnet. The same purpose as
testnet but more reliably resembling mainnet behavior. You are well
within your rights to demand more than that but my preference would be
to push some of those demands to custom signets rather than the
default Signet.

Testing out proposed soft forks in advance of them being considered
for activation would already be introducing a dimension of complexity
that is going to be hard to manage [0]. I'm generally of the view that
if you are going to introduce a complexity dimension, keep the other
dimensions as vanilla as possible. Otherwise you are battling
complexity in multiple different dimensions and it becomes hard or
impossible to maintain it and meet your initial objectives.

But if this feature of extremely regular re-orgs is an in demand
feature for testers I think the question then becomes what the default
be (I would suggest re-orgs every 8 hours rather than no re-orgs at
all) and then the alternative which you can switch to, re-orgs every
block or every 6 blocks or whatever.

> I believe my suggestion was not correctly understood. I'm not suggesting *users* sign blocks or
otherwise do anything manually here, only that the existing block
producers each generate a new key,
and we then only sign reorgs with *those* keys. Users will be able to
set a flag to indicate "I want
to accept sigs from either sets of keys, and see reorgs" or "I only
want sigs from the non-reorg
keys, and will consider the reorg keys-signed blocks invalid"

Ah I did misunderstand, yes this makes more sense. Thanks for the correction.

[0] https://bitcoin.stackexchange.com/questions/98642/can-we-experiment-on-signet-with-multiple-proposed-soft-forks-whilst-maintaining

On Fri, Sep 10, 2021 at 7:24 PM Matt Corallo <lf-lists at mattcorallo.com> wrote:
>
>
>
> On 9/10/21 06:05, Michael Folkson wrote:
> >> I see zero reason whatsoever to not simply reorg ~every block, or as often as is practical. If users opt in to wanting to test with reorgs, they should be able to test with reorgs, not wait a day to test with reorgs.
> >
> > One of the goals of the default Signet was to make the default Signet
> > resemble mainnet as much as possible. (You can do whatever you want on
> > a custom signet you set up yourself including manufacturing a re-org
> > every block if you wish.) Hence I'm a bit wary of making the behavior
> > on the default Signet deviate significantly from what you might
> > experience on mainnet. Given re-orgs don't occur that often on mainnet
> > I can see the argument for making them more regular (every 8 hours
> > seems reasonable to me) on the default Signet but every block seems
> > excessive. It makes the default Signet into an environment for purely
> > testing whether your application can withstand various flavors of edge
> > case re-orgs. You may want to test whether your application can
> > withstand normal mainnet behavior (no re-orgs for long periods of
> > time) first before you concern yourself with re-orgs.
>
> Huh? Why would the goal be to match mainnet? The goal, as I understand it, is to allow software to
> use SigNet without modification *to make testing simpler* - keep the header format the same to let
> SPV clients function without (significant) modification, etc. The point of the whole thing is to
> make testing as easy as possible, why would we do otherwise.
>
> Further, because one goal here is to enable clients to opt in or out of the reorg chain at will
> (presumably by just changing one config flag in bitcoin.conf), why would we worry about making it
> "similar to mainnet". If users want an experience "similar to mainnet", they can simply turn off
> reorgs and they'll see a consistent chain moving forward which never reorgs, similar to the
> practical experience of mainnet.
>
> Once you've opted into reorgs, you almost certainly are looking to *test* reorgs - you just
> restarted Bitcoin Core with the reorg flag set, waiting around for a reorg after doing that seems
> like the experience of testnet3 today, and the whole reason why we wanted signet to begin with -
> things happen sporadically and inconsistently, making developers wait around forever. Please lets
> not replicate the "gotta wait for blocks before I can go to lunch" experience of testnet today on
> signet, I'm tired of eating lunch late.
>
> >> Why bother with a version bit? This seems substantially more complicated than the original proposal that surfaced many times before signet launched to just have a different reorg signing key. Thus, users who wish to follow reorgs can use a 1-of-2 (or higher multisig) and users who wish to not follow reorgs would use a 1-of-1 (or higher multisig), simply marking the reorg blocks as invalid without touching any header bits that non-full clients will ever see.
> >
> > If I understand this correctly this is introducing a need for users to
> > sign blocks when currently with the default Signet the user does not
> > need to concern themselves with signing blocks. That is entirely left
> > to the network block signers of the default Signet (who were AJ and
> > Kalle last time I checked). Again I don't think this additional
> > complexity is needed on the default Signet when you can set up your
> > own custom Signet if you want to test edge case scenarios that deviate
> > significantly from what you are likely to experience on mainnet. A
> > flag set via a configuration argument (the AJ, 0xB10C proposal) with
> > no-reorgs (or 8 hour re-orgs) as the default seems to me like it would
> > introduce no additional complexity to the casual (or alpha stage)
> > tester experience though of course it introduces implementation
> > complexity.
> >
> > To move the default Signet in the direction of resembling mainnet even
> > closer would be to randomly generate batches of transactions to fill
> > up blocks and create a fee market. It would be great to be able to
> > test features like RBF and Lightning unhappy paths (justice
> > transactions, perhaps even pinning attacks etc) on the default Signet
> > in future.
>
> I believe my suggestion was not correctly understood. I'm not suggesting *users* sign blocks or
> otherwise do anything manually here, only that the existing block producers each generate a new key,
> and we then only sign reorgs with *those* keys. Users will be able to set a flag to indicate "I want
> to accept sigs from either sets of keys, and see reorgs" or "I only want sigs from the non-reorg
> keys, and will consider the reorg keys-signed blocks invalid"
>
> Matt



-- 
Michael Folkson
Email: michaelfolkson at gmail.com
Keybase: michaelfolkson
PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3

From ZmnSCPxj at protonmail.com  Sat Sep 11 01:09:30 2021
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Sat, 11 Sep 2021 01:09:30 +0000
Subject: [bitcoin-dev] Braidpool: Proposal for a decentralised mining
	pool
In-Reply-To: <CAO1K=nnGXasdu_M4NgCkcCFMB16sW5r-Xd462d6jfR9mBBCgSA@mail.gmail.com>
References: <MiuahdA--3-2@tutanota.de>
 <ceFmn7ZHyPHN70rDuE66lnPEwjgjQ7LtZLwyFgIVUpPvPDvSZSsLHUf_yiBvXTpjdEju4UxAOnDgilZaQAMvQzYcUbOkZsYvOIpuBG7japo=@protonmail.com>
 <edbbb44e247d4e639659e1b9b989dd84-kohli@ctemplar.com>
 <CAO1K=nnGXasdu_M4NgCkcCFMB16sW5r-Xd462d6jfR9mBBCgSA@mail.gmail.com>
Message-ID: <pqkX9ft1aIX7oRHcgAL2jxwO1VZlnSpWrwNiwhD0ru_-zH9LpQbc5008jmR3dg_z0q_k5zwCQPrhPryLRIYP7aUn8EvjpSeX7zfMztLsfzs=@protonmail.com>

Good morning Filippo,

> Hi!
>
> From the proposal it is not clear why a miner must reference other miners' shares in his shares.
> What I mean is that there is a huge incentive for a rogue miner to not reference any share from
> other miner so he won't share the reward with anyone, but it will be paid for the share that he
> create because good miners will reference his shares.
> The pool will probably become unprofitable for good miners.
>
> Another thing that I do not understand is how to resolve conflicts. For example, using figure 1 at
> page 1, a node could be receive this 2 valid states:
>
> 1. L -> a1 -> a2 -> a3 -> R
> 2. L -> a1* -> a2* -> R
>
> To resolve the above fork the only two method that comes to my mind are:
>
> 1. use the one that has more work
> 2. use the longest one
> Btw both methods present an issue IMHO.
>
> If the longest chain is used:
> When a block (L) is find, a miner (a) could easily create a lot of share with low difficulty
> (L -> a1* -> a2* -> ... -> an*), then start to mine shares with his real hashrate (L -> a1 -> a2)
> and publish them so they get referenced. If someone else finds a block he gets the reward cause he
> has been referenced. If he finds the block he just attaches the funded block to the longest chain
> (that reference no one) and publishes it without sharing the reward
> (L -> a1* -> a2* -> ... -> an* -> R).
>
> If is used the one with more work:
> A miner that has published the shares (L -> a1 -> a2 -> a3) when find a block R that alone has more
> work than a1 + a2 + a3 it just publish (L -> R) and he do not share the reward with anyone.


My understanding from the "Braid" in braidpool is that every share can reference more than one previous share.

In your proposed attack, a single hasher refers only to shares that the hasher itself makes.

However, a good hasher will refer not only to its own shares, but also to shares of the "bad" hasher.

And all honest hashers will be based, not on a single chain, but on the share that refers to the most total work.

So consider these shares from a bad hasher:

     BAD1 <- BAD2 <- BAD3

A good hasher will refer to those, and also to its own shares:

     BAD1 <- BAD2 <- BAD3
       ^       ^       ^
       |       |       |
       |       |       +------+
       |       +-----+        |
       |             |        |
       +--- GOOD1 <- GOOD2 <- GOOD3

`GOOD3` refers to 5 other shares, whereas `BAD3` refers to only 2 shares, so `GOOD3` will be considered weightier, thus removing this avenue of attack and resolving the issue.
Even if measured in terms of total work, `GOOD3` also contains the work that `BAD3` does, so it would still win.

Regards,
ZmnSCPxj


From aj at erisian.com.au  Sat Sep 11 03:26:44 2021
From: aj at erisian.com.au (Anthony Towns)
Date: Sat, 11 Sep 2021 13:26:44 +1000
Subject: [bitcoin-dev] TAPLEAF_UPDATE_VERIFY covenant opcode
In-Reply-To: <CALZpt+FnnbGJC4=KO_OPiKxt0Ey9Bzh1gxP1dQSDz2aBi9WyOA@mail.gmail.com>
References: <20210909064138.GA22496@erisian.com.au>
 <CALZpt+FnnbGJC4=KO_OPiKxt0Ey9Bzh1gxP1dQSDz2aBi9WyOA@mail.gmail.com>
Message-ID: <20210911032644.GB23578@erisian.com.au>

On Fri, Sep 10, 2021 at 12:12:24AM -0400, Antoine Riard wrote:
> "Talk is cheap. Show me the code" :p
> ? ? case OP_MERKLESUB:

I'm not entirely clear on what your opcode there is trying to do. I
think it's taking

   <N> <P> MERKLESUB

and checking that output N has the same scripts as the current input
except with the current script removed, and with its internal pubkey as
the current input's internal pubkey plus P.

> ? ? ? ? txTo->vout[out_pos].scriptPubKey.IsWitnessProgram(witnessversion,
> witnessprogram);
> ? ? ? ? //! The committed to output must be a witness v1 program at least

That would mean anyone who could do a valid spend of the tx could
violate the covenant by spending to an unencumbered witness v2 output
and (by collaborating with a miner) steal the funds. I don't think
there's a reasonable way to have existing covenants be forward
compatible with future destination addresses (beyond something like CTV
that strictly hardcodes them).

> One could also imagine a list of output positions to force the taproot update
> on multiple outputs ("OP_MULTIMERKLESUB").

Having the output position parameter might be an interesting way to
merge/split a vault/pool, but it's not clear to me how much sense it
makes sense to optimise for that, rather than just doing that via the key
path. For pools, you want the key path to be common anyway (for privacy
and efficiency), so it shouldn't be a problem; but even for vaults,
you want the cold wallet accessible enough to be useful for the case
where theft is attempted, and maybe that's also accessible enough for
the ocassional merge/split to keep your utxo count/sizes reasonable.

> For the merkle branches extension, I was thinking of introducing a separate
> OP_MERKLEADD, maybe to *add* a point to the internal pubkey group signer. If
> you're only interested in leaf pruning, using OP_MERKLESUB only should save you
> one byte of empty vector ?

Saving a byte of witness data at the cost of specifying additional
opcodes seems like optimising the wrong thing to me.

> One solution I was thinking about was introducing a new tapscript version
> (`TAPROOT_INTERNAL_TAPSCRIPT`) signaling that VerifyTaprootCommitment must
> compute the TapTweak with a new TapTweak=(internal_pubkey || merkle_root ||
> parity_bit). A malicious participant wouldn't be able to interfere with the
> updated internal key as it would break its own spending taproot commitment
> verification ?

I don't think that works, because different scripts in the same merkle
tree can have different script versions, which would here indicate
different parities for the same internal pub key.

> > That's useless without some way of verifying that the new utxo retains
> > the bitcoin that was in the old utxo, so also include a new opcode
> > IN_OUT_AMOUNT that pushes two items onto the stack: the amount from this
> > input's utxo, and the amount in the corresponding output, and then expect
> > anyone using TLUV to use maths operators to verify that funds are being
> > appropriately retained in the updated scriptPubKey.
> Credit to you for the SIGHASH_GROUP design, here the code, with
> SIGHASH_ANYPUBKEY/ANYAMOUNT extensions.
> 
> I think it's achieving the same effect as IN_OUT_AMOUNT, at least for CoinPool
> use-case.

The IN_OUT_AMOUNT opcode lets you do maths on the values, so you can
specify "hot wallets can withdraw up to X" rather than "hot wallets
must withdraw exactly X". I don't think there's a way of doing that with
SIGHASH_GROUP, even with a modifier like ANYPUBKEY?

> (I think I could come with some use-case from lex mercatoria where if you play
> out a hardship provision you want to tweak all the other provisions by a CSV
> delay while conserving the rest of their policy)

If you want to tweak all the scripts, I think you should be using the
key path.

One way you could do somthing like that without changing the scripts
though, is have the timelock on most of the scripts be something like
"[3 months] CSV", and have a "delay" script that doesn't require a CSV,
does require a signature from someone able to authorise the delay,
and requires the output to have the same scriptPubKey and amount. Then
you can use that path to delay resolution by 3 months however often,
even if you can't coordinate a key path spend.

> > And second, it doesn't provide a way for utxos to "interact", which is
> > something that is interesting for automated market makers [5], but perhaps
> > only interesting for chains aiming to support multiple asset types,
> > and not bitcoin directly. On the other hand, perhaps combining it with
> > CTV might be enough to solve that, particularly if the hash passed to
> > CTV is constructed via script/CAT/etc.
> That's where SIGHASH_GROUP might be more interesting as you could generate
> transaction "puzzles".
> IIUC, the problem is how to have a set of ratios between x/f(x).

Normal way to do it is specify a formula, eg

   outBTC * outUSDT >= inBTC * inUSDT

that's a constant product market maker without a profit margin. There's
lots of research in the ethereum world about doing these things, and
bitmatrix is trying to do it on liquid. It's not clear to me if there's
anywhere in bitcoin per se that it would make sense.

Then your relative balances of each token imply a price, and traders will
rebalance anytime that price is out of whack with the rest of the market.

You can tweak the formula so that you make a profit, which also ends up
meaning the fund pool becomes more liquid overtime. But that means that
you want to cope with 100 BTC and 5M USDT at $50k, but also 200 BTC and
10M USDT at $50k, and many values in between. So I don't think:

> The maker generates a Taproot tree where each leaf is committing to a different
> "strike price".

really works that well.

One irritating thing I realised while reading Jeremy's mail is that

  CAT "TapBranch" SHA256 DUP CAT SWAP CAT SHA256

doesn't actually work -- the first CAT needs to sort the two branches
first, and "LESSTHAN" etc want to compare values numerically rather
than lexically. So maybe it would make more sense to introduce an opcode
that builds a merkle root from tagged hashes directly, rather than one
that lets you compare to 32B strings so that you can do the TapBranch
logic manually.

Cheers,
aj


From fmerli1 at gmail.com  Sat Sep 11 07:54:58 2021
From: fmerli1 at gmail.com (Filippo Merli)
Date: Sat, 11 Sep 2021 09:54:58 +0200
Subject: [bitcoin-dev] Braidpool: Proposal for a decentralised mining
	pool
In-Reply-To: <pqkX9ft1aIX7oRHcgAL2jxwO1VZlnSpWrwNiwhD0ru_-zH9LpQbc5008jmR3dg_z0q_k5zwCQPrhPryLRIYP7aUn8EvjpSeX7zfMztLsfzs=@protonmail.com>
References: <MiuahdA--3-2@tutanota.de>
 <ceFmn7ZHyPHN70rDuE66lnPEwjgjQ7LtZLwyFgIVUpPvPDvSZSsLHUf_yiBvXTpjdEju4UxAOnDgilZaQAMvQzYcUbOkZsYvOIpuBG7japo=@protonmail.com>
 <edbbb44e247d4e639659e1b9b989dd84-kohli@ctemplar.com>
 <CAO1K=nnGXasdu_M4NgCkcCFMB16sW5r-Xd462d6jfR9mBBCgSA@mail.gmail.com>
 <pqkX9ft1aIX7oRHcgAL2jxwO1VZlnSpWrwNiwhD0ru_-zH9LpQbc5008jmR3dg_z0q_k5zwCQPrhPryLRIYP7aUn8EvjpSeX7zfMztLsfzs=@protonmail.com>
Message-ID: <CAO1K=nmhhMuisAXdddC1OSDUP2q8XsQjAUO4CVnyx8+BBvvwTw@mail.gmail.com>

>From my understanding of the posted proposal, a share to get rewarded must
"prove" to be created before the rewarded share. (between L and R)
If GOOD3 refers to BAD2 the only thing that I can prove is that BAD2 has
been mined before GOOD2.

So if  BAD3 is a valid block (I call it R like in the pdf) the only thing
that I can certainly know is that between L and R there is BAD1 and BAD2
and they should be the ones that get the rewards.
Btw rereading the proposal I'm not sure about how the rewards are
calculated, what let me think that is the way that I illustrate above is
the figure 2: c3 is referring to a3 but is not included in the first reward.

To clarify I'm talking about two things in my previous email, the first one
is: what if a bad miner does not refer to good miners' shares?
The second thing is: what if a bad miner publishes two (or more)
conflicting versions of the DAG?


On Sat, Sep 11, 2021 at 3:09 AM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:

> Good morning Filippo,
>
> > Hi!
> >
> > From the proposal it is not clear why a miner must reference other
> miners' shares in his shares.
> > What I mean is that there is a huge incentive for a rogue miner to not
> reference any share from
> > other miner so he won't share the reward with anyone, but it will be
> paid for the share that he
> > create because good miners will reference his shares.
> > The pool will probably become unprofitable for good miners.
> >
> > Another thing that I do not understand is how to resolve conflicts. For
> example, using figure 1 at
> > page 1, a node could be receive this 2 valid states:
> >
> > 1. L -> a1 -> a2 -> a3 -> R
> > 2. L -> a1* -> a2* -> R
> >
> > To resolve the above fork the only two method that comes to my mind are:
> >
> > 1. use the one that has more work
> > 2. use the longest one
> > Btw both methods present an issue IMHO.
> >
> > If the longest chain is used:
> > When a block (L) is find, a miner (a) could easily create a lot of share
> with low difficulty
> > (L -> a1* -> a2* -> ... -> an*), then start to mine shares with his real
> hashrate (L -> a1 -> a2)
> > and publish them so they get referenced. If someone else finds a block
> he gets the reward cause he
> > has been referenced. If he finds the block he just attaches the funded
> block to the longest chain
> > (that reference no one) and publishes it without sharing the reward
> > (L -> a1* -> a2* -> ... -> an* -> R).
> >
> > If is used the one with more work:
> > A miner that has published the shares (L -> a1 -> a2 -> a3) when find a
> block R that alone has more
> > work than a1 + a2 + a3 it just publish (L -> R) and he do not share the
> reward with anyone.
>
>
> My understanding from the "Braid" in braidpool is that every share can
> reference more than one previous share.
>
> In your proposed attack, a single hasher refers only to shares that the
> hasher itself makes.
>
> However, a good hasher will refer not only to its own shares, but also to
> shares of the "bad" hasher.
>
> And all honest hashers will be based, not on a single chain, but on the
> share that refers to the most total work.
>
> So consider these shares from a bad hasher:
>
>      BAD1 <- BAD2 <- BAD3
>
> A good hasher will refer to those, and also to its own shares:
>
>      BAD1 <- BAD2 <- BAD3
>        ^       ^       ^
>        |       |       |
>        |       |       +------+
>        |       +-----+        |
>        |             |        |
>        +--- GOOD1 <- GOOD2 <- GOOD3
>
> `GOOD3` refers to 5 other shares, whereas `BAD3` refers to only 2 shares,
> so `GOOD3` will be considered weightier, thus removing this avenue of
> attack and resolving the issue.
> Even if measured in terms of total work, `GOOD3` also contains the work
> that `BAD3` does, so it would still win.
>
> Regards,
> ZmnSCPxj
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210911/4012ae1d/attachment.html>

From shymaa.arafat at gmail.com  Sat Sep 11 03:00:12 2021
From: shymaa.arafat at gmail.com (shymaa arafat)
Date: Sat, 11 Sep 2021 05:00:12 +0200
Subject: [bitcoin-dev] Storing the Merkle Tree in a compact way
Message-ID: <CAM98U8=r+DGW46O5Srp5SzqB38suYnZY8DR06dqx-_gdB5Ruxw@mail.gmail.com>

Allow me to introduce this simple idea that could be useful ...

-The Intuition was some discussion on Utreexo project about storage saving
and some traversing issues in handling the UTXOS Merkle Tree/ forest; that
is  N internal nodes need to be stored along with 2N pointers (left&right),
+ maybe 1 more pointer in the leaves special nodes to handle different
traversing options (insert, delete, & differently proof fetch that traverse
aunt or niece node according to your implementation
https://github.com/mit-dci/utreexo/discussions/316)
.
Then, I thought of a simple idea that gets rid of all the pointers;
specially appealing when we have all trees are full (complete) in the
forest, but can work for any Merkle Tree:

- 2D array with variable row size; R[j] is of length (N/2^j)
-For example when N=8 nodes
R[0]=0,1,2,...,7
R[1]=8,9,10,11
R[2]=12,13
R[3]=14
.
-We can see that total storage is just 2N-1 nodes,
no need for pointers, and traversing could be neat in any direction with
the right formula:

-Pseudo code to fetch proof[i] ...

//direction to know + or -
If ((i mod 2)==0) drct=1;
            else drct=-1;
// first, the sibling node
proof[i]=R[0,i+drct]

//add the rest thru loop
For(j=1; j?logN; j++)
 { index= i/(2^j)+drct;
    proof[i]=Add(R[j,index]);
 }

-In fact it's just the simple primitive approach of transforming a
recursion to an iteration, and even if Utreexo team solved their problem
differently I thought it is worth telling as it can work for any Merkle Tree
.
Thanks for your time,
Shymaa M Arafat
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210911/1ca4abf1/attachment-0001.html>

From aj at erisian.com.au  Sun Sep 12 07:53:05 2021
From: aj at erisian.com.au (Anthony Towns)
Date: Sun, 12 Sep 2021 17:53:05 +1000
Subject: [bitcoin-dev] Reorgs on SigNet - Looking for feedback on
 approach and parameters
In-Reply-To: <e11d718f-2bb7-335a-80dc-7d44244a0e98@mattcorallo.com>
References: <83272afb-ed87-15b6-e02c-16bb1102beb4@gmail.com>
 <e11d718f-2bb7-335a-80dc-7d44244a0e98@mattcorallo.com>
Message-ID: <20210912075305.GA23673@erisian.com.au>

On Thu, Sep 09, 2021 at 05:50:08PM -0700, Matt Corallo via bitcoin-dev wrote:
> > AJ proposed to allow SigNet users to opt-out of reorgs in case they
> > explicitly want to remain unaffected. This can be done by setting a
> > to-be-reorged version bit [...]
> Why bother with a version bit? This seems substantially more complicated
> than the original proposal that surfaced many times before signet launched
> to just have a different reorg signing key.

Yeah, that was the original idea, but there ended up being two problems
with that approach. The simplest is that the signet block signature
encodes the signet challenge, so if you have two different challenges, eg

  "<normal> CHECKSIG"
  "0 SWAP 1 <normal> <reorg> 2 CHECKMULTISIG"

then while both challenges will accept a signature by normal as the
block solution, the signature by "normal" will be different between the
two. This is a fairly natural result of reusing the tx-signing code for
the block signatures and not having a noinput/anyprevout tx-signing mode.

More generally, though, this would mean that a node that's opting out
of reorgs will see the to-be-reorged blocks as simply invalid due to a
bad signature, and will follow the "this node sent me an invalid block"
path in the p2p code, and start marking peers that are following reorgs
as discouraged and worth disconnecting. I think that would make it pretty
hard to avoid partitioning the network between peers that do and don't
accept reorgs, and generally be a pain.

So using the RECENT_CONSENSUS_CHANGE behaviour that avoids the
discourage/disconnect logic seems the way to avoid that problem, and that
means making it so that nodes that that opt-out of reorgs can distinguish
valid-but-will-become-stale blocks from invalid blocks. Using a versionbit
seems like the easiest way of doing that.

> > The reorg-interval X very much depends on the user's needs. One could
> > argue that there should be, for example, three reorgs per day, each 48
> > blocks apart. Such a short reorg interval allows developers in all time
> > zones to be awake during one or two reorgs per day. Developers don't
> > need to wait for, for example, a week until they can test their reorgs
> > next. However, too frequent reorgs could hinder other SigNet users.
> I see zero reason whatsoever to not simply reorg ~every block, or as often
> as is practical. If users opt in to wanting to test with reorgs, they should
> be able to test with reorgs, not wait a day to test with reorgs.

Blocks on signet get mined at a similar rate to mainnet, so you'll always
have to wait a little bit (up to an hour) -- if you don't want to wait
at all, that's what regtest (or perhaps a custom signet) is for.

I guess it would be super easy to say something like:

 - miner 1 ignores blocks marked for reorg
 - miner 2 marks its blocks for reorg, mines on top of the most work
   block
 - miner 2 never mines a block which would have (height % 10 == 1)
 - miner 1 and miner 2 have the same hashrate, but mine at randomly
   different times

which would mean there's almost always a reorg being mined, people that
follow reorgs will see fewer than 1.9x as many blocks as non-reorg nodes,
and reorgs won't go on for more than 10 blocks.

Cheers,
aj


From vjudeu at gazeta.pl  Sun Sep 12 14:29:08 2021
From: vjudeu at gazeta.pl (vjudeu at gazeta.pl)
Date: Sun, 12 Sep 2021 16:29:08 +0200
Subject: [bitcoin-dev] Reorgs on SigNet - Looking for feedback on
 approach and parameters
In-Reply-To: <20210908075903.GA21644@erisian.com.au>
Message-ID: <140049304-b536fa7b4b29a5afe6fe058ef76145cb@pmq7v.m5r2.onet>

> - 1 block reorgs: these are a regular feature on mainnet, everyone
   should cope with them; having them happen multiple times a day to
   make testing easier should be great

Anyone can do 1 block reorg, because nonce is not signed, so anyone can replace that with better value. For example, if you have block 00000086d6b2636cb2a392d45edc4ec544a10024d30141c9adf4bfd9de533b53 with 0x0007f4cc nonce, you can replace that with 0x00110241 nonce and get 000000096a1c4239d994547185c80308a552cba85d5bd28a51e9dc583ae5eadb block, where everything is identical, except the nonce.

Sometimes that reorg could be deeper if you would be lucky enough to get a block with more work than N following blocks combined.

On 2021-09-08 09:59:29 user Anthony Towns via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
> On Tue, Sep 07, 2021 at 06:07:47PM +0200, 0xB10C via bitcoin-dev wrote:
> The reorg-interval X very much depends on the user's needs. One could
> argue that there should be, for example, three reorgs per day, each 48
> blocks apart.

Oh, wow, I think the last suggestion was every 100 blocks (every
~16h40m). Once every ~8h sounds very convenient.

> Such a short reorg interval allows developers in all time
> zones to be awake during one or two reorgs per day.

And also for there to reliably be reorgs when they're not awake, which
might be a useful thing to be able to handle, too :)

> Developers don't
> need to wait for, for example, a week until they can test their reorgs
> next. However, too frequent reorgs could hinder other SigNet users.

Being able to run `bitcoind -signet -signetacceptreorg=0` and never
seeing any reorgs should presumably make this not a problem?

For people who do see reorgs, having an average of 2 or 3 additional
blocks every 48 blocks is perhaps a 6% increase in storage/traffic.

> # Scenario 1: Race between two chains
> 
> For this scenario, at least two nodes and miner scripts need to be
> running. An always-miner A continuously produces blocks and rejects
> blocks with the to-be-reorged version bit flag set. And a race-miner R
> that only mines D blocks at the start of each interval and then waits X
> blocks. A and R both have the same hash rate. Assuming both are well
> connected to the network, it's random which miner will first mine and
> propagate a block. In the end, the A miner chain will always win the race.

I think this description is missing that all the blocks R mines have
the to-be-reorged flag set.

>     3. How deep should the reorgs be on average? Do you want to test
>        deeper reorgs (10+ blocks) too?

Super interested in input on this -- perhaps we should get optech to
send a survey out to their members, or so?

My feeling is:

 - 1 block reorgs: these are a regular feature on mainnet, everyone
   should cope with them; having them happen multiple times a day to
   make testing easier should be great

 - 2-3 block reorgs: good for testing the "your tx didn't get enough
   confirms to be credited to your account" case, even though it barely
   ever happens on mainnet

 - 4-6 block reorgs: likely to violate business assumptions, but
   completely technically plausible, especially if there's an attack
   against the network

 - 7-100 block reorgs: for this to happen on mainnet, it would probably
   mean there was a bug and pools/miners agree the chain has to
   be immediately reverted -- eg, someone discovers and exploits an
   inflation bug, minting themselves free bitcoins and breaking the 21M
   limit (eg, the 51 block reorg in Aug 2010); or someone discovers a
   bug that splits the chain, and the less compatible chain is reverted
   (eg, the 24 block reorg due to the bdb lock limit in Mar 2013);
   or something similar. Obviously the bug would have to have been
   discovered pretty quickly after it was exploited for the reorg to be
   under a day's worth of blocks.

 - 100-2000+ block reorgs: severe bug that wasn't found quickly, or where
   getting >50% of miners organised took more than a few hours. This will
   start breaking protocol assumptions, like pool payouts, lightning's
   relative locktimes, or liquid's peg-in confirmation requirements, and
   result in hundres of MBs of changes to the utxo set

Maybe it would be good to do reorgs of 15, 150 or 1500 blocks as a
special fire-drill event, perhaps once a month/quarter/year or so,
in some pre-announced window?

I think sticking to 1-6 block reorgs initially is a fine way to start
though.

> After enough testing, the default SigNet can start to do periodical
> reorgs, too.

FWIW, the only thing that concerns me about doing this on the default
signet is making sure that nodes that set -signetacceptreorg=0 don't
end up partitioning the p2p network due to either rejecting a higher
work chain or rejecting txs due to double-spends across the two chains.

A quick draft of code for -signetacceptreorg=0 is available at 

  https://github.com/ajtowns/bitcoin/commits/202108-signetreorg

Cheers,
aj

_______________________________________________
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


From gsanders87 at gmail.com  Sun Sep 12 14:54:33 2021
From: gsanders87 at gmail.com (Greg Sanders)
Date: Sun, 12 Sep 2021 22:54:33 +0800
Subject: [bitcoin-dev] Reorgs on SigNet - Looking for feedback on
 approach and parameters
In-Reply-To: <140049304-b536fa7b4b29a5afe6fe058ef76145cb@pmq7v.m5r2.onet>
References: <20210908075903.GA21644@erisian.com.au>
 <140049304-b536fa7b4b29a5afe6fe058ef76145cb@pmq7v.m5r2.onet>
Message-ID: <CAB3F3DvvdC=+zTWatSRXdEYnwrG-nxt88j2sWF2RBdustn+yNw@mail.gmail.com>

> Sometimes that reorg could be deeper if you would be lucky enough to get
a block with more work than N following blocks combined

Each block is credited for its contribution to total chainwork by the
difficulty target, not the hash of the block itself.

On Sun, Sep 12, 2021 at 10:42 PM vjudeu via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> > - 1 block reorgs: these are a regular feature on mainnet, everyone
>    should cope with them; having them happen multiple times a day to
>    make testing easier should be great
>
> Anyone can do 1 block reorg, because nonce is not signed, so anyone can
> replace that with better value. For example, if you have block
> 00000086d6b2636cb2a392d45edc4ec544a10024d30141c9adf4bfd9de533b53 with
> 0x0007f4cc nonce, you can replace that with 0x00110241 nonce and get
> 000000096a1c4239d994547185c80308a552cba85d5bd28a51e9dc583ae5eadb block,
> where everything is identical, except the nonce.
>
> Sometimes that reorg could be deeper if you would be lucky enough to get a
> block with more work than N following blocks combined.
>
> On 2021-09-08 09:59:29 user Anthony Towns via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
> > On Tue, Sep 07, 2021 at 06:07:47PM +0200, 0xB10C via bitcoin-dev wrote:
> > The reorg-interval X very much depends on the user's needs. One could
> > argue that there should be, for example, three reorgs per day, each 48
> > blocks apart.
>
> Oh, wow, I think the last suggestion was every 100 blocks (every
> ~16h40m). Once every ~8h sounds very convenient.
>
> > Such a short reorg interval allows developers in all time
> > zones to be awake during one or two reorgs per day.
>
> And also for there to reliably be reorgs when they're not awake, which
> might be a useful thing to be able to handle, too :)
>
> > Developers don't
> > need to wait for, for example, a week until they can test their reorgs
> > next. However, too frequent reorgs could hinder other SigNet users.
>
> Being able to run `bitcoind -signet -signetacceptreorg=0` and never
> seeing any reorgs should presumably make this not a problem?
>
> For people who do see reorgs, having an average of 2 or 3 additional
> blocks every 48 blocks is perhaps a 6% increase in storage/traffic.
>
> > # Scenario 1: Race between two chains
> >
> > For this scenario, at least two nodes and miner scripts need to be
> > running. An always-miner A continuously produces blocks and rejects
> > blocks with the to-be-reorged version bit flag set. And a race-miner R
> > that only mines D blocks at the start of each interval and then waits X
> > blocks. A and R both have the same hash rate. Assuming both are well
> > connected to the network, it's random which miner will first mine and
> > propagate a block. In the end, the A miner chain will always win the
> race.
>
> I think this description is missing that all the blocks R mines have
> the to-be-reorged flag set.
>
> >     3. How deep should the reorgs be on average? Do you want to test
> >        deeper reorgs (10+ blocks) too?
>
> Super interested in input on this -- perhaps we should get optech to
> send a survey out to their members, or so?
>
> My feeling is:
>
>  - 1 block reorgs: these are a regular feature on mainnet, everyone
>    should cope with them; having them happen multiple times a day to
>    make testing easier should be great
>
>  - 2-3 block reorgs: good for testing the "your tx didn't get enough
>    confirms to be credited to your account" case, even though it barely
>    ever happens on mainnet
>
>  - 4-6 block reorgs: likely to violate business assumptions, but
>    completely technically plausible, especially if there's an attack
>    against the network
>
>  - 7-100 block reorgs: for this to happen on mainnet, it would probably
>    mean there was a bug and pools/miners agree the chain has to
>    be immediately reverted -- eg, someone discovers and exploits an
>    inflation bug, minting themselves free bitcoins and breaking the 21M
>    limit (eg, the 51 block reorg in Aug 2010); or someone discovers a
>    bug that splits the chain, and the less compatible chain is reverted
>    (eg, the 24 block reorg due to the bdb lock limit in Mar 2013);
>    or something similar. Obviously the bug would have to have been
>    discovered pretty quickly after it was exploited for the reorg to be
>    under a day's worth of blocks.
>
>  - 100-2000+ block reorgs: severe bug that wasn't found quickly, or where
>    getting >50% of miners organised took more than a few hours. This will
>    start breaking protocol assumptions, like pool payouts, lightning's
>    relative locktimes, or liquid's peg-in confirmation requirements, and
>    result in hundres of MBs of changes to the utxo set
>
> Maybe it would be good to do reorgs of 15, 150 or 1500 blocks as a
> special fire-drill event, perhaps once a month/quarter/year or so,
> in some pre-announced window?
>
> I think sticking to 1-6 block reorgs initially is a fine way to start
> though.
>
> > After enough testing, the default SigNet can start to do periodical
> > reorgs, too.
>
> FWIW, the only thing that concerns me about doing this on the default
> signet is making sure that nodes that set -signetacceptreorg=0 don't
> end up partitioning the p2p network due to either rejecting a higher
> work chain or rejecting txs due to double-spends across the two chains.
>
> A quick draft of code for -signetacceptreorg=0 is available at
>
>   https://github.com/ajtowns/bitcoin/commits/202108-signetreorg
>
> Cheers,
> aj
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210912/22dac873/attachment.html>

From jamtlu at gmail.com  Sun Sep 12 15:26:44 2021
From: jamtlu at gmail.com (James Lu)
Date: Sun, 12 Sep 2021 11:26:44 -0400
Subject: [bitcoin-dev] Proposal: Auto-shutdown as 5-year fork window
Message-ID: <CANQHGB2SwZGrFAWC0Rd88qACTjPa1L+hbV54LG87M9RJZni+4Q@mail.gmail.com>

If MTP-11 is greater than 5 years after the release date of the current
software version, the full node should shut down automatically.

This would allow writing code that gives the community ~5 years to upgrade
to a version that executes a new hard fork while keeping everyone in
consensus, provided the change is non-controversial.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210912/9e083ba8/attachment-0001.html>

From vjudeu at gazeta.pl  Sun Sep 12 19:38:47 2021
From: vjudeu at gazeta.pl (vjudeu at gazeta.pl)
Date: Sun, 12 Sep 2021 21:38:47 +0200
Subject: [bitcoin-dev] Proposal: Auto-shutdown as 5-year fork window
In-Reply-To: <CANQHGB2SwZGrFAWC0Rd88qACTjPa1L+hbV54LG87M9RJZni+4Q@mail.gmail.com>
Message-ID: <140815317-3f36d45615e153403b3c83e26c266773@pmq4v.m5r2.onet>

You can do that kind of change in your own Bitcoin-compatible client, but you cannot be sure that other people will run that version and that it will shut down when you want. Many miners use their own custom software for mining blocks, the same for mining pools. There are many clients that are compatible with consensus, but different than Bitcoin Core.
Also you should notice that Bitcoin community make changes by using soft-forks, not hard-forks. Backward compatibility is preserved as often as possible and there is no reason to change that. Any change can be deployed in a soft-fork way, even "evil soft-forks" are possible, as described in https://petertodd.org/2016/forced-soft-forks. I think that kind of soft-fork is still better than hard-fork.
On 2021-09-12 21:16:00 user James Lu via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
If MTP-11 is greater than 5 years after the release date of the current software version, the full node should shut down automatically.
?
This would?allow writing code that?gives the community ~5 years to upgrade to a version that executes a new hard fork while keeping everyone in consensus, provided the change is non-controversial.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210912/4805d7b0/attachment.html>

From antoine.riard at gmail.com  Sun Sep 12 23:37:56 2021
From: antoine.riard at gmail.com (Antoine Riard)
Date: Sun, 12 Sep 2021 19:37:56 -0400
Subject: [bitcoin-dev] TAPLEAF_UPDATE_VERIFY covenant opcode
In-Reply-To: <20210911032644.GB23578@erisian.com.au>
References: <20210909064138.GA22496@erisian.com.au>
 <CALZpt+FnnbGJC4=KO_OPiKxt0Ey9Bzh1gxP1dQSDz2aBi9WyOA@mail.gmail.com>
 <20210911032644.GB23578@erisian.com.au>
Message-ID: <CALZpt+HzM__OJntegOhDqkg5zU=PQXtKgQoB518A2qP9=foovw@mail.gmail.com>

Sorry for the lack of clarity, sometimes it sounds easier to explain ideas
with code.

While MERKLESUB is still WIP, here the semantic. If the input spent is a
SegWit v1 Taproot output, and the script path spending is used, the top
stack item is interpreted as an output position of the spending
transaction. The second top stack item is interpreted as a 32-byte x-only
pubkey to be negated and added to the spent internal pubkey.

The spent tapscript is removed from the merkle tree of tapscripts and a new
merkle root is recomputed with the first node element of the spending
control block as the tapleaf hash. From then, this new merkle root is added
as the taproot tweak to the updated internal pubkey, while correcting for
parity. This new tweaked pubkey is interpreted as a v1 witness program and
must match the scriptPubKey of the spending transaction output as the
passed position. Otherwise, MERKLESUB returns a failure.

I believe this is matching your description and the main difference
compared to your TLUV proposal is the lack of merkle tree extension, where
a new merkle path is added in place of the removed tapscript. Motivation is
saving up the one byte of the new merkle path step, which is not necessary
for our CoinPool use-case.

> That would mean anyone who could do a valid spend of the tx could
> violate the covenant by spending to an unencumbered witness v2 output
> and (by collaborating with a miner) steal the funds. I don't think
> there's a reasonable way to have existing covenants be forward
> compatible with future destination addresses (beyond something like CTV
> that strictly hardcodes them).

That's a good catch, thanks for raising it :)

Depends how you define reasonable, but I think one straightforward fix is
to extend the signature digest algorithm to encompass the segwit version
(and maybe program-size ?) of the spending transaction outputs.

Then you add a "contract" aggregated-key in every tapscript where a
TLUV/MERKLESUB covenant is present. The off-chain contract participant can
exchange signatures at initial setup committing to the segwit version. I
think this addresses the sent-to-unknown-witness-output point ?

When future destination addresses are deployed, assuming a new round of
interactivity, the participants can send the fund to a v1+ by exchanging
signatures with SIGHASH_ALL, that way authorizing the bypass of
TLUV/MERKLESUB.

Of course, in case of v1+ deployment, the key path could be used. Though
this path could have been "burnt" by picking up an internal point with an
unknown scalar following the off-chain contract/use-case semantic ?

> Having the output position parameter might be an interesting way to
> merge/split a vault/pool, but it's not clear to me how much sense it
> makes sense to optimise for that, rather than just doing that via the key
> path. For pools, you want the key path to be common anyway (for privacy
> and efficiency), so it shouldn't be a problem; but even for vaults,
> you want the cold wallet accessible enough to be useful for the case
> where theft is attempted, and maybe that's also accessible enough for
> the ocassional merge/split to keep your utxo count/sizes reasonable.

I think you can come up with interesting contract policies. Let's say you
want to authorize the emergency path of your pool/vault balances if X
happens (e.g a massive drop in USDT price signed by DLC oracles). You have
(A+B+C+D) forking into (A+B) and (C+D) pooled funds. To conserve the
contracts pre-negotiated economic equilibrium, all the participants would
like the emergency path to be inherited on both forks. Without relying on
the key path interactivity, which is ultimately a trust on the post-fork
cooperation of your counterparty ?

> Saving a byte of witness data at the cost of specifying additional
> opcodes seems like optimising the wrong thing to me.

I think we should keep in mind that any overhead cost in the usage of a
script primitive is echoed to the user of off-chain contract/payment
channels. If the tapscripts are bigger, your average on-chain spends in
case of non-cooperative scenarios are increased in consequence, and as such
your fee-bumping reserve. Thus making those systems less economically
accessible.

If we really envision having billions of Bitcoin users owning a utxo or
shards of them, we should also think that those users might have limited
means to pay on-chain fees. Where should be the line between resource
optimizations and protocol/implementation complexity ? Hard to tell.

> I don't think that works, because different scripts in the same merkle
> tree can have different script versions, which would here indicate
> different parities for the same internal pub key.

Let me make it clearer. We introduce a new tapscript version 0x20, forcing
a new bit in the first byte of the control block to be interpreted as the
parity bit of the spent internal pubkey. To ensure this parity bit is
faithful and won't break the updated key path, it's committed in the spent
taptweak. A malicious counterparty while having malleability on the control
block, by setting the parity bit to the wrong value will break the taptweak
and fail the taproot commitment verification ?

I think the correct commitment of different script versions in the merkle
tree can be verified by tree participants at setup ?

> The IN_OUT_AMOUNT opcode lets you do maths on the values, so you can
> specify "hot wallets can withdraw up to X" rather than "hot wallets
> must withdraw exactly X". I don't think there's a way of doing that with
> SIGHASH_GROUP, even with a modifier like ANYPUBKEY?

You can exchange signatures for withdraw outputs with multiples `nValue`
covering the authorized range, assuming the ANYAMOUNT modifier ? One
advantage of leveraging sighash is the ability to update a withdraw policy
in real-time. Vaults participants might be willing to bump the withdraw
policy beyond X, assuming you have N-of-M consents.

> If you want to tweak all the scripts, I think you should be using the
> key path.
>
> One way you could do somthing like that without changing the scripts
> though, is have the timelock on most of the scripts be something like
> "[3 months] CSV", and have a "delay" script that doesn't require a CSV,
> does require a signature from someone able to authorise the delay,
> and requires the output to have the same scriptPubKey and amount. Then
> you can use that path to delay resolution by 3 months however often,
> even if you can't coordinate a key path spend

I think I would like to express the following contract policy. Let's say
you have 1) a one-time conditional script path to withdraw fund ("a put on
strike price X"), 2) a conditional script path to tweak by 3 months all the
usual withdraw path and 3) those remaining withdraw paths. Once played out,
you would like the one-time path to be removed from your merkle tree. And
this removal to be inherited on the tweaked tree if 2) plays out.

I agree that's advanced Bitcoin contracting and we might not require from
one script primitive to cover the whole expressivity we're aiming to.

> that's a constant product market maker without a profit margin. There's
> lots of research in the ethereum world about doing these things, and
> bitmatrix is trying to do it on liquid. It's not clear to me if there's
> anywhere in bitcoin per se that it would make sense.

Good with the more detailed explanation. Yeah I know it's widely deployed
on the ethereum-side, still late on catching up with literature/resources
on that. Assuming we have a widely-deployed token protocol on the
bitcoin-side, you could couple it with a DLC-style of security model and
that might be enough to bootstrap a fruitful token trading ecosystem ?
Though I agree, expressing an AMM in bitcoin primitives is an interesting
design challenge!

> So maybe it would make more sense to introduce an opcode
> that builds a merkle root from tagged hashes directly, rather than one
> that lets you compare to 32B strings so that you can do the TapBranch
> logic manually.

IIUC, you would like an opcode to edit the spent merkle root or build a new
one from stack elements ? E.g adding new withdraw tapleaf if the input
amount is over X. I think that the design description gives more
flexibility but I'm worried you will need more than one opcode. Like
OP_TWEAKADD, to add the tweak on the updated internal key and
OP_SCRIPTPUBKEY_VERIFY (or at least OP_CSFS though more expensive) ?

Le ven. 10 sept. 2021 ? 23:26, Anthony Towns <aj at erisian.com.au> a ?crit :

> On Fri, Sep 10, 2021 at 12:12:24AM -0400, Antoine Riard wrote:
> > "Talk is cheap. Show me the code" :p
> >     case OP_MERKLESUB:
>
> I'm not entirely clear on what your opcode there is trying to do. I
> think it's taking
>
>    <N> <P> MERKLESUB
>
> and checking that output N has the same scripts as the current input
> except with the current script removed, and with its internal pubkey as
> the current input's internal pubkey plus P.
>
> >         txTo->vout[out_pos].scriptPubKey.IsWitnessProgram(witnessversion,
> > witnessprogram);
> >         //! The committed to output must be a witness v1 program at least
>
> That would mean anyone who could do a valid spend of the tx could
> violate the covenant by spending to an unencumbered witness v2 output
> and (by collaborating with a miner) steal the funds. I don't think
> there's a reasonable way to have existing covenants be forward
> compatible with future destination addresses (beyond something like CTV
> that strictly hardcodes them).
>
> > One could also imagine a list of output positions to force the taproot
> update
> > on multiple outputs ("OP_MULTIMERKLESUB").
>
> Having the output position parameter might be an interesting way to
> merge/split a vault/pool, but it's not clear to me how much sense it
> makes sense to optimise for that, rather than just doing that via the key
> path. For pools, you want the key path to be common anyway (for privacy
> and efficiency), so it shouldn't be a problem; but even for vaults,
> you want the cold wallet accessible enough to be useful for the case
> where theft is attempted, and maybe that's also accessible enough for
> the ocassional merge/split to keep your utxo count/sizes reasonable.
>
> > For the merkle branches extension, I was thinking of introducing a
> separate
> > OP_MERKLEADD, maybe to *add* a point to the internal pubkey group
> signer. If
> > you're only interested in leaf pruning, using OP_MERKLESUB only should
> save you
> > one byte of empty vector ?
>
> Saving a byte of witness data at the cost of specifying additional
> opcodes seems like optimising the wrong thing to me.
>
> > One solution I was thinking about was introducing a new tapscript version
> > (`TAPROOT_INTERNAL_TAPSCRIPT`) signaling that VerifyTaprootCommitment
> must
> > compute the TapTweak with a new TapTweak=(internal_pubkey || merkle_root
> ||
> > parity_bit). A malicious participant wouldn't be able to interfere with
> the
> > updated internal key as it would break its own spending taproot
> commitment
> > verification ?
>
> I don't think that works, because different scripts in the same merkle
> tree can have different script versions, which would here indicate
> different parities for the same internal pub key.
>
> > > That's useless without some way of verifying that the new utxo retains
> > > the bitcoin that was in the old utxo, so also include a new opcode
> > > IN_OUT_AMOUNT that pushes two items onto the stack: the amount from
> this
> > > input's utxo, and the amount in the corresponding output, and then
> expect
> > > anyone using TLUV to use maths operators to verify that funds are being
> > > appropriately retained in the updated scriptPubKey.
> > Credit to you for the SIGHASH_GROUP design, here the code, with
> > SIGHASH_ANYPUBKEY/ANYAMOUNT extensions.
> >
> > I think it's achieving the same effect as IN_OUT_AMOUNT, at least for
> CoinPool
> > use-case.
>
> The IN_OUT_AMOUNT opcode lets you do maths on the values, so you can
> specify "hot wallets can withdraw up to X" rather than "hot wallets
> must withdraw exactly X". I don't think there's a way of doing that with
> SIGHASH_GROUP, even with a modifier like ANYPUBKEY?
>
> > (I think I could come with some use-case from lex mercatoria where if
> you play
> > out a hardship provision you want to tweak all the other provisions by a
> CSV
> > delay while conserving the rest of their policy)
>
> If you want to tweak all the scripts, I think you should be using the
> key path.
>
> One way you could do somthing like that without changing the scripts
> though, is have the timelock on most of the scripts be something like
> "[3 months] CSV", and have a "delay" script that doesn't require a CSV,
> does require a signature from someone able to authorise the delay,
> and requires the output to have the same scriptPubKey and amount. Then
> you can use that path to delay resolution by 3 months however often,
> even if you can't coordinate a key path spend.
>
> > > And second, it doesn't provide a way for utxos to "interact", which is
> > > something that is interesting for automated market makers [5], but
> perhaps
> > > only interesting for chains aiming to support multiple asset types,
> > > and not bitcoin directly. On the other hand, perhaps combining it with
> > > CTV might be enough to solve that, particularly if the hash passed to
> > > CTV is constructed via script/CAT/etc.
> > That's where SIGHASH_GROUP might be more interesting as you could
> generate
> > transaction "puzzles".
> > IIUC, the problem is how to have a set of ratios between x/f(x).
>
> Normal way to do it is specify a formula, eg
>
>    outBTC * outUSDT >= inBTC * inUSDT
>
> that's a constant product market maker without a profit margin. There's
> lots of research in the ethereum world about doing these things, and
> bitmatrix is trying to do it on liquid. It's not clear to me if there's
> anywhere in bitcoin per se that it would make sense.
>
> Then your relative balances of each token imply a price, and traders will
> rebalance anytime that price is out of whack with the rest of the market.
>
> You can tweak the formula so that you make a profit, which also ends up
> meaning the fund pool becomes more liquid overtime. But that means that
> you want to cope with 100 BTC and 5M USDT at $50k, but also 200 BTC and
> 10M USDT at $50k, and many values in between. So I don't think:
>
> > The maker generates a Taproot tree where each leaf is committing to a
> different
> > "strike price".
>
> really works that well.
>
> One irritating thing I realised while reading Jeremy's mail is that
>
>   CAT "TapBranch" SHA256 DUP CAT SWAP CAT SHA256
>
> doesn't actually work -- the first CAT needs to sort the two branches
> first, and "LESSTHAN" etc want to compare values numerically rather
> than lexically. So maybe it would make more sense to introduce an opcode
> that builds a merkle root from tagged hashes directly, rather than one
> that lets you compare to 32B strings so that you can do the TapBranch
> logic manually.
>
> Cheers,
> aj
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210912/63905a99/attachment-0001.html>

From lf-lists at mattcorallo.com  Mon Sep 13 05:33:24 2021
From: lf-lists at mattcorallo.com (Matt Corallo)
Date: Sun, 12 Sep 2021 22:33:24 -0700
Subject: [bitcoin-dev] Reorgs on SigNet - Looking for feedback on
	approach and parameters
In-Reply-To: <20210912075305.GA23673@erisian.com.au>
References: <20210912075305.GA23673@erisian.com.au>
Message-ID: <571244AD-B8F4-4F2A-BF4B-31EED3AB7713@mattcorallo.com>



> On Sep 12, 2021, at 00:53, Anthony Towns <aj at erisian.com.au> wrote:
> 
> ?On Thu, Sep 09, 2021 at 05:50:08PM -0700, Matt Corallo via bitcoin-dev wrote:
>>> AJ proposed to allow SigNet users to opt-out of reorgs in case they
>>> explicitly want to remain unaffected. This can be done by setting a
>>> to-be-reorged version bit [...]
>> Why bother with a version bit? This seems substantially more complicated
>> than the original proposal that surfaced many times before signet launched
>> to just have a different reorg signing key.
> 
> Yeah, that was the original idea, but there ended up being two problems
> with that approach. The simplest is that the signet block signature
> encodes the signet challenge,

But if that was the originally proposal, why is the challenge committed to in the block? :)

> So using the RECENT_CONSENSUS_CHANGE behaviour that avoids the
> discourage/disconnect logic seems the way to avoid that problem, and that
> means making it so that nodes that that opt-out of reorgs can distinguish
> valid-but-will-become-stale blocks from invalid blocks. Using a versionbit
> seems like the easiest way of doing that.

Sure, you could set that for invalid block signatures as well though. It?s not really a material DoS protection one way or the other.

>>> The reorg-interval X very much depends on the user's needs. One could
>>> argue that there should be, for example, three reorgs per day, each 48
>>> blocks apart. Such a short reorg interval allows developers in all time
>>> zones to be awake during one or two reorgs per day. Developers don't
>>> need to wait for, for example, a week until they can test their reorgs
>>> next. However, too frequent reorgs could hinder other SigNet users.
>> I see zero reason whatsoever to not simply reorg ~every block, or as often
>> as is practical. If users opt in to wanting to test with reorgs, they should
>> be able to test with reorgs, not wait a day to test with reorgs.
> 
> Blocks on signet get mined at a similar rate to mainnet, so you'll always
> have to wait a little bit (up to an hour) -- if you don't want to wait
> at all, that's what regtest (or perhaps a custom signet) is for.

Can you explain the motivation for this? From where I sit, as far as I know, I should basically be a prime example of the target market for public signet - someone developing bitcoin applications with regular requirements to test those applications with other developers without jumping through hoops to configure software the same across the globe and set up miners. With blocks being slow and irregular, I?m basically not benefited at all by signet and will stick with testnet3/mainnet testing, which both suck.

From kohli at ctemplar.com  Mon Sep 13 08:03:42 2021
From: kohli at ctemplar.com (pool2win)
Date: Mon, 13 Sep 2021 08:03:42 -0000
Subject: [bitcoin-dev] Braidpool: Proposal for a decentralised mining
 pool
In-Reply-To: <CAO1K=nmhhMuisAXdddC1OSDUP2q8XsQjAUO4CVnyx8+BBvvwTw@mail.gmail.com>
References: <MiuahdA--3-2@tutanota.de>
 <ceFmn7ZHyPHN70rDuE66lnPEwjgjQ7LtZLwyFgIVUpPvPDvSZSsLHUf_yiBvXTpjdEju4UxAOnDgilZaQAMvQzYcUbOkZsYvOIpuBG7japo=@protonmail.com>
 <edbbb44e247d4e639659e1b9b989dd84-kohli@ctemplar.com>
 <CAO1K=nnGXasdu_M4NgCkcCFMB16sW5r-Xd462d6jfR9mBBCgSA@mail.gmail.com>
 <pqkX9ft1aIX7oRHcgAL2jxwO1VZlnSpWrwNiwhD0ru_-zH9LpQbc5008jmR3dg_z0q_k5zwCQPrhPryLRIYP7aUn8EvjpSeX7zfMztLsfzs=@protonmail.com>
 <CAO1K=nmhhMuisAXdddC1OSDUP2q8XsQjAUO4CVnyx8+BBvvwTw@mail.gmail.com>
Message-ID: <06d43253aa86489989352c0dfa2bcf8e-kohli@ctemplar.com>

Hi Filippo,

If a malicious miner, M broadcasts {m1, m2 ... mn} at a regular interval, *and* also broadcasts {m1*, mn*} where mn* is  bitcoin block then M will cheat all other miners of their reward. You correctly identified this attack. The problem stems from the fact that I wanted to use the bitcoin block as the sentinel to mark the shares from the DAG that need to be rewarded. There's a few approaches we can take here, but I think the best one is that the hub broadcasts a "sentinel" to mark out the point in logical time up to which shares will be rewarded.

m1* <-------------------- mn*<--------+
                                      |
m1 <----m2 <---m3 <-------------------+
^        ^      ^                     |
|        |      |                     |
|        |      +-----------+         |
|        |                  |         |
|        +--------+         |    SENTINEL
+-----+           |         |         |
      |           |         |         |
      a1  <------ a2 <-----a3  <------+


In the above diagram, when hub receives mn*, the bitcoin block to be rewarded, the hub has also received {m1...m3, a1...a3} and therefore rewards all those shares and broadcasts this logical time to the p2p by sending a sentinel announcement.

This solution will also scale to the multiple hubs construction, as each hub will define their own sentinel and the miners working with each hub can independently verify their shares are being correctly rewarded. The solution also handles the case where M is not referencing any other shares.

The above alternative, might also answer your question about why we need to build a DAG. With a DAG we can capture logical time. Without a DAG, the above solution will require the hub to announce the hash of shares from each miner that have been rewarded.

I really appreciate you taking the time to go through the proposal and pointing out the attack. I hope the above solution addresses your concerns.

Thanks and best regards
pool2win

From michaelfolkson at gmail.com  Mon Sep 13 12:30:31 2021
From: michaelfolkson at gmail.com (Michael Folkson)
Date: Mon, 13 Sep 2021 13:30:31 +0100
Subject: [bitcoin-dev] Reorgs on SigNet - Looking for feedback on
 approach and parameters
Message-ID: <CAFvNmHQmH8S4JFa6xQNbFt0b4PjmqHx5ii6Jd9T5bfSWmPW0KA@mail.gmail.com>

> Can you explain the motivation for this? From where I sit, as far as I know, I should basically be > a prime example of the target market for public signet - someone developing bitcoin applications > with regular requirements to test those applications with other developers without
> jumping through hoops to configure software the same across the globe and set up miners.
> With blocks > being slow and irregular, I?m basically not benefited at all by signet and will
> stick with testnet3/mainnet testing, which both suck.

On testnet3 you can realistically go days without blocks being found
(and conversely thousands of blocks can be found in a day), the block
discovery time variance is huge. Of course this is probabilistically
possible on mainnet too but the probability of this happening is close
to zero. Here[0] is an example of 16,000 blocks being found in a day
on testnet3.

On signet block discovery time variance mirrors mainnet.

On mainnet you are risking Bitcoin with actual monetary value. If you
don't mind doing this then you don't need testnet3, signet or anything
else. In addition proposed soft forks may be activated on signet (and
could also be on testnet3) well before they are considered for
activation on mainnet for testing and experimentation purposes.

[0] https://web.archive.org/web/20160910173004/https://blog.blocktrail.com/2015/04/in-the-darkest-depths-of-testnet3-16k-blocks-were-found-in-1-day/

-- 
Michael Folkson
Email: michaelfolkson at gmail.com
Keybase: michaelfolkson
PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3

From laanwj at protonmail.com  Mon Sep 13 14:31:16 2021
From: laanwj at protonmail.com (W. J. van der Laan)
Date: Mon, 13 Sep 2021 14:31:16 +0000
Subject: [bitcoin-dev] Bitcoin Core 22.0 released
Message-ID: <5VOuodE0IUyG3hFxoB5287JJ2n5oSvvpPWjUV8eJVPSxHgQnhuZWwPmAIPkvw9IxBEGAiEy11pQFJEq8L4sAx0ub9D2ZxhakKHMr0lKV1xM=@protonmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512

22.0 Release Notes
==================

Bitcoin Core version 22.0 is now available from:

  <https://bitcoincore.org/bin/bitcoin-core-22.0/>

Or through bittorrent:

  magnet:?xt=urn:btih:1538a3b3962215f12e0e5f60105457332cf8fee4&dn=bitcoin-core-22.0&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.leechers-paradise.org%3A6969%2Fannounce&tr=udp%3A%2F%2Fexplodie.org%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.torrent.eu.org%3A451%2Fannounce&tr=udp%3A%2F%2Ftracker.bitcoin.sprovoost.nl%3A6969

This release includes new features, various bug fixes and performance
improvements, as well as updated translations.

Please report bugs using the issue tracker at GitHub:

  <https://github.com/bitcoin/bitcoin/issues>

To receive security and update notifications, please subscribe to:

  <https://bitcoincore.org/en/list/announcements/join/>

How to Upgrade
==============

If you are running an older version, shut it down. Wait until it has completely
shut down (which might take a few minutes in some cases), then run the
installer (on Windows) or just copy over `/Applications/Bitcoin-Qt` (on Mac)
or `bitcoind`/`bitcoin-qt` (on Linux).

Upgrading directly from a version of Bitcoin Core that has reached its EOL is
possible, but it might take some time if the data directory needs to be migrated. Old
wallet versions of Bitcoin Core are generally supported.

Compatibility
==============

Bitcoin Core is supported and extensively tested on operating systems
using the Linux kernel, macOS 10.14+, and Windows 7 and newer.  Bitcoin
Core should also work on most other Unix-like systems but is not as
frequently tested on them.  It is not recommended to use Bitcoin Core on
unsupported systems.

- From Bitcoin Core 22.0 onwards, macOS versions earlier than 10.14 are no longer supported.

Notable changes
===============

P2P and network changes
- -----------------------
- - Added support for running Bitcoin Core as an
  [I2P (Invisible Internet Project)](https://en.wikipedia.org/wiki/I2P) service
  and connect to such services. See [i2p.md](https://github.com/bitcoin/bitcoin/blob/22.x/doc/i2p.md) for details. (#20685)
- - This release removes support for Tor version 2 hidden services in favor of Tor
  v3 only, as the Tor network [dropped support for Tor
  v2](https://blog.torproject.org/v2-deprecation-timeline) with the release of
  Tor version 0.4.6.  Henceforth, Bitcoin Core ignores Tor v2 addresses; it
  neither rumors them over the network to other peers, nor stores them in memory
  or to `peers.dat`.  (#22050)

- - Added NAT-PMP port mapping support via
  [`libnatpmp`](https://miniupnp.tuxfamily.org/libnatpmp.html). (#18077)

New and Updated RPCs
- --------------------

- - Due to [BIP 350](https://github.com/bitcoin/bips/blob/master/bip-0350.mediawiki)
  being implemented, behavior for all RPCs that accept addresses is changed when
  a native witness version 1 (or higher) is passed. These now require a Bech32m
  encoding instead of a Bech32 one, and Bech32m encoding will be used for such
  addresses in RPC output as well. No version 1 addresses should be created
  for mainnet until consensus rules are adopted that give them meaning
  (as will happen through [BIP 341](https://github.com/bitcoin/bips/blob/master/bip-0341.mediawiki)).
  Once that happens, Bech32m is expected to be used for them, so this shouldn't
  affect any production systems, but may be observed on other networks where such
  addresses already have meaning (like signet). (#20861)

- - The `getpeerinfo` RPC returns two new boolean fields, `bip152_hb_to` and
  `bip152_hb_from`, that respectively indicate whether we selected a peer to be
  in compact blocks high-bandwidth mode or whether a peer selected us as a
  compact blocks high-bandwidth peer. High-bandwidth peers send new block
  announcements via a `cmpctblock` message rather than the usual inv/headers
  announcements. See BIP 152 for more details. (#19776)

- - `getpeerinfo` no longer returns the following fields: `addnode`, `banscore`,
  and `whitelisted`, which were previously deprecated in 0.21. Instead of
  `addnode`, the `connection_type` field returns manual. Instead of
  `whitelisted`, the `permissions` field indicates if the peer has special
  privileges. The `banscore` field has simply been removed. (#20755)

- - The following RPCs:  `gettxout`, `getrawtransaction`, `decoderawtransaction`,
  `decodescript`, `gettransaction`, and REST endpoints: `/rest/tx`,
  `/rest/getutxos`, `/rest/block` deprecated the following fields (which are no
  longer returned in the responses by default): `addresses`, `reqSigs`.
  The `-deprecatedrpc=addresses` flag must be passed for these fields to be
  included in the RPC response. This flag/option will be available only for this major release, after which
  the deprecation will be removed entirely. Note that these fields are attributes of
  the `scriptPubKey` object returned in the RPC response. However, in the response
  of `decodescript` these fields are top-level attributes, and included again as attributes
  of the `scriptPubKey` object. (#20286)

- - When creating a hex-encoded bitcoin transaction using the `bitcoin-tx` utility
  with the `-json` option set, the following fields: `addresses`, `reqSigs` are no longer
  returned in the tx output of the response. (#20286)

- - The `listbanned` RPC now returns two new numeric fields: `ban_duration` and `time_remaining`.
  Respectively, these new fields indicate the duration of a ban and the time remaining until a ban expires,
  both in seconds. Additionally, the `ban_created` field is repositioned to come before `banned_until`. (#21602)

- - The `setban` RPC can ban onion addresses again. This fixes a regression introduced in version 0.21.0. (#20852)

- - The `getnodeaddresses` RPC now returns a "network" field indicating the
  network type (ipv4, ipv6, onion, or i2p) for each address.  (#21594)

- - `getnodeaddresses` now also accepts a "network" argument (ipv4, ipv6, onion,
  or i2p) to return only addresses of the specified network.  (#21843)

- - The `testmempoolaccept` RPC now accepts multiple transactions (still experimental at the moment,
  API may be unstable). This is intended for testing transaction packages with dependency
  relationships; it is not recommended for batch-validating independent transactions. In addition to
  mempool policy, package policies apply: the list cannot contain more than 25 transactions or have a
  total size exceeding 101K virtual bytes, and cannot conflict with (spend the same inputs as) each other or
  the mempool, even if it would be a valid BIP125 replace-by-fee. There are some known limitations to
  the accuracy of the test accept: it's possible for `testmempoolaccept` to return "allowed"=True for a
  group of transactions, but "too-long-mempool-chain" if they are actually submitted. (#20833)

- - `addmultisigaddress` and `createmultisig` now support up to 20 keys for
  Segwit addresses. (#20867)

Changes to Wallet or GUI related RPCs can be found in the GUI or Wallet section below.

Build System
- ------------

- - Release binaries are now produced using the new `guix`-based build system.
  The [/doc/release-process.md](https://github.com/bitcoin/bitcoin/blob/master/doc/release-process.md) document has been updated accordingly.

Files
- -----

- - The list of banned hosts and networks (via `setban` RPC) is now saved on disk
  in JSON format in `banlist.json` instead of `banlist.dat`. `banlist.dat` is
  only read on startup if `banlist.json` is not present. Changes are only written to the new
  `banlist.json`. A future version of Bitcoin Core may completely ignore
  `banlist.dat`. (#20966)

New settings
- ------------

- - The `-natpmp` option has been added to use NAT-PMP to map the listening port.
  If both UPnP and NAT-PMP are enabled, a successful allocation from UPnP
  prevails over one from NAT-PMP. (#18077)

Updated settings
- ----------------

Changes to Wallet or GUI related settings can be found in the GUI or Wallet section below.

- - Passing an invalid `-rpcauth` argument now cause bitcoind to fail to start.  (#20461)

Tools and Utilities
- -------------------

- - A new CLI `-addrinfo` command returns the number of addresses known to the
  node per network type (including Tor v2 versus v3) and total. This can be
  useful to see if the node knows enough addresses in a network to use options
  like `-onlynet=<network>` or to upgrade to this release of Bitcoin Core 22.0
  that supports Tor v3 only.  (#21595)

- - A new `-rpcwaittimeout` argument to `bitcoin-cli` sets the timeout
  in seconds to use with `-rpcwait`. If the timeout expires,
  `bitcoin-cli` will report a failure. (#21056)

Wallet
- ------

- - External signers such as hardware wallets can now be used through the new RPC methods `enumeratesigners` and `displayaddress`. Support is also added to the `send` RPC call. This feature is experimental. See [external-signer.md](https://github.com/bitcoin/bitcoin/blob/22.x/doc/external-signer.md) for details. (#16546)

- - A new `listdescriptors` RPC is available to inspect the contents of descriptor-enabled wallets.
  The RPC returns public versions of all imported descriptors, including their timestamp and flags.
  For ranged descriptors, it also returns the range boundaries and the next index to generate addresses from. (#20226)

- - The `bumpfee` RPC is not available with wallets that have private keys
  disabled. `psbtbumpfee` can be used instead. (#20891)

- - The `fundrawtransaction`, `send` and `walletcreatefundedpsbt` RPCs now support an `include_unsafe` option
  that when `true` allows using unsafe inputs to fund the transaction.
  Note that the resulting transaction may become invalid if one of the unsafe inputs disappears.
  If that happens, the transaction must be funded with different inputs and republished. (#21359)

- - We now support up to 20 keys in `multi()` and `sortedmulti()` descriptors
  under `wsh()`. (#20867)

- - Taproot descriptors can be imported into the wallet only after activation has occurred on the network (e.g. mainnet, testnet, signet) in use. See [descriptors.md](https://github.com/bitcoin/bitcoin/blob/22.x/doc/descriptors.md) for supported descriptors.

GUI changes
- -----------

- - External signers such as hardware wallets can now be used. These require an external tool such as [HWI](https://github.com/bitcoin-core/HWI) to be installed and configured under Options -> Wallet. When creating a new wallet a new option "External signer" will appear in the dialog. If the device is detected, its name is suggested as the wallet name. The watch-only keys are then automatically imported. Receive addresses can be verified on the device. The send dialog will automatically use the connected device. This feature is experimental and the UI may freeze for a few seconds when performing these actions.

Low-level changes
=================

RPC
- ---

- - The RPC server can process a limited number of simultaneous RPC requests.
  Previously, if this limit was exceeded, the RPC server would respond with
  [status code 500 (`HTTP_INTERNAL_SERVER_ERROR`)](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes#5xx_server_errors).
  Now it returns status code 503 (`HTTP_SERVICE_UNAVAILABLE`). (#18335)

- - Error codes have been updated to be more accurate for the following error cases (#18466):
  - `signmessage` now returns RPC_INVALID_ADDRESS_OR_KEY (-5) if the
    passed address is invalid. Previously returned RPC_TYPE_ERROR (-3).
  - `verifymessage` now returns RPC_INVALID_ADDRESS_OR_KEY (-5) if the
    passed address is invalid. Previously returned RPC_TYPE_ERROR (-3).
  - `verifymessage` now returns RPC_TYPE_ERROR (-3) if the passed signature
    is malformed. Previously returned RPC_INVALID_ADDRESS_OR_KEY (-5).

Tests
- -----

22.0 change log
===============

A detailed list of changes in this version follows. To keep the list to a manageable length, small refactors and typo fixes are not included, and similar changes are sometimes condensed into one line.

### Consensus
- - bitcoin/bitcoin#19438 Introduce deploymentstatus (ajtowns)
- - bitcoin/bitcoin#20207 Follow-up extra comments on taproot code and tests (sipa)
- - bitcoin/bitcoin#21330 Deal with missing data in signature hashes more consistently (sipa)

### Policy
- - bitcoin/bitcoin#18766 Disable fee estimation in blocksonly mode (by removing the fee estimates global) (darosior)
- - bitcoin/bitcoin#20497 Add `MAX_STANDARD_SCRIPTSIG_SIZE` to policy (sanket1729)
- - bitcoin/bitcoin#20611 Move `TX_MAX_STANDARD_VERSION` to policy (MarcoFalke)

### Mining
- - bitcoin/bitcoin#19937, bitcoin/bitcoin#20923 Signet mining utility (ajtowns)

### Block and transaction handling
- - bitcoin/bitcoin#14501 Fix possible data race when committing block files (luke-jr)
- - bitcoin/bitcoin#15946 Allow maintaining the blockfilterindex when using prune (jonasschnelli)
- - bitcoin/bitcoin#18710 Add local thread pool to CCheckQueue (hebasto)
- - bitcoin/bitcoin#19521 Coinstats Index (fjahr)
- - bitcoin/bitcoin#19806 UTXO snapshot activation (jamesob)
- - bitcoin/bitcoin#19905 Remove dead CheckForkWarningConditionsOnNewFork (MarcoFalke)
- - bitcoin/bitcoin#19935 Move SaltedHashers to separate file and add some new ones (achow101)
- - bitcoin/bitcoin#20054 Remove confusing and useless "unexpected version" warning (MarcoFalke)
- - bitcoin/bitcoin#20519 Handle rename failure in `DumpMempool(?)` by using the `RenameOver(?)` return value (practicalswift)
- - bitcoin/bitcoin#20749, bitcoin/bitcoin#20750, bitcoin/bitcoin#21055, bitcoin/bitcoin#21270, bitcoin/bitcoin#21525, bitcoin/bitcoin#21391, bitcoin/bitcoin#21767, bitcoin/bitcoin#21866 Prune `g_chainman` usage (dongcarl)
- - bitcoin/bitcoin#20833 rpc/validation: enable packages through testmempoolaccept (glozow)
- - bitcoin/bitcoin#20834 Locks and docs in ATMP and CheckInputsFromMempoolAndCache (glozow)
- - bitcoin/bitcoin#20854 Remove unnecessary try-block (amitiuttarwar)
- - bitcoin/bitcoin#20868 Remove redundant check on pindex (jarolrod)
- - bitcoin/bitcoin#20921 Don't try to invalidate genesis block in CChainState::InvalidateBlock (theStack)
- - bitcoin/bitcoin#20972 Locks: Annotate CTxMemPool::check to require `cs_main` (dongcarl)
- - bitcoin/bitcoin#21009 Remove RewindBlockIndex logic (dhruv)
- - bitcoin/bitcoin#21025 Guard chainman chainstates with `cs_main` (dongcarl)
- - bitcoin/bitcoin#21202 Two small clang lock annotation improvements (amitiuttarwar)
- - bitcoin/bitcoin#21523 Run VerifyDB on all chainstates (jamesob)
- - bitcoin/bitcoin#21573 Update libsecp256k1 subtree to latest master (sipa)
- - bitcoin/bitcoin#21582, bitcoin/bitcoin#21584, bitcoin/bitcoin#21585 Fix assumeutxo crashes (MarcoFalke)
- - bitcoin/bitcoin#21681 Fix ActivateSnapshot to use hardcoded nChainTx (jamesob)
- - bitcoin/bitcoin#21796 index: Avoid async shutdown on init error (MarcoFalke)
- - bitcoin/bitcoin#21946 Document and test lack of inherited signaling in RBF policy (ariard)
- - bitcoin/bitcoin#22084 Package testmempoolaccept followups (glozow)
- - bitcoin/bitcoin#22102 Remove `Warning:` from warning message printed for unknown new rules (prayank23)
- - bitcoin/bitcoin#22112 Force port 0 in I2P (vasild)
- - bitcoin/bitcoin#22135 CRegTestParams: Use `args` instead of `gArgs` (kiminuo)
- - bitcoin/bitcoin#22146 Reject invalid coin height and output index when loading assumeutxo (MarcoFalke)
- - bitcoin/bitcoin#22253 Distinguish between same tx and same-nonwitness-data tx in mempool (glozow)
- - bitcoin/bitcoin#22261 Two small fixes to node broadcast logic (jnewbery)
- - bitcoin/bitcoin#22415 Make `m_mempool` optional in CChainState (jamesob)
- - bitcoin/bitcoin#22499 Update assumed chain params (sriramdvt)
- - bitcoin/bitcoin#22589 net, doc: update I2P hardcoded seeds and docs for 22.0 (jonatack)

### P2P protocol and network code
- - bitcoin/bitcoin#18077 Add NAT-PMP port forwarding support (hebasto)
- - bitcoin/bitcoin#18722 addrman: improve performance by using more suitable containers (vasild)
- - bitcoin/bitcoin#18819 Replace `cs_feeFilter` with simple std::atomic (MarcoFalke)
- - bitcoin/bitcoin#19203 Add regression fuzz harness for CVE-2017-18350. Add FuzzedSocket (practicalswift)
- - bitcoin/bitcoin#19288 fuzz: Add fuzzing harness for TorController (practicalswift)
- - bitcoin/bitcoin#19415 Make DNS lookup mockable, add fuzzing harness (practicalswift)
- - bitcoin/bitcoin#19509 Per-Peer Message Capture (troygiorshev)
- - bitcoin/bitcoin#19763 Don't try to relay to the address' originator (vasild)
- - bitcoin/bitcoin#19771 Replace enum CConnMan::NumConnections with enum class ConnectionDirection (luke-jr)
- - bitcoin/bitcoin#19776 net, rpc: expose high bandwidth mode state via getpeerinfo (theStack)
- - bitcoin/bitcoin#19832 Put disconnecting logs into BCLog::NET category (hebasto)
- - bitcoin/bitcoin#19858 Periodically make block-relay connections and sync headers (sdaftuar)
- - bitcoin/bitcoin#19884 No delay in adding fixed seeds if -dnsseed=0 and peers.dat is empty (dhruv)
- - bitcoin/bitcoin#20079 Treat handshake misbehavior like unknown message (MarcoFalke)
- - bitcoin/bitcoin#20138 Assume that SetCommonVersion is called at most once per peer (MarcoFalke)
- - bitcoin/bitcoin#20162 p2p: declare Announcement::m_state as uint8_t, add getter/setter (jonatack)
- - bitcoin/bitcoin#20197 Protect onions in AttemptToEvictConnection(), add eviction protection test coverage (jonatack)
- - bitcoin/bitcoin#20210 assert `CNode::m_inbound_onion` is inbound in ctor, add getter, unit tests (jonatack)
- - bitcoin/bitcoin#20228 addrman: Make addrman a top-level component (jnewbery)
- - bitcoin/bitcoin#20234 Don't bind on 0.0.0.0 if binds are restricted to Tor (vasild)
- - bitcoin/bitcoin#20477 Add unit testing of node eviction logic (practicalswift)
- - bitcoin/bitcoin#20516 Well-defined CAddress disk serialization, and addrv2 anchors.dat (sipa)
- - bitcoin/bitcoin#20557 addrman: Fix new table bucketing during unserialization (jnewbery)
- - bitcoin/bitcoin#20561 Periodically clear `m_addr_known` (sdaftuar)
- - bitcoin/bitcoin#20599 net processing: Tolerate sendheaders and sendcmpct messages before verack (jnewbery)
- - bitcoin/bitcoin#20616 Check CJDNS address is valid (lontivero)
- - bitcoin/bitcoin#20617 Remove `m_is_manual_connection` from CNodeState (ariard)
- - bitcoin/bitcoin#20624 net processing: Remove nStartingHeight check from block relay (jnewbery)
- - bitcoin/bitcoin#20651 Make p2p recv buffer timeout 20 minutes for all peers (jnewbery)
- - bitcoin/bitcoin#20661 Only select from addrv2-capable peers for torv3 address relay (sipa)
- - bitcoin/bitcoin#20685 Add I2P support using I2P SAM (vasild)
- - bitcoin/bitcoin#20690 Clean up logging of outbound connection type (sdaftuar)
- - bitcoin/bitcoin#20721 Move ping data to `net_processing` (jnewbery)
- - bitcoin/bitcoin#20724 Cleanup of -debug=net log messages (ajtowns)
- - bitcoin/bitcoin#20747 net processing: Remove dropmessagestest (jnewbery)
- - bitcoin/bitcoin#20764 cli -netinfo peer connections dashboard updates ? ? (jonatack)
- - bitcoin/bitcoin#20788 add RAII socket and use it instead of bare SOCKET (vasild)
- - bitcoin/bitcoin#20791 remove unused legacyWhitelisted in AcceptConnection() (jonatack)
- - bitcoin/bitcoin#20816 Move RecordBytesSent() call out of `cs_vSend` lock (jnewbery)
- - bitcoin/bitcoin#20845 Log to net debug in MaybeDiscourageAndDisconnect except for noban and manual peers (MarcoFalke)
- - bitcoin/bitcoin#20864 Move SocketSendData lock annotation to header (MarcoFalke)
- - bitcoin/bitcoin#20965 net, rpc:  return `NET_UNROUTABLE` as `not_publicly_routable`, automate helps (jonatack)
- - bitcoin/bitcoin#20966 banman: save the banlist in a JSON format on disk (vasild)
- - bitcoin/bitcoin#21015 Make all of `net_processing` (and some of net) use std::chrono types (dhruv)
- - bitcoin/bitcoin#21029 bitcoin-cli: Correct docs (no "generatenewaddress" exists) (luke-jr)
- - bitcoin/bitcoin#21148 Split orphan handling from `net_processing` into txorphanage (ajtowns)
- - bitcoin/bitcoin#21162 Net Processing: Move RelayTransaction() into PeerManager (jnewbery)
- - bitcoin/bitcoin#21167 make `CNode::m_inbound_onion` public, initialize explicitly (jonatack)
- - bitcoin/bitcoin#21186 net/net processing: Move addr data into `net_processing` (jnewbery)
- - bitcoin/bitcoin#21187 Net processing: Only call PushAddress() from `net_processing` (jnewbery)
- - bitcoin/bitcoin#21198 Address outstanding review comments from PR20721 (jnewbery)
- - bitcoin/bitcoin#21222 log: Clarify log message when file does not exist (MarcoFalke)
- - bitcoin/bitcoin#21235 Clarify disconnect log message in ProcessGetBlockData, remove send bool (MarcoFalke)
- - bitcoin/bitcoin#21236 Net processing: Extract `addr` send functionality into MaybeSendAddr() (jnewbery)
- - bitcoin/bitcoin#21261 update inbound eviction protection for multiple networks, add I2P peers (jonatack)
- - bitcoin/bitcoin#21328 net, refactor: pass uint16 CService::port as uint16 (jonatack)
- - bitcoin/bitcoin#21387 Refactor sock to add I2P fuzz and unit tests (vasild)
- - bitcoin/bitcoin#21395 Net processing: Remove unused CNodeState.address member (jnewbery)
- - bitcoin/bitcoin#21407 i2p: limit the size of incoming messages (vasild)
- - bitcoin/bitcoin#21506 p2p, refactor: make NetPermissionFlags an enum class (jonatack)
- - bitcoin/bitcoin#21509 Don't send FEEFILTER in blocksonly mode (mzumsande)
- - bitcoin/bitcoin#21560 Add Tor v3 hardcoded seeds (laanwj)
- - bitcoin/bitcoin#21563 Restrict period when `cs_vNodes` mutex is locked (hebasto)
- - bitcoin/bitcoin#21564 Avoid calling getnameinfo when formatting IPv4 addresses in CNetAddr::ToStringIP (practicalswift)
- - bitcoin/bitcoin#21631 i2p: always check the return value of Sock::Wait() (vasild)
- - bitcoin/bitcoin#21644 p2p, bugfix: use NetPermissions::HasFlag() in CConnman::Bind() (jonatack)
- - bitcoin/bitcoin#21659 flag relevant Sock methods with [[nodiscard]] (vasild)
- - bitcoin/bitcoin#21750 remove unnecessary check of `CNode::cs_vSend` (vasild)
- - bitcoin/bitcoin#21756 Avoid calling `getnameinfo` when formatting IPv6 addresses in `CNetAddr::ToStringIP` (practicalswift)
- - bitcoin/bitcoin#21775 Limit `m_block_inv_mutex` (MarcoFalke)
- - bitcoin/bitcoin#21825 Add I2P hardcoded seeds (jonatack)
- - bitcoin/bitcoin#21843 p2p, rpc: enable GetAddr, GetAddresses, and getnodeaddresses by network (jonatack)
- - bitcoin/bitcoin#21845 net processing: Don't require locking `cs_main` before calling RelayTransactions() (jnewbery)
- - bitcoin/bitcoin#21872 Sanitize message type for logging (laanwj)
- - bitcoin/bitcoin#21914 Use stronger AddLocal() for our I2P address (vasild)
- - bitcoin/bitcoin#21985 Return IPv6 scope id in `CNetAddr::ToStringIP()` (laanwj)
- - bitcoin/bitcoin#21992 Remove -feefilter option (amadeuszpawlik)
- - bitcoin/bitcoin#21996 Pass strings to NetPermissions::TryParse functions by const ref (jonatack)
- - bitcoin/bitcoin#22013 ignore block-relay-only peers when skipping DNS seed (ajtowns)
- - bitcoin/bitcoin#22050 Remove tor v2 support (jonatack)
- - bitcoin/bitcoin#22096 AddrFetch - don't disconnect on self-announcements (mzumsande)
- - bitcoin/bitcoin#22141 net processing: Remove hash and fValidatedHeaders from QueuedBlock (jnewbery)
- - bitcoin/bitcoin#22144 Randomize message processing peer order (sipa)
- - bitcoin/bitcoin#22147 Protect last outbound HB compact block peer (sdaftuar)
- - bitcoin/bitcoin#22179 Torv2 removal followups (vasild)
- - bitcoin/bitcoin#22211 Relay I2P addresses even if not reachable (by us) (vasild)
- - bitcoin/bitcoin#22284 Performance improvements to ProtectEvictionCandidatesByRatio() (jonatack)
- - bitcoin/bitcoin#22387 Rate limit the processing of rumoured addresses (sipa)
- - bitcoin/bitcoin#22455 addrman: detect on-disk corrupted nNew and nTried during unserialization (vasild)

### Wallet
- - bitcoin/bitcoin#15710 Catch `ios_base::failure` specifically (Bushstar)
- - bitcoin/bitcoin#16546 External signer support - Wallet Box edition (Sjors)
- - bitcoin/bitcoin#17331 Use effective values throughout coin selection (achow101)
- - bitcoin/bitcoin#18418 Increase `OUTPUT_GROUP_MAX_ENTRIES` to 100 (fjahr)
- - bitcoin/bitcoin#18842 Mark replaced tx to not be in the mempool anymore (MarcoFalke)
- - bitcoin/bitcoin#19136 Add `parent_desc` to `getaddressinfo` (achow101)
- - bitcoin/bitcoin#19137 wallettool: Add dump and createfromdump commands (achow101)
- - bitcoin/bitcoin#19651 `importdescriptor`s update existing (S3RK)
- - bitcoin/bitcoin#20040 Refactor OutputGroups to handle fees and spending eligibility on grouping (achow101)
- - bitcoin/bitcoin#20202 Make BDB support optional (achow101)
- - bitcoin/bitcoin#20226, bitcoin/bitcoin#21277, - bitcoin/bitcoin#21063 Add `listdescriptors` command (S3RK)
- - bitcoin/bitcoin#20267 Disable and fix tests for when BDB is not compiled (achow101)
- - bitcoin/bitcoin#20275 List all wallets in non-SQLite and non-BDB builds (ryanofsky)
- - bitcoin/bitcoin#20365 wallettool: Add parameter to create descriptors wallet (S3RK)
- - bitcoin/bitcoin#20403 `upgradewallet` fixes, improvements, test coverage (jonatack)
- - bitcoin/bitcoin#20448 `unloadwallet`: Allow specifying `wallet_name` param matching RPC endpoint wallet (luke-jr)
- - bitcoin/bitcoin#20536 Error with "Transaction too large" if the funded tx will end up being too large after signing (achow101)
- - bitcoin/bitcoin#20687 Add missing check for -descriptors wallet tool option (MarcoFalke)
- - bitcoin/bitcoin#20952 Add BerkeleyDB version sanity check at init time (laanwj)
- - bitcoin/bitcoin#21127 Load flags before everything else (Sjors)
- - bitcoin/bitcoin#21141 Add new format string placeholders for walletnotify (maayank)
- - bitcoin/bitcoin#21238 A few descriptor improvements to prepare for Taproot support (sipa)
- - bitcoin/bitcoin#21302 `createwallet` examples for descriptor wallets (S3RK)
- - bitcoin/bitcoin#21329 descriptor wallet: Cache last hardened xpub and use in normalized descriptors (achow101)
- - bitcoin/bitcoin#21365 Basic Taproot signing support for descriptor wallets (sipa)
- - bitcoin/bitcoin#21417 Misc external signer improvement and HWI 2 support (Sjors)
- - bitcoin/bitcoin#21467 Move external signer out of wallet module (Sjors)
- - bitcoin/bitcoin#21572 Fix wrong wallet RPC context set after #21366 (ryanofsky)
- - bitcoin/bitcoin#21574 Drop JSONRPCRequest constructors after #21366 (ryanofsky)
- - bitcoin/bitcoin#21666 Miscellaneous external signer changes (fanquake)
- - bitcoin/bitcoin#21759 Document coin selection code (glozow)
- - bitcoin/bitcoin#21786 Ensure sat/vB feerates are in range (mantissa of 3) (jonatack)
- - bitcoin/bitcoin#21944 Fix issues when `walletdir` is root directory (prayank23)
- - bitcoin/bitcoin#22042 Replace size/weight estimate tuple with struct for named fields (instagibbs)
- - bitcoin/bitcoin#22051 Basic Taproot derivation support for descriptors (sipa)
- - bitcoin/bitcoin#22154 Add OutputType::BECH32M and related wallet support for fetching bech32m addresses (achow101)
- - bitcoin/bitcoin#22156 Allow tr() import only when Taproot is active (achow101)
- - bitcoin/bitcoin#22166 Add support for inferring tr() descriptors (sipa)
- - bitcoin/bitcoin#22173 Do not load external signers wallets when unsupported (achow101)
- - bitcoin/bitcoin#22308 Add missing BlockUntilSyncedToCurrentChain (MarcoFalke)
- - bitcoin/bitcoin#22334 Do not spam about non-existent spk managers (S3RK)
- - bitcoin/bitcoin#22379 Erase spkmans rather than setting to nullptr (achow101)
- - bitcoin/bitcoin#22421 Make IsSegWitOutput return true for taproot outputs (sipa)
- - bitcoin/bitcoin#22461 Change ScriptPubKeyMan::Upgrade default to True (achow101)
- - bitcoin/bitcoin#22492 Reorder locks in dumpwallet to avoid lock order assertion (achow101)
- - bitcoin/bitcoin#22686 Use GetSelectionAmount in ApproximateBestSubset (achow101)

### RPC and other APIs
- - bitcoin/bitcoin#18335, bitcoin/bitcoin#21484 cli: Print useful error if bitcoind rpc work queue exceeded (LarryRuane)
- - bitcoin/bitcoin#18466 Fix invalid parameter error codes for `{sign,verify}message` RPCs (theStack)
- - bitcoin/bitcoin#18772 Calculate fees in `getblock` using BlockUndo data (robot-visions)
- - bitcoin/bitcoin#19033 http: Release work queue after event base finish (promag)
- - bitcoin/bitcoin#19055 Add MuHash3072 implementation (fjahr)
- - bitcoin/bitcoin#19145 Add `hash_type` MUHASH for gettxoutsetinfo (fjahr)
- - bitcoin/bitcoin#19847 Avoid duplicate set lookup in `gettxoutproof` (promag)
- - bitcoin/bitcoin#20286 Deprecate `addresses` and `reqSigs` from RPC outputs (mjdietzx)
- - bitcoin/bitcoin#20459 Fail to return undocumented return values (MarcoFalke)
- - bitcoin/bitcoin#20461 Validate `-rpcauth` arguments (promag)
- - bitcoin/bitcoin#20556 Properly document return values (`submitblock`, `gettxout`, `getblocktemplate`, `scantxoutset`) (MarcoFalke)
- - bitcoin/bitcoin#20755 Remove deprecated fields from `getpeerinfo` (amitiuttarwar)
- - bitcoin/bitcoin#20832 Better error messages for invalid addresses (eilx2)
- - bitcoin/bitcoin#20867 Support up to 20 keys for multisig under Segwit context (darosior)
- - bitcoin/bitcoin#20877 cli: `-netinfo` user help and argument parsing improvements (jonatack)
- - bitcoin/bitcoin#20891 Remove deprecated bumpfee behavior (achow101)
- - bitcoin/bitcoin#20916 Return wtxid from `testmempoolaccept` (MarcoFalke)
- - bitcoin/bitcoin#20917 Add missing signet mentions in network name lists (theStack)
- - bitcoin/bitcoin#20941 Document `RPC_TRANSACTION_ALREADY_IN_CHAIN` exception (jarolrod)
- - bitcoin/bitcoin#20944 Return total fee in `getmempoolinfo` (MarcoFalke)
- - bitcoin/bitcoin#20964 Add specific error code for "wallet already loaded" (laanwj)
- - bitcoin/bitcoin#21053 Document {previous,next}blockhash as optional (theStack)
- - bitcoin/bitcoin#21056 Add a `-rpcwaittimeout` parameter to limit time spent waiting (cdecker)
- - bitcoin/bitcoin#21192 cli: Treat high detail levels as maximum in `-netinfo` (laanwj)
- - bitcoin/bitcoin#21311 Document optional fields for `getchaintxstats` result (theStack)
- - bitcoin/bitcoin#21359 `include_unsafe` option for fundrawtransaction (t-bast)
- - bitcoin/bitcoin#21426 Remove `scantxoutset` EXPERIMENTAL warning (jonatack)
- - bitcoin/bitcoin#21544 Missing doc updates for bumpfee psbt update (MarcoFalke)
- - bitcoin/bitcoin#21594 Add `network` field to `getnodeaddresses` (jonatack)
- - bitcoin/bitcoin#21595, bitcoin/bitcoin#21753 cli: Create `-addrinfo` (jonatack)
- - bitcoin/bitcoin#21602 Add additional ban time fields to `listbanned` (jarolrod)
- - bitcoin/bitcoin#21679 Keep default argument value in correct type (promag)
- - bitcoin/bitcoin#21718 Improve error message for `getblock` invalid datatype (klementtan)
- - bitcoin/bitcoin#21913 RPCHelpMan fixes (kallewoof)
- - bitcoin/bitcoin#22021 `bumpfee`/`psbtbumpfee` fixes and updates (jonatack)
- - bitcoin/bitcoin#22043 `addpeeraddress` test coverage, code simplify/constness (jonatack)
- - bitcoin/bitcoin#22327 cli: Avoid truncating `-rpcwaittimeout` (MarcoFalke)

### GUI
- - bitcoin/bitcoin#18948 Call setParent() in the parent's context (hebasto)
- - bitcoin/bitcoin#20482 Add depends qt fix for ARM macs (jonasschnelli)
- - bitcoin/bitcoin#21836 scripted-diff: Replace three dots with ellipsis in the ui strings (hebasto)
- - bitcoin/bitcoin#21935 Enable external signer support for GUI builds (Sjors)
- - bitcoin/bitcoin#22133 Make QWindowsVistaStylePlugin available again (regression) (hebasto)
- - bitcoin-core/gui#4 UI external signer support (e.g. hardware wallet) (Sjors)
- - bitcoin-core/gui#13 Hide peer detail view if multiple are selected (promag)
- - bitcoin-core/gui#18 Add peertablesortproxy module (hebasto)
- - bitcoin-core/gui#21 Improve pruning tooltip (fluffypony, BitcoinErrorLog)
- - bitcoin-core/gui#72 Log static plugins meta data and used style (hebasto)
- - bitcoin-core/gui#79 Embed monospaced font (hebasto)
- - bitcoin-core/gui#85 Remove unused "What's This" button in dialogs on Windows OS (hebasto)
- - bitcoin-core/gui#115 Replace "Hide tray icon" option with positive "Show tray icon" one (hebasto)
- - bitcoin-core/gui#118 Remove BDB version from the Information tab (hebasto)
- - bitcoin-core/gui#121 Early subscribe core signals in transaction table model (promag)
- - bitcoin-core/gui#123 Do not accept command while executing another one (hebasto)
- - bitcoin-core/gui#125 Enable changing the autoprune block space size in intro dialog (luke-jr)
- - bitcoin-core/gui#138 Unlock encrypted wallet "OK" button bugfix (mjdietzx)
- - bitcoin-core/gui#139 doc: Improve gui/src/qt README.md (jarolrod)
- - bitcoin-core/gui#154 Support macOS Dark mode (goums, Uplab)
- - bitcoin-core/gui#162 Add network to peers window and peer details (jonatack)
- - bitcoin-core/gui#163, bitcoin-core/gui#180 Peer details: replace Direction with Connection Type (jonatack)
- - bitcoin-core/gui#164 Handle peer addition/removal in a right way (hebasto)
- - bitcoin-core/gui#165 Save QSplitter state in QSettings (hebasto)
- - bitcoin-core/gui#173 Follow Qt docs when implementing rowCount and columnCount (hebasto)
- - bitcoin-core/gui#179 Add Type column to peers window, update peer details name/tooltip (jonatack)
- - bitcoin-core/gui#186 Add information to "Confirm fee bump" window (prayank23)
- - bitcoin-core/gui#189 Drop workaround for QTBUG-42503 which was fixed in Qt 5.5.0 (prusnak)
- - bitcoin-core/gui#194 Save/restore RPCConsole geometry only for window (hebasto)
- - bitcoin-core/gui#202 Fix right panel toggle in peers tab (RandyMcMillan)
- - bitcoin-core/gui#203 Display plain "Inbound" in peer details (jonatack)
- - bitcoin-core/gui#204 Drop buggy TableViewLastColumnResizingFixer class (hebasto)
- - bitcoin-core/gui#205, bitcoin-core/gui#229 Save/restore TransactionView and recentRequestsView tables column sizes (hebasto)
- - bitcoin-core/gui#206 Display fRelayTxes and `bip152_highbandwidth_{to, from}` in peer details (jonatack)
- - bitcoin-core/gui#213 Add Copy Address Action to Payment Requests (jarolrod)
- - bitcoin-core/gui#214 Disable requests context menu actions when appropriate (jarolrod)
- - bitcoin-core/gui#217 Make warning label look clickable (jarolrod)
- - bitcoin-core/gui#219 Prevent the main window popup menu (hebasto)
- - bitcoin-core/gui#220 Do not translate file extensions (hebasto)
- - bitcoin-core/gui#221 RPCConsole translatable string fixes and improvements (jonatack)
- - bitcoin-core/gui#226 Add "Last Block" and "Last Tx" rows to peer details area (jonatack)
- - bitcoin-core/gui#233 qt test: Don't bind to regtest port (achow101)
- - bitcoin-core/gui#243 Fix issue when disabling the auto-enabled blank wallet checkbox (jarolrod)
- - bitcoin-core/gui#246 Revert "qt: Use "fusion" style on macOS Big Sur with old Qt" (hebasto)
- - bitcoin-core/gui#248 For values of "Bytes transferred" and "Bytes/s" with 1000-based prefix names use 1000-based divisor instead of 1024-based (wodry)
- - bitcoin-core/gui#251 Improve URI/file handling message (hebasto)
- - bitcoin-core/gui#256 Save/restore column sizes of the tables in the Peers tab (hebasto)
- - bitcoin-core/gui#260 Handle exceptions isntead of crash (hebasto)
- - bitcoin-core/gui#263 Revamp context menus (hebasto)
- - bitcoin-core/gui#271 Don't clear console prompt when font resizing (jarolrod)
- - bitcoin-core/gui#275 Support runtime appearance adjustment on macOS (hebasto)
- - bitcoin-core/gui#276 Elide long strings in their middle in the Peers tab (hebasto)
- - bitcoin-core/gui#281 Set shortcuts for console's resize buttons (jarolrod)
- - bitcoin-core/gui#293 Enable wordWrap for Services (RandyMcMillan)
- - bitcoin-core/gui#296 Do not use QObject::tr plural syntax for numbers with a unit symbol (hebasto)
- - bitcoin-core/gui#297 Avoid unnecessary translations (hebasto)
- - bitcoin-core/gui#298 Peertableview alternating row colors (RandyMcMillan)
- - bitcoin-core/gui#300 Remove progress bar on modal overlay (brunoerg)
- - bitcoin-core/gui#309 Add access to the Peers tab from the network icon (hebasto)
- - bitcoin-core/gui#311 Peers Window rename 'Peer id' to 'Peer' (jarolrod)
- - bitcoin-core/gui#313 Optimize string concatenation by default (hebasto)
- - bitcoin-core/gui#325 Align numbers in the "Peer Id" column to the right (hebasto)
- - bitcoin-core/gui#329 Make console buttons look clickable (jarolrod)
- - bitcoin-core/gui#330 Allow prompt icon to be colorized (jarolrod)
- - bitcoin-core/gui#331 Make RPC console welcome message translation-friendly (hebasto)
- - bitcoin-core/gui#332 Replace disambiguation strings with translator comments (hebasto)
- - bitcoin-core/gui#335 test: Use QSignalSpy instead of QEventLoop (jarolrod)
- - bitcoin-core/gui#343 Improve the GUI responsiveness when progress dialogs are used (hebasto)
- - bitcoin-core/gui#361 Fix GUI segfault caused by bitcoin/bitcoin#22216 (ryanofsky)
- - bitcoin-core/gui#362 Add keyboard shortcuts to context menus (luke-jr)
- - bitcoin-core/gui#366 Dark Mode fixes/portability (luke-jr)
- - bitcoin-core/gui#375 Emit dataChanged signal to dynamically re-sort Peers table (hebasto)
- - bitcoin-core/gui#393 Fix regression in "Encrypt Wallet" menu item (hebasto)
- - bitcoin-core/gui#396 Ensure external signer option remains disabled without signers (achow101)
- - bitcoin-core/gui#406 Handle new added plurals in `bitcoin_en.ts` (hebasto)

### Build system
- - bitcoin/bitcoin#17227 Add Android packaging support (icota)
- - bitcoin/bitcoin#17920 guix: Build support for macOS (dongcarl)
- - bitcoin/bitcoin#18298 Fix Qt processing of configure script for depends with DEBUG=1 (hebasto)
- - bitcoin/bitcoin#19160 multiprocess: Add basic spawn and IPC support (ryanofsky)
- - bitcoin/bitcoin#19504 Bump minimum python version to 3.6 (ajtowns)
- - bitcoin/bitcoin#19522 fix building libconsensus with reduced exports for Darwin targets (fanquake)
- - bitcoin/bitcoin#19683 Pin clang search paths for darwin host (dongcarl)
- - bitcoin/bitcoin#19764 Split boost into build/host packages + bump + cleanup (dongcarl)
- - bitcoin/bitcoin#19817 libtapi 1100.0.11 (fanquake)
- - bitcoin/bitcoin#19846 enable unused member function diagnostic (Zero-1729)
- - bitcoin/bitcoin#19867 Document and cleanup Qt hacks (fanquake)
- - bitcoin/bitcoin#20046 Set `CMAKE_INSTALL_RPATH` for native packages (ryanofsky)
- - bitcoin/bitcoin#20223 Drop the leading 0 from the version number (achow101)
- - bitcoin/bitcoin#20333 Remove `native_biplist` dependency (fanquake)
- - bitcoin/bitcoin#20353 configure: Support -fdebug-prefix-map and -fmacro-prefix-map (ajtowns)
- - bitcoin/bitcoin#20359 Various config.site.in improvements and linting (dongcarl)
- - bitcoin/bitcoin#20413 Require C++17 compiler (MarcoFalke)
- - bitcoin/bitcoin#20419 Set minimum supported macOS to 10.14 (fanquake)
- - bitcoin/bitcoin#20421 miniupnpc 2.2.2 (fanquake)
- - bitcoin/bitcoin#20422 Mac deployment unification (fanquake)
- - bitcoin/bitcoin#20424 Update univalue subtree (MarcoFalke)
- - bitcoin/bitcoin#20449 Fix Windows installer build (achow101)
- - bitcoin/bitcoin#20468 Warn when generating man pages for binaries built from a dirty branch (tylerchambers)
- - bitcoin/bitcoin#20469 Avoid secp256k1.h include from system (dergoegge)
- - bitcoin/bitcoin#20470 Replace genisoimage with xorriso (dongcarl)
- - bitcoin/bitcoin#20471 Use C++17 in depends (fanquake)
- - bitcoin/bitcoin#20496 Drop unneeded macOS framework dependencies (hebasto)
- - bitcoin/bitcoin#20520 Do not force Precompiled Headers (PCH) for building Qt on Linux (hebasto)
- - bitcoin/bitcoin#20549 Support make src/bitcoin-node and src/bitcoin-gui (promag)
- - bitcoin/bitcoin#20565 Ensure PIC build for bdb on Android (BlockMechanic)
- - bitcoin/bitcoin#20594 Fix getauxval calls in randomenv.cpp (jonasschnelli)
- - bitcoin/bitcoin#20603 Update crc32c subtree (MarcoFalke)
- - bitcoin/bitcoin#20609 configure: output notice that test binary is disabled by fuzzing (apoelstra)
- - bitcoin/bitcoin#20619 guix: Quality of life improvements (dongcarl)
- - bitcoin/bitcoin#20629 Improve id string robustness (dongcarl)
- - bitcoin/bitcoin#20641 Use Qt top-level build facilities (hebasto)
- - bitcoin/bitcoin#20650 Drop workaround for a fixed bug in Qt build system (hebasto)
- - bitcoin/bitcoin#20673 Use more legible qmake commands in qt package (hebasto)
- - bitcoin/bitcoin#20684 Define .INTERMEDIATE target once only (hebasto)
- - bitcoin/bitcoin#20720 more robustly check for fcf-protection support (fanquake)
- - bitcoin/bitcoin#20734 Make platform-specific targets available for proper platform builds only (hebasto)
- - bitcoin/bitcoin#20936 build fuzz tests by default (danben)
- - bitcoin/bitcoin#20937 guix: Make nsis reproducible by respecting SOURCE-DATE-EPOCH (dongcarl)
- - bitcoin/bitcoin#20938 fix linking against -latomic when building for riscv (fanquake)
- - bitcoin/bitcoin#20939 fix `RELOC_SECTION` security check for bitcoin-util (fanquake)
- - bitcoin/bitcoin#20963 gitian-linux: Build binaries for 64-bit POWER (continued) (laanwj)
- - bitcoin/bitcoin#21036 gitian: Bump descriptors to focal for 22.0 (fanquake)
- - bitcoin/bitcoin#21045 Adds switch to enable/disable randomized base address in MSVC builds (EthanHeilman)
- - bitcoin/bitcoin#21065 make macOS HOST in download-osx generic (fanquake)
- - bitcoin/bitcoin#21078 guix: only download sources for hosts being built (fanquake)
- - bitcoin/bitcoin#21116 Disable --disable-fuzz-binary for gitian/guix builds (hebasto)
- - bitcoin/bitcoin#21182 remove mostly pointless `BOOST_PROCESS` macro (fanquake)
- - bitcoin/bitcoin#21205 actually fail when Boost is missing (fanquake)
- - bitcoin/bitcoin#21209 use newer source for libnatpmp (fanquake)
- - bitcoin/bitcoin#21226 Fix fuzz binary compilation under windows (danben)
- - bitcoin/bitcoin#21231 Add /opt/homebrew to path to look for boost libraries (fyquah)
- - bitcoin/bitcoin#21239 guix: Add codesignature attachment support for osx+win (dongcarl)
- - bitcoin/bitcoin#21250 Make `HAVE_O_CLOEXEC` available outside LevelDB (bugfix) (theStack)
- - bitcoin/bitcoin#21272 guix: Passthrough `SDK_PATH` into container (dongcarl)
- - bitcoin/bitcoin#21274 assumptions:  Assume C++17 (fanquake)
- - bitcoin/bitcoin#21286 Bump minimum Qt version to 5.9.5 (hebasto)
- - bitcoin/bitcoin#21298 guix: Bump time-machine, glibc, and linux-headers (dongcarl)
- - bitcoin/bitcoin#21304 guix: Add guix-clean script + establish gc-root for container profiles (dongcarl)
- - bitcoin/bitcoin#21320 fix libnatpmp macos cross compile (fanquake)
- - bitcoin/bitcoin#21321 guix: Add curl to required tool list (hebasto)
- - bitcoin/bitcoin#21333 set Unicode true for NSIS installer (fanquake)
- - bitcoin/bitcoin#21339 Make `AM_CONDITIONAL([ENABLE_EXTERNAL_SIGNER])` unconditional (hebasto)
- - bitcoin/bitcoin#21349 Fix fuzz-cuckoocache cross-compiling with DEBUG=1 (hebasto)
- - bitcoin/bitcoin#21354 build, doc: Drop no longer required packages from macOS cross-compiling dependencies (hebasto)
- - bitcoin/bitcoin#21363 build, qt: Improve Qt static plugins/libs check code (hebasto)
- - bitcoin/bitcoin#21375 guix: Misc feedback-based fixes + hier restructuring (dongcarl)
- - bitcoin/bitcoin#21376 Qt 5.12.10 (fanquake)
- - bitcoin/bitcoin#21382 Clean remnants of QTBUG-34748 fix (hebasto)
- - bitcoin/bitcoin#21400 Fix regression introduced in #21363 (hebasto)
- - bitcoin/bitcoin#21403 set --build when configuring packages in depends (fanquake)
- - bitcoin/bitcoin#21421 don't try and use -fstack-clash-protection on Windows (fanquake)
- - bitcoin/bitcoin#21423 Cleanups and follow ups after bumping Qt to 5.12.10 (hebasto)
- - bitcoin/bitcoin#21427 Fix `id_string` invocations (dongcarl)
- - bitcoin/bitcoin#21430 Add -Werror=implicit-fallthrough compile flag (hebasto)
- - bitcoin/bitcoin#21457 Split libtapi and clang out of `native_cctools` (fanquake)
- - bitcoin/bitcoin#21462 guix: Add guix-{attest,verify} scripts (dongcarl)
- - bitcoin/bitcoin#21495 build, qt: Fix static builds on macOS Big Sur (hebasto)
- - bitcoin/bitcoin#21497 Do not opt-in unused CoreWLAN stuff in depends for macOS (hebasto)
- - bitcoin/bitcoin#21543 Enable safe warnings for msvc builds (hebasto)
- - bitcoin/bitcoin#21565 Make `bitcoin_qt.m4` more generic (fanquake)
- - bitcoin/bitcoin#21610 remove -Wdeprecated-register from NOWARN flags (fanquake)
- - bitcoin/bitcoin#21613 enable -Wdocumentation (fanquake)
- - bitcoin/bitcoin#21629 Fix configuring when building depends with `NO_BDB=1` (fanquake)
- - bitcoin/bitcoin#21654 build, qt: Make Qt rcc output always deterministic (hebasto)
- - bitcoin/bitcoin#21655 build, qt: No longer need to set `QT_RCC_TEST=1` for determinism (hebasto)
- - bitcoin/bitcoin#21658 fix make deploy for arm64-darwin (sgulls)
- - bitcoin/bitcoin#21694 Use XLIFF file to provide more context to Transifex translators (hebasto)
- - bitcoin/bitcoin#21708, bitcoin/bitcoin#21593 Drop pointless sed commands (hebasto)
- - bitcoin/bitcoin#21731 Update msvc build to use Qt5.12.10 binaries (sipsorcery)
- - bitcoin/bitcoin#21733 Re-add command to install vcpkg (dplusplus1024)
- - bitcoin/bitcoin#21793 Use `-isysroot` over `--sysroot` on macOS (fanquake)
- - bitcoin/bitcoin#21869 Add missing `-D_LIBCPP_DEBUG=1` to debug flags (MarcoFalke)
- - bitcoin/bitcoin#21889 macho: check for control flow instrumentation (fanquake)
- - bitcoin/bitcoin#21920 Improve macro for testing -latomic requirement (MarcoFalke)
- - bitcoin/bitcoin#21991 libevent 2.1.12-stable (fanquake)
- - bitcoin/bitcoin#22054 Bump Qt version to 5.12.11 (hebasto)
- - bitcoin/bitcoin#22063 Use Qt archive of the same version as the compiled binaries (hebasto)
- - bitcoin/bitcoin#22070 Don't use cf-protection when targeting arm-apple-darwin (fanquake)
- - bitcoin/bitcoin#22071 Latest config.guess and config.sub (fanquake)
- - bitcoin/bitcoin#22075 guix: Misc leftover usability improvements (dongcarl)
- - bitcoin/bitcoin#22123 Fix qt.mk for mac arm64 (promag)
- - bitcoin/bitcoin#22174 build, qt: Fix libraries linking order for Linux hosts (hebasto)
- - bitcoin/bitcoin#22182 guix: Overhaul how guix-{attest,verify} works and hierarchy (dongcarl)
- - bitcoin/bitcoin#22186 build, qt: Fix compiling qt package in depends with GCC 11 (hebasto)
- - bitcoin/bitcoin#22199 macdeploy: minor fixups and simplifications (fanquake)
- - bitcoin/bitcoin#22230 Fix MSVC linker /SubSystem option for bitcoin-qt.exe (hebasto)
- - bitcoin/bitcoin#22234 Mark print-% target as phony (dgoncharov)
- - bitcoin/bitcoin#22238 improve detection of eBPF support (fanquake)
- - bitcoin/bitcoin#22258 Disable deprecated-copy warning only when external warnings are enabled (MarcoFalke)
- - bitcoin/bitcoin#22320 set minimum required Boost to 1.64.0 (fanquake)
- - bitcoin/bitcoin#22348 Fix cross build for Windows with Boost Process (hebasto)
- - bitcoin/bitcoin#22365 guix: Avoid relying on newer symbols by rebasing our cross toolchains on older glibcs (dongcarl)
- - bitcoin/bitcoin#22381 guix: Test security-check sanity before performing them (with macOS) (fanquake)
- - bitcoin/bitcoin#22405 Remove --enable-glibc-back-compat from Guix build (fanquake)
- - bitcoin/bitcoin#22406 Remove --enable-determinism configure option (fanquake)
- - bitcoin/bitcoin#22410 Avoid GCC 7.1 ABI change warning in guix build (sipa)
- - bitcoin/bitcoin#22436 use aarch64 Clang if cross-compiling for darwin on aarch64 (fanquake)
- - bitcoin/bitcoin#22465 guix: Pin kernel-header version, time-machine to upstream 1.3.0 commit (dongcarl)
- - bitcoin/bitcoin#22511 guix: Silence `getent(1)` invocation, doc fixups (dongcarl)
- - bitcoin/bitcoin#22531 guix: Fixes to guix-{attest,verify} (achow101)
- - bitcoin/bitcoin#22642 release: Release with separate sha256sums and sig files (dongcarl)
- - bitcoin/bitcoin#22685 clientversion: No suffix `#if CLIENT_VERSION_IS_RELEASE` (dongcarl)
- - bitcoin/bitcoin#22713 Fix build with Boost 1.77.0 (sizeofvoid)

### Tests and QA
- - bitcoin/bitcoin#14604 Add test and refactor `feature_block.py` (sanket1729)
- - bitcoin/bitcoin#17556 Change `feature_config_args.py` not to rely on strange regtest=0 behavior (ryanofsky)
- - bitcoin/bitcoin#18795 wallet issue with orphaned rewards (domob1812)
- - bitcoin/bitcoin#18847 compressor: Use a prevector in CompressScript serialization (jb55)
- - bitcoin/bitcoin#19259 fuzz: Add fuzzing harness for LoadMempool(?) and DumpMempool(?) (practicalswift)
- - bitcoin/bitcoin#19315 Allow outbound & block-relay-only connections in functional tests. (amitiuttarwar)
- - bitcoin/bitcoin#19698 Apply strict verification flags for transaction tests and assert backwards compatibility (glozow)
- - bitcoin/bitcoin#19801 Check for all possible `OP_CLTV` fail reasons in `feature_cltv.py` (BIP 65) (theStack)
- - bitcoin/bitcoin#19893 Remove or explain syncwithvalidationinterfacequeue (MarcoFalke)
- - bitcoin/bitcoin#19972 fuzz: Add fuzzing harness for node eviction logic (practicalswift)
- - bitcoin/bitcoin#19982 Fix inconsistent lock order in `wallet_tests/CreateWallet` (hebasto)
- - bitcoin/bitcoin#20000 Fix creation of "std::string"s with \0s (vasild)
- - bitcoin/bitcoin#20047 Use `wait_for_{block,header}` helpers in `p2p_fingerprint.py` (theStack)
- - bitcoin/bitcoin#20171 Add functional test `test_txid_inv_delay` (ariard)
- - bitcoin/bitcoin#20189 Switch to BIP341's suggested scheme for outputs without script (sipa)
- - bitcoin/bitcoin#20248 Fix length of R check in `key_signature_tests` (dgpv)
- - bitcoin/bitcoin#20276, bitcoin/bitcoin#20385, bitcoin/bitcoin#20688, bitcoin/bitcoin#20692 Run various mempool tests even with wallet disabled (mjdietzx)
- - bitcoin/bitcoin#20323 Create or use existing properly initialized NodeContexts (dongcarl)
- - bitcoin/bitcoin#20354 Add `feature_taproot.py --previous_release` (MarcoFalke)
- - bitcoin/bitcoin#20370 fuzz: Version handshake (MarcoFalke)
- - bitcoin/bitcoin#20377 fuzz: Fill various small fuzzing gaps (practicalswift)
- - bitcoin/bitcoin#20425 fuzz: Make CAddrMan fuzzing harness deterministic (practicalswift)
- - bitcoin/bitcoin#20430 Sanitizers: Add suppression for unsigned-integer-overflow in libstdc++ (jonasschnelli)
- - bitcoin/bitcoin#20437 fuzz: Avoid time-based "non-determinism" in fuzzing harnesses by using mocked GetTime() (practicalswift)
- - bitcoin/bitcoin#20458 Add `is_bdb_compiled` helper (Sjors)
- - bitcoin/bitcoin#20466 Fix intermittent `p2p_fingerprint` issue (MarcoFalke)
- - bitcoin/bitcoin#20472 Add testing of ParseInt/ParseUInt edge cases with leading +/-/0:s (practicalswift)
- - bitcoin/bitcoin#20507 sync: print proper lock order location when double lock is detected (vasild)
- - bitcoin/bitcoin#20522 Fix sync issue in `disconnect_p2ps` (amitiuttarwar)
- - bitcoin/bitcoin#20524 Move `MIN_VERSION_SUPPORTED` to p2p.py (jnewbery)
- - bitcoin/bitcoin#20540 Fix `wallet_multiwallet` issue on windows (MarcoFalke)
- - bitcoin/bitcoin#20560 fuzz: Link all targets once (MarcoFalke)
- - bitcoin/bitcoin#20567 Add option to git-subtree-check to do full check, add help (laanwj)
- - bitcoin/bitcoin#20569 Fix intermittent `wallet_multiwallet` issue with `got_loading_error` (MarcoFalke)
- - bitcoin/bitcoin#20613 Use Popen.wait instead of RPC in `assert_start_raises_init_error` (MarcoFalke)
- - bitcoin/bitcoin#20663 fuzz: Hide `script_assets_test_minimizer` (MarcoFalke)
- - bitcoin/bitcoin#20674 fuzz: Call SendMessages after ProcessMessage to increase coverage (MarcoFalke)
- - bitcoin/bitcoin#20683 Fix restart node race (MarcoFalke)
- - bitcoin/bitcoin#20686 fuzz: replace CNode code with fuzz/util.h::ConsumeNode() (jonatack)
- - bitcoin/bitcoin#20733 Inline non-member functions with body in fuzzing headers (pstratem)
- - bitcoin/bitcoin#20737 Add missing assignment in `mempool_resurrect.py` (MarcoFalke)
- - bitcoin/bitcoin#20745 Correct `epoll_ctl` data race suppression (hebasto)
- - bitcoin/bitcoin#20748 Add race:SendZmqMessage tsan suppression (MarcoFalke)
- - bitcoin/bitcoin#20760 Set correct nValue for multi-op-return policy check (MarcoFalke)
- - bitcoin/bitcoin#20761 fuzz: Check that `NULL_DATA` is unspendable (MarcoFalke)
- - bitcoin/bitcoin#20765 fuzz: Check that certain script TxoutType are nonstandard (mjdietzx)
- - bitcoin/bitcoin#20772 fuzz: Bolster ExtractDestination(s) checks (mjdietzx)
- - bitcoin/bitcoin#20789 fuzz: Rework strong and weak net enum fuzzing (MarcoFalke)
- - bitcoin/bitcoin#20828 fuzz: Introduce CallOneOf helper to replace switch-case (MarcoFalke)
- - bitcoin/bitcoin#20839 fuzz: Avoid extraneous copy of input data, using Span<> (MarcoFalke)
- - bitcoin/bitcoin#20844 Add sanitizer suppressions for AMD EPYC CPUs (MarcoFalke)
- - bitcoin/bitcoin#20857 Update documentation in `feature_csv_activation.py` (PiRK)
- - bitcoin/bitcoin#20876 Replace getmempoolentry with testmempoolaccept in MiniWallet (MarcoFalke)
- - bitcoin/bitcoin#20881 fuzz: net permission flags in net processing (MarcoFalke)
- - bitcoin/bitcoin#20882 fuzz: Add missing muhash registration (MarcoFalke)
- - bitcoin/bitcoin#20908 fuzz: Use mocktime in `process_message*` fuzz targets (MarcoFalke)
- - bitcoin/bitcoin#20915 fuzz: Fail if message type is not fuzzed (MarcoFalke)
- - bitcoin/bitcoin#20946 fuzz: Consolidate fuzzing TestingSetup initialization (dongcarl)
- - bitcoin/bitcoin#20954 Declare `nodes` type `in test_framework.py` (kiminuo)
- - bitcoin/bitcoin#20955 Fix `get_previous_releases.py` for aarch64 (MarcoFalke)
- - bitcoin/bitcoin#20969 check that getblockfilter RPC fails without block filter index (theStack)
- - bitcoin/bitcoin#20971 Work around libFuzzer deadlock (MarcoFalke)
- - bitcoin/bitcoin#20993 Store subversion (user agent) as string in `msg_version` (theStack)
- - bitcoin/bitcoin#20995 fuzz: Avoid initializing version to less than `MIN_PEER_PROTO_VERSION` (MarcoFalke)
- - bitcoin/bitcoin#20998 Fix BlockToJsonVerbose benchmark (martinus)
- - bitcoin/bitcoin#21003 Move MakeNoLogFileContext to `libtest_util`, and use it in bench (MarcoFalke)
- - bitcoin/bitcoin#21008 Fix zmq test flakiness, improve speed (theStack)
- - bitcoin/bitcoin#21023 fuzz: Disable shuffle when merge=1 (MarcoFalke)
- - bitcoin/bitcoin#21037 fuzz: Avoid designated initialization (C++20) in fuzz tests (practicalswift)
- - bitcoin/bitcoin#21042 doc, test: Improve `setup_clean_chain` documentation (fjahr)
- - bitcoin/bitcoin#21080 fuzz: Configure check for main function (take 2) (MarcoFalke)
- - bitcoin/bitcoin#21084 Fix timeout decrease in `feature_assumevalid` (brunoerg)
- - bitcoin/bitcoin#21096 Re-add dead code detection (flack)
- - bitcoin/bitcoin#21100 Remove unused function `xor_bytes` (theStack)
- - bitcoin/bitcoin#21115 Fix Windows cross build (hebasto)
- - bitcoin/bitcoin#21117 Remove `assert_blockchain_height` (MarcoFalke)
- - bitcoin/bitcoin#21121 Small unit test improvements, including helper to make mempool transaction (amitiuttarwar)
- - bitcoin/bitcoin#21124 Remove unnecessary assignment in bdb (brunoerg)
- - bitcoin/bitcoin#21125 Change `BOOST_CHECK` to `BOOST_CHECK_EQUAL` for paths (kiminuo)
- - bitcoin/bitcoin#21142, bitcoin/bitcoin#21512 fuzz: Add `tx_pool` fuzz target (MarcoFalke)
- - bitcoin/bitcoin#21165 Use mocktime in `test_seed_peers` (dhruv)
- - bitcoin/bitcoin#21169 fuzz: Add RPC interface fuzzing. Increase fuzzing coverage from 65% to 70% (practicalswift)
- - bitcoin/bitcoin#21170 bench: Add benchmark to write json into a string (martinus)
- - bitcoin/bitcoin#21178 Run `mempool_reorg.py` even with wallet disabled (DariusParvin)
- - bitcoin/bitcoin#21185 fuzz: Remove expensive and redundant muhash from crypto fuzz target (MarcoFalke)
- - bitcoin/bitcoin#21200 Speed up `rpc_blockchain.py` by removing miniwallet.generate() (MarcoFalke)
- - bitcoin/bitcoin#21211 Move `P2WSH_OP_TRUE` to shared test library (MarcoFalke)
- - bitcoin/bitcoin#21228 Avoid comparision of integers with different signs (jonasschnelli)
- - bitcoin/bitcoin#21230 Fix `NODE_NETWORK_LIMITED_MIN_BLOCKS` disconnection (MarcoFalke)
- - bitcoin/bitcoin#21252 Add missing wait for sync to `feature_blockfilterindex_prune` (MarcoFalke)
- - bitcoin/bitcoin#21254 Avoid connecting to real network when running tests (MarcoFalke)
- - bitcoin/bitcoin#21264 fuzz: Two scripted diff renames (MarcoFalke)
- - bitcoin/bitcoin#21280 Bug fix in `transaction_tests` (glozow)
- - bitcoin/bitcoin#21293 Replace accidentally placed bit-OR with logical-OR (hebasto)
- - bitcoin/bitcoin#21297 `feature_blockfilterindex_prune.py` improvements (jonatack)
- - bitcoin/bitcoin#21310 zmq test: fix sync-up by matching notification to generated block (theStack)
- - bitcoin/bitcoin#21334 Additional BIP9 tests (Sjors)
- - bitcoin/bitcoin#21338 Add functional test for anchors.dat (brunoerg)
- - bitcoin/bitcoin#21345 Bring `p2p_leak.py` up to date (mzumsande)
- - bitcoin/bitcoin#21357 Unconditionally check for fRelay field in test framework (jarolrod)
- - bitcoin/bitcoin#21358 fuzz: Add missing include (`test/util/setup_common.h`) (MarcoFalke)
- - bitcoin/bitcoin#21371 fuzz: fix gcc Woverloaded-virtual build warnings (jonatack)
- - bitcoin/bitcoin#21373 Generate fewer blocks in `feature_nulldummy` to fix timeouts, speed up (jonatack)
- - bitcoin/bitcoin#21390 Test improvements for UTXO set hash tests (fjahr)
- - bitcoin/bitcoin#21410 increase `rpc_timeout` for fundrawtx `test_transaction_too_large` (jonatack)
- - bitcoin/bitcoin#21411 add logging, reduce blocks, move `sync_all` in `wallet_` groups (jonatack)
- - bitcoin/bitcoin#21438 Add ParseUInt8() test coverage (jonatack)
- - bitcoin/bitcoin#21443 fuzz: Implement `fuzzed_dns_lookup_function` as a lambda (practicalswift)
- - bitcoin/bitcoin#21445 cirrus: Use SSD cluster for speedup (MarcoFalke)
- - bitcoin/bitcoin#21477 Add test for CNetAddr::ToString IPv6 address formatting (RFC 5952) (practicalswift)
- - bitcoin/bitcoin#21487 fuzz: Use ConsumeWeakEnum in addrman for service flags (MarcoFalke)
- - bitcoin/bitcoin#21488 Add ParseUInt16() unit test and fuzz coverage (jonatack)
- - bitcoin/bitcoin#21491 test: remove duplicate assertions in util_tests (jonatack)
- - bitcoin/bitcoin#21522 fuzz: Use PickValue where possible (MarcoFalke)
- - bitcoin/bitcoin#21531 remove qt byteswap compattests (fanquake)
- - bitcoin/bitcoin#21557 small cleanup in RPCNestedTests tests (fanquake)
- - bitcoin/bitcoin#21586 Add missing suppression for signed-integer-overflow:txmempool.cpp (MarcoFalke)
- - bitcoin/bitcoin#21592 Remove option to make TestChain100Setup non-deterministic (MarcoFalke)
- - bitcoin/bitcoin#21597 Document `race:validation_chainstatemanager_tests` suppression (MarcoFalke)
- - bitcoin/bitcoin#21599 Replace file level integer overflow suppression with function level suppression (practicalswift)
- - bitcoin/bitcoin#21604 Document why no symbol names can be used for suppressions (MarcoFalke)
- - bitcoin/bitcoin#21606 fuzz: Extend psbt fuzz target a bit (MarcoFalke)
- - bitcoin/bitcoin#21617 fuzz: Fix uninitialized read in i2p test (MarcoFalke)
- - bitcoin/bitcoin#21630 fuzz: split FuzzedSock interface and implementation (vasild)
- - bitcoin/bitcoin#21634 Skip SQLite fsyncs while testing (achow101)
- - bitcoin/bitcoin#21669 Remove spurious double lock tsan suppressions by bumping to clang-12 (MarcoFalke)
- - bitcoin/bitcoin#21676 Use mocktime to avoid intermittent failure in `rpc_tests` (MarcoFalke)
- - bitcoin/bitcoin#21677 fuzz: Avoid use of low file descriptor ids (which may be in use) in FuzzedSock (practicalswift)
- - bitcoin/bitcoin#21678 Fix TestPotentialDeadLockDetected suppression (hebasto)
- - bitcoin/bitcoin#21689 Remove intermittently failing and not very meaningful `BOOST_CHECK` in `cnetaddr_basic` (practicalswift)
- - bitcoin/bitcoin#21691 Check that no versionbits are re-used (MarcoFalke)
- - bitcoin/bitcoin#21707 Extend functional tests for addr relay (mzumsande)
- - bitcoin/bitcoin#21712 Test default `include_mempool` value of gettxout (promag)
- - bitcoin/bitcoin#21738 Use clang-12 for ASAN, Add missing suppression (MarcoFalke)
- - bitcoin/bitcoin#21740 add new python linter to check file names and permissions (windsok)
- - bitcoin/bitcoin#21749 Bump shellcheck version (hebasto)
- - bitcoin/bitcoin#21754 Run `feature_cltv` with MiniWallet (MarcoFalke)
- - bitcoin/bitcoin#21762 Speed up `mempool_spend_coinbase.py` (MarcoFalke)
- - bitcoin/bitcoin#21773 fuzz: Ensure prevout is consensus-valid (MarcoFalke)
- - bitcoin/bitcoin#21777 Fix `feature_notifications.py` intermittent issue (MarcoFalke)
- - bitcoin/bitcoin#21785 Fix intermittent issue in `p2p_addr_relay.py` (MarcoFalke)
- - bitcoin/bitcoin#21787 Fix off-by-ones in `rpc_fundrawtransaction` assertions (jonatack)
- - bitcoin/bitcoin#21792 Fix intermittent issue in `p2p_segwit.py` (MarcoFalke)
- - bitcoin/bitcoin#21795 fuzz: Terminate immediately if a fuzzing harness tries to perform a DNS lookup (belt and suspenders) (practicalswift)
- - bitcoin/bitcoin#21798 fuzz: Create a block template in `tx_pool` targets (MarcoFalke)
- - bitcoin/bitcoin#21804 Speed up `p2p_segwit.py` (jnewbery)
- - bitcoin/bitcoin#21810 fuzz: Various RPC fuzzer follow-ups (practicalswift)
- - bitcoin/bitcoin#21814 Fix `feature_config_args.py` intermittent issue (MarcoFalke)
- - bitcoin/bitcoin#21821 Add missing test for empty P2WSH redeem (MarcoFalke)
- - bitcoin/bitcoin#21822 Resolve bug in `interface_bitcoin_cli.py` (klementtan)
- - bitcoin/bitcoin#21846 fuzz: Add `-fsanitize=integer` suppression needed for RPC fuzzer (`generateblock`) (practicalswift)
- - bitcoin/bitcoin#21849 fuzz: Limit toxic test globals to their respective scope (MarcoFalke)
- - bitcoin/bitcoin#21867 use MiniWallet for `p2p_blocksonly.py` (theStack)
- - bitcoin/bitcoin#21873 minor fixes & improvements for files linter test (windsok)
- - bitcoin/bitcoin#21874 fuzz: Add `WRITE_ALL_FUZZ_TARGETS_AND_ABORT` (MarcoFalke)
- - bitcoin/bitcoin#21884 fuzz: Remove unused --enable-danger-fuzz-link-all option (MarcoFalke)
- - bitcoin/bitcoin#21890 fuzz: Limit ParseISO8601DateTime fuzzing to 32-bit (MarcoFalke)
- - bitcoin/bitcoin#21891 fuzz: Remove strprintf test cases that are known to fail (MarcoFalke)
- - bitcoin/bitcoin#21892 fuzz: Avoid excessively large min fee rate in `tx_pool` (MarcoFalke)
- - bitcoin/bitcoin#21895 Add TSA annotations to the WorkQueue class members (hebasto)
- - bitcoin/bitcoin#21900 use MiniWallet for `feature_csv_activation.py` (theStack)
- - bitcoin/bitcoin#21909 fuzz: Limit max insertions in timedata fuzz test (MarcoFalke)
- - bitcoin/bitcoin#21922 fuzz: Avoid timeout in EncodeBase58 (MarcoFalke)
- - bitcoin/bitcoin#21927 fuzz: Run const CScript member functions only once (MarcoFalke)
- - bitcoin/bitcoin#21929 fuzz: Remove incorrect float round-trip serialization test (MarcoFalke)
- - bitcoin/bitcoin#21936 fuzz: Terminate immediately if a fuzzing harness tries to create a TCP socket (belt and suspenders) (practicalswift)
- - bitcoin/bitcoin#21941 fuzz: Call const member functions in addrman fuzz test only once (MarcoFalke)
- - bitcoin/bitcoin#21945 add P2PK support to MiniWallet (theStack)
- - bitcoin/bitcoin#21948 Fix off-by-one in mockscheduler test RPC (MarcoFalke)
- - bitcoin/bitcoin#21953 fuzz: Add `utxo_snapshot` target (MarcoFalke)
- - bitcoin/bitcoin#21970 fuzz: Add missing CheckTransaction before CheckTxInputs (MarcoFalke)
- - bitcoin/bitcoin#21989 Use `COINBASE_MATURITY` in functional tests (kiminuo)
- - bitcoin/bitcoin#22003 Add thread safety annotations (ajtowns)
- - bitcoin/bitcoin#22004 fuzz: Speed up transaction fuzz target (MarcoFalke)
- - bitcoin/bitcoin#22005 fuzz: Speed up banman fuzz target (MarcoFalke)
- - bitcoin/bitcoin#22029 [fuzz] Improve transport deserialization fuzz test coverage (dhruv)
- - bitcoin/bitcoin#22048 MiniWallet: introduce enum type for output mode (theStack)
- - bitcoin/bitcoin#22057 use MiniWallet (P2PK mode) for `feature_dersig.py` (theStack)
- - bitcoin/bitcoin#22065 Mark `CheckTxInputs` `[[nodiscard]]`. Avoid UUM in fuzzing harness `coins_view` (practicalswift)
- - bitcoin/bitcoin#22069 fuzz: don't try and use fopencookie() when building for Android (fanquake)
- - bitcoin/bitcoin#22082 update nanobench from release 4.0.0 to 4.3.4 (martinus)
- - bitcoin/bitcoin#22086 remove BasicTestingSetup from unit tests that don't need it (fanquake)
- - bitcoin/bitcoin#22089 MiniWallet: fix fee calculation for P2PK and check tx vsize (theStack)
- - bitcoin/bitcoin#21107, bitcoin/bitcoin#22092 Convert documentation into type annotations (fanquake)
- - bitcoin/bitcoin#22095 Additional BIP32 test vector for hardened derivation with leading zeros (kristapsk)
- - bitcoin/bitcoin#22103 Fix IPv6 check on BSD systems (n-thumann)
- - bitcoin/bitcoin#22118 check anchors.dat when node starts for the first time (brunoerg)
- - bitcoin/bitcoin#22120 `p2p_invalid_block`: Check that a block rejected due to too-new tim? (willcl-ark)
- - bitcoin/bitcoin#22153 Fix `p2p_leak.py` intermittent failure (mzumsande)
- - bitcoin/bitcoin#22169 p2p, rpc, fuzz: various tiny follow-ups (jonatack)
- - bitcoin/bitcoin#22176 Correct outstanding -Werror=sign-compare errors (Empact)
- - bitcoin/bitcoin#22180 fuzz: Increase branch coverage of the float fuzz target (MarcoFalke)
- - bitcoin/bitcoin#22187 Add `sync_blocks` in `wallet_orphanedreward.py` (domob1812)
- - bitcoin/bitcoin#22201 Fix TestShell to allow running in Jupyter Notebook (josibake)
- - bitcoin/bitcoin#22202 Add temporary coinstats suppressions (MarcoFalke)
- - bitcoin/bitcoin#22203 Use ConnmanTestMsg from test lib in `denialofservice_tests` (MarcoFalke)
- - bitcoin/bitcoin#22210 Use MiniWallet in `test_no_inherited_signaling` RBF test (MarcoFalke)
- - bitcoin/bitcoin#22224 Update msvc and appveyor builds to use Qt5.12.11 binaries (sipsorcery)
- - bitcoin/bitcoin#22249 Kill process group to avoid dangling processes when using `--failfast` (S3RK)
- - bitcoin/bitcoin#22267 fuzz: Speed up crypto fuzz target (MarcoFalke)
- - bitcoin/bitcoin#22270 Add bitcoin-util tests (+refactors) (MarcoFalke)
- - bitcoin/bitcoin#22271 fuzz: Assert roundtrip equality for `CPubKey` (theStack)
- - bitcoin/bitcoin#22279 fuzz: add missing ECCVerifyHandle to `base_encode_decode` (apoelstra)
- - bitcoin/bitcoin#22292 bench, doc: benchmarking updates and fixups (jonatack)
- - bitcoin/bitcoin#22306 Improvements to `p2p_addr_relay.py` (amitiuttarwar)
- - bitcoin/bitcoin#22310 Add functional test for replacement relay fee check (ariard)
- - bitcoin/bitcoin#22311 Add missing syncwithvalidationinterfacequeue in `p2p_blockfilters` (MarcoFalke)
- - bitcoin/bitcoin#22313 Add missing `sync_all` to `feature_coinstatsindex` (MarcoFalke)
- - bitcoin/bitcoin#22322 fuzz: Check banman roundtrip (MarcoFalke)
- - bitcoin/bitcoin#22363 Use `script_util` helpers for creating P2{PKH,SH,WPKH,WSH} scripts (theStack)
- - bitcoin/bitcoin#22399 fuzz: Rework CTxDestination fuzzing (MarcoFalke)
- - bitcoin/bitcoin#22408 add tests for `bad-txns-prevout-null` reject reason (theStack)
- - bitcoin/bitcoin#22445 fuzz: Move implementations of non-template fuzz helpers from util.h to util.cpp (sriramdvt)
- - bitcoin/bitcoin#22446 Fix `wallet_listdescriptors.py` if bdb is not compiled (hebasto)
- - bitcoin/bitcoin#22447 Whitelist `rpc_rawtransaction` peers to speed up tests (jonatack)
- - bitcoin/bitcoin#22742 Use proper target in `do_fund_send` (S3RK)

### Miscellaneous
- - bitcoin/bitcoin#19337 sync: Detect double lock from the same thread (vasild)
- - bitcoin/bitcoin#19809 log: Prefix log messages with function name and source code location if -logsourcelocations is set (practicalswift)
- - bitcoin/bitcoin#19866 eBPF Linux tracepoints (jb55)
- - bitcoin/bitcoin#20024 init: Fix incorrect warning "Reducing -maxconnections from N to N-1, because of system limitations" (practicalswift)
- - bitcoin/bitcoin#20145 contrib: Add getcoins.py script to get coins from (signet) faucet (kallewoof)
- - bitcoin/bitcoin#20255 util: Add assume() identity function (MarcoFalke)
- - bitcoin/bitcoin#20288 script, doc: Contrib/seeds updates (jonatack)
- - bitcoin/bitcoin#20358 src/randomenv.cpp: Fix build on uclibc (ffontaine)
- - bitcoin/bitcoin#20406 util: Avoid invalid integer negation in formatmoney and valuefromamount (practicalswift)
- - bitcoin/bitcoin#20434 contrib: Parse elf directly for symbol and security checks (laanwj)
- - bitcoin/bitcoin#20451 lint: Run mypy over contrib/devtools (fanquake)
- - bitcoin/bitcoin#20476 contrib: Add test for elf symbol-check (laanwj)
- - bitcoin/bitcoin#20530 lint: Update cppcheck linter to c++17 and improve explicit usage (fjahr)
- - bitcoin/bitcoin#20589 log: Clarify that failure to read/write `fee_estimates.dat` is non-fatal (MarcoFalke)
- - bitcoin/bitcoin#20602 util: Allow use of c++14 chrono literals (MarcoFalke)
- - bitcoin/bitcoin#20605 init: Signal-safe instant shutdown (laanwj)
- - bitcoin/bitcoin#20608 contrib: Add symbol check test for PE binaries (fanquake)
- - bitcoin/bitcoin#20689 contrib: Replace binary verification script verify.sh with python rewrite (theStack)
- - bitcoin/bitcoin#20715 util: Add argsmanager::getcommand() and use it in bitcoin-wallet (MarcoFalke)
- - bitcoin/bitcoin#20735 script: Remove outdated extract-osx-sdk.sh (hebasto)
- - bitcoin/bitcoin#20817 lint: Update list of spelling linter false positives, bump to codespell 2.0.0 (theStack)
- - bitcoin/bitcoin#20884 script: Improve robustness of bitcoind.service on startup (hebasto)
- - bitcoin/bitcoin#20906 contrib: Embed c++11 patch in `install_db4.sh` (gruve-p)
- - bitcoin/bitcoin#21004 contrib: Fix docker args conditional in gitian-build (setpill)
- - bitcoin/bitcoin#21007 bitcoind: Add -daemonwait option to wait for initialization (laanwj)
- - bitcoin/bitcoin#21041 log: Move "Pre-allocating up to position 0x[?] in [?].dat" log message to debug category (practicalswift)
- - bitcoin/bitcoin#21059 Drop boost/preprocessor dependencies (hebasto)
- - bitcoin/bitcoin#21087 guix: Passthrough `BASE_CACHE` into container (dongcarl)
- - bitcoin/bitcoin#21088 guix: Jump forwards in time-machine and adapt (dongcarl)
- - bitcoin/bitcoin#21089 guix: Add support for powerpc64{,le} (dongcarl)
- - bitcoin/bitcoin#21110 util: Remove boost `posix_time` usage from `gettime*` (fanquake)
- - bitcoin/bitcoin#21111 Improve OpenRC initscript (parazyd)
- - bitcoin/bitcoin#21123 code style: Add EditorConfig file (kiminuo)
- - bitcoin/bitcoin#21173 util: Faster hexstr => 13% faster blocktojson (martinus)
- - bitcoin/bitcoin#21221 tools: Allow argument/parameter bin packing in clang-format (jnewbery)
- - bitcoin/bitcoin#21244 Move GetDataDir to ArgsManager (kiminuo)
- - bitcoin/bitcoin#21255 contrib: Run test-symbol-check for risc-v (fanquake)
- - bitcoin/bitcoin#21271 guix: Explicitly set umask in build container (dongcarl)
- - bitcoin/bitcoin#21300 script: Add explanatory comment to tc.sh (dscotese)
- - bitcoin/bitcoin#21317 util: Make assume() usable as unary expression (MarcoFalke)
- - bitcoin/bitcoin#21336 Make .gitignore ignore src/test/fuzz/fuzz.exe (hebasto)
- - bitcoin/bitcoin#21337 guix: Update darwin native packages dependencies (hebasto)
- - bitcoin/bitcoin#21405 compat: remove memcpy -> memmove backwards compatibility alias (fanquake)
- - bitcoin/bitcoin#21418 contrib: Make systemd invoke dependencies only when ready (laanwj)
- - bitcoin/bitcoin#21447 Always add -daemonwait to known command line arguments (hebasto)
- - bitcoin/bitcoin#21471 bugfix: Fix `bech32_encode` calls in `gen_key_io_test_vectors.py` (sipa)
- - bitcoin/bitcoin#21615 script: Add trusted key for hebasto (hebasto)
- - bitcoin/bitcoin#21664 contrib: Use lief for macos and windows symbol & security checks (fanquake)
- - bitcoin/bitcoin#21695 contrib: Remove no longer used contrib/bitcoin-qt.pro (hebasto)
- - bitcoin/bitcoin#21711 guix: Add full installation and usage documentation (dongcarl)
- - bitcoin/bitcoin#21799 guix: Use `gcc-8` across the board (dongcarl)
- - bitcoin/bitcoin#21802 Avoid UB in util/asmap (advance a dereferenceable iterator outside its valid range) (MarcoFalke)
- - bitcoin/bitcoin#21823 script: Update reviewers (jonatack)
- - bitcoin/bitcoin#21850 Remove `GetDataDir(net_specific)` function (kiminuo)
- - bitcoin/bitcoin#21871 scripts: Add checks for minimum required os versions (fanquake)
- - bitcoin/bitcoin#21966 Remove double serialization; use software encoder for fee estimation (sipa)
- - bitcoin/bitcoin#22060 contrib: Add torv3 seed nodes for testnet, drop v2 ones (laanwj)
- - bitcoin/bitcoin#22244 devtools: Correctly extract symbol versions in symbol-check (laanwj)
- - bitcoin/bitcoin#22533 guix/build: Remove vestigial SKIPATTEST.TAG (dongcarl)
- - bitcoin/bitcoin#22643 guix-verify: Non-zero exit code when anything fails (dongcarl)
- - bitcoin/bitcoin#22654 guix: Don't include directory name in SHA256SUMS (achow101)

### Documentation
- - bitcoin/bitcoin#15451 clarify getdata limit after #14897 (HashUnlimited)
- - bitcoin/bitcoin#15545 Explain why CheckBlock() is called before AcceptBlock (Sjors)
- - bitcoin/bitcoin#17350 Add developer documentation to isminetype (HAOYUatHZ)
- - bitcoin/bitcoin#17934 Use `CONFIG_SITE` variable instead of --prefix option (hebasto)
- - bitcoin/bitcoin#18030 Coin::IsSpent() can also mean never existed (Sjors)
- - bitcoin/bitcoin#18096 IsFinalTx comment about nSequence & `OP_CLTV` (nothingmuch)
- - bitcoin/bitcoin#18568 Clarify developer notes about constant naming (ryanofsky)
- - bitcoin/bitcoin#19961 doc: tor.md updates (jonatack)
- - bitcoin/bitcoin#19968 Clarify CRollingBloomFilter size estimate (robot-dreams)
- - bitcoin/bitcoin#20200 Rename CODEOWNERS to REVIEWERS (adamjonas)
- - bitcoin/bitcoin#20329 docs/descriptors.md: Remove hardened marker in the path after xpub (dgpv)
- - bitcoin/bitcoin#20380 Add instructions on how to fuzz the P2P layer using Honggfuzz NetDriver (practicalswift)
- - bitcoin/bitcoin#20414 Remove generated manual pages from master branch (laanwj)
- - bitcoin/bitcoin#20473 Document current boost dependency as 1.71.0 (laanwj)
- - bitcoin/bitcoin#20512 Add bash as an OpenBSD dependency (emilengler)
- - bitcoin/bitcoin#20568 Use FeeModes doc helper in estimatesmartfee (MarcoFalke)
- - bitcoin/bitcoin#20577 libconsensus: add missing error code description, fix NBitcoin link (theStack)
- - bitcoin/bitcoin#20587 Tidy up Tor doc (more stringent) (wodry)
- - bitcoin/bitcoin#20592 Update wtxidrelay documentation per BIP339 (jonatack)
- - bitcoin/bitcoin#20601 Update for FreeBSD 12.2, add GUI Build Instructions (jarolrod)
- - bitcoin/bitcoin#20635 fix misleading comment about call to non-existing function (pox)
- - bitcoin/bitcoin#20646 Refer to BIPs 339/155 in feature negotiation (jonatack)
- - bitcoin/bitcoin#20653 Move addr relay comment in net to correct place (MarcoFalke)
- - bitcoin/bitcoin#20677 Remove shouty enums in `net_processing` comments (sdaftuar)
- - bitcoin/bitcoin#20741 Update 'Secure string handling' (prayank23)
- - bitcoin/bitcoin#20757 tor.md and -onlynet help updates (jonatack)
- - bitcoin/bitcoin#20829 Add -netinfo help (jonatack)
- - bitcoin/bitcoin#20830 Update developer notes with signet (jonatack)
- - bitcoin/bitcoin#20890 Add explicit macdeployqtplus dependencies install step (hebasto)
- - bitcoin/bitcoin#20913 Add manual page generation for bitcoin-util (laanwj)
- - bitcoin/bitcoin#20985 Add xorriso to macOS depends packages (fanquake)
- - bitcoin/bitcoin#20986 Update developer notes to discourage very long lines (jnewbery)
- - bitcoin/bitcoin#20987 Add instructions for generating RPC docs (ben-kaufman)
- - bitcoin/bitcoin#21026 Document use of make-tag script to make tags (laanwj)
- - bitcoin/bitcoin#21028 doc/bips: Add BIPs 43, 44, 49, and 84 (luke-jr)
- - bitcoin/bitcoin#21049 Add release notes for listdescriptors RPC (S3RK)
- - bitcoin/bitcoin#21060 More precise -debug and -debugexclude doc (wodry)
- - bitcoin/bitcoin#21077 Clarify -timeout and -peertimeout config options (glozow)
- - bitcoin/bitcoin#21105 Correctly identify script type (niftynei)
- - bitcoin/bitcoin#21163 Guix is shipped in Debian and Ubuntu (MarcoFalke)
- - bitcoin/bitcoin#21210 Rework internal and external links (MarcoFalke)
- - bitcoin/bitcoin#21246 Correction for VerifyTaprootCommitment comments (roconnor-blockstream)
- - bitcoin/bitcoin#21263 Clarify that squashing should happen before review (MarcoFalke)
- - bitcoin/bitcoin#21323 guix, doc: Update default HOSTS value (hebasto)
- - bitcoin/bitcoin#21324 Update build instructions for Fedora (hebasto)
- - bitcoin/bitcoin#21343 Revamp macOS build doc (jarolrod)
- - bitcoin/bitcoin#21346 install qt5 when building on macOS (fanquake)
- - bitcoin/bitcoin#21384 doc: add signet to bitcoin.conf documentation (jonatack)
- - bitcoin/bitcoin#21394 Improve comment about protected peers (amitiuttarwar)
- - bitcoin/bitcoin#21398 Update fuzzing docs for afl-clang-lto (MarcoFalke)
- - bitcoin/bitcoin#21444 net, doc: Doxygen updates and fixes in netbase.{h,cpp} (jonatack)
- - bitcoin/bitcoin#21481 Tell howto install clang-format on Debian/Ubuntu (wodry)
- - bitcoin/bitcoin#21567 Fix various misleading comments (glozow)
- - bitcoin/bitcoin#21661 Fix name of script guix-build (Emzy)
- - bitcoin/bitcoin#21672 Remove boostrap info from `GUIX_COMMON_FLAGS` doc (fanquake)
- - bitcoin/bitcoin#21688 Note on SDK for macOS depends cross-compile (jarolrod)
- - bitcoin/bitcoin#21709 Update reduce-memory.md and bitcoin.conf -maxconnections info (jonatack)
- - bitcoin/bitcoin#21710 update helps for addnode rpc and -addnode/-maxconnections config options (jonatack)
- - bitcoin/bitcoin#21752 Clarify that feerates are per virtual size (MarcoFalke)
- - bitcoin/bitcoin#21811 Remove Visual Studio 2017 reference from readme (sipsorcery)
- - bitcoin/bitcoin#21818 Fixup -coinstatsindex help, update bitcoin.conf and files.md (jonatack)
- - bitcoin/bitcoin#21856 add OSS-Fuzz section to fuzzing.md doc (adamjonas)
- - bitcoin/bitcoin#21912 Remove mention of priority estimation (MarcoFalke)
- - bitcoin/bitcoin#21925 Update bips.md for 0.21.1 (MarcoFalke)
- - bitcoin/bitcoin#21942 improve make with parallel jobs description (klementtan)
- - bitcoin/bitcoin#21947 Fix OSS-Fuzz links (MarcoFalke)
- - bitcoin/bitcoin#21988 note that brew installed qt is not supported (jarolrod)
- - bitcoin/bitcoin#22056 describe in fuzzing.md how to reproduce a CI crash (jonatack)
- - bitcoin/bitcoin#22080 add maxuploadtarget to bitcoin.conf example (jarolrod)
- - bitcoin/bitcoin#22088 Improve note on choosing posix mingw32 (jarolrod)
- - bitcoin/bitcoin#22109 Fix external links (IRC, ?) (MarcoFalke)
- - bitcoin/bitcoin#22121 Various validation doc fixups (MarcoFalke)
- - bitcoin/bitcoin#22172 Update tor.md, release notes with removal of tor v2 support (jonatack)
- - bitcoin/bitcoin#22204 Remove obsolete `okSafeMode` RPC guideline from developer notes (theStack)
- - bitcoin/bitcoin#22208 Update `REVIEWERS` (practicalswift)
- - bitcoin/bitcoin#22250 add basic I2P documentation (vasild)
- - bitcoin/bitcoin#22296 Final merge of release notes snippets, mv to wiki (MarcoFalke)
- - bitcoin/bitcoin#22335 recommend `--disable-external-signer` in OpenBSD build guide (theStack)
- - bitcoin/bitcoin#22339 Document minimum required libc++ version (hebasto)
- - bitcoin/bitcoin#22349 Repository IRC updates (jonatack)
- - bitcoin/bitcoin#22360 Remove unused section from release process (MarcoFalke)
- - bitcoin/bitcoin#22369 Add steps for Transifex to release process (jonatack)
- - bitcoin/bitcoin#22393 Added info to bitcoin.conf doc (bliotti)
- - bitcoin/bitcoin#22402 Install Rosetta on M1-macOS for qt in depends (hebasto)
- - bitcoin/bitcoin#22432 Fix incorrect `testmempoolaccept` doc (glozow)
- - bitcoin/bitcoin#22648 doc, test: improve i2p/tor docs and i2p reachable unit tests (jonatack)

Credits
=======

Thanks to everyone who directly contributed to this release:

- - Aaron Clauson
- - Adam Jonas
- - amadeuszpawlik
- - Amiti Uttarwar
- - Andrew Chow
- - Andrew Poelstra
- - Anthony Towns
- - Antoine Poinsot
- - Antoine Riard
- - apawlik
- - apitko
- - Ben Carman
- - Ben Woosley
- - benk10
- - Bezdrighin
- - Block Mechanic
- - Brian Liotti
- - Bruno Garcia
- - Carl Dong
- - Christian Decker
- - coinforensics
- - Cory Fields
- - Dan Benjamin
- - Daniel Kraft
- - Darius Parvin
- - Dhruv Mehta
- - Dmitry Goncharov
- - Dmitry Petukhov
- - dplusplus1024
- - dscotese
- - Duncan Dean
- - Elle Mouton
- - Elliott Jin
- - Emil Engler
- - Ethan Heilman
- - eugene
- - Evan Klitzke
- - Fabian Jahr
- - Fabrice Fontaine
- - fanquake
- - fdov
- - flack
- - Fotis Koutoupas
- - Fu Yong Quah
- - fyquah
- - glozow
- - Gregory Sanders
- - Guido Vranken
- - Gunar C. Gessner
- - h
- - HAOYUatHZ
- - Hennadii Stepanov
- - Igor Cota
- - Ikko Ashimine
- - Ivan Metlushko
- - jackielove4u
- - James O'Beirne
- - Jarol Rodriguez
- - Joel Klabo
- - John Newbery
- - Jon Atack
- - Jonas Schnelli
- - Jo?o Barbosa
- - Josiah Baker
- - Karl-Johan Alm
- - Kiminuo
- - Klement Tan
- - Kristaps Kaupe
- - Larry Ruane
- - lisa neigut
- - Lucas Ontivero
- - Luke Dashjr
- - Maayan Keshet
- - MarcoFalke
- - Martin Ankerl
- - Martin Zumsande
- - Michael Dietz
- - Michael Polzer
- - Michael Tidwell
- - Niklas G?gge
- - nthumann
- - Oliver Gugger
- - parazyd
- - Patrick Strateman
- - Pavol Rusnak
- - Peter Bushnell
- - Pierre K
- - Pieter Wuille
- - PiRK
- - pox
- - practicalswift
- - Prayank
- - R E Broadley
- - Rafael Sadowski
- - randymcmillan
- - Raul Siles
- - Riccardo Spagni
- - Russell O'Connor
- - Russell Yanofsky
- - S3RK
- - saibato
- - Samuel Dobson
- - sanket1729
- - Sawyer Billings
- - Sebastian Falbesoner
- - setpill
- - sgulls
- - sinetek
- - Sjors Provoost
- - Sriram
- - Stephan Oeste
- - Suhas Daftuar
- - Sylvain Goumy
- - t-bast
- - Troy Giorshev
- - Tushar Singla
- - Tyler Chambers
- - Uplab
- - Vasil Dimov
- - W. J. van der Laan
- - willcl-ark
- - William Bright
- - William Casarin
- - windsok
- - wodry
- - Yerzhan Mazhkenov
- - Yuval Kogman
- - Zero

As well as to everyone that helped with translations on
[Transifex](https://www.transifex.com/bitcoin/bitcoin/).
-----BEGIN PGP SIGNATURE-----

iQEzBAEBCgAdFiEEnerg3HBjJJ+wVHRoHkrtYphs0l0FAmE/X9UACgkQHkrtYphs
0l2ezQf+JD5g0NVVNdLuvNf+bz59zBMf7seNi385h6sd74hRDHYYN/whYLZwRl+w
0zvCvzbDy3AFULI/laoaUHAP1sz7/5H01je+BH/hzAKCflGQfZYz3y+fBftye6ag
OKesWunMJdmU58nj1AQjcueXu8JmolH73GeJFRlNsVYyiYRndyA+5osF2oqBUTdP
c9rtUIOQx6O/YjEhFZeIXnER2YhLIYaVf06FkGRUTS6coYI6dhhYbFrv3NcD1+rf
U2XMeTaiDoDnQrSaSxd/czJ85oNdF8kjsZpfbVwcRU0M2HW8AqBpp+AgDIdmznCt
Nac3q+5b0JHiZ2tNfyUiixto+OMSVA==
=DIZ5
-----END PGP SIGNATURE-----

From lf-lists at mattcorallo.com  Mon Sep 13 16:24:29 2021
From: lf-lists at mattcorallo.com (Matt Corallo)
Date: Mon, 13 Sep 2021 09:24:29 -0700
Subject: [bitcoin-dev] Reorgs on SigNet - Looking for feedback on
	approach and parameters
In-Reply-To: <CAFvNmHQmH8S4JFa6xQNbFt0b4PjmqHx5ii6Jd9T5bfSWmPW0KA@mail.gmail.com>
References: <CAFvNmHQmH8S4JFa6xQNbFt0b4PjmqHx5ii6Jd9T5bfSWmPW0KA@mail.gmail.com>
Message-ID: <68B066E6-0D96-49F0-88D2-D716E81BFDE8@mattcorallo.com>



> On Sep 13, 2021, at 05:30, Michael Folkson <michaelfolkson at gmail.com> wrote:
> 
> ?
>> 
>> Can you explain the motivation for this? From where I sit, as far as I know, I should basically be > a prime example of the target market for public signet - someone developing bitcoin applications > with regular requirements to test those applications with other developers without
>> jumping through hoops to configure software the same across the globe and set up miners.
>> With blocks > being slow and irregular, I?m basically not benefited at all by signet and will
>> stick with testnet3/mainnet testing, which both suck.
> 
> On testnet3 you can realistically go days without blocks being found
> (and conversely thousands of blocks can be found in a day), the block
> discovery time variance is huge. Of course this is probabilistically
> possible on mainnet too but the probability of this happening is close
> to zero. Here[0] is an example of 16,000 blocks being found in a day
> on testnet3.

Blocks too fast isn?t generally an issue when waiting for blocks to test, and hooking up a miner is probably less work on testnet3 than creating a multi-party private signet with miners. In any case, you didn?t address the substance of the point - we can do better to make it a good platform for testing?. Why aren?t we?

From aj at erisian.com.au  Tue Sep 14 04:56:10 2021
From: aj at erisian.com.au (Anthony Towns)
Date: Tue, 14 Sep 2021 14:56:10 +1000
Subject: [bitcoin-dev] Reorgs on SigNet - Looking for feedback on
 approach and parameters
In-Reply-To: <571244AD-B8F4-4F2A-BF4B-31EED3AB7713@mattcorallo.com>
References: <20210912075305.GA23673@erisian.com.au>
 <571244AD-B8F4-4F2A-BF4B-31EED3AB7713@mattcorallo.com>
Message-ID: <20210914045610.GA25475@erisian.com.au>

On Sun, Sep 12, 2021 at 10:33:24PM -0700, Matt Corallo via bitcoin-dev wrote:
> > On Sep 12, 2021, at 00:53, Anthony Towns <aj at erisian.com.au> wrote:
> >> Why bother with a version bit? This seems substantially more complicated
> >> than the original proposal that surfaced many times before signet launched
> >> to just have a different reorg signing key.
> > Yeah, that was the original idea, but there ended up being two problems
> > with that approach. The simplest is that the signet block signature
> > encodes the signet challenge,
> But if that was the originally proposal, why is the challenge committed to in the block? :)

The answer to your question was in the text after the comma, that you
deleted...

> > Blocks on signet get mined at a similar rate to mainnet, so you'll always
> > have to wait a little bit (up to an hour) -- if you don't want to wait
> > at all, that's what regtest (or perhaps a custom signet) is for.
> Can you explain the motivation for this? 

I'm not sure that's really the question you want answered? Mostly
it's just "this is how mainnet works" plus "these are the smallest
changes to have blocks be chosen by a signature, rather than entirely
by PoW competition".

For integration testing across many services, I think a ten-minute-average
between blocks still makes sense -- protocols relying on CSV/CLTV to
ensure there's a delay they can use to recover funds, if they specify
that in blocks (as lightning's to_self_delay does), then significant
surges of blocks will cause uninteresting bugs. 

It would be easy enough to change things to target an average of 2 or
5 minutes, I suppose, but then you'd probably need to propogate that
logic back into your apps that would otherwise think 144 blocks is around
about a day.

We could switch back to doing blocks exactly every 10 minutes, rather
than a poisson-ish distribution in the range of 1min to 60min, but that
doesn't seem like that huge a win, and makes it hard to test that things
behave properly when blocks arrive in bursts.

> From where I sit, as far as I know, I should basically be a prime
> example of the target market for public signet - someone developing
> bitcoin applications with regular requirements to test those applications
> with other developers without jumping through hoops to configure software
> the same across the globe and set up miners. With blocks being slow and
> irregular, I?m basically not benefited at all by signet and will stick
> with testnet3/mainnet testing, which both suck.

Best of luck to you then? Nobody's trying to sell you on a subscription
plan to using signet. Signet's less expensive in fees (or risk) than
mainnet, and takes far less time for IBD than testnet, but if those
aren't blockers for you, that's great.

Cheers,
aj

From prayank at tutanota.de  Tue Sep 14 12:17:57 2021
From: prayank at tutanota.de (Prayank)
Date: Tue, 14 Sep 2021 14:17:57 +0200 (CEST)
Subject: [bitcoin-dev] BIP process meeting - Tuesday September 14th
 23:00 UTC on #bitcoin-dev Libera IRC
Message-ID: <MjZEVeZ--3-2@tutanota.de>

Hi Michael,

Thanks for sharing the details about the meeting.

Wishlist has some interesting points. I would like to suggest few things:

1.BIP process: 

A. Plan and document a proposal

 B. Open PR in?https://github.com/bitcoin/bips and edit everything properly

 C. BIP is assigned a number and merged

 D. Share the proposal on bitcoin dev mailing list

bitcoin-dev mailing list link can be considered a BIP and saved in a BIP directory. Anyone can create such directories. So BIP is nothing but a proposal shared on bitcoin-dev mailing list.

Who implements the BIP? When is it implemented? How is it implemented? Opinions on proposal etc. will be different for each BIP. This will avoid the 'bitcoin/bips' repository being considered as some BIP authority that approves BIPs and proposals can improve Bitcoin without using the repository. Repository will only be helpful in documenting BIP correctly.

2. Bot in `bitcoin/bips` repository that notifies about pull requests based on different things. This will help maintainer(s) and contributors.

3. BIP Gallery: I tried sharing things in a different way so that newbies can understand importance of BIPs in Bitcoin and relate to it:?https://prayank23.github.io/BIPsGallery/?however couldn't complete it with all the BIPs because not many people considered it helpful. There were few suggestions to improve it by adding some text for each BIP and better image gallery. Maybe someone else can create a better project.?


-- 
Prayank

A3B1 E430 2298 178F
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210914/cee843bc/attachment.html>

From michaelfolkson at gmail.com  Tue Sep 14 14:07:46 2021
From: michaelfolkson at gmail.com (Michael Folkson)
Date: Tue, 14 Sep 2021 15:07:46 +0100
Subject: [bitcoin-dev] BIP process meeting - Tuesday September 14th
 23:00 UTC on #bitcoin-dev Libera IRC
In-Reply-To: <MjZEVeZ--3-2@tutanota.de>
References: <MjZEVeZ--3-2@tutanota.de>
Message-ID: <CAFvNmHRt2L+D1jkVJmUsiZ8Fpeqqioygk+eZkP7+8r2p3Dx6_Q@mail.gmail.com>

Hey Prayank

Thanks for the suggestions.

> bitcoin-dev mailing list link can be considered a BIP and saved in a BIP directory. Anyone can create such directories. So BIP is nothing but a proposal shared on bitcoin-dev mailing list.

A mailing list post is static and a BIP will go normally go through
multiple edits and revisions so you do need to take advantage of the
Git version control system. It gets quite unwieldy to attempt to do
that via a mailing list with every minor suggested edit getting sent
to all subscribers. Also allowing the entire global population
(billions of people) to be able to create a directory doesn't sound
like a good idea to me :)

> This will avoid the 'bitcoin/bips' repository being considered as some BIP authority that approves BIPs and proposals can improve Bitcoin without using the repository. Repository will only be helpful in documenting BIP correctly.

I can only speak for myself here but I am not particularly concerned
about this perception of authority. We need a central repo that we can
all refer to (rather than BIPs being distributed across a large number
of repos) and that central repo needs to managed and maintained by
somebody (in this case the two BIP editors Kalle and Luke). In the
same way as there are limits on the ability of Core maintainers to
unilaterally merge in contentious code changes there are similar
limits on the ability of BIP editors. Ultimately anyone merging a PR
has to consider process/consensus and concerns can (and have been in
the past) be raised on this mailing list or elsewhere.

> 2. Bot in `bitcoin/bips` repository that notifies about pull requests based on different things. This will help maintainer(s) and contributors.

I'm not sure where you are suggesting a bot should be. On IRC? There
is a BIP merges bot on Mastodon[0] that I'm aware of and obviously you
can subscribe to GitHub repo notification emails.

> 3. BIP Gallery: I tried sharing things in a different way so that newbies can understand importance of BIPs in Bitcoin and relate to it: https://prayank23.github.io/BIPsGallery/ however couldn't complete it with all the BIPs because not many people considered it helpful. There were few suggestions to improve it by adding some text for each BIP and better image gallery. Maybe someone else can create a better project.

This looks cool. I think we can definitely do better in encouraging
more people to engage with the BIP process especially as the ideas
start flowing in post Taproot activation brainstorming what should be
in the "next soft fork" (trademark!). Some of the BIPs (e.g. the
Taproot BIPs 340-342) are quite technically dense so someone on IRC
suggested making greater use of informational BIPs to supplement the
standard BIPs for new implementers or even casual readers.

[0] https://x0f.org/@bipmerges

On Tue, Sep 14, 2021 at 1:17 PM Prayank <prayank at tutanota.de> wrote:
>
> Hi Michael,
>
> Thanks for sharing the details about the meeting.
>
> Wishlist has some interesting points. I would like to suggest few things:
>
> 1.BIP process:
>
> A. Plan and document a proposal
>
> B. Open PR in https://github.com/bitcoin/bips and edit everything properly
>
> C. BIP is assigned a number and merged
>
> D. Share the proposal on bitcoin dev mailing list
>
> bitcoin-dev mailing list link can be considered a BIP and saved in a BIP directory. Anyone can create such directories. So BIP is nothing but a proposal shared on bitcoin-dev mailing list.
>
> Who implements the BIP? When is it implemented? How is it implemented? Opinions on proposal etc. will be different for each BIP. This will avoid the 'bitcoin/bips' repository being considered as some BIP authority that approves BIPs and proposals can improve Bitcoin without using the repository. Repository will only be helpful in documenting BIP correctly.
>
> 2. Bot in `bitcoin/bips` repository that notifies about pull requests based on different things. This will help maintainer(s) and contributors.
>
> 3. BIP Gallery: I tried sharing things in a different way so that newbies can understand importance of BIPs in Bitcoin and relate to it: https://prayank23.github.io/BIPsGallery/ however couldn't complete it with all the BIPs because not many people considered it helpful. There were few suggestions to improve it by adding some text for each BIP and better image gallery. Maybe someone else can create a better project.
>
>
> --
> Prayank
>
> A3B1 E430 2298 178F



-- 
Michael Folkson
Email: michaelfolkson at gmail.com
Keybase: michaelfolkson
PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3

From prayank at tutanota.de  Tue Sep 14 14:50:16 2021
From: prayank at tutanota.de (Prayank)
Date: Tue, 14 Sep 2021 16:50:16 +0200 (CEST)
Subject: [bitcoin-dev] BIP process meeting - Tuesday September 14th
 23:00 UTC on #bitcoin-dev Libera IRC
In-Reply-To: <CAFvNmHRt2L+D1jkVJmUsiZ8Fpeqqioygk+eZkP7+8r2p3Dx6_Q@mail.gmail.com>
References: <MjZEVeZ--3-2@tutanota.de>
 <CAFvNmHRt2L+D1jkVJmUsiZ8Fpeqqioygk+eZkP7+8r2p3Dx6_Q@mail.gmail.com>
Message-ID: <MjZmMzE--3-2@tutanota.de>

> A mailing list post is static and a BIP will go normally go through multiple edits and revisions so you do need to take advantage of the Git version control system. It gets quite unwieldy to attempt to do that via a mailing list with every minor suggested edit getting sent to all subscribers.

 Mailing list post will have the link to BIP documentation. Post itself doesn't need to be updated but same link can be used to share updated information. Example: https://gist.github.com/prayank23/95b4804777fefd015d7cc4f847675d7f?(Image can be changed in gist when required or add new information)
Mailing list post will help in reading discussions related to proposal.

>Also allowing the entire global population
(billions of people) to be able to create a directory doesn't sound
like a good idea to me :)

There is nothing to allow/disallow. That's the whole point. People are free to save links and organize things which can be called a BIP directory.

> I can only speak for myself here but I am not particularly concerned about this perception of authority. 

This perception affects Bitcoin. 

> In the same way as there are limits on the ability of Core maintainers to unilaterally merge in contentious code changes there are similar limits on the ability of BIP editors. Ultimately anyone merging a PR has to consider process/consensus and concerns can (and have been in the past) be raised on this mailing list or elsewhere.

Bitcoin Core is an implementation (used by most of the nodes right now). BIPs are proposals for Bitcoin. Using same organization on GitHub and such comparisons can be misleading for many. I don't think we need ACKs/NACKs in BIPs repository and I feel weird to be a part of discussions, ACKing this pull request:?https://github.com/bitcoin/bips/pull/1104. Not sure any Bitcoin project needs a pull request merged in this repository to implement a proposal.

>?I'm not sure where you are suggesting a bot should be.

A bot similar to DrahtBot in Bitcoin Core repository.?
Few other developers had suggested similar thing earlier:

https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-April/018859.html

https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-April/018868.html

https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-April/018869.html

https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-April/018871.html

-- 
Prayank

A3B1 E430 2298 178F



Sep 14, 2021, 19:37 by michaelfolkson at gmail.com:

> Hey Prayank
>
> Thanks for the suggestions.
>
>> bitcoin-dev mailing list link can be considered a BIP and saved in a BIP directory. Anyone can create such directories. So BIP is nothing but a proposal shared on bitcoin-dev mailing list.
>>
>
> A mailing list post is static and a BIP will go normally go through
> multiple edits and revisions so you do need to take advantage of the
> Git version control system. It gets quite unwieldy to attempt to do
> that via a mailing list with every minor suggested edit getting sent
> to all subscribers. Also allowing the entire global population
> (billions of people) to be able to create a directory doesn't sound
> like a good idea to me :)
>
>> This will avoid the 'bitcoin/bips' repository being considered as some BIP authority that approves BIPs and proposals can improve Bitcoin without using the repository. Repository will only be helpful in documenting BIP correctly.
>>
>
> I can only speak for myself here but I am not particularly concerned
> about this perception of authority. We need a central repo that we can
> all refer to (rather than BIPs being distributed across a large number
> of repos) and that central repo needs to managed and maintained by
> somebody (in this case the two BIP editors Kalle and Luke). In the
> same way as there are limits on the ability of Core maintainers to
> unilaterally merge in contentious code changes there are similar
> limits on the ability of BIP editors. Ultimately anyone merging a PR
> has to consider process/consensus and concerns can (and have been in
> the past) be raised on this mailing list or elsewhere.
>
>> 2. Bot in `bitcoin/bips` repository that notifies about pull requests based on different things. This will help maintainer(s) and contributors.
>>
>
> I'm not sure where you are suggesting a bot should be. On IRC? There
> is a BIP merges bot on Mastodon[0] that I'm aware of and obviously you
> can subscribe to GitHub repo notification emails.
>
>> 3. BIP Gallery: I tried sharing things in a different way so that newbies can understand importance of BIPs in Bitcoin and relate to it: https://prayank23.github.io/BIPsGallery/ however couldn't complete it with all the BIPs because not many people considered it helpful. There were few suggestions to improve it by adding some text for each BIP and better image gallery. Maybe someone else can create a better project.
>>
>
> This looks cool. I think we can definitely do better in encouraging
> more people to engage with the BIP process especially as the ideas
> start flowing in post Taproot activation brainstorming what should be
> in the "next soft fork" (trademark!). Some of the BIPs (e.g. the
> Taproot BIPs 340-342) are quite technically dense so someone on IRC
> suggested making greater use of informational BIPs to supplement the
> standard BIPs for new implementers or even casual readers.
>
> [0] https://x0f.org/@bipmerges
>
> On Tue, Sep 14, 2021 at 1:17 PM Prayank <prayank at tutanota.de> wrote:
>
>>
>> Hi Michael,
>>
>> Thanks for sharing the details about the meeting.
>>
>> Wishlist has some interesting points. I would like to suggest few things:
>>
>> 1.BIP process:
>>
>> A. Plan and document a proposal
>>
>> B. Open PR in https://github.com/bitcoin/bips and edit everything properly
>>
>> C. BIP is assigned a number and merged
>>
>> D. Share the proposal on bitcoin dev mailing list
>>
>> bitcoin-dev mailing list link can be considered a BIP and saved in a BIP directory. Anyone can create such directories. So BIP is nothing but a proposal shared on bitcoin-dev mailing list.
>>
>> Who implements the BIP? When is it implemented? How is it implemented? Opinions on proposal etc. will be different for each BIP. This will avoid the 'bitcoin/bips' repository being considered as some BIP authority that approves BIPs and proposals can improve Bitcoin without using the repository. Repository will only be helpful in documenting BIP correctly.
>>
>> 2. Bot in `bitcoin/bips` repository that notifies about pull requests based on different things. This will help maintainer(s) and contributors.
>>
>> 3. BIP Gallery: I tried sharing things in a different way so that newbies can understand importance of BIPs in Bitcoin and relate to it: https://prayank23.github.io/BIPsGallery/ however couldn't complete it with all the BIPs because not many people considered it helpful. There were few suggestions to improve it by adding some text for each BIP and better image gallery. Maybe someone else can create a better project.
>>
>>
>> --
>> Prayank
>>
>> A3B1 E430 2298 178F
>>
>
>
>
> -- 
> Michael Folkson
> Email: michaelfolkson at gmail.com
> Keybase: michaelfolkson
> PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210914/261a6313/attachment-0001.html>

From michaelfolkson at gmail.com  Tue Sep 14 15:43:59 2021
From: michaelfolkson at gmail.com (Michael Folkson)
Date: Tue, 14 Sep 2021 16:43:59 +0100
Subject: [bitcoin-dev] BIP process meeting - Tuesday September 14th
 23:00 UTC on #bitcoin-dev Libera IRC
In-Reply-To: <MjZmMzE--3-2@tutanota.de>
References: <MjZEVeZ--3-2@tutanota.de>
 <CAFvNmHRt2L+D1jkVJmUsiZ8Fpeqqioygk+eZkP7+8r2p3Dx6_Q@mail.gmail.com>
 <MjZmMzE--3-2@tutanota.de>
Message-ID: <CAFvNmHRV1sk5o9bLAxDw4qcarhG5zymrt9Oo4_emKJprtYrtMA@mail.gmail.com>

>> I can only speak for myself here but I am not particularly concerned about this perception of authority.

> This perception affects Bitcoin.

Personally I would rather have an optimal process that provides
clarity and helps us build better software than be sensitive to
inaccurate perceptions that hinder that ultimate goal.

> Bitcoin Core is an implementation (used by most of the nodes right now). BIPs are proposals for Bitcoin.

Indeed, thanks for pointing this out. I take it as a given that
everyone knows this but yeah when making such a comparison it is good
to make this clear.

> Using same organization on GitHub and such comparisons can be misleading for many.

I think there's an argument that BIPs could be under a different
GitHub organization but it would be pretty low on my list or
priorities. There is a clear divide between the group of Core
maintainers and the group of BIP editors and in the absence of a
reason to change that I would rather maintain the status quo.

> I don't think we need ACKs/NACKs in BIPs repository and I feel weird to be a part of discussions, ACKing this pull request: https://github.com/bitcoin/bips/pull/1104.

With BIP champions having more latitude in getting their BIP PR merged
than they would for example getting their Core PR merged I agree
ACK/NACKs on BIP PRs are less relevant. However, I still think some
BIP champions would like to have their changes reviewed especially by
subject matter experts. And if there are strong disagreements over the
changes made an alternative BIP is always an option. I don't see the
harm in having discussion with reviewers on BIP PRs and reviewers
registering an ACK/NACK as long as we are all clear on what the BIP
process is.

> Not sure any Bitcoin project needs a pull request merged in this repository to implement a proposal.

I agree it is optional for some/many Bitcoin projects whether they are
BIPed or not. Would you be comfortable with a soft fork/consensus code
change going into Bitcoin Core without a BIP? I personally wouldn't.

We should probably leave it at that to ensure we are not spamming the
email list but hope to see you at the meeting later :)

On Tue, Sep 14, 2021 at 3:50 PM Prayank <prayank at tutanota.de> wrote:
>
> > A mailing list post is static and a BIP will go normally go through multiple edits and revisions so you do need to take advantage of the Git version control system. It gets quite unwieldy to attempt to do that via a mailing list with every minor suggested edit getting sent to all subscribers.
>
> Mailing list post will have the link to BIP documentation. Post itself doesn't need to be updated but same link can be used to share updated information. Example: https://gist.github.com/prayank23/95b4804777fefd015d7cc4f847675d7f (Image can be changed in gist when required or add new information)
>
> Mailing list post will help in reading discussions related to proposal.
>
> >Also allowing the entire global population
> (billions of people) to be able to create a directory doesn't sound
> like a good idea to me :)
>
> There is nothing to allow/disallow. That's the whole point. People are free to save links and organize things which can be called a BIP directory.
>
> > I can only speak for myself here but I am not particularly concerned about this perception of authority.
>
> This perception affects Bitcoin.
>
> > In the same way as there are limits on the ability of Core maintainers to unilaterally merge in contentious code changes there are similar limits on the ability of BIP editors. Ultimately anyone merging a PR has to consider process/consensus and concerns can (and have been in the past) be raised on this mailing list or elsewhere.
>
> Bitcoin Core is an implementation (used by most of the nodes right now). BIPs are proposals for Bitcoin. Using same organization on GitHub and such comparisons can be misleading for many. I don't think we need ACKs/NACKs in BIPs repository and I feel weird to be a part of discussions, ACKing this pull request: https://github.com/bitcoin/bips/pull/1104. Not sure any Bitcoin project needs a pull request merged in this repository to implement a proposal.
>
> > I'm not sure where you are suggesting a bot should be.
>
> A bot similar to DrahtBot in Bitcoin Core repository.
>
> Few other developers had suggested similar thing earlier:
>
> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-April/018859.html
>
> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-April/018868.html
>
> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-April/018869.html
>
> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-April/018871.html
>
> --
> Prayank
>
> A3B1 E430 2298 178F
>
>
>
> Sep 14, 2021, 19:37 by michaelfolkson at gmail.com:
>
> Hey Prayank
>
> Thanks for the suggestions.
>
> bitcoin-dev mailing list link can be considered a BIP and saved in a BIP directory. Anyone can create such directories. So BIP is nothing but a proposal shared on bitcoin-dev mailing list.
>
>
> A mailing list post is static and a BIP will go normally go through
> multiple edits and revisions so you do need to take advantage of the
> Git version control system. It gets quite unwieldy to attempt to do
> that via a mailing list with every minor suggested edit getting sent
> to all subscribers. Also allowing the entire global population
> (billions of people) to be able to create a directory doesn't sound
> like a good idea to me :)
>
> This will avoid the 'bitcoin/bips' repository being considered as some BIP authority that approves BIPs and proposals can improve Bitcoin without using the repository. Repository will only be helpful in documenting BIP correctly.
>
>
> I can only speak for myself here but I am not particularly concerned
> about this perception of authority. We need a central repo that we can
> all refer to (rather than BIPs being distributed across a large number
> of repos) and that central repo needs to managed and maintained by
> somebody (in this case the two BIP editors Kalle and Luke). In the
> same way as there are limits on the ability of Core maintainers to
> unilaterally merge in contentious code changes there are similar
> limits on the ability of BIP editors. Ultimately anyone merging a PR
> has to consider process/consensus and concerns can (and have been in
> the past) be raised on this mailing list or elsewhere.
>
> 2. Bot in `bitcoin/bips` repository that notifies about pull requests based on different things. This will help maintainer(s) and contributors.
>
>
> I'm not sure where you are suggesting a bot should be. On IRC? There
> is a BIP merges bot on Mastodon[0] that I'm aware of and obviously you
> can subscribe to GitHub repo notification emails.
>
> 3. BIP Gallery: I tried sharing things in a different way so that newbies can understand importance of BIPs in Bitcoin and relate to it: https://prayank23.github.io/BIPsGallery/ however couldn't complete it with all the BIPs because not many people considered it helpful. There were few suggestions to improve it by adding some text for each BIP and better image gallery. Maybe someone else can create a better project.
>
>
> This looks cool. I think we can definitely do better in encouraging
> more people to engage with the BIP process especially as the ideas
> start flowing in post Taproot activation brainstorming what should be
> in the "next soft fork" (trademark!). Some of the BIPs (e.g. the
> Taproot BIPs 340-342) are quite technically dense so someone on IRC
> suggested making greater use of informational BIPs to supplement the
> standard BIPs for new implementers or even casual readers.
>
> [0] https://x0f.org/@bipmerges
>
> On Tue, Sep 14, 2021 at 1:17 PM Prayank <prayank at tutanota.de> wrote:
>
>
> Hi Michael,
>
> Thanks for sharing the details about the meeting.
>
> Wishlist has some interesting points. I would like to suggest few things:
>
> 1.BIP process:
>
> A. Plan and document a proposal
>
> B. Open PR in https://github.com/bitcoin/bips and edit everything properly
>
> C. BIP is assigned a number and merged
>
> D. Share the proposal on bitcoin dev mailing list
>
> bitcoin-dev mailing list link can be considered a BIP and saved in a BIP directory. Anyone can create such directories. So BIP is nothing but a proposal shared on bitcoin-dev mailing list.
>
> Who implements the BIP? When is it implemented? How is it implemented? Opinions on proposal etc. will be different for each BIP. This will avoid the 'bitcoin/bips' repository being considered as some BIP authority that approves BIPs and proposals can improve Bitcoin without using the repository. Repository will only be helpful in documenting BIP correctly.
>
> 2. Bot in `bitcoin/bips` repository that notifies about pull requests based on different things. This will help maintainer(s) and contributors.
>
> 3. BIP Gallery: I tried sharing things in a different way so that newbies can understand importance of BIPs in Bitcoin and relate to it: https://prayank23.github.io/BIPsGallery/ however couldn't complete it with all the BIPs because not many people considered it helpful. There were few suggestions to improve it by adding some text for each BIP and better image gallery. Maybe someone else can create a better project.
>
>
> --
> Prayank
>
> A3B1 E430 2298 178F
>
>
>
>
> --
> Michael Folkson
> Email: michaelfolkson at gmail.com
> Keybase: michaelfolkson
> PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3
>
>


-- 
Michael Folkson
Email: michaelfolkson at gmail.com
Keybase: michaelfolkson
PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3

From karl at dglab.com  Wed Sep 15 06:14:31 2021
From: karl at dglab.com (Karl-Johan Alm)
Date: Wed, 15 Sep 2021 15:14:31 +0900
Subject: [bitcoin-dev] BIP extensions
Message-ID: <CALJw2w7f2JfskLQPwEBsKTqpJBo+qccP-y6oAX5EEzJ3vo0Grw@mail.gmail.com>

BIPs are proposals.

They begin as ideas, are formulated and discussed on this list, and
assuming no glaring flaws are observed, turned into pull requests to
the bips repository, assigned a BIP number by the editors, and merged.

It is then organically incorporated into the various entities that
exist in the Bitcoin space. At this point, it is not merely a
proposal, but a standard. As entities place their weight behind a BIP,
it makes less and less sense to consider its author the "maintainer"
of the BIP, with rights to modify it at their whim. Someone may have
agreed to the proposal in its original form, but they may disagree
with it if it is altered from under their feet.

BIPs are modified for primarily three reasons:

1. Because of spelling errors, or to otherwise improve on their
description without changing what is actually proposed.
2. To improve the proposal in some way, e.g. after discussion or after
getting feedback on the proposed approach.
3. To add missing content, such as activation strategy.

I propose that changes of the second and third type, unless they are
absolutely free from contention, are done as BIP extensions.

BIP extensions are separate BIPs that extend on or an existing BIP.
BIP extensions do not require the approval of the extended-upon BIP's
author, and are considered independent proposals entirely. A BIP that
extends on BIP XXX is referred to as BIP-XXX-Y, e.g. BIP-123-1, and
their introductory section must include the wording "

This BIP extends on (link: BIP-XXX).

".

By making extensions to BIPs, rather than modifying them long after
review, we are giving the community
1. the assurance that a BIP will mostly remain in its form forever,
except if an obvious win is discovered,
2. the ability to judge modifications to BIPs, such as activation
parameters, on their merits alone, and
3. the path to propose modifications to BIPs even if their authors
have gone inactive and cease to provide feedback, as is the case for
many BIPs today, as BIP extensions do not require the approval of the
extended-upon BIP.

(Apologies if this has been proposed already. If so, feel free to
ignore this message, and sorry to have wasted your time.)

From aj at erisian.com.au  Wed Sep 15 06:50:51 2021
From: aj at erisian.com.au (Anthony Towns)
Date: Wed, 15 Sep 2021 16:50:51 +1000
Subject: [bitcoin-dev] TAPLEAF_UPDATE_VERIFY covenant opcode
In-Reply-To: <CALZpt+HzM__OJntegOhDqkg5zU=PQXtKgQoB518A2qP9=foovw@mail.gmail.com>
References: <20210909064138.GA22496@erisian.com.au>
 <CALZpt+FnnbGJC4=KO_OPiKxt0Ey9Bzh1gxP1dQSDz2aBi9WyOA@mail.gmail.com>
 <20210911032644.GB23578@erisian.com.au>
 <CALZpt+HzM__OJntegOhDqkg5zU=PQXtKgQoB518A2qP9=foovw@mail.gmail.com>
Message-ID: <20210915065051.GA26119@erisian.com.au>

On Sun, Sep 12, 2021 at 07:37:56PM -0400, Antoine Riard via bitcoin-dev wrote:
> While MERKLESUB is still WIP, here the semantic. [...]
> I believe this is matching your description and the main difference compared to
> your TLUV proposal is the lack of merkle tree extension, where a new merkle
> path is added in place of the removed tapscript.

I think "<I> <P> MERKLESUB" is the same as "<P> OP_0 2 TLUV", provided
<I> happens to be the same index as the current input. So it misses the
ability to add branches (replacing OP_0 with a hash), the ability to
preserve the current script (replacing 2 with 0), and the ability to
remove some of the parent paths (replacing 2 with 4*n); but gains the
ability to refer to non-corresponding outputs.

> > That would mean anyone who could do a valid spend of the tx could
> > violate the covenant by spending to an unencumbered witness v2 output
> > and (by collaborating with a miner) steal the funds. I don't think
> > there's a reasonable way to have existing covenants be forward
> > compatible with future destination addresses (beyond something like CTV
> > that strictly hardcodes them).
> That's a good catch, thanks for raising it :)
> Depends how you define reasonable, but I think one straightforward fix is to
> extend the signature digest algorithm to encompass the segwit version (and
> maybe program-size ?) of the spending transaction outputs.

That... doesn't sound very straightforward to me; it's basically
introducing a new covenant approach, that's getting fixed into a
signature, rather than being a separate opcode.

I think a better approach for that would be to introduce the opcode (eg,
PUSH_OUTPUT_SCRIPTPUBKEY, and SUBSTR to be able to analyse the segwit
version), and make use of graftroot to allow a signature to declare that
it's conditional on some extra script code. But it feels like it's going
a bit off topic.

> > Having the output position parameter might be an interesting way to
> > merge/split a vault/pool, but it's not clear to me how much sense it
> > makes sense to optimise for that, rather than just doing that via the key
> > path. For pools, you want the key path to be common anyway (for privacy
> > and efficiency), so it shouldn't be a problem; but even for vaults,
> > you want the cold wallet accessible enough to be useful for the case
> > where theft is attempted, and maybe that's also accessible enough for
> > the ocassional merge/split to keep your utxo count/sizes reasonable.
> I think you can come up with interesting contract policies. Let's say you want
> to authorize the emergency path of your pool/vault balances if X happens (e.g a
> massive drop in USDT price signed by DLC oracles). You have (A+B+C+D) forking
> into (A+B) and (C+D) pooled funds. To conserve the contracts pre-negotiated
> economic equilibrium, all the participants would like the emergency path to be
> inherited on both forks. Without relying on the key path interactivity, which
> is ultimately a trust on the post-fork cooperation of your counterparty ?

I'm not really sure what you're saying there; is that any different to a
pool of (A and B) where A suddenly wants to withdraw funds ASAP and can't
wait for a key path signature? In that case A authorises the withdrawal
and does whatever she wants with the funds (including form a new pool),
and B remains in the pool.

I don't think you can reliably have some arbitrary subset of the pool
able to withdraw atomically without using the key path -- if A,B,C,D have
individual scripts allowing withdrawal, then there's no way of setting
the tree up so that every pair of members can have their scripts cut
off without also cutting off one or both of the other members withdrawal
scripts.

If you know in advance which groups want to stick together, you could
set things up as:

  (((A, B), AB), C)

where:

  A =   "A DUP H(B') 10 TLUV CHECKSIG"  -> (B', C)
  B =   "B DUP H(A') 10 TLUV CHECKSIG"  -> (A', C)
  A' =  "A DUP 0 2 TLUV CHECKSIG"   -> (C)
  B' =  "B DUP 0 2 TLUV CHECKSIG"   -> (C)
  AB =  "(A+B) DUP 6 TLUV CHECKSIG  -> (C)
  C  =  "C DUP 0 2 TLUV CHECKSIG"   -> ((A,B), AB)

(10 = 2+4*2 = drop my script, my sibling and my uncle; 6 = 2+4*1 =
drop my script and my sibling; 2 = drop my script only)

Which would let A and B exit together in a single tx rather than needing two
transactions to exit separately.

> > Saving a byte of witness data at the cost of specifying additional
> > opcodes seems like optimising the wrong thing to me.
> I think we should keep in mind that any overhead cost in the usage of a script
> primitive is echoed to the user of off-chain contract/payment channels. If the
> tapscripts are bigger, your average on-chain spends in case of non-cooperative
> scenarios are increased in consequence, and as such your fee-bumping reserve.
> Thus making those systems less economically accessible.

If you're worried about the cost of a single byte of witness data you
probably can't afford to do script path spends at all -- certainly
having to do 64 bytes of witness data to add a signature that commits
to an amount and the like will be infeasible in that case.

> > I don't think that works, because different scripts in the same merkle
> > tree can have different script versions, which would here indicate
> > different parities for the same internal pub key.
> Let me make it clearer. We introduce a new tapscript version 0x20, forcing a
> new bit in the first byte of the control block to be interpreted as the parity
> bit of the spent internal pubkey.

That doesn't work. Suppose you start off with an even internal pubkey,
with three scripts, (A, (B,C)). All of those scripts have tapscript
version 0xc0 because the internal pubkey is even. You spend using A and
calculate the new internal pubkey which turns out to be odd. You then
need to change B and C's script version from 0xc0 to 0x20, but you can't
do that (at least, you can't do it without revealing every script).

> To ensure this parity bit is faithful and
> won't break the updated key path, it's committed in the spent taptweak.

Changing the TapTweak calculation is a hard fork; existing software
already verifies the calculation even if the script version is unknown.

> > The IN_OUT_AMOUNT opcode lets you do maths on the values, so you can
> > specify "hot wallets can withdraw up to X" rather than "hot wallets
> > must withdraw exactly X". I don't think there's a way of doing that with
> > SIGHASH_GROUP, even with a modifier like ANYPUBKEY?
> You can exchange signatures for withdraw outputs with multiples `nValue`
> covering the authorized range, assuming the ANYAMOUNT modifier ?

If you want your hotwallet to be able to withdraw up to $2000, that's
around 4,000,000 sats, so you'd be doing up to 4M signatures there if you
wanted to get the exact value you're trying to send, without having to
either overpay, or first pay yourself then have another tx that splits
your withdrawal into what you're spending and change that's no longer
in your vault.

> One advantage
> of leveraging sighash is the ability to update a withdraw policy in real-time.
> Vaults participants might be willing to bump the withdraw policy beyond X,
> assuming you have N-of-M consents.

I mean, maybe? It seems like a very heavy weight construct where a more
general approach would probably be better (eg, graftroot to attach a
script to a signature; or checkdatasig or whatever so you push a value
to the stack then check it's signature, then reuse the authenticated
data against other checks) so that you only have to supply a signature
when you want to be able to approve things after the fact.

> I think I would like to express the following contract policy. Let's say you
> have 1) a one-time conditional script path to withdraw fund ("a put on strike
> price X"), 2) a conditional script path to tweak by 3 months all the usual
> withdraw path and 3) those remaining withdraw paths. Once played out, you would
> like the one-time path to be removed from your merkle tree. And this removal to
> be inherited on the tweaked tree if 2) plays out.

Okay, so I think that means we've got the unconditional withdraw path
"U" (your 1), the delay path "D" (your 2) and some normal path(s) "N"
(your 3). I think you can get that behaviour with:

   S1 = Merkle( U, (D, N) )
   S2 = Merkle( U, W )
   S3 = Merkle( N )

that is, you start off with the funds in scriptPubKey S1, then spend
using D to get to S2, then spend using W to get to S3, then presumably
spend using N at some point.

The script for W is just:

   "IN_OUT_AMOUNT EQUALVERIFY 0 <N> 6 TLUV <3 months> CSV"   
       (drop the script, drop its sibling, add N, wait 3 months)

The script for D is:

   "IN_OUT_AMOUNT EQUALVERIFY 0 <W> 6 TLUV <sigcheck...>"
       (drop the script, drop its sibling, add W, extra conditions
        to avoid anyone being able to delay things)

That is, the strategy isn't "tweak the scripts by delaying them 3 months"
it's "tweak the merkle tree, to replace the scripts that would be delayed
with a new script that has a delay and then allows itself to be replaced
by the original scripts that we now want back".

Cheers,
aj


From me at federicociro.com  Wed Sep 15 05:47:57 2021
From: me at federicociro.com (Federico Berrone)
Date: Wed, 15 Sep 2021 07:47:57 +0200
Subject: [bitcoin-dev] BIP extensions
In-Reply-To: <CALJw2w7f2JfskLQPwEBsKTqpJBo+qccP-y6oAX5EEzJ3vo0Grw@mail.gmail.com>
References: <CALJw2w7f2JfskLQPwEBsKTqpJBo+qccP-y6oAX5EEzJ3vo0Grw@mail.gmail.com>
Message-ID: <10670b5b-2c3f-61d0-c2e4-0d496d14f2cd@federicociro.com>

Hi Karl-Johan,
I fully agree with your proposal. In order to de-clutter BIPs and make a 
more understandable proposal, we can add the additional information in a 
separate piece. Also, this would maintain the original proposal without 
any modifications, showing the original spirit of it.
Let me know how can I help you with your proposal.

Regards,
Federico Berrone.

P/D: This is my first participation in the bitcoin-dev list, sorry if I 
am breaking any rule, I would be glad to know if that is the case.

El 15/09/2021 a las 8:14, Karl-Johan Alm via bitcoin-dev escribi?:
> BIPs are proposals.
>
> They begin as ideas, are formulated and discussed on this list, and
> assuming no glaring flaws are observed, turned into pull requests to
> the bips repository, assigned a BIP number by the editors, and merged.
>
> It is then organically incorporated into the various entities that
> exist in the Bitcoin space. At this point, it is not merely a
> proposal, but a standard. As entities place their weight behind a BIP,
> it makes less and less sense to consider its author the "maintainer"
> of the BIP, with rights to modify it at their whim. Someone may have
> agreed to the proposal in its original form, but they may disagree
> with it if it is altered from under their feet.
>
> BIPs are modified for primarily three reasons:
>
> 1. Because of spelling errors, or to otherwise improve on their
> description without changing what is actually proposed.
> 2. To improve the proposal in some way, e.g. after discussion or after
> getting feedback on the proposed approach.
> 3. To add missing content, such as activation strategy.
>
> I propose that changes of the second and third type, unless they are
> absolutely free from contention, are done as BIP extensions.
>
> BIP extensions are separate BIPs that extend on or an existing BIP.
> BIP extensions do not require the approval of the extended-upon BIP's
> author, and are considered independent proposals entirely. A BIP that
> extends on BIP XXX is referred to as BIP-XXX-Y, e.g. BIP-123-1, and
> their introductory section must include the wording "
>
> This BIP extends on (link: BIP-XXX).
>
> ".
>
> By making extensions to BIPs, rather than modifying them long after
> review, we are giving the community
> 1. the assurance that a BIP will mostly remain in its form forever,
> except if an obvious win is discovered,
> 2. the ability to judge modifications to BIPs, such as activation
> parameters, on their merits alone, and
> 3. the path to propose modifications to BIPs even if their authors
> have gone inactive and cease to provide feedback, as is the case for
> many BIPs today, as BIP extensions do not require the approval of the
> extended-upon BIP.
>
> (Apologies if this has been proposed already. If so, feel free to
> ignore this message, and sorry to have wasted your time.)
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------- next part --------------
A non-text attachment was scrubbed...
Name: OpenPGP_0xB4B16B2D677120AF.asc
Type: application/pgp-keys
Size: 3147 bytes
Desc: OpenPGP public key
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210915/f2ca346f/attachment.bin>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: OpenPGP_signature
Type: application/pgp-signature
Size: 840 bytes
Desc: OpenPGP digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210915/f2ca346f/attachment.sig>

From prayank at tutanota.de  Wed Sep 15 09:50:34 2021
From: prayank at tutanota.de (Prayank)
Date: Wed, 15 Sep 2021 11:50:34 +0200 (CEST)
Subject: [bitcoin-dev] Reminder on the Purpose of BIPs
Message-ID: <MjcrMLe--3-2@tutanota.de>

> I like the idea of decentralizing the BIPs process. It is a historical artifact that the bips repository is part of the same organization that bitcoin core is part of. But there shouldn't be the perception that standardization is driven by that, or that there is any kind of (non-trivial) gatekeeping.

I had suggested few changes in BIP process and repository yesterday. Meeting was disappointing because of few reasons: 

1.Its been 12 years since Bitcoin came in to existence and I am surprised that during such important conversations I still see only 4 people out of which 2 are maintainers.

2.None of the people who participated in meeting agree that we need to create multiple BIP directories and let people decide what works best for them. Reduce dependency on one repository or few people. At the end of the day these are just proposals, implementations are more important and there are so many ways to document things online, archive etc.

Playing ACK/NACK game in 'bitcoin/bips' repository will be a waste of time so I created this as an example:

https://github.com/prayank23/bips/blob/master/README.md

https://prayank23.github.io/bips/

I respect everyone involved in Bitcoin development however neither I trust anyone nor I expect anyone to trust me. Bitcoin is not just another open source software. Its a protocol for decentralized network which can be used to settle payments. We are trying to redefine MONEY, many cypherpunks, activists, hacktivists, privacy advocates etc. are involved and trying to separate money from state. The same money that is needed for almost everything you do in this world from birth to death, love to war and same money that makes some people more powerful. So, I won't be surprised with anything in future and will be prepared for everything.

-- 
Prayank

A3B1 E430 2298 178F
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210915/a1478d8f/attachment.html>

From karl at dglab.com  Wed Sep 15 10:18:41 2021
From: karl at dglab.com (Karl-Johan Alm)
Date: Wed, 15 Sep 2021 19:18:41 +0900
Subject: [bitcoin-dev] BIP extensions
In-Reply-To: <10670b5b-2c3f-61d0-c2e4-0d496d14f2cd@federicociro.com>
References: <CALJw2w7f2JfskLQPwEBsKTqpJBo+qccP-y6oAX5EEzJ3vo0Grw@mail.gmail.com>
 <10670b5b-2c3f-61d0-c2e4-0d496d14f2cd@federicociro.com>
Message-ID: <CALJw2w4v5weerSGBCj0v=QB=0uG7YmkOtEs_kXrPydoS-_hibw@mail.gmail.com>

Hi Frederico,

Welcome to the bitcoin-dev list. :)

Michael Folkson is currently pushing for a revision to BIP 2, which is
discussed in the "BIP process meeting" thread here. You could help out
by participating in that process. There's a wiki page with ideas for
this in [1] and the current plan is to modify [2] or some other pull
request to reflect what everyone decides.

[1] https://github.com/bitcoin/bips/wiki/BIP-Process-wishlist
[2] https://github.com/bitcoin/bips/pull/1015

-Kalle.

On Wed, 15 Sept 2021 at 17:29, Federico Berrone via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
>
> Hi Karl-Johan,
> I fully agree with your proposal. In order to de-clutter BIPs and make a
> more understandable proposal, we can add the additional information in a
> separate piece. Also, this would maintain the original proposal without
> any modifications, showing the original spirit of it.
> Let me know how can I help you with your proposal.
>
> Regards,
> Federico Berrone.
>
> P/D: This is my first participation in the bitcoin-dev list, sorry if I
> am breaking any rule, I would be glad to know if that is the case.
>
> El 15/09/2021 a las 8:14, Karl-Johan Alm via bitcoin-dev escribi?:
> > BIPs are proposals.
> >
> > They begin as ideas, are formulated and discussed on this list, and
> > assuming no glaring flaws are observed, turned into pull requests to
> > the bips repository, assigned a BIP number by the editors, and merged.
> >
> > It is then organically incorporated into the various entities that
> > exist in the Bitcoin space. At this point, it is not merely a
> > proposal, but a standard. As entities place their weight behind a BIP,
> > it makes less and less sense to consider its author the "maintainer"
> > of the BIP, with rights to modify it at their whim. Someone may have
> > agreed to the proposal in its original form, but they may disagree
> > with it if it is altered from under their feet.
> >
> > BIPs are modified for primarily three reasons:
> >
> > 1. Because of spelling errors, or to otherwise improve on their
> > description without changing what is actually proposed.
> > 2. To improve the proposal in some way, e.g. after discussion or after
> > getting feedback on the proposed approach.
> > 3. To add missing content, such as activation strategy.
> >
> > I propose that changes of the second and third type, unless they are
> > absolutely free from contention, are done as BIP extensions.
> >
> > BIP extensions are separate BIPs that extend on or an existing BIP.
> > BIP extensions do not require the approval of the extended-upon BIP's
> > author, and are considered independent proposals entirely. A BIP that
> > extends on BIP XXX is referred to as BIP-XXX-Y, e.g. BIP-123-1, and
> > their introductory section must include the wording "
> >
> > This BIP extends on (link: BIP-XXX).
> >
> > ".
> >
> > By making extensions to BIPs, rather than modifying them long after
> > review, we are giving the community
> > 1. the assurance that a BIP will mostly remain in its form forever,
> > except if an obvious win is discovered,
> > 2. the ability to judge modifications to BIPs, such as activation
> > parameters, on their merits alone, and
> > 3. the path to propose modifications to BIPs even if their authors
> > have gone inactive and cease to provide feedback, as is the case for
> > many BIPs today, as BIP extensions do not require the approval of the
> > extended-upon BIP.
> >
> > (Apologies if this has been proposed already. If so, feel free to
> > ignore this message, and sorry to have wasted your time.)
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev at lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From aj at erisian.com.au  Wed Sep 15 14:34:54 2021
From: aj at erisian.com.au (Anthony Towns)
Date: Thu, 16 Sep 2021 00:34:54 +1000
Subject: [bitcoin-dev] BIP extensions
In-Reply-To: <CALJw2w7f2JfskLQPwEBsKTqpJBo+qccP-y6oAX5EEzJ3vo0Grw@mail.gmail.com>
References: <CALJw2w7f2JfskLQPwEBsKTqpJBo+qccP-y6oAX5EEzJ3vo0Grw@mail.gmail.com>
Message-ID: <20210915143454.GA27129@erisian.com.au>

On Wed, Sep 15, 2021 at 03:14:31PM +0900, Karl-Johan Alm via bitcoin-dev wrote:
> BIPs are proposals.

> It is then organically incorporated into the various entities that
> exist in the Bitcoin space. At this point, it is not merely a
> proposal, but a standard.

Thinking of BIPs that have reach "Final" status as a "standard" might
be reasonable, but I'd be pretty careful about even going that far,
let alone further.

But as you said, "BIPs are proposals". If your conclusion is somehow
that a BIP "is not merely a proposal", you're reached a contradiction,
which means you've made a logic error somewhere in between...

> Someone may have
> agreed to the proposal in its original form, but they may disagree
> with it if it is altered from under their feet.

> 2. To improve the proposal in some way, e.g. after discussion or after
> getting feedback on the proposed approach.
> 3. To add missing content, such as activation strategy.

> I propose that changes of the second and third type, unless they are
> absolutely free from contention, are done as BIP extensions.

If you were proposing this just for BIPs that are marked final, then
sure, maybe, I guess -- though why mark them final if you still want
to add missing content or make further improvements? But if you want to
apply it as soon as a BIP number is assigned or text is merged into the
repo, I think that just means requesting number assignment gets delayed
until the end of the development process rather than near the beginning,
which doesn't sound particularly helpful.

That's essentially how the lightning BOLTs are set up -- you only get to
publish a BOLT after you've got support from multiple implementations
[0]; but that has meant they don't have published docs for the various
things individual teams have implemented, making interoperability harder
rather than easier. There's been talk about creating bLIPs [1] to remedy
this lack.

> BIP extensions are separate BIPs that extend on or an existing BIP.

So as an alternative, how about more clearly separating out draft BIPs
from those in Active/Final state? ie:

 * brand new BIP draft comes in from its authors/champions/whatever
 * number xxx gets assigned, it becomes "Draft BIP xxx"
 * authors modify it as they see fit
 * once the authors are happy with the text, they can move it
   to Final status, at which point it is no longer a draft and is
   just "BIP xxx", and doesn't get modified anymore
 * go to step 1

(I'm doubtful that it's very useful to have an "Active" state as distinct
from "Final"; that just gives the editors an excuse to play favourites
by deciding whose objections count and whose don't (or perhaps which
implementations count and which ones don't). It's currently only used for
BIPs about the BIP process, which makes it seem particularly pointless...)

> By making extensions to BIPs, rather than modifying them long after
> review, we are giving the community [...]

As described, I think you would be giving people an easy way to actively
obstruct the BIP process by making it harder to "improve the proposal"
and "add missing content", and encouraging contentiousness as a result.

For adding on to BIPs that have reached Final status, I think just
assigning completely new numbers is fine, as occurred with bech32 and
bech32m (BIPs 173 and 350).

Even beyond that, having BIP maintainers exercising judgement by trying
to reserve/assign "pretty" numbers (like "BIP 3" for the new BIP process)
seems like a mistake to me. If it were up to me, I'd make the setup be
something like:

 * new BIP? make a PR, putting the text into
   "drafts/bip-authorname-description.mediawiki" (with corresponding
   directory for images etc). Have the word "Draft" appear in the "BIP:
   xxx" header as well as in the Status: header.

 * if that passes CI and isn't incoherent, it gets merged

 * only after the draft is already merged is a BIP number assigned.
   the number is chosen by a script, and the BIP maintainers rename it
   to "drafts/bip-xxx.mediawiki" in a followup commit including internal
   links to bip-authorname-description/foo.png and add it to the README
   (automatically at the same time as the merge, ideally)

 * when a BIP becomes Final, it gets moved from drafts/ into
   the main directory [2], and to avoid breaking external links,
   drafts/bip-xxx.mediawiki is changed to just have a link to the
   main doc.

 * likewise when a BIP becomes rejected/deprecated/whatever, it's moved
   into historical/ and drafts/bip-xxx.mediawiki and bip-xxx.mediawiki
   are updated with a link to the new location

 * otherwise, don't allow any modifications to bips outside of
   drafts/, with the possible exception of adding additional info in
   Acknowledgements or See also section or similar, adding Superseded-By:
   links, and updating additional tables that are deliberately designed
   to be updated, eg bip-0009/assignments.mediawiki

It's better to remove incentives to introduce friction rather than
add more.

Cheers,
aj

[0] https://github.com/lightningnetwork/lightning-rfc/blob/master/CONTRIBUTING.md

[1] https://github.com/ryanthegentry/lightning-rfc/blob/blip-0001/blips/blip-0001.md

[2] Maybe moving the files between directories is too much, but I think
    having "drafts/" in the URL is likely to help ensure people referring
    to draft BIPs actually realise they're drafts, and thus subject to
    large changes, cf
    https://twitter.com/BobMcElrath/status/1281606259863629824

    Likewise, people probably might not want to implement/deploy BIPs
    marked "draft", which is a good reason for the authors to mark them
    final, which in turn might help ensure they're actually complete
    and finished before they're deployed, all of which seems like a
    good thing.


From lf-lists at mattcorallo.com  Wed Sep 15 15:24:43 2021
From: lf-lists at mattcorallo.com (Matt Corallo)
Date: Wed, 15 Sep 2021 08:24:43 -0700
Subject: [bitcoin-dev] Reorgs on SigNet - Looking for feedback on
	approach and parameters
In-Reply-To: <20210914045610.GA25475@erisian.com.au>
References: <20210914045610.GA25475@erisian.com.au>
Message-ID: <90AD5816-4B44-4BBB-A2FC-39CD381D6395@mattcorallo.com>



> On Sep 13, 2021, at 21:56, Anthony Towns <aj at erisian.com.au> wrote:
> I'm not sure that's really the question you want answered?

Of course it is? I?d like to understand the initial thinking and design analysis that went into this decision. That seems like an important question to ask when seeking changes in an existing system :).

> Mostly
> it's just "this is how mainnet works" plus "these are the smallest
> changes to have blocks be chosen by a signature, rather than entirely
> by PoW competition".
> 
> For integration testing across many services, I think a ten-minute-average
> between blocks still makes sense -- protocols relying on CSV/CLTV to
> ensure there's a delay they can use to recover funds, if they specify
> that in blocks (as lightning's to_self_delay does), then significant
> surges of blocks will cause uninteresting bugs. 

Hmm, why would blocks coming quicker lead to a bug? I certainly hope no one has a bug if their block time is faster than per ten minutes. I presume here, you mean something like ?if the node can?t keep up with the block rate?, but I certainly hope the benchmark for may isn?t 10 minutes, or really even one.

> It would be easy enough to change things to target an average of 2 or
> 5 minutes, I suppose, but then you'd probably need to propogate that
> logic back into your apps that would otherwise think 144 blocks is around
> about a day.

Why? One useful thing for testing is compressing real time. More broadly, the only issues that I?ve heard around block times in testnet3 are the inconsistency and, rarely software failing to keep up at all.

> We could switch back to doing blocks exactly every 10 minutes, rather
> than a poisson-ish distribution in the range of 1min to 60min, but that
> doesn't seem like that huge a win, and makes it hard to test that things
> behave properly when blocks arrive in bursts.

Hmm, I suppose? If you want to test that the upper bound doesn?t need to be 100 minutes, though, it could be 10.

> Best of luck to you then? Nobody's trying to sell you on a subscription
> plan to using signet.


lol, yes, I?m aware of that, nor did I mean to imply that anything has to be targeted at a specific person?s requirements. Rather, my point here is that I?m really confused as to who  the target user *is*, because we should be building products with target users in mind, even if those targets are often ?me? for open source projects.

From gloriajzhao at gmail.com  Thu Sep 16 07:51:25 2021
From: gloriajzhao at gmail.com (Gloria Zhao)
Date: Thu, 16 Sep 2021 08:51:25 +0100
Subject: [bitcoin-dev] Proposal: Package Mempool Accept and Package RBF
Message-ID: <CAFXO6=+cHyQKM2n9yn4EhwLZO+AUB0ZD81qWPxmpN27rjUoU3w@mail.gmail.com>

Hi there,

I'm writing to propose a set of mempool policy changes to enable package
validation (in preparation for package relay) in Bitcoin Core. These would
not
be consensus or P2P protocol changes. However, since mempool policy
significantly affects transaction propagation, I believe this is relevant
for
the mailing list.

My proposal enables packages consisting of multiple parents and 1 child. If
you
develop software that relies on specific transaction relay assumptions
and/or
are interested in using package relay in the future, I'm very interested to
hear
your feedback on the utility or restrictiveness of these package policies
for
your use cases.

A draft implementation of this proposal can be found in [Bitcoin Core
PR#22290][1].

An illustrated version of this post can be found at
https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.
I have also linked the images below.

## Background

Feel free to skip this section if you are already familiar with mempool
policy
and package relay terminology.

### Terminology Clarifications

* Package = an ordered list of related transactions, representable by a
Directed
  Acyclic Graph.
* Package Feerate = the total modified fees divided by the total virtual
size of
  all transactions in the package.
    - Modified fees = a transaction's base fees + fee delta applied by the
user
      with `prioritisetransaction`. As such, we expect this to vary across
mempools.
    - Virtual Size = the maximum of virtual sizes calculated using [BIP141
      virtual size][2] and sigop weight. [Implemented here in Bitcoin
Core][3].
    - Note that feerate is not necessarily based on the base fees and
serialized
      size.

* Fee-Bumping = user/wallet actions that take advantage of miner incentives
to
  boost a transaction's candidacy for inclusion in a block, including Child
Pays
for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our intention in
mempool policy is to recognize when the new transaction is more economical
to
mine than the original one(s) but not open DoS vectors, so there are some
limitations.

### Policy

The purpose of the mempool is to store the best (to be most
incentive-compatible
with miners, highest feerate) candidates for inclusion in a block. Miners
use
the mempool to build block templates. The mempool is also useful as a cache
for
boosting block relay and validation performance, aiding transaction relay,
and
generating feerate estimations.

Ideally, all consensus-valid transactions paying reasonable fees should
make it
to miners through normal transaction relay, without any special
connectivity or
relationships with miners. On the other hand, nodes do not have unlimited
resources, and a P2P network designed to let any honest node broadcast their
transactions also exposes the transaction validation engine to DoS attacks
from
malicious peers.

As such, for unconfirmed transactions we are considering for our mempool, we
apply a set of validation rules in addition to consensus, primarily to
protect
us from resource exhaustion and aid our efforts to keep the highest fee
transactions. We call this mempool _policy_: a set of (configurable,
node-specific) rules that transactions must abide by in order to be accepted
into our mempool. Transaction "Standardness" rules and mempool restrictions
such
as "too-long-mempool-chain" are both examples of policy.

### Package Relay and Package Mempool Accept

In transaction relay, we currently consider transactions one at a time for
submission to the mempool. This creates a limitation in the node's ability
to
determine which transactions have the highest feerates, since we cannot take
into account descendants (i.e. cannot use CPFP) until all the transactions
are
in the mempool. Similarly, we cannot use a transaction's descendants when
considering it for RBF. When an individual transaction does not meet the
mempool
minimum feerate and the user isn't able to create a replacement transaction
directly, it will not be accepted by mempools.

This limitation presents a security issue for applications and users
relying on
time-sensitive transactions. For example, Lightning and other protocols
create
UTXOs with multiple spending paths, where one counterparty's spending path
opens
up after a timelock, and users are protected from cheating scenarios as
long as
they redeem on-chain in time. A key security assumption is that all parties'
transactions will propagate and confirm in a timely manner. This assumption
can
be broken if fee-bumping does not work as intended.

The end goal for Package Relay is to consider multiple transactions at the
same
time, e.g. a transaction with its high-fee child. This may help us better
determine whether transactions should be accepted to our mempool,
especially if
they don't meet fee requirements individually or are better RBF candidates
as a
package. A combination of changes to mempool validation logic, policy, and
transaction relay allows us to better propagate the transactions with the
highest package feerates to miners, and makes fee-bumping tools more
powerful
for users.

The "relay" part of Package Relay suggests P2P messaging changes, but a
large
part of the changes are in the mempool's package validation logic. We call
this
*Package Mempool Accept*.

### Previous Work

* Given that mempool validation is DoS-sensitive and complex, it would be
  dangerous to haphazardly tack on package validation logic. Many efforts
have
been made to make mempool validation less opaque (see [#16400][4],
[#21062][5],
[#22675][6], [#22796][7]).
* [#20833][8] Added basic capabilities for package validation, test accepts
only
  (no submission to mempool).
* [#21800][9] Implemented package ancestor/descendant limit checks for
arbitrary
  packages. Still test accepts only.
* Previous package relay proposals (see [#16401][10], [#19621][11]).

### Existing Package Rules

These are in master as introduced in [#20833][8] and [#21800][9]. I'll
consider
them as "given" in the rest of this document, though they can be changed,
since
package validation is test-accept only right now.

1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and
`MAX_PACKAGE_SIZE=101KvB` total size [8]

   *Rationale*: This is already enforced as mempool ancestor/descendant
limits.
Presumably, transactions in a package are all related, so exceeding this
limit
would mean that the package can either be split up or it wouldn't pass this
mempool policy.

2. Packages must be topologically sorted: if any dependencies exist between
transactions, parents must appear somewhere before children. [8]

3. A package cannot have conflicting transactions, i.e. none of them can
spend
the same inputs. This also means there cannot be duplicate transactions. [8]

4. When packages are evaluated against ancestor/descendant limits in a test
accept, the union of all of their descendants and ancestors is considered.
This
is essentially a "worst case" heuristic where every transaction in the
package
is treated as each other's ancestor and descendant. [8]
Packages for which ancestor/descendant limits are accurately captured by
this
heuristic: [19]

There are also limitations such as the fact that CPFP carve out is not
applied
to package transactions. #20833 also disables RBF in package validation;
this
proposal overrides that to allow packages to use RBF.

## Proposed Changes

The next step in the Package Mempool Accept project is to implement
submission
to mempool, initially through RPC only. This allows us to test the
submission
logic before exposing it on P2P.

### Summary

- Packages may contain already-in-mempool transactions.
- Packages are 2 generations, Multi-Parent-1-Child.
- Fee-related checks use the package feerate. This means that wallets can
create a package that utilizes CPFP.
- Parents are allowed to RBF mempool transactions with a set of rules
similar
  to BIP125. This enables a combination of CPFP and RBF, where a
transaction's descendant fees pay for replacing mempool conflicts.

There is a draft implementation in [#22290][1]. It is WIP, but feedback is
always welcome.

### Details

#### Packages May Contain Already-in-Mempool Transactions

A package may contain transactions that are already in the mempool. We
remove
("deduplicate") those transactions from the package for the purposes of
package
mempool acceptance. If a package is empty after deduplication, we do
nothing.

*Rationale*: Mempools vary across the network. It's possible for a parent
to be
accepted to the mempool of a peer on its own due to differences in policy
and
fee market fluctuations. We should not reject or penalize the entire
package for
an individual transaction as that could be a censorship vector.

#### Packages Are Multi-Parent-1-Child

Only packages of a specific topology are permitted. Namely, a package is
exactly
1 child with all of its unconfirmed parents. After deduplication, the
package
may be exactly the same, empty, 1 child, 1 child with just some of its
unconfirmed parents, etc. Note that it's possible for the parents to be
indirect
descendants/ancestors of one another, or for parent and child to share a
parent,
so we cannot make any other topology assumptions.

*Rationale*: This allows for fee-bumping by CPFP. Allowing multiple parents
makes it possible to fee-bump a batch of transactions. Restricting packages
to a
defined topology is also easier to reason about and simplifies the
validation
logic greatly. Multi-parent-1-child allows us to think of the package as
one big
transaction, where:

- Inputs = all the inputs of parents + inputs of the child that come from
  confirmed UTXOs
- Outputs = all the outputs of the child + all outputs of the parents that
  aren't spent by other transactions in the package

Examples of packages that follow this rule (variations of example A show
some
possibilities after deduplication): ![image][15]

#### Fee-Related Checks Use Package Feerate

Package Feerate = the total modified fees divided by the total virtual size
of
all transactions in the package.

To meet the two feerate requirements of a mempool, i.e., the pre-configured
minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum
feerate, the
total package feerate is used instead of the individual feerate. The
individual
transactions are allowed to be below feerate requirements if the package
meets
the feerate requirements. For example, the parent(s) in the package can
have 0
fees but be paid for by the child.

*Rationale*: This can be thought of as "CPFP within a package," solving the
issue of a parent not meeting minimum fees on its own. This allows L2
applications to adjust their fees at broadcast time instead of overshooting
or
risking getting stuck/pinned.

We use the package feerate of the package *after deduplication*.

*Rationale*:  It would be incorrect to use the fees of transactions that are
already in the mempool, as we do not want a transaction's fees to be
double-counted for both its individual RBF and package RBF.

Examples F and G [14] show the same package, but P1 is submitted
individually before
the package in example G. In example F, we can see that the 300vB package
pays
an additional 200sat in fees, which is not enough to pay for its own
bandwidth
(BIP125#4). In example G, we can see that P1 pays enough to replace M1, but
using P1's fees again during package submission would make it look like a
300sat
increase for a 200vB package. Even including its fees and size would not be
sufficient in this example, since the 300sat looks like enough for the 300vB
package. The calculcation after deduplication is 100sat increase for a
package
of size 200vB, which correctly fails BIP125#4. Assume all transactions have
a
size of 100vB.

#### Package RBF

If a package meets feerate requirements as a package, the parents in the
transaction are allowed to replace-by-fee mempool transactions. The child
cannot
replace mempool transactions. Multiple transactions can replace the same
transaction, but in order to be valid, none of the transactions can try to
replace an ancestor of another transaction in the same package (which would
thus
make its inputs unavailable).

*Rationale*: Even if we are using package feerate, a package will not
propagate
as intended if RBF still requires each individual transaction to meet the
feerate requirements.

We use a set of rules slightly modified from BIP125 as follows:

##### Signaling (Rule #1)

All mempool transactions to be replaced must signal replaceability.

*Rationale*: Package RBF signaling logic should be the same for package RBF
and
single transaction acceptance. This would be updated if single transaction
validation moves to full RBF.

##### New Unconfirmed Inputs (Rule #2)

A package may include new unconfirmed inputs, but the ancestor feerate of
the
child must be at least as high as the ancestor feerates of every transaction
being replaced. This is contrary to BIP125#2, which states "The replacement
transaction may only include an unconfirmed input if that input was
included in
one of the original transactions. (An unconfirmed input spends an output
from a
currently-unconfirmed transaction.)"

*Rationale*: The purpose of BIP125#2 is to ensure that the replacement
transaction has a higher ancestor score than the original transaction(s)
(see
[comment][13]). Example H [16] shows how adding a new unconfirmed input can
lower the
ancestor score of the replacement transaction. P1 is trying to replace M1,
and
spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat, and M2
pays
100sat. Assume all transactions have a size of 100vB. While, in isolation,
P1
looks like a better mining candidate than M1, it must be mined with M2, so
its
ancestor feerate is actually 4.5sat/vB.  This is lower than M1's ancestor
feerate, which is 6sat/vB.

In package RBF, the rule analogous to BIP125#2 would be "none of the
transactions in the package can spend new unconfirmed inputs." Example J
[17] shows
why, if any of the package transactions have ancestors, package feerate is
no
longer accurate. Even though M2 and M3 are not ancestors of P1 (which is the
replacement transaction in an RBF), we're actually interested in the entire
package. A miner should mine M1 which is 5sat/vB instead of M2, M3, P1, P2,
and
P3, which is only 4sat/vB. The Package RBF rule cannot be loosened to only
allow
the child to have new unconfirmed inputs, either, because it can still
cause us
to overestimate the package's ancestor score.

However, enforcing a rule analogous to BIP125#2 would not only make Package
RBF
less useful, but would also break Package RBF for packages with parents
already
in the mempool: if a package parent has already been submitted, it would
look
like the child is spending a "new" unconfirmed input. In example K [18],
we're
looking to replace M1 with the entire package including P1, P2, and P3. We
must
consider the case where one of the parents is already in the mempool (in
this
case, P2), which means we must allow P3 to have new unconfirmed inputs.
However,
M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not replace M1
with this package.

Thus, the package RBF rule regarding new unconfirmed inputs is less strict
than
BIP125#2. However, we still achieve the same goal of requiring the
replacement
transactions to have a ancestor score at least as high as the original
ones. As
a result, the entire package is required to be a higher feerate mining
candidate
than each of the replaced transactions.

Another note: the [comment][13] above the BIP125#2 code in the original RBF
implementation suggests that the rule was intended to be temporary.

##### Absolute Fee (Rule #3)

The package must increase the absolute fee of the mempool, i.e. the total
fees
of the package must be higher than the absolute fees of the mempool
transactions
it replaces. Combined with the CPFP rule above, this differs from BIP125
Rule #3
- an individual transaction in the package may have lower fees than the
  transaction(s) it is replacing. In fact, it may have 0 fees, and the child
pays for RBF.

##### Feerate (Rule #4)

The package must pay for its own bandwidth; the package feerate must be
higher
than the replaced transactions by at least minimum relay feerate
(`incrementalRelayFee`). Combined with the CPFP rule above, this differs
from
BIP125 Rule #4 - an individual transaction in the package can have a lower
feerate than the transaction(s) it is replacing. In fact, it may have 0
fees,
and the child pays for RBF.

##### Total Number of Replaced Transactions (Rule #5)

The package cannot replace more than 100 mempool transactions. This is
identical
to BIP125 Rule #5.

### Expected FAQs

1. Is it possible for only some of the package to make it into the mempool?

   Yes, it is. However, since we evict transactions from the mempool by
descendant score and the package child is supposed to be sponsoring the
fees of
its parents, the most common scenario would be all-or-nothing. This is
incentive-compatible. In fact, to be conservative, package validation should
begin by trying to submit all of the transactions individually, and only
use the
package mempool acceptance logic if the parents fail due to low feerate.

2. Should we allow packages to contain already-confirmed transactions?

    No, for practical reasons. In mempool validation, we actually aren't
able to
tell with 100% confidence if we are looking at a transaction that has
already
confirmed, because we look up inputs using a UTXO set. If we have historical
block data, it's possible to look for it, but this is inefficient, not
always
possible for pruning nodes, and unnecessary because we're not going to do
anything with the transaction anyway. As such, we already have the
expectation
that transaction relay is somewhat "stateful" i.e. nobody should be relaying
transactions that have already been confirmed. Similarly, we shouldn't be
relaying packages that contain already-confirmed transactions.

[1]: https://github.com/bitcoin/bitcoin/pull/22290
[2]:
https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations
[3]:
https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282
[4]: https://github.com/bitcoin/bitcoin/pull/16400
[5]: https://github.com/bitcoin/bitcoin/pull/21062
[6]: https://github.com/bitcoin/bitcoin/pull/22675
[7]: https://github.com/bitcoin/bitcoin/pull/22796
[8]: https://github.com/bitcoin/bitcoin/pull/20833
[9]: https://github.com/bitcoin/bitcoin/pull/21800
[10]: https://github.com/bitcoin/bitcoin/pull/16401
[11]: https://github.com/bitcoin/bitcoin/pull/19621
[12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki
[13]:
https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104
[14]:
https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png
[15]:
https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png
[16]:
https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png
[17]:
https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png
[18]:
https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png
[19]:
https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png
[20]:
https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210916/d71208a2/attachment-0001.html>

From shymaa.arafat at gmail.com  Thu Sep 16 15:05:24 2021
From: shymaa.arafat at gmail.com (shymaa arafat)
Date: Thu, 16 Sep 2021 17:05:24 +0200
Subject: [bitcoin-dev] [Lightning-dev] Storing the Merkle Tree in a
	compact way
In-Reply-To: <f6RaXDTOElzKjWTLDhUPFKXs1AeJnaxdC2HGbaO8NponuccadSaInyzyBNIoH-Wa3_cyv4SKOoWNx2gTj1jdhyGLwFExyGF4q1d9UJ68skI=@protonmail.com>
References: <CAM98U8=r+DGW46O5Srp5SzqB38suYnZY8DR06dqx-_gdB5Ruxw@mail.gmail.com>
 <f6RaXDTOElzKjWTLDhUPFKXs1AeJnaxdC2HGbaO8NponuccadSaInyzyBNIoH-Wa3_cyv4SKOoWNx2gTj1jdhyGLwFExyGF4q1d9UJ68skI=@protonmail.com>
Message-ID: <CAM98U8==V1igTn9UuM2w4SHOJ+G6McO-nEkAiV-93-tqf5+bKw@mail.gmail.com>

It could be viewed as the simple complete tree to 1D array  with no
pointers described in lecture 8 here
https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-spring-2020/lecture-notes/index.htm
starting from min 15 in this video
https://youtu.be/Xnpo1atN-Iw

Since all trees in Utreexo forest are full binary trees, this is perfect to
use, and we can save *76*10?*2*size of pointer(probably4bytes)*
*~600MB *with almost no effort.

However, I suggest to put it in a 2D array to make it more easy to handle
(the indexing math) as we, different than the lecture, traverse in many
ways ( normally to delete or insert, and the parent siblings for the proofs)

I wrote more details here
https://bitcointalk.org/index.php?topic=5360009.0

On Thu, Sep 16, 2021, 14:37 Vincent <vincent.palazzo at protonmail.com> wrote:

> Hi.
>
> Thanks for the reference, but I missed where you want save space with this
> compression on the Merkle Tree.
>
> Regards.
>
> Vincent.
> vincenzo.palazzo at protonmail.com
> https://github.com/vincenzopalazzo
> ??????? Original Message ???????
> On Thursday, September 16th, 2021 at 5:15 AM, shymaa arafat <
> shymaa.arafat at gmail.com> wrote:
>
> Allow me to introduce this simple idea that could be useful ...
>
> -The Intuition was some discussion on Utreexo project about storage saving
> and some traversing issues in handling the UTXOS Merkle Tree/ forest; that
> is  N internal nodes need to be stored along with 2N pointers (left&right),
> + maybe 1 more pointer in the leaves special nodes to handle different
> traversing options (insert, delete, & differently proof fetch that traverse
> aunt or niece node according to your implementation
> https://github.com/mit-dci/utreexo/discussions/316)
> .
> Then, I thought of a simple idea that gets rid of all the pointers;
> specially appealing when we have all trees are full (complete) in the
> forest, but can work for any Merkle Tree:
>
> - 2D array with variable row size; R[j] is of length (N/2^j)
> -For example when N=8 nodes
> R[0]=0,1,2,...,7
> R[1]=8,9,10,11
> R[2]=12,13
> R[3]=14
> .
> -We can see that total storage is just 2N-1 nodes,
> no need for pointers, and traversing could be neat in any direction with
> the right formula:
>
> -Pseudo code to fetch proof[i] ...
>
> //direction to know + or -
> If ((i mod 2)==0) drct=1;
>             else drct=-1;
> // first, the sibling node
> proof[i]=R[0,i+drct]
>
> //add the rest thru loop
> For(j=1; j?logN; j++)
>  { index= i/(2^j)+drct;
>     proof[i]=Add(R[j,index]);
>  }
>
> -In fact it's just the simple primitive approach of transforming a
> recursion to an iteration, and even if Utreexo team solved their problem
> differently I thought it is worth telling as it can work for any Merkle Tree
> .
> Thanks for your time,
> Shymaa M Arafat
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210916/c0fa90ad/attachment-0001.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: IMG_20210913_194256.jpg
Type: image/jpeg
Size: 279763 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210916/c0fa90ad/attachment-0002.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: IMG_20210913_193322.jpg
Type: image/jpeg
Size: 164592 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210916/c0fa90ad/attachment-0003.jpg>

From giacomo.caironi at gmail.com  Thu Sep 16 21:36:48 2021
From: giacomo.caironi at gmail.com (Giacomo Caironi)
Date: Thu, 16 Sep 2021 23:36:48 +0200
Subject: [bitcoin-dev] Test cases for Taproot signature message
Message-ID: <CACHAfwcJrf8kc9+=2+ekjuPTPjW8T6qJS538QQ2DJedAn-XxKA@mail.gmail.com>

Hi,
recently I have worked on a python implementation of bitcoin signature
messages, and I have found that there was way better documentation about
Segwit signature message than Taproot.

1) Segwit signature message got its own BIP, completed with test cases
regarding only that specific function; Taproot on the other hand has the
signature message function defined in BIP 341 and the test vectors in a
different BIP (341). This is confusing. Shouldn't we create a different BIP
only for Taproot signature message exactly like Segwit?

2) The test vectors for Taproot have no documentation and, most
importantly, they are not atomic, in the sense that they do not target a
specific part of the taproot code but all of it. This may not be a very big
problem, but for signature verification it is. Because there are hashes
involved, we can't really debug why a signature message doesn't pass
validation, either it is valid or it is not. BIP 143 in this case is really
good, because it provides hash preimages, so it is possible to debug the
function and see where something went wrong. Because of this, writing the
Segwit signature hash function took a fraction of the time compared to
Taproot.

If this idea is accepted I will be more than happy to write the test cases
for Taproot.

BTW this is the first time I contribute to Bitcoin, let me know if I was
rude or did something wrong. Moreover english is not my first language, so
I apologize if I wrote something awful above
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210916/2ec10acd/attachment.html>

From bitcoin-dev at wuille.net  Thu Sep 16 22:30:19 2021
From: bitcoin-dev at wuille.net (Pieter Wuille)
Date: Thu, 16 Sep 2021 22:30:19 +0000
Subject: [bitcoin-dev] Test cases for Taproot signature message
In-Reply-To: <CACHAfwcJrf8kc9+=2+ekjuPTPjW8T6qJS538QQ2DJedAn-XxKA@mail.gmail.com>
References: <CACHAfwcJrf8kc9+=2+ekjuPTPjW8T6qJS538QQ2DJedAn-XxKA@mail.gmail.com>
Message-ID: <NgpYOVuE_3u6zfAZI6cxpc7iB5L_cGtTUrdCJfSdRgChJxOXsY3w0veIk0ZayeEvSeu3SE4AX_E27C6-Yu3MjCFJzMO6AR9g_1CLMJYVG1o=@wuille.net>

On Thursday, September 16th, 2021 at 5:36 PM, Giacomo Caironi via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:

> Hi,
> recently I have worked on a python implementation of bitcoin signature messages, and I have found that there was way better documentation about Segwit signature message than Taproot.
>
> 1) Segwit signature message got its own BIP, completed with test cases regarding only that specific function; Taproot on the other hand has the signature message function defined in BIP 341 and the test vectors in a different BIP (341). This is confusing. Shouldn't we create a different BIP only for Taproot signature message exactly like Segwit?

I'm not entirely sure what you mean; you're saying BIP 341 twice.

Still, you're right overall - there is no separate BIP for the signature message function. The reason is that the message function is different for BIP341 and BIP342. BIP 341 defines a basic common message function, which is then built up for BIP 341 key path spending, and for BIP 342 tapscript spending. This common part could have been a separate BIP, but that'd still not be a very clean separation. I'm not very inclined to support changing that at this point, given the state of deployment the BIPs have, but that doesn't mean the documentation/vectors can't be improved in the existing documents.

> 2) The test vectors for Taproot have no documentation and, most importantly, they are not atomic, in the sense that they do not target a specific part of the taproot code but all of it. This may not be a very big problem, but for signature verification it is. Because there are hashes involved, we can't really debug why a signature message doesn't pass validation, either it is valid or it is not. BIP 143 in this case is really good, because it provides hash preimages, so it is possible to debug the function and see where something went wrong. Because of this, writing the Segwit signature hash function took a fraction of the time compared to Taproot.

You're right. The existing tests are really intended for verifying an implementation against (and for making sure future code changes don't break anything). They have much higher coverage than the segwit tests had. But they aren't useful as documentation; the code that generates them (https://github.com/bitcoin/bitcoin/blob/v22.0/test/functional/feature_taproot.py#L605L1122) is probably better at that even, but still pretty dense.

> If this idea is accepted I will be more than happy to write the test cases for Taproot.

If you're interested in writing test vectors that are more aimed at helping debugging issues, by all means, do. You've already brought up the sighash code as an example. Another idea, primarily aimed at developers of signing code, is test vectors for certain P2TR scriptPubKeys, derived from certain internal keys and script trees. I'm happy to help to integrate such in Bitcoin Core and the BIP(s).

Thanks!

Cheers,

--
Pieter
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210916/75d36938/attachment.html>

From riccardo.casatta at gmail.com  Fri Sep 17 07:07:48 2021
From: riccardo.casatta at gmail.com (Riccardo Casatta)
Date: Fri, 17 Sep 2021 09:07:48 +0200
Subject: [bitcoin-dev] Test cases for Taproot signature message
In-Reply-To: <CACHAfwcJrf8kc9+=2+ekjuPTPjW8T6qJS538QQ2DJedAn-XxKA@mail.gmail.com>
References: <CACHAfwcJrf8kc9+=2+ekjuPTPjW8T6qJS538QQ2DJedAn-XxKA@mail.gmail.com>
Message-ID: <CADabwBB4jmwqyJAqfghWTGLiCHFYXset5ZvKB36Q2FkMomJCtQ@mail.gmail.com>

Hi Giacomo,

I wrote the rust implementation of bitcoin signature messages and to
double-check I created some test vectors you can see at
https://github.com/rust-bitcoin/rust-bitcoin/blob/b7f984972ad6cb4942827c2b7c401f590588cdcf/src/util/sighash.rs#L689-L799.
These vectors have been created printing intermediate results from
https://github.com/bitcoin/bitcoin/blob/6401de0133e32a641ed9e78a85b3aa337c75d190/test/functional/feature_taproot.py

Il giorno gio 16 set 2021 alle ore 23:40 Giacomo Caironi via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> ha scritto:

> Hi,
> recently I have worked on a python implementation of bitcoin signature
> messages, and I have found that there was way better documentation about
> Segwit signature message than Taproot.
>
> 1) Segwit signature message got its own BIP, completed with test cases
> regarding only that specific function; Taproot on the other hand has the
> signature message function defined in BIP 341 and the test vectors in a
> different BIP (341). This is confusing. Shouldn't we create a different BIP
> only for Taproot signature message exactly like Segwit?
>
> 2) The test vectors for Taproot have no documentation and, most
> importantly, they are not atomic, in the sense that they do not target a
> specific part of the taproot code but all of it. This may not be a very big
> problem, but for signature verification it is. Because there are hashes
> involved, we can't really debug why a signature message doesn't pass
> validation, either it is valid or it is not. BIP 143 in this case is really
> good, because it provides hash preimages, so it is possible to debug the
> function and see where something went wrong. Because of this, writing the
> Segwit signature hash function took a fraction of the time compared to
> Taproot.
>
> If this idea is accepted I will be more than happy to write the test cases
> for Taproot.
>
> BTW this is the first time I contribute to Bitcoin, let me know if I was
> rude or did something wrong. Moreover english is not my first language, so
> I apologize if I wrote something awful above
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>


-- 
Riccardo Casatta - @RCasatta <https://twitter.com/RCasatta>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210917/ff486528/attachment-0001.html>

From michaelfolkson at protonmail.com  Fri Sep 17 12:56:37 2021
From: michaelfolkson at protonmail.com (Michael Folkson)
Date: Fri, 17 Sep 2021 12:56:37 +0000
Subject: [bitcoin-dev] Tuesday's BIP process meeting
Message-ID: <yhts1Qr7NFB61xBzoZsCywlvJTGcBvLPruD38mwSX7hLcyfEzRck3ygA7ZbhFTnSYntSB_900YM6UHwFk8wRJhAjfjPlwRdfSVg9R82fhQM=@protonmail.com>

Tuesday's BIP process meeting was announced previously here [0].

A conversation log of the meeting is available here [1].

It looks promising at this stage that we'll eventually have a bundle of changes to warrant a new proposed BIP (BIP 3) to replace the current BIP process that is described in BIP 2 [2]. Obviously there was only a very small group of attendees at this week's IRC meeting and so there should (and will) be ample time for the community (and this list) to review whatever changes are proposed before they are considered for merging into the BIPs repository and before they take effect.

The following proposed changes were discussed:

1) An end to the 3 year rejection rule. In BIP 2 a BIP enters the "Rejected" state after 3 years if no progress had been made. The BIP champion then needs to address public criticism of the BIP to be able to leave the "Rejected" state. It is proposed instead that after 3 years the BIP would enter an "Inactive" state that only requires activity from the BIP champion to leave the "Inactive" state.

2) Currently BIP champions need to ACK pull requests with basic spelling/grammar changes before they can be merged. This is time consuming not only for the BIP editors who have to chase the BIP champions but it can be irritating for the BIP champions too especially if they champion multiple BIPs. It is proposed that BIP editors instead can use their judgment to merge in changes that don't impact the meaning of the BIP cc'ing the BIP champions and with reversions possible if the BIP champion is unhappy with the change. A single pull request making changes across multiple BIPs (e.g. spelling corrections) will not be considered for merging however.

3) BIP comments were introduced so that subject matter experts and informed critics of a proposal could make it clear to BIP readers and implementers of possible defects with the proposal. However, they have been rarely used and the few comments submitted on BIPs seem to have been widely ignored. Instead it is proposed that link(s) to bitcoin-dev mailing list post(s) with criticism or outlines of defects can be included within a BIP by the BIP editors such that the interested reader can easily be directed to the source of that information.

4) BIP champion(s) of soft fork BIPs containing consensus changes could theoretically include an activation method and parameters in their BIP unilaterally without consulting the broader community. (To be clear this is not necessarily what happened with Taproot activation parameters but there was confusion and disagreement about the role of BIPs and BIP editors in the perceived "finalization" of activation parameters.) This needs further discussion but proposed changes include sharpening the wording around activation parameters to make it clear that any parameters included are merely those recommended by the BIP champion(s) and don't necessarily have community consensus. Alternative proposals would be to not include activation methods or parameters within the BIP at all or to give BIP editors latitude to highlight concerns in a bitcoin-dev mailing list post and then include a link to that post within the BIP.

For details of other changes discussed in the meeting please see the conversation log [1]. Kalle Alm has also sent an email [3] on BIP extensions to this list.

The next meeting is on Wednesday September 29th (23:00 UTC) on the Libera IRC channel #bitcoin-dev.

[0]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019412.html
[1]: https://gist.github.com/michaelfolkson/f2870851bb812b4ac86006ea54ca78a2
[2]: https://github.com/bitcoin/bips/blob/master/bip-0002.mediawiki
[3]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019457.html

Michael Folkson
Email: michaelfolkson at protonmail.com
Keybase: michaelfolkson
PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210917/4e11d144/attachment.html>

From jlrubin at mit.edu  Fri Sep 17 16:58:45 2021
From: jlrubin at mit.edu (Jeremy)
Date: Fri, 17 Sep 2021 09:58:45 -0700
Subject: [bitcoin-dev] Inherited IDs - A safer,
 more powerful alternative to BIP-118 (ANYPREVOUT) for scaling
 Bitcoin
Message-ID: <CAD5xwhh-1zUbPgYW6hE8q3CmhFZFdEqjx5pB7+VFM4mV=1FfaQ@mail.gmail.com>

Bitcoin & LN Devs,

The below is a message that was shared to me by an anon account on Telegram
(nym: John Law). You can chat with them directly in the https://t.me/op_ctv
or https://t.me/bips_activation group. I'm reproducing it here at their
request as they were unsure of how to post to the mailing list without
compromising their identity (perhaps we should publish a guideline on how
to do so?).

Best,

Jeremy


Hi,

I'd like to propose an alternative to BIP-118 [1] that is both safer and
more
powerful. The proposal is called Inherited IDs (IIDs) and is described in a
paper that can be found here [2]. The paper presents IIDs and Layer 2
protocols
using IIDs that are far more scalable and usable than those proposed for
BIP-118
(including eltoo [3]).

Like BIP-118, IIDs are a proposal for a softfork that changes the rules for
calculating certain signatures. BIP-118 supports signatures that do not
commit to the transaction ID of the parent transaction, thus allowing
"floating
transactions". In contrast, the IID proposal does not allow floating
transactions, but it does allow an output to specify that child transaction
signatures commit to the parent transaction's IID, rather than its
transaction
ID.

IID Definitions
===============
* If T is a transaction, TXID(T) is the transaction ID of T.
* An output is an "IID output" if it is a native SegWit output with version
2
  and a 32-byte witness program, and is a "non-IID output" otherwise.
* A transaction is an "IID transaction" if it has at least one IID output.
* If T is a non-IID transaction, or a coinbase transaction, IID(T) =
TXID(T).
* If T is a non-coinbase IID transaction, first_parent(T) = F is the
transaction
  referenced by the OutPoint in T's input 0, and IID(T) = hash(IID(F) ||
F_idx)
  where F_idx is the index field in the OutPoint in T's input 0 (that is,
T's
  input 0 spends F's output F_idx).

IID Signature Validation
========================
* Signatures that spend IID outputs commit to signature messages in which
IIDs
  replace transaction IDs in all OutPoints of the child transaction that
spend
  IID outputs.

Note that IID(T) can be calculated from T (if it is a non-IID or a coinbase
transaction) or from T and F (otherwise). Therefore, as long as nodes store
(or
calculate) the IID of each transaction in the UTXO set, they can validate
signatures of transactions that spend IID outputs. Thus, the IID proposal
fits
Bitcoin's existing UTXO model, at the small cost of adding a 32-byte IID
value
for certain unspent outputs. Also, note that the IID of a transaction may
not
commit to the exact contents of the transaction, but it does commit to how
the
transaction is related to some exactly-specified transaction (such as being
the
first child of the second child of a specific transaction). As a result, a
transaction that is signed using IIDs cannot be used more than once or in an
unanticipated location, thus making it much safer than a floating
transaction.

2-Party Channel Protocols
=========================
BIP-118 supports the eltoo protocol [3] for 2-party channels, which improves
upon the Lightning protocol for 2-party channels [4] by:
1) simplifying the protocol,
2) eliminating penalty transactions, and
3) supporting late determination of transaction fees [1, Sec. 4.1.5].

The IID proposal does not support the eltoo protocol. However, the IID
proposal
does support a 2-party channel protocol, called 2Stage [2, Sec. 3.3], that
is
arguably better than eltoo. Specifically, 2Stage achieves eltoo's 3
improvements
listed above, plus it:
4) eliminates the need for watchtowers [2, Sec. 3.6], and
5) has constant (rather than linear) worst-case on-chain costs [2, Sec.
3.4].

Channel Factories
=================
In general, an on-chain transaction is required to create or close a 2-party
channel. Multi-party channel factories have been proposed in order to allow
a
fixed set of parties to create and close numerous 2-party channels between
them,
thus amortizing the on-channel costs of those channels [5]. BIP-118 also
supports simple and efficient multi-party channel factories via the eltoo
protocol [1, Sec. 5.2] (which are called "multi-party channels" in that
paper).

While the IID proposal does not support the eltoo protocol, it does support
channel factories that are far more scalable and powerful than any
previously-
proposed channel factories (including eltoo factories). Specifically, IIDs
support a simple factory protocol in which not all parties need to sign the
factory's funding transaction [2, Sec. 5.3], thus greatly improving the
scale
of the factory (at the expense of requiring an on-chain transaction to
update
the set of channels created by the factory). These channel factories can be
combined with the 2Stage protocol to create trust-free and watchtower-free
channels including very large numbers of casual users.

Furthermore, IIDs support channel factories with an unbounded number of
parties
that allow all of the channels in the factory to be bought and sold by
anyone
(including parties not originally in the factory) with a single on-chain
transaction in a trust-free manner [2, Secs. 6 and 7]. As a result, a single
on-chain transaction can be used in place of thousands, or even millions, of
Lightning or eltoo on-chain transactions. These channel factory protocols
make
critical use of IIDs and do not appear to be possible with BIP-118.

Next Steps
==========
If IIDs sounds interesting, please take a look at the IID paper [2]. It
contains
many results not listed above, including rules for SVP nodes, protocols for
off-chain channel networks, Layer 2 protocol extensions, support for
covenants
(including vaults), and nearly matching lower and upper bounds on
multi-party
channels.

The paper also includes 3 options for how IIDs could be added to Bitcoin
via a
softfork [2, Appendix A]. I'm new to Bitcoin and am not sure which of these
3
options is best. If anyone finds the IID proposal valuable, I would greatly
appreciate it if they were willing to pick the best option (or invent an
even
better option) for adding IIDs to Bitcoin and create a BIP for that option.
Hopefully, IIDs will provide a safe way to dramatically scale Bitcoin while
improving its usability.

Thanks,
John


References
==========

[1] BIP-118: https://anyprevout.xyz and
https://github.com/bitcoin/bips/pull/943

[2] Scaling Bitcoing with Inherited IDs, by John Law:
    iids13.pdf at https://github.com/JohnLaw2/btc-iids

[3] eltoo: A Simple Layer2 Protocol for Bitcoin, by Decker, Russell &
Osuntokun:
    https://blockstream.com/eltoo.pdf

[4] The Bitcoin Lightning Network, by Poon & Dryja:
    https://lightning.network/lightning-network-paper.pdf

[5] Scalable Funding of Bitcoin Micropayment Channel Networks, by Burchert,
    Decker & Wattenhofer: http://dx.doi.org/10.1098/rsos.180089

Acknowledgments
===============
Thanks to Ruben Somsen and Jeremy Rubin for their helpful comments.

Also, thanks to Bob McElrath for his original brainstorm that led to the
creation of the IID concept:
https://diyhpl.us/wiki/transcripts/2019-02-09-mcelrath-on-chain-defense-in-depth

<https://twitter.com/JeremyRubin>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210917/34fb43eb/attachment-0001.html>

From aj at erisian.com.au  Sat Sep 18 11:37:40 2021
From: aj at erisian.com.au (Anthony Towns)
Date: Sat, 18 Sep 2021 21:37:40 +1000
Subject: [bitcoin-dev] Inherited IDs - A safer,
 more powerful alternative to BIP-118 (ANYPREVOUT) for scaling
 Bitcoin
In-Reply-To: <CAD5xwhh-1zUbPgYW6hE8q3CmhFZFdEqjx5pB7+VFM4mV=1FfaQ@mail.gmail.com>
References: <CAD5xwhh-1zUbPgYW6hE8q3CmhFZFdEqjx5pB7+VFM4mV=1FfaQ@mail.gmail.com>
Message-ID: <20210918113740.GA27989@erisian.com.au>

On Fri, Sep 17, 2021 at 09:58:45AM -0700, Jeremy via bitcoin-dev wrote,
on behalf of John Law:

> I'd like to propose an alternative to BIP-118 [1] that is both safer and more
> powerful. The proposal is called Inherited IDs (IIDs) and is described in a
> paper that can be found here [2]. [...]

Pretty sure I've skimmed this before but hadn't given it a proper look.
Saying "X is more powerful" and then saying it can't actually do the
same stuff as the thing it's "more powerful" than always strikes me as
a red flag. Anyhoo..

I think the basic summary is that you add to each utxo a new resettable
"structural" tx id called an "iid" and indetify input txs that way when
signing, so that if the details of the transaction changes but not the
structure, the signature remains valid.

In particular, if you've got a tx with inputs tx1:n1, tx2:n2, tx3:n3, etc;
and outputs out1, out2, out3, etc, then its structual id is hash(iid(tx1),
n1) if any of its outputs are "tagged" and it's not a coinbase tx, and
otherwise it's just its txid.  (The proposed tagging is to use a segwit
v2 output in the tx, though I don't think that's an essential detail)

So if you have a tx A with 3 outputs, then tx B spends "A:0, A:1" and
tx C spends "B:0" and tx D spends "C:0", if you replace B with B',
then if both B and B' were tagged, and the signatures for C (and D,
assuming C was tagged) will still be valid for spending from B'.

So the question is what you can do with that.

The "2stage" protocol is proposed as an alternative to eltoo is
essentially just:

 a) funding tx gets dropped to the chain
 b) closing state is proposed by one party
 c) other party can immediately finalise by confirming a final state
    that matches the proposed closing state, or was after it
 d) if the other party's not around for whatever delay, the party that
    proposed the close can finalise it

That doesn't work for more than two participants, because two of
the participants could collude to take the fast path in (c) with some
earlier state, robbing any other participants. That said, this is a fine
protocol for two participants, and might be better than doing the full
eltoo arrangement if you only have a two participant channel.

To make channel factories work in this model, I think the key step is
using invalidation trees to allow updating the split of funds between
groups of participants. I think invalidation trees introduce a tradeoff
between (a) how many updates you can make, and (b) how long you have to
notice a close is proposed and correct it, before an invalidated state
can be posted, and (c) how long it will take to be able to extract your
funds from the factory if there are problems initially. You reduce those
delays substantially (to a log() factor) by introducing a hierarchy of
update txs (giving you a log() number of txs), I think.

That's the "multisig factories" section anyway, if I'm
following correctly. The "timeout trees", "update-forest" and
"challenge-and-response" approaches both introduce a trusted user ("the
operator"), I think, so are perhaps more comparable to statechains
than eltoo?

So how does that compare, in my opinion?

If you consider special casing two-party channels with eltoo, then I
think eltoo-2party and 2stage are equally effective. Comparing
eltoo-nparty and the multisig iid factories approach, I think the
uncooperative case looks like:

 ms-iid:
   log(n) txs (for the invalidation tree)
   log(n) time (?) (for the delays to ensure invalidated states don't
                    get published)

 eltoo: 1 tx from you
        1 block after you notice, plus the fixed csv delay

A malicious counterparty can post many old update states prior to you
poisting the latest state, but those don't introduce extra csv delays
and you aren't paying the fees for those states, so I don't think it
makes sense to call that an O(n) delay or cost.

An additional practical problem with lightning is dealing with layered
commitments; that's a problem both for the delays while waiting for a
potential rejection in 2stage and for the invalidation tree delays in the
factory construction. But it's not a solved problem for eltoo yet, either.

As far as implementation goes, introducing the "iid" concept would mean
that info would need to be added to the utxo database -- if every utxo
got an iid, that would be perhaps a 1.4GB increase to the utxo db (going
by unique transaction rather than unique output), but presumably iid txs
would end up being both uncommon and short-lived, so the cost is probably
really mostly just in the additional complexity. Both iid and ANYPREVOUT
require changes to how signatures are evaluated and apps that use the
new feature are written, but ANYPREVOUT doesn't need changes beyond that.

(Also, the description of OP_CODESEPARATOR (footnote 13 on page 13,
ominous!) doesn't match its implementation in taproot. It also says BIP
118 introduces a new address type for floating transactions, but while
this was floated on the list, the current draft of 118 just introduces
a new tapscript key type for normal taproot addresses)

I think you can pretty easily simulate this construction with
anyprevout. Where you would have had A:1 spent by B, and B:2 and B:3
spent by C, change the derivation paths for the keys a1, b2, and b3
to append "/1", "/1/2" and "/1/3" and don't reuse them, and sign with
anyprevout when constructing B and C and any replacement transactions
for B and C.  So I don't think this allows any new constructions that
anyprevout wouldn't.

Cheers,
aj


From giacomo.caironi at gmail.com  Sat Sep 18 11:32:28 2021
From: giacomo.caironi at gmail.com (Giacomo Caironi)
Date: Sat, 18 Sep 2021 13:32:28 +0200
Subject: [bitcoin-dev] Test cases for Taproot signature message
In-Reply-To: <NgpYOVuE_3u6zfAZI6cxpc7iB5L_cGtTUrdCJfSdRgChJxOXsY3w0veIk0ZayeEvSeu3SE4AX_E27C6-Yu3MjCFJzMO6AR9g_1CLMJYVG1o=@wuille.net>
References: <CACHAfwcJrf8kc9+=2+ekjuPTPjW8T6qJS538QQ2DJedAn-XxKA@mail.gmail.com>
 <NgpYOVuE_3u6zfAZI6cxpc7iB5L_cGtTUrdCJfSdRgChJxOXsY3w0veIk0ZayeEvSeu3SE4AX_E27C6-Yu3MjCFJzMO6AR9g_1CLMJYVG1o=@wuille.net>
Message-ID: <CACHAfwfPTQvUwzqs1mg3Z-FwtcuwGgJfBeK6r0ovtRZKB=rA5A@mail.gmail.com>

Ok I have created three test cases, you can find them here
<https://gist.github.com/giacomocaironi/e41a45195b2ac6863ec46e8f86324757>.
They cover most of the SigMsg function but they don't cover the ext_flag,
so they are only for taproot key path; but if you want to test for script
paths you have to implement more than this function so you would use the
official test vector.
Could someone please take a look at them? I think that they are right but I
am not too sure

Il giorno ven 17 set 2021 alle ore 00:30 Pieter Wuille <
bitcoin-dev at wuille.net> ha scritto:

> On Thursday, September 16th, 2021 at 5:36 PM, Giacomo Caironi via
> bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
>
> Hi,
> recently I have worked on a python implementation of bitcoin signature
> messages, and I have found that there was way better documentation about
> Segwit signature message than Taproot.
>
> 1) Segwit signature message got its own BIP, completed with test cases
> regarding only that specific function; Taproot on the other hand has the
> signature message function defined in BIP 341 and the test vectors in a
> different BIP (341). This is confusing. Shouldn't we create a different BIP
> only for Taproot signature message exactly like Segwit?
>
>
> I'm not entirely sure what you mean; you're saying BIP 341 twice.
>
> Still, you're right overall - there is no separate BIP for the signature
> message function. The reason is that the message function is different for
> BIP341 and BIP342. BIP 341 defines a basic common message function, which
> is then built up for BIP 341 key path spending, and for BIP 342 tapscript
> spending. This common part could have been a separate BIP, but that'd still
> not be a very clean separation. I'm not very inclined to support changing
> that at this point, given the state of deployment the BIPs have, but that
> doesn't mean the documentation/vectors can't be improved in the existing
> documents.
>
> 2) The test vectors for Taproot have no documentation and, most
> importantly, they are not atomic, in the sense that they do not target a
> specific part of the taproot code but all of it. This may not be a very big
> problem, but for signature verification it is. Because there are hashes
> involved, we can't really debug why a signature message doesn't pass
> validation, either it is valid or it is not. BIP 143 in this case is really
> good, because it provides hash preimages, so it is possible to debug the
> function and see where something went wrong. Because of this, writing the
> Segwit signature hash function took a fraction of the time compared to
> Taproot.
>
>
> You're right. The existing tests are really intended for verifying an
> implementation against (and for making sure future code changes don't break
> anything). They have much higher coverage than the segwit tests had. But
> they aren't useful as documentation; the code that generates them (
> https://github.com/bitcoin/bitcoin/blob/v22.0/test/functional/feature_taproot.py#L605L1122)
> is probably better at that even, but still pretty dense.
>
> If this idea is accepted I will be more than happy to write the test cases
> for Taproot.
>
>
> If you're interested in writing test vectors that are more aimed at
> helping debugging issues, by all means, do. You've already brought up the
> sighash code as an example. Another idea, primarily aimed at developers of
> signing code, is test vectors for certain P2TR scriptPubKeys, derived from
> certain internal keys and script trees. I'm happy to help to integrate such
> in Bitcoin Core and the BIP(s).
>
> Thanks!
>
> Cheers,
>
> --
> Pieter
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210918/40269758/attachment-0001.html>

From antoine.riard at gmail.com  Sat Sep 18 14:11:10 2021
From: antoine.riard at gmail.com (Antoine Riard)
Date: Sat, 18 Sep 2021 10:11:10 -0400
Subject: [bitcoin-dev] TAPLEAF_UPDATE_VERIFY covenant opcode
In-Reply-To: <20210915065051.GA26119@erisian.com.au>
References: <20210909064138.GA22496@erisian.com.au>
 <CALZpt+FnnbGJC4=KO_OPiKxt0Ey9Bzh1gxP1dQSDz2aBi9WyOA@mail.gmail.com>
 <20210911032644.GB23578@erisian.com.au>
 <CALZpt+HzM__OJntegOhDqkg5zU=PQXtKgQoB518A2qP9=foovw@mail.gmail.com>
 <20210915065051.GA26119@erisian.com.au>
Message-ID: <CALZpt+Hczvy1Fxu40cCKKC8bR9fouQ+sAiqV65-Z4VuLp+Bi7w@mail.gmail.com>

> I think "<I> <P> MERKLESUB" is the same as "<P> OP_0 2 TLUV", provided
> <I> happens to be the same index as the current input. So it misses the
> ability to add branches (replacing OP_0 with a hash), the ability to
> preserve the current script (replacing 2 with 0), and the ability to
> remove some of the parent paths (replacing 2 with 4*n); but gains the
> ability to refer to non-corresponding outputs.

Yes, I agree.

> That... doesn't sound very straightforward to me; it's basically
> introducing a new covenant approach, that's getting fixed into a
> signature, rather than being a separate opcode.

I think one design advantage of combining scope-minimal opcodes like
MERKLESUB with sighash malleability is the ability to update a subset of
the off-chain contract transactions fields after the funding phase. With a
lower level of cooperation than required by the key path. I think not an
ability offered by templated covenants.

> I'm not really sure what you're saying there; is that any different to a
> pool of (A and B) where A suddenly wants to withdraw funds ASAP and can't
> wait for a key path signature? In that case A authorises the withdrawal
> and does whatever she wants with the funds (including form a new pool),
> and B remains in the pool.

Yes this is a different contract policy that I would like to set up.

Let's say you would like to express the following set of capabilities.

C0="Split the 4 BTC funds between Alice/Bob and Caroll/Dave"
C1="Alice can withdraw 1 BTC after 2 weeks"
C2="Bob can withdraw 1 BTC after 2 weeks"
C3="Caroll can withdraw 1 BTC after 2 weeks"
C4="Dave can withdraw 1 BTC after 2 weeks"
C5="If USDT price=X, Alice can withdraw 2 BTC or Caroll can withdraw 2 BTC"

If C4 is exercised, to avoid trust in the remaining counterparty, both
Alice or Caroll should be able to conserve the C5 option, without relying
on the updated key path.

As you're saying, as we know the group in advance, one way to setup the tree
could be:

       (A, (((((B, C), BC), D), BCD), ((((E, F), EF), G), EFG)))

where:
A="1 <alice> <caroll> 2 CHECKMULTISIG <usdt_oracle> CHECKSIG"
B="<alice> DUP 0 2 TLUV CHECKSIG"
C="<bob> DUP 0 2 TLUV CHECKSIG"
D="<alice+bob> 0 6 TLUV 1 <caroll> <dave> 2 CHECKMULTISIG"
E="<caroll> DUP 0 2 TLUV CHECKSIG"
F="<dave> DUP 0 2 TLUV CHECKSIG"
G="<caroll+dave> 0 6 TLUV 1 <alice> <bob> 2 CHECKMULTISIG"

E.g, if D is exercised, B+C+D are removed from the tree and A, E, F, G are
conserved in the Caroll/Dave fork. Then Caroll can exercise the USDT option
without trusting Dave.

Note, this solution isn't really satisfying as the G path isn't neutralized
on the Caroll/Dave fork and could be replayed by Alice or Bob... One
improvement could be to have the "withdraw" script path (C,D,F,G) expressed
redundantly. That way when a "split" script path is exercised the uncle
split path and all the siblings "withdraw" paths can be removed.

Echoing your point about the difficulty of reliably composing arbitrary
subsets of the pool, I lean to agree that merkle trees aren't the most
straightforward way to encode that kind of contract policy.

> If you're worried about the cost of a single byte of witness data you
> probably can't afford to do script path spends at all -- certainly
> having to do 64 bytes of witness data to add a signature that commits
> to an amount and the like will be infeasible in that case.

Yes, I agree fully templated covenants are more efficient to save witness
data.

I still like the idea of inserting a key as you might have an interesting
ability.
Like a N-of-M, a subset of the vault/pool able to update the withdraw
pubkey.

> That doesn't work. Suppose you start off with an even internal pubkey,
> with three scripts, (A, (B,C)). All of those scripts have tapscript
> version 0xc0 because the internal pubkey is even. You spend using A and
> calculate the new internal pubkey which turns out to be odd. You then
> need to change B and C's script version from 0xc0 to 0x20, but you can't
> do that (at least, you can't do it without revealing every script).

I'm not sure we're aligned on the mechanism.

We introduce a new tapscript version 0x20.

At spent taproot commitment verification, if the tapscript version=0x20,
the second-lowest bit of the first byte of the control block is interpreted
as the parity bit of the spent internal pubkey (i.e control[0] & 0x2).

This parity bit is used to compute a new format of TapTweakV2=H(p || m ||
bit) and commitment verification keep proceeding unmodified.

As the leaf version is committed as part of every TapLeaf, I think any
usage of MERKLESUB would require to use tapscript version 0x20 for the
whole set of leaves.

If you build a tree blurring 0xc0 leaves and TapTweakV2, I think those
leaves will be unspendable as they will always fail the commitment
verification.

> Changing the TapTweak calculation is a hard fork; existing software
> already verifies the calculation even if the script version is unknown.

Thinking more, you're right...

In case of TapTweakV2, non-upgraded nodes won't be able to pass the
validation of unknown script version (0x20), and the failure will provoke a
fork.

Could we commit the spent internal pubkey parity bit as a one-more-tweak
transparent to non-upgrades nodes ?

For upgraded, P = R + (t2 * G) and Q = P + (t1 * G)
For non-upgraded, Q = P + (t1 * G).

Could we add a new validation rule (e.g VerifyInternalPubkeyCommitment)
conditional on a newer tapscript version just before
VerifyTaprootCommitment ?

> That is, the strategy isn't "tweak the scripts by delaying them 3 months"
> it's "tweak the merkle tree, to replace the scripts that would be delayed
> with a new script that has a delay and then allows itself to be replaced
> by the original scripts that we now want back".

Yes, that's a good strategy to have logically equivalent subtree embedded
in the modifying tapscript.

If you have multiple modifying scripts and you can't predict the order, I
think the tree complexity will be quickly too high and grafroot-like
approaches are likely better

Le mer. 15 sept. 2021 ? 02:51, Anthony Towns <aj at erisian.com.au> a ?crit :

> On Sun, Sep 12, 2021 at 07:37:56PM -0400, Antoine Riard via bitcoin-dev
> wrote:
> > While MERKLESUB is still WIP, here the semantic. [...]
> > I believe this is matching your description and the main difference
> compared to
> > your TLUV proposal is the lack of merkle tree extension, where a new
> merkle
> > path is added in place of the removed tapscript.
>
> I think "<I> <P> MERKLESUB" is the same as "<P> OP_0 2 TLUV", provided
> <I> happens to be the same index as the current input. So it misses the
> ability to add branches (replacing OP_0 with a hash), the ability to
> preserve the current script (replacing 2 with 0), and the ability to
> remove some of the parent paths (replacing 2 with 4*n); but gains the
> ability to refer to non-corresponding outputs.
>
> > > That would mean anyone who could do a valid spend of the tx could
> > > violate the covenant by spending to an unencumbered witness v2 output
> > > and (by collaborating with a miner) steal the funds. I don't think
> > > there's a reasonable way to have existing covenants be forward
> > > compatible with future destination addresses (beyond something like CTV
> > > that strictly hardcodes them).
> > That's a good catch, thanks for raising it :)
> > Depends how you define reasonable, but I think one straightforward fix
> is to
> > extend the signature digest algorithm to encompass the segwit version
> (and
> > maybe program-size ?) of the spending transaction outputs.
>
> That... doesn't sound very straightforward to me; it's basically
> introducing a new covenant approach, that's getting fixed into a
> signature, rather than being a separate opcode.
>
> I think a better approach for that would be to introduce the opcode (eg,
> PUSH_OUTPUT_SCRIPTPUBKEY, and SUBSTR to be able to analyse the segwit
> version), and make use of graftroot to allow a signature to declare that
> it's conditional on some extra script code. But it feels like it's going
> a bit off topic.
>
> > > Having the output position parameter might be an interesting way to
> > > merge/split a vault/pool, but it's not clear to me how much sense it
> > > makes sense to optimise for that, rather than just doing that via the
> key
> > > path. For pools, you want the key path to be common anyway (for privacy
> > > and efficiency), so it shouldn't be a problem; but even for vaults,
> > > you want the cold wallet accessible enough to be useful for the case
> > > where theft is attempted, and maybe that's also accessible enough for
> > > the ocassional merge/split to keep your utxo count/sizes reasonable.
> > I think you can come up with interesting contract policies. Let's say
> you want
> > to authorize the emergency path of your pool/vault balances if X happens
> (e.g a
> > massive drop in USDT price signed by DLC oracles). You have (A+B+C+D)
> forking
> > into (A+B) and (C+D) pooled funds. To conserve the contracts
> pre-negotiated
> > economic equilibrium, all the participants would like the emergency path
> to be
> > inherited on both forks. Without relying on the key path interactivity,
> which
> > is ultimately a trust on the post-fork cooperation of your counterparty ?
>
> I'm not really sure what you're saying there; is that any different to a
> pool of (A and B) where A suddenly wants to withdraw funds ASAP and can't
> wait for a key path signature? In that case A authorises the withdrawal
> and does whatever she wants with the funds (including form a new pool),
> and B remains in the pool.
>
> I don't think you can reliably have some arbitrary subset of the pool
> able to withdraw atomically without using the key path -- if A,B,C,D have
> individual scripts allowing withdrawal, then there's no way of setting
> the tree up so that every pair of members can have their scripts cut
> off without also cutting off one or both of the other members withdrawal
> scripts.
>
> If you know in advance which groups want to stick together, you could
> set things up as:
>
>   (((A, B), AB), C)
>
> where:
>
>   A =   "A DUP H(B') 10 TLUV CHECKSIG"  -> (B', C)
>   B =   "B DUP H(A') 10 TLUV CHECKSIG"  -> (A', C)
>   A' =  "A DUP 0 2 TLUV CHECKSIG"   -> (C)
>   B' =  "B DUP 0 2 TLUV CHECKSIG"   -> (C)
>   AB =  "(A+B) DUP 6 TLUV CHECKSIG  -> (C)
>   C  =  "C DUP 0 2 TLUV CHECKSIG"   -> ((A,B), AB)
>
> (10 = 2+4*2 = drop my script, my sibling and my uncle; 6 = 2+4*1 =
> drop my script and my sibling; 2 = drop my script only)
>
> Which would let A and B exit together in a single tx rather than needing
> two
> transactions to exit separately.
>
> > > Saving a byte of witness data at the cost of specifying additional
> > > opcodes seems like optimising the wrong thing to me.
> > I think we should keep in mind that any overhead cost in the usage of a
> script
> > primitive is echoed to the user of off-chain contract/payment channels.
> If the
> > tapscripts are bigger, your average on-chain spends in case of
> non-cooperative
> > scenarios are increased in consequence, and as such your fee-bumping
> reserve.
> > Thus making those systems less economically accessible.
>
> If you're worried about the cost of a single byte of witness data you
> probably can't afford to do script path spends at all -- certainly
> having to do 64 bytes of witness data to add a signature that commits
> to an amount and the like will be infeasible in that case.
>
> > > I don't think that works, because different scripts in the same merkle
> > > tree can have different script versions, which would here indicate
> > > different parities for the same internal pub key.
> > Let me make it clearer. We introduce a new tapscript version 0x20,
> forcing a
> > new bit in the first byte of the control block to be interpreted as the
> parity
> > bit of the spent internal pubkey.
>
> That doesn't work. Suppose you start off with an even internal pubkey,
> with three scripts, (A, (B,C)). All of those scripts have tapscript
> version 0xc0 because the internal pubkey is even. You spend using A and
> calculate the new internal pubkey which turns out to be odd. You then
> need to change B and C's script version from 0xc0 to 0x20, but you can't
> do that (at least, you can't do it without revealing every script).
>
> > To ensure this parity bit is faithful and
> > won't break the updated key path, it's committed in the spent taptweak.
>
> Changing the TapTweak calculation is a hard fork; existing software
> already verifies the calculation even if the script version is unknown.
>
> > > The IN_OUT_AMOUNT opcode lets you do maths on the values, so you can
> > > specify "hot wallets can withdraw up to X" rather than "hot wallets
> > > must withdraw exactly X". I don't think there's a way of doing that
> with
> > > SIGHASH_GROUP, even with a modifier like ANYPUBKEY?
> > You can exchange signatures for withdraw outputs with multiples `nValue`
> > covering the authorized range, assuming the ANYAMOUNT modifier ?
>
> If you want your hotwallet to be able to withdraw up to $2000, that's
> around 4,000,000 sats, so you'd be doing up to 4M signatures there if you
> wanted to get the exact value you're trying to send, without having to
> either overpay, or first pay yourself then have another tx that splits
> your withdrawal into what you're spending and change that's no longer
> in your vault.
>
> > One advantage
> > of leveraging sighash is the ability to update a withdraw policy in
> real-time.
> > Vaults participants might be willing to bump the withdraw policy beyond
> X,
> > assuming you have N-of-M consents.
>
> I mean, maybe? It seems like a very heavy weight construct where a more
> general approach would probably be better (eg, graftroot to attach a
> script to a signature; or checkdatasig or whatever so you push a value
> to the stack then check it's signature, then reuse the authenticated
> data against other checks) so that you only have to supply a signature
> when you want to be able to approve things after the fact.
>
> > I think I would like to express the following contract policy. Let's say
> you
> > have 1) a one-time conditional script path to withdraw fund ("a put on
> strike
> > price X"), 2) a conditional script path to tweak by 3 months all the
> usual
> > withdraw path and 3) those remaining withdraw paths. Once played out,
> you would
> > like the one-time path to be removed from your merkle tree. And this
> removal to
> > be inherited on the tweaked tree if 2) plays out.
>
> Okay, so I think that means we've got the unconditional withdraw path
> "U" (your 1), the delay path "D" (your 2) and some normal path(s) "N"
> (your 3). I think you can get that behaviour with:
>
>    S1 = Merkle( U, (D, N) )
>    S2 = Merkle( U, W )
>    S3 = Merkle( N )
>
> that is, you start off with the funds in scriptPubKey S1, then spend
> using D to get to S2, then spend using W to get to S3, then presumably
> spend using N at some point.
>
> The script for W is just:
>
>    "IN_OUT_AMOUNT EQUALVERIFY 0 <N> 6 TLUV <3 months> CSV"
>        (drop the script, drop its sibling, add N, wait 3 months)
>
> The script for D is:
>
>    "IN_OUT_AMOUNT EQUALVERIFY 0 <W> 6 TLUV <sigcheck...>"
>        (drop the script, drop its sibling, add W, extra conditions
>         to avoid anyone being able to delay things)
>
> That is, the strategy isn't "tweak the scripts by delaying them 3 months"
> it's "tweak the merkle tree, to replace the scripts that would be delayed
> with a new script that has a delay and then allows itself to be replaced
> by the original scripts that we now want back".
>
> Cheers,
> aj
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210918/1e644c1b/attachment-0001.html>

From antoine.riard at gmail.com  Sun Sep 19 23:16:45 2021
From: antoine.riard at gmail.com (Antoine Riard)
Date: Sun, 19 Sep 2021 19:16:45 -0400
Subject: [bitcoin-dev] Proposal: Package Mempool Accept and Package RBF
In-Reply-To: <CAFXO6=+cHyQKM2n9yn4EhwLZO+AUB0ZD81qWPxmpN27rjUoU3w@mail.gmail.com>
References: <CAFXO6=+cHyQKM2n9yn4EhwLZO+AUB0ZD81qWPxmpN27rjUoU3w@mail.gmail.com>
Message-ID: <CALZpt+HpvmEHUEOgye34T6pVQ+wnKKn-_8cTJTQXYQb9t1jOTA@mail.gmail.com>

Hi Gloria,

> A package may contain transactions that are already in the mempool. We
> remove
> ("deduplicate") those transactions from the package for the purposes of
> package
> mempool acceptance. If a package is empty after deduplication, we do
> nothing.

IIUC, you have a package A+B+C submitted for acceptance and A is already in
your mempool. You trim out A from the package and then evaluate B+C.

I think this might be an issue if A is the higher-fee element of the ABC
package. B+C package fees might be under the mempool min fee and will be
rejected, potentially breaking the acceptance expectations of the package
issuer ?

Further, I think the dedup should be done on wtxid, as you might have
multiple valid witnesses. Though with varying vsizes and as such offering
different feerates.

E.g you're going to evaluate the package A+B and A' is already in your
mempool with a bigger valid witness. You trim A based on txid, then you
evaluate A'+B, which fails the fee checks. However, evaluating A+B would
have been a success.

AFAICT, the dedup rationale would be to save on CPU time/IO disk, to avoid
repeated signatures verification and parent UTXOs fetches ? Can we achieve
the same goal by bypassing tx-level checks for already-in txn while
conserving the package integrity for package-level checks ?

> Note that it's possible for the parents to be
> indirect
> descendants/ancestors of one another, or for parent and child to share a
> parent,
> so we cannot make any other topology assumptions.

I'm not clearly understanding the accepted topologies. By "parent and child
to share a parent", do you mean the set of transactions A, B, C, where B is
spending A and C is spending A and B would be correct ?

If yes, is there a width-limit introduced or we fallback on
MAX_PACKAGE_COUNT=25 ?

IIRC, one rationale to come with this topology limitation was to lower the
DoS risks when potentially deploying p2p packages.

Considering the current Core's mempool acceptance rules, I think CPFP
batching is unsafe for LN time-sensitive closure. A malicious tx-relay
jamming successful on one channel commitment transaction would contamine
the remaining commitments sharing the same package.

E.g, you broadcast the package A+B+C+D+E where A,B,C,D are commitment
transactions and E a shared CPFP. If a malicious A' transaction has a
better feerate than A, the whole package acceptance will fail. Even if A'
confirms in the following block,
the propagation and confirmation of B+C+D have been delayed. This could
carry on a loss of funds.

That said, if you're broadcasting commitment transactions without
time-sensitive HTLC outputs, I think the batching is effectively a fee
saving as you don't have to duplicate the CPFP.

IMHO, I'm leaning towards deploying during a first phase 1-parent/1-child.
I think it's the most conservative step still improving second-layer safety.

> *Rationale*:  It would be incorrect to use the fees of transactions that
are
> already in the mempool, as we do not want a transaction's fees to be
> double-counted for both its individual RBF and package RBF.

I'm unsure about the logical order of the checks proposed.

If A+B is submitted to replace A', where A pays 0 sats, B pays 200 sats and
A' pays 100 sats. If we apply the individual RBF on A, A+B acceptance
fails. For this reason I think the individual RBF should be bypassed and
only the package RBF apply ?

Note this situation is plausible, with current LN design, your counterparty
can have a commitment transaction with a better fee just by selecting a
higher `dust_limit_satoshis` than yours.

> Examples F and G [14] show the same package, but P1 is submitted
> individually before
> the package in example G. In example F, we can see that the 300vB package
> pays
> an additional 200sat in fees, which is not enough to pay for its own
> bandwidth
> (BIP125#4). In example G, we can see that P1 pays enough to replace M1,
but
> using P1's fees again during package submission would make it look like a
> 300sat
> increase for a 200vB package. Even including its fees and size would not
be
> sufficient in this example, since the 300sat looks like enough for the
300vB
> package. The calculcation after deduplication is 100sat increase for a
> package
> of size 200vB, which correctly fails BIP125#4. Assume all transactions
have
> a
> size of 100vB.

What problem are you trying to solve by the package feerate *after* dedup
rule ?

My understanding is that an in-package transaction might be already in the
mempool. Therefore, to compute a correct RBF penalty replacement, the vsize
of this transaction could be discarded lowering the cost of package RBF.

If we keep a "safe" dedup mechanism (see my point above), I think this
discount is justified, as the validation cost of node operators is paid for
?

> The child cannot replace mempool transactions.

Let's say you issue package A+B, then package C+B', where B' is a child of
both A and C. This rule fails the acceptance of C+B' ?

I think this is a footgunish API, as if a package issuer send the
multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.
Then try to broadcast the higher-feerate C'+D' package, it should be
rejected. So it's breaking the naive broadcaster assumption that a
higher-feerate/higher-fee package always replaces ? And it might be unsafe
in protocols where states are symmetric. E.g a malicious counterparty
broadcasts first S+A, then you honestly broadcast S+B, where B pays better
fees.

> All mempool transactions to be replaced must signal replaceability.

I think this is unsafe for L2s if counterparties have malleability of the
child transaction. They can block your package replacement by opting-out
from RBF signaling. IIRC, LN's "anchor output" presents such an ability.

I think it's better to either fix inherited signaling or move towards
full-rbf.

> if a package parent has already been submitted, it would
> look
>like the child is spending a "new" unconfirmed input.

I think this is an issue brought by the trimming during the dedup phase. If
we preserve the package integrity, only re-using the tx-level checks
results of already in-mempool transactions to gain in CPU time we won't
have this issue. Package childs can add unconfirmed inputs as long as
they're in-package, the bip125 rule2 is only evaluated against parents ?

> However, we still achieve the same goal of requiring the
> replacement
> transactions to have a ancestor score at least as high as the original
> ones.

I'm not sure if this holds...

Let's say you have in-mempool A, B where A pays 10 sat/vb for 100 vbytes
and B pays 10 sat/vb for 100 vbytes. You have the candidate replacement D
spending both A and C where D pays 15sat/vb for 100 vbytes and C pays 1
sat/vb for 1000 vbytes.

Package A + B ancestor score is 10 sat/vb.

D has a higher feerate/absolute fee than B.

Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's 1000
sats + D's 1500 sats) /
A's 100 vb + C's 1000 vb + D's 100 vb)

Overall, this is a review through the lenses of LN requirements. I think
other L2 protocols/applications
could be candidates to using package accept/relay such as:
* https://github.com/lightninglabs/pool
* https://github.com/discreetlogcontracts/dlcspecs
* https://github.com/bitcoin-teleport/teleport-transactions/
* https://github.com/sapio-lang/sapio
* https://github.com/commerceblock/mercury/blob/master/doc/statechains.md
* https://github.com/revault/practical-revault

Thanks for rolling forward the ball on this subject.

Antoine

Le jeu. 16 sept. 2021 ? 03:55, Gloria Zhao via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> a ?crit :

> Hi there,
>
> I'm writing to propose a set of mempool policy changes to enable package
> validation (in preparation for package relay) in Bitcoin Core. These would
> not
> be consensus or P2P protocol changes. However, since mempool policy
> significantly affects transaction propagation, I believe this is relevant
> for
> the mailing list.
>
> My proposal enables packages consisting of multiple parents and 1 child.
> If you
> develop software that relies on specific transaction relay assumptions
> and/or
> are interested in using package relay in the future, I'm very interested
> to hear
> your feedback on the utility or restrictiveness of these package policies
> for
> your use cases.
>
> A draft implementation of this proposal can be found in [Bitcoin Core
> PR#22290][1].
>
> An illustrated version of this post can be found at
> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.
> I have also linked the images below.
>
> ## Background
>
> Feel free to skip this section if you are already familiar with mempool
> policy
> and package relay terminology.
>
> ### Terminology Clarifications
>
> * Package = an ordered list of related transactions, representable by a
> Directed
>   Acyclic Graph.
> * Package Feerate = the total modified fees divided by the total virtual
> size of
>   all transactions in the package.
>     - Modified fees = a transaction's base fees + fee delta applied by the
> user
>       with `prioritisetransaction`. As such, we expect this to vary across
> mempools.
>     - Virtual Size = the maximum of virtual sizes calculated using [BIP141
>       virtual size][2] and sigop weight. [Implemented here in Bitcoin
> Core][3].
>     - Note that feerate is not necessarily based on the base fees and
> serialized
>       size.
>
> * Fee-Bumping = user/wallet actions that take advantage of miner
> incentives to
>   boost a transaction's candidacy for inclusion in a block, including
> Child Pays
> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our intention in
> mempool policy is to recognize when the new transaction is more economical
> to
> mine than the original one(s) but not open DoS vectors, so there are some
> limitations.
>
> ### Policy
>
> The purpose of the mempool is to store the best (to be most
> incentive-compatible
> with miners, highest feerate) candidates for inclusion in a block. Miners
> use
> the mempool to build block templates. The mempool is also useful as a
> cache for
> boosting block relay and validation performance, aiding transaction relay,
> and
> generating feerate estimations.
>
> Ideally, all consensus-valid transactions paying reasonable fees should
> make it
> to miners through normal transaction relay, without any special
> connectivity or
> relationships with miners. On the other hand, nodes do not have unlimited
> resources, and a P2P network designed to let any honest node broadcast
> their
> transactions also exposes the transaction validation engine to DoS attacks
> from
> malicious peers.
>
> As such, for unconfirmed transactions we are considering for our mempool,
> we
> apply a set of validation rules in addition to consensus, primarily to
> protect
> us from resource exhaustion and aid our efforts to keep the highest fee
> transactions. We call this mempool _policy_: a set of (configurable,
> node-specific) rules that transactions must abide by in order to be
> accepted
> into our mempool. Transaction "Standardness" rules and mempool
> restrictions such
> as "too-long-mempool-chain" are both examples of policy.
>
> ### Package Relay and Package Mempool Accept
>
> In transaction relay, we currently consider transactions one at a time for
> submission to the mempool. This creates a limitation in the node's ability
> to
> determine which transactions have the highest feerates, since we cannot
> take
> into account descendants (i.e. cannot use CPFP) until all the transactions
> are
> in the mempool. Similarly, we cannot use a transaction's descendants when
> considering it for RBF. When an individual transaction does not meet the
> mempool
> minimum feerate and the user isn't able to create a replacement transaction
> directly, it will not be accepted by mempools.
>
> This limitation presents a security issue for applications and users
> relying on
> time-sensitive transactions. For example, Lightning and other protocols
> create
> UTXOs with multiple spending paths, where one counterparty's spending path
> opens
> up after a timelock, and users are protected from cheating scenarios as
> long as
> they redeem on-chain in time. A key security assumption is that all
> parties'
> transactions will propagate and confirm in a timely manner. This
> assumption can
> be broken if fee-bumping does not work as intended.
>
> The end goal for Package Relay is to consider multiple transactions at the
> same
> time, e.g. a transaction with its high-fee child. This may help us better
> determine whether transactions should be accepted to our mempool,
> especially if
> they don't meet fee requirements individually or are better RBF candidates
> as a
> package. A combination of changes to mempool validation logic, policy, and
> transaction relay allows us to better propagate the transactions with the
> highest package feerates to miners, and makes fee-bumping tools more
> powerful
> for users.
>
> The "relay" part of Package Relay suggests P2P messaging changes, but a
> large
> part of the changes are in the mempool's package validation logic. We call
> this
> *Package Mempool Accept*.
>
> ### Previous Work
>
> * Given that mempool validation is DoS-sensitive and complex, it would be
>   dangerous to haphazardly tack on package validation logic. Many efforts
> have
> been made to make mempool validation less opaque (see [#16400][4],
> [#21062][5],
> [#22675][6], [#22796][7]).
> * [#20833][8] Added basic capabilities for package validation, test
> accepts only
>   (no submission to mempool).
> * [#21800][9] Implemented package ancestor/descendant limit checks for
> arbitrary
>   packages. Still test accepts only.
> * Previous package relay proposals (see [#16401][10], [#19621][11]).
>
> ### Existing Package Rules
>
> These are in master as introduced in [#20833][8] and [#21800][9]. I'll
> consider
> them as "given" in the rest of this document, though they can be changed,
> since
> package validation is test-accept only right now.
>
> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and
> `MAX_PACKAGE_SIZE=101KvB` total size [8]
>
>    *Rationale*: This is already enforced as mempool ancestor/descendant
> limits.
> Presumably, transactions in a package are all related, so exceeding this
> limit
> would mean that the package can either be split up or it wouldn't pass this
> mempool policy.
>
> 2. Packages must be topologically sorted: if any dependencies exist between
> transactions, parents must appear somewhere before children. [8]
>
> 3. A package cannot have conflicting transactions, i.e. none of them can
> spend
> the same inputs. This also means there cannot be duplicate transactions.
> [8]
>
> 4. When packages are evaluated against ancestor/descendant limits in a test
> accept, the union of all of their descendants and ancestors is considered.
> This
> is essentially a "worst case" heuristic where every transaction in the
> package
> is treated as each other's ancestor and descendant. [8]
> Packages for which ancestor/descendant limits are accurately captured by
> this
> heuristic: [19]
>
> There are also limitations such as the fact that CPFP carve out is not
> applied
> to package transactions. #20833 also disables RBF in package validation;
> this
> proposal overrides that to allow packages to use RBF.
>
> ## Proposed Changes
>
> The next step in the Package Mempool Accept project is to implement
> submission
> to mempool, initially through RPC only. This allows us to test the
> submission
> logic before exposing it on P2P.
>
> ### Summary
>
> - Packages may contain already-in-mempool transactions.
> - Packages are 2 generations, Multi-Parent-1-Child.
> - Fee-related checks use the package feerate. This means that wallets can
> create a package that utilizes CPFP.
> - Parents are allowed to RBF mempool transactions with a set of rules
> similar
>   to BIP125. This enables a combination of CPFP and RBF, where a
> transaction's descendant fees pay for replacing mempool conflicts.
>
> There is a draft implementation in [#22290][1]. It is WIP, but feedback is
> always welcome.
>
> ### Details
>
> #### Packages May Contain Already-in-Mempool Transactions
>
> A package may contain transactions that are already in the mempool. We
> remove
> ("deduplicate") those transactions from the package for the purposes of
> package
> mempool acceptance. If a package is empty after deduplication, we do
> nothing.
>
> *Rationale*: Mempools vary across the network. It's possible for a parent
> to be
> accepted to the mempool of a peer on its own due to differences in policy
> and
> fee market fluctuations. We should not reject or penalize the entire
> package for
> an individual transaction as that could be a censorship vector.
>
> #### Packages Are Multi-Parent-1-Child
>
> Only packages of a specific topology are permitted. Namely, a package is
> exactly
> 1 child with all of its unconfirmed parents. After deduplication, the
> package
> may be exactly the same, empty, 1 child, 1 child with just some of its
> unconfirmed parents, etc. Note that it's possible for the parents to be
> indirect
> descendants/ancestors of one another, or for parent and child to share a
> parent,
> so we cannot make any other topology assumptions.
>
> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple parents
> makes it possible to fee-bump a batch of transactions. Restricting
> packages to a
> defined topology is also easier to reason about and simplifies the
> validation
> logic greatly. Multi-parent-1-child allows us to think of the package as
> one big
> transaction, where:
>
> - Inputs = all the inputs of parents + inputs of the child that come from
>   confirmed UTXOs
> - Outputs = all the outputs of the child + all outputs of the parents that
>   aren't spent by other transactions in the package
>
> Examples of packages that follow this rule (variations of example A show
> some
> possibilities after deduplication): ![image][15]
>
> #### Fee-Related Checks Use Package Feerate
>
> Package Feerate = the total modified fees divided by the total virtual
> size of
> all transactions in the package.
>
> To meet the two feerate requirements of a mempool, i.e., the pre-configured
> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum
> feerate, the
> total package feerate is used instead of the individual feerate. The
> individual
> transactions are allowed to be below feerate requirements if the package
> meets
> the feerate requirements. For example, the parent(s) in the package can
> have 0
> fees but be paid for by the child.
>
> *Rationale*: This can be thought of as "CPFP within a package," solving the
> issue of a parent not meeting minimum fees on its own. This allows L2
> applications to adjust their fees at broadcast time instead of
> overshooting or
> risking getting stuck/pinned.
>
> We use the package feerate of the package *after deduplication*.
>
> *Rationale*:  It would be incorrect to use the fees of transactions that
> are
> already in the mempool, as we do not want a transaction's fees to be
> double-counted for both its individual RBF and package RBF.
>
> Examples F and G [14] show the same package, but P1 is submitted
> individually before
> the package in example G. In example F, we can see that the 300vB package
> pays
> an additional 200sat in fees, which is not enough to pay for its own
> bandwidth
> (BIP125#4). In example G, we can see that P1 pays enough to replace M1, but
> using P1's fees again during package submission would make it look like a
> 300sat
> increase for a 200vB package. Even including its fees and size would not be
> sufficient in this example, since the 300sat looks like enough for the
> 300vB
> package. The calculcation after deduplication is 100sat increase for a
> package
> of size 200vB, which correctly fails BIP125#4. Assume all transactions
> have a
> size of 100vB.
>
> #### Package RBF
>
> If a package meets feerate requirements as a package, the parents in the
> transaction are allowed to replace-by-fee mempool transactions. The child
> cannot
> replace mempool transactions. Multiple transactions can replace the same
> transaction, but in order to be valid, none of the transactions can try to
> replace an ancestor of another transaction in the same package (which
> would thus
> make its inputs unavailable).
>
> *Rationale*: Even if we are using package feerate, a package will not
> propagate
> as intended if RBF still requires each individual transaction to meet the
> feerate requirements.
>
> We use a set of rules slightly modified from BIP125 as follows:
>
> ##### Signaling (Rule #1)
>
> All mempool transactions to be replaced must signal replaceability.
>
> *Rationale*: Package RBF signaling logic should be the same for package
> RBF and
> single transaction acceptance. This would be updated if single transaction
> validation moves to full RBF.
>
> ##### New Unconfirmed Inputs (Rule #2)
>
> A package may include new unconfirmed inputs, but the ancestor feerate of
> the
> child must be at least as high as the ancestor feerates of every
> transaction
> being replaced. This is contrary to BIP125#2, which states "The replacement
> transaction may only include an unconfirmed input if that input was
> included in
> one of the original transactions. (An unconfirmed input spends an output
> from a
> currently-unconfirmed transaction.)"
>
> *Rationale*: The purpose of BIP125#2 is to ensure that the replacement
> transaction has a higher ancestor score than the original transaction(s)
> (see
> [comment][13]). Example H [16] shows how adding a new unconfirmed input
> can lower the
> ancestor score of the replacement transaction. P1 is trying to replace M1,
> and
> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat, and M2
> pays
> 100sat. Assume all transactions have a size of 100vB. While, in isolation,
> P1
> looks like a better mining candidate than M1, it must be mined with M2, so
> its
> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's ancestor
> feerate, which is 6sat/vB.
>
> In package RBF, the rule analogous to BIP125#2 would be "none of the
> transactions in the package can spend new unconfirmed inputs." Example J
> [17] shows
> why, if any of the package transactions have ancestors, package feerate is
> no
> longer accurate. Even though M2 and M3 are not ancestors of P1 (which is
> the
> replacement transaction in an RBF), we're actually interested in the entire
> package. A miner should mine M1 which is 5sat/vB instead of M2, M3, P1,
> P2, and
> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened to only
> allow
> the child to have new unconfirmed inputs, either, because it can still
> cause us
> to overestimate the package's ancestor score.
>
> However, enforcing a rule analogous to BIP125#2 would not only make
> Package RBF
> less useful, but would also break Package RBF for packages with parents
> already
> in the mempool: if a package parent has already been submitted, it would
> look
> like the child is spending a "new" unconfirmed input. In example K [18],
> we're
> looking to replace M1 with the entire package including P1, P2, and P3. We
> must
> consider the case where one of the parents is already in the mempool (in
> this
> case, P2), which means we must allow P3 to have new unconfirmed inputs.
> However,
> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not replace
> M1
> with this package.
>
> Thus, the package RBF rule regarding new unconfirmed inputs is less strict
> than
> BIP125#2. However, we still achieve the same goal of requiring the
> replacement
> transactions to have a ancestor score at least as high as the original
> ones. As
> a result, the entire package is required to be a higher feerate mining
> candidate
> than each of the replaced transactions.
>
> Another note: the [comment][13] above the BIP125#2 code in the original RBF
> implementation suggests that the rule was intended to be temporary.
>
> ##### Absolute Fee (Rule #3)
>
> The package must increase the absolute fee of the mempool, i.e. the total
> fees
> of the package must be higher than the absolute fees of the mempool
> transactions
> it replaces. Combined with the CPFP rule above, this differs from BIP125
> Rule #3
> - an individual transaction in the package may have lower fees than the
>   transaction(s) it is replacing. In fact, it may have 0 fees, and the
> child
> pays for RBF.
>
> ##### Feerate (Rule #4)
>
> The package must pay for its own bandwidth; the package feerate must be
> higher
> than the replaced transactions by at least minimum relay feerate
> (`incrementalRelayFee`). Combined with the CPFP rule above, this differs
> from
> BIP125 Rule #4 - an individual transaction in the package can have a lower
> feerate than the transaction(s) it is replacing. In fact, it may have 0
> fees,
> and the child pays for RBF.
>
> ##### Total Number of Replaced Transactions (Rule #5)
>
> The package cannot replace more than 100 mempool transactions. This is
> identical
> to BIP125 Rule #5.
>
> ### Expected FAQs
>
> 1. Is it possible for only some of the package to make it into the mempool?
>
>    Yes, it is. However, since we evict transactions from the mempool by
> descendant score and the package child is supposed to be sponsoring the
> fees of
> its parents, the most common scenario would be all-or-nothing. This is
> incentive-compatible. In fact, to be conservative, package validation
> should
> begin by trying to submit all of the transactions individually, and only
> use the
> package mempool acceptance logic if the parents fail due to low feerate.
>
> 2. Should we allow packages to contain already-confirmed transactions?
>
>     No, for practical reasons. In mempool validation, we actually aren't
> able to
> tell with 100% confidence if we are looking at a transaction that has
> already
> confirmed, because we look up inputs using a UTXO set. If we have
> historical
> block data, it's possible to look for it, but this is inefficient, not
> always
> possible for pruning nodes, and unnecessary because we're not going to do
> anything with the transaction anyway. As such, we already have the
> expectation
> that transaction relay is somewhat "stateful" i.e. nobody should be
> relaying
> transactions that have already been confirmed. Similarly, we shouldn't be
> relaying packages that contain already-confirmed transactions.
>
> [1]: https://github.com/bitcoin/bitcoin/pull/22290
> [2]:
> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations
> [3]:
> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282
> [4]: https://github.com/bitcoin/bitcoin/pull/16400
> [5]: https://github.com/bitcoin/bitcoin/pull/21062
> [6]: https://github.com/bitcoin/bitcoin/pull/22675
> [7]: https://github.com/bitcoin/bitcoin/pull/22796
> [8]: https://github.com/bitcoin/bitcoin/pull/20833
> [9]: https://github.com/bitcoin/bitcoin/pull/21800
> [10]: https://github.com/bitcoin/bitcoin/pull/16401
> [11]: https://github.com/bitcoin/bitcoin/pull/19621
> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki
> [13]:
> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104
> [14]:
> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png
> [15]:
> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png
> [16]:
> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png
> [17]:
> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png
> [18]:
> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png
> [19]:
> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png
> [20]:
> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210919/f11e1976/attachment-0001.html>

From bastien at acinq.fr  Mon Sep 20 09:19:38 2021
From: bastien at acinq.fr (Bastien TEINTURIER)
Date: Mon, 20 Sep 2021 11:19:38 +0200
Subject: [bitcoin-dev] Proposal: Package Mempool Accept and Package RBF
In-Reply-To: <CAFXO6=+cHyQKM2n9yn4EhwLZO+AUB0ZD81qWPxmpN27rjUoU3w@mail.gmail.com>
References: <CAFXO6=+cHyQKM2n9yn4EhwLZO+AUB0ZD81qWPxmpN27rjUoU3w@mail.gmail.com>
Message-ID: <CACdvm3NdqYFvJ9t4ocXjdLT09fPu40YYwdvpvOnyYCmk5QXyrQ@mail.gmail.com>

Hi Gloria,

Thanks for this detailed post!

The illustrations you provided are very useful for this kind of graph
topology problems.

The rules you lay out for package RBF look good to me at first glance
as there are some subtle improvements compared to BIP 125.

> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and
> `MAX_PACKAGE_SIZE=101KvB` total size [8]

I have a question regarding this rule, as your example 2C could be
concerning for LN (unless I didn't understand it correctly).

This also touches on the package RBF rule 5 ("The package cannot
replace more than 100 mempool transactions.")

In your example we have a parent transaction A already in the mempool
and an unrelated child B. We submit a package C + D where C spends
another of A's inputs. You're highlighting that this package may be
rejected because of the unrelated transaction(s) B.

The way I see this, an attacker can abuse this rule to ensure
transaction A stays pinned in the mempool without confirming by
broadcasting a set of child transactions that reach these limits
and pay low fees (where A would be a commit tx in LN).

We had to create the CPFP carve-out rule explicitly to work around
this limitation, and I think it would be necessary for package RBF
as well, because in such cases we do want to be able to submit a
package A + C where C pays high fees to speed up A's confirmation,
regardless of unrelated unconfirmed children of A...

We could submit only C to benefit from the existing CPFP carve-out
rule, but that wouldn't work if our local mempool doesn't have A yet,
but other remote mempools do.

Is my concern justified? Is this something that we should dig into a
bit deeper?

Thanks,
Bastien

Le jeu. 16 sept. 2021 ? 09:55, Gloria Zhao via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> a ?crit :

> Hi there,
>
> I'm writing to propose a set of mempool policy changes to enable package
> validation (in preparation for package relay) in Bitcoin Core. These would
> not
> be consensus or P2P protocol changes. However, since mempool policy
> significantly affects transaction propagation, I believe this is relevant
> for
> the mailing list.
>
> My proposal enables packages consisting of multiple parents and 1 child.
> If you
> develop software that relies on specific transaction relay assumptions
> and/or
> are interested in using package relay in the future, I'm very interested
> to hear
> your feedback on the utility or restrictiveness of these package policies
> for
> your use cases.
>
> A draft implementation of this proposal can be found in [Bitcoin Core
> PR#22290][1].
>
> An illustrated version of this post can be found at
> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.
> I have also linked the images below.
>
> ## Background
>
> Feel free to skip this section if you are already familiar with mempool
> policy
> and package relay terminology.
>
> ### Terminology Clarifications
>
> * Package = an ordered list of related transactions, representable by a
> Directed
>   Acyclic Graph.
> * Package Feerate = the total modified fees divided by the total virtual
> size of
>   all transactions in the package.
>     - Modified fees = a transaction's base fees + fee delta applied by the
> user
>       with `prioritisetransaction`. As such, we expect this to vary across
> mempools.
>     - Virtual Size = the maximum of virtual sizes calculated using [BIP141
>       virtual size][2] and sigop weight. [Implemented here in Bitcoin
> Core][3].
>     - Note that feerate is not necessarily based on the base fees and
> serialized
>       size.
>
> * Fee-Bumping = user/wallet actions that take advantage of miner
> incentives to
>   boost a transaction's candidacy for inclusion in a block, including
> Child Pays
> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our intention in
> mempool policy is to recognize when the new transaction is more economical
> to
> mine than the original one(s) but not open DoS vectors, so there are some
> limitations.
>
> ### Policy
>
> The purpose of the mempool is to store the best (to be most
> incentive-compatible
> with miners, highest feerate) candidates for inclusion in a block. Miners
> use
> the mempool to build block templates. The mempool is also useful as a
> cache for
> boosting block relay and validation performance, aiding transaction relay,
> and
> generating feerate estimations.
>
> Ideally, all consensus-valid transactions paying reasonable fees should
> make it
> to miners through normal transaction relay, without any special
> connectivity or
> relationships with miners. On the other hand, nodes do not have unlimited
> resources, and a P2P network designed to let any honest node broadcast
> their
> transactions also exposes the transaction validation engine to DoS attacks
> from
> malicious peers.
>
> As such, for unconfirmed transactions we are considering for our mempool,
> we
> apply a set of validation rules in addition to consensus, primarily to
> protect
> us from resource exhaustion and aid our efforts to keep the highest fee
> transactions. We call this mempool _policy_: a set of (configurable,
> node-specific) rules that transactions must abide by in order to be
> accepted
> into our mempool. Transaction "Standardness" rules and mempool
> restrictions such
> as "too-long-mempool-chain" are both examples of policy.
>
> ### Package Relay and Package Mempool Accept
>
> In transaction relay, we currently consider transactions one at a time for
> submission to the mempool. This creates a limitation in the node's ability
> to
> determine which transactions have the highest feerates, since we cannot
> take
> into account descendants (i.e. cannot use CPFP) until all the transactions
> are
> in the mempool. Similarly, we cannot use a transaction's descendants when
> considering it for RBF. When an individual transaction does not meet the
> mempool
> minimum feerate and the user isn't able to create a replacement transaction
> directly, it will not be accepted by mempools.
>
> This limitation presents a security issue for applications and users
> relying on
> time-sensitive transactions. For example, Lightning and other protocols
> create
> UTXOs with multiple spending paths, where one counterparty's spending path
> opens
> up after a timelock, and users are protected from cheating scenarios as
> long as
> they redeem on-chain in time. A key security assumption is that all
> parties'
> transactions will propagate and confirm in a timely manner. This
> assumption can
> be broken if fee-bumping does not work as intended.
>
> The end goal for Package Relay is to consider multiple transactions at the
> same
> time, e.g. a transaction with its high-fee child. This may help us better
> determine whether transactions should be accepted to our mempool,
> especially if
> they don't meet fee requirements individually or are better RBF candidates
> as a
> package. A combination of changes to mempool validation logic, policy, and
> transaction relay allows us to better propagate the transactions with the
> highest package feerates to miners, and makes fee-bumping tools more
> powerful
> for users.
>
> The "relay" part of Package Relay suggests P2P messaging changes, but a
> large
> part of the changes are in the mempool's package validation logic. We call
> this
> *Package Mempool Accept*.
>
> ### Previous Work
>
> * Given that mempool validation is DoS-sensitive and complex, it would be
>   dangerous to haphazardly tack on package validation logic. Many efforts
> have
> been made to make mempool validation less opaque (see [#16400][4],
> [#21062][5],
> [#22675][6], [#22796][7]).
> * [#20833][8] Added basic capabilities for package validation, test
> accepts only
>   (no submission to mempool).
> * [#21800][9] Implemented package ancestor/descendant limit checks for
> arbitrary
>   packages. Still test accepts only.
> * Previous package relay proposals (see [#16401][10], [#19621][11]).
>
> ### Existing Package Rules
>
> These are in master as introduced in [#20833][8] and [#21800][9]. I'll
> consider
> them as "given" in the rest of this document, though they can be changed,
> since
> package validation is test-accept only right now.
>
> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and
> `MAX_PACKAGE_SIZE=101KvB` total size [8]
>
>    *Rationale*: This is already enforced as mempool ancestor/descendant
> limits.
> Presumably, transactions in a package are all related, so exceeding this
> limit
> would mean that the package can either be split up or it wouldn't pass this
> mempool policy.
>
> 2. Packages must be topologically sorted: if any dependencies exist between
> transactions, parents must appear somewhere before children. [8]
>
> 3. A package cannot have conflicting transactions, i.e. none of them can
> spend
> the same inputs. This also means there cannot be duplicate transactions.
> [8]
>
> 4. When packages are evaluated against ancestor/descendant limits in a test
> accept, the union of all of their descendants and ancestors is considered.
> This
> is essentially a "worst case" heuristic where every transaction in the
> package
> is treated as each other's ancestor and descendant. [8]
> Packages for which ancestor/descendant limits are accurately captured by
> this
> heuristic: [19]
>
> There are also limitations such as the fact that CPFP carve out is not
> applied
> to package transactions. #20833 also disables RBF in package validation;
> this
> proposal overrides that to allow packages to use RBF.
>
> ## Proposed Changes
>
> The next step in the Package Mempool Accept project is to implement
> submission
> to mempool, initially through RPC only. This allows us to test the
> submission
> logic before exposing it on P2P.
>
> ### Summary
>
> - Packages may contain already-in-mempool transactions.
> - Packages are 2 generations, Multi-Parent-1-Child.
> - Fee-related checks use the package feerate. This means that wallets can
> create a package that utilizes CPFP.
> - Parents are allowed to RBF mempool transactions with a set of rules
> similar
>   to BIP125. This enables a combination of CPFP and RBF, where a
> transaction's descendant fees pay for replacing mempool conflicts.
>
> There is a draft implementation in [#22290][1]. It is WIP, but feedback is
> always welcome.
>
> ### Details
>
> #### Packages May Contain Already-in-Mempool Transactions
>
> A package may contain transactions that are already in the mempool. We
> remove
> ("deduplicate") those transactions from the package for the purposes of
> package
> mempool acceptance. If a package is empty after deduplication, we do
> nothing.
>
> *Rationale*: Mempools vary across the network. It's possible for a parent
> to be
> accepted to the mempool of a peer on its own due to differences in policy
> and
> fee market fluctuations. We should not reject or penalize the entire
> package for
> an individual transaction as that could be a censorship vector.
>
> #### Packages Are Multi-Parent-1-Child
>
> Only packages of a specific topology are permitted. Namely, a package is
> exactly
> 1 child with all of its unconfirmed parents. After deduplication, the
> package
> may be exactly the same, empty, 1 child, 1 child with just some of its
> unconfirmed parents, etc. Note that it's possible for the parents to be
> indirect
> descendants/ancestors of one another, or for parent and child to share a
> parent,
> so we cannot make any other topology assumptions.
>
> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple parents
> makes it possible to fee-bump a batch of transactions. Restricting
> packages to a
> defined topology is also easier to reason about and simplifies the
> validation
> logic greatly. Multi-parent-1-child allows us to think of the package as
> one big
> transaction, where:
>
> - Inputs = all the inputs of parents + inputs of the child that come from
>   confirmed UTXOs
> - Outputs = all the outputs of the child + all outputs of the parents that
>   aren't spent by other transactions in the package
>
> Examples of packages that follow this rule (variations of example A show
> some
> possibilities after deduplication): ![image][15]
>
> #### Fee-Related Checks Use Package Feerate
>
> Package Feerate = the total modified fees divided by the total virtual
> size of
> all transactions in the package.
>
> To meet the two feerate requirements of a mempool, i.e., the pre-configured
> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum
> feerate, the
> total package feerate is used instead of the individual feerate. The
> individual
> transactions are allowed to be below feerate requirements if the package
> meets
> the feerate requirements. For example, the parent(s) in the package can
> have 0
> fees but be paid for by the child.
>
> *Rationale*: This can be thought of as "CPFP within a package," solving the
> issue of a parent not meeting minimum fees on its own. This allows L2
> applications to adjust their fees at broadcast time instead of
> overshooting or
> risking getting stuck/pinned.
>
> We use the package feerate of the package *after deduplication*.
>
> *Rationale*:  It would be incorrect to use the fees of transactions that
> are
> already in the mempool, as we do not want a transaction's fees to be
> double-counted for both its individual RBF and package RBF.
>
> Examples F and G [14] show the same package, but P1 is submitted
> individually before
> the package in example G. In example F, we can see that the 300vB package
> pays
> an additional 200sat in fees, which is not enough to pay for its own
> bandwidth
> (BIP125#4). In example G, we can see that P1 pays enough to replace M1, but
> using P1's fees again during package submission would make it look like a
> 300sat
> increase for a 200vB package. Even including its fees and size would not be
> sufficient in this example, since the 300sat looks like enough for the
> 300vB
> package. The calculcation after deduplication is 100sat increase for a
> package
> of size 200vB, which correctly fails BIP125#4. Assume all transactions
> have a
> size of 100vB.
>
> #### Package RBF
>
> If a package meets feerate requirements as a package, the parents in the
> transaction are allowed to replace-by-fee mempool transactions. The child
> cannot
> replace mempool transactions. Multiple transactions can replace the same
> transaction, but in order to be valid, none of the transactions can try to
> replace an ancestor of another transaction in the same package (which
> would thus
> make its inputs unavailable).
>
> *Rationale*: Even if we are using package feerate, a package will not
> propagate
> as intended if RBF still requires each individual transaction to meet the
> feerate requirements.
>
> We use a set of rules slightly modified from BIP125 as follows:
>
> ##### Signaling (Rule #1)
>
> All mempool transactions to be replaced must signal replaceability.
>
> *Rationale*: Package RBF signaling logic should be the same for package
> RBF and
> single transaction acceptance. This would be updated if single transaction
> validation moves to full RBF.
>
> ##### New Unconfirmed Inputs (Rule #2)
>
> A package may include new unconfirmed inputs, but the ancestor feerate of
> the
> child must be at least as high as the ancestor feerates of every
> transaction
> being replaced. This is contrary to BIP125#2, which states "The replacement
> transaction may only include an unconfirmed input if that input was
> included in
> one of the original transactions. (An unconfirmed input spends an output
> from a
> currently-unconfirmed transaction.)"
>
> *Rationale*: The purpose of BIP125#2 is to ensure that the replacement
> transaction has a higher ancestor score than the original transaction(s)
> (see
> [comment][13]). Example H [16] shows how adding a new unconfirmed input
> can lower the
> ancestor score of the replacement transaction. P1 is trying to replace M1,
> and
> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat, and M2
> pays
> 100sat. Assume all transactions have a size of 100vB. While, in isolation,
> P1
> looks like a better mining candidate than M1, it must be mined with M2, so
> its
> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's ancestor
> feerate, which is 6sat/vB.
>
> In package RBF, the rule analogous to BIP125#2 would be "none of the
> transactions in the package can spend new unconfirmed inputs." Example J
> [17] shows
> why, if any of the package transactions have ancestors, package feerate is
> no
> longer accurate. Even though M2 and M3 are not ancestors of P1 (which is
> the
> replacement transaction in an RBF), we're actually interested in the entire
> package. A miner should mine M1 which is 5sat/vB instead of M2, M3, P1,
> P2, and
> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened to only
> allow
> the child to have new unconfirmed inputs, either, because it can still
> cause us
> to overestimate the package's ancestor score.
>
> However, enforcing a rule analogous to BIP125#2 would not only make
> Package RBF
> less useful, but would also break Package RBF for packages with parents
> already
> in the mempool: if a package parent has already been submitted, it would
> look
> like the child is spending a "new" unconfirmed input. In example K [18],
> we're
> looking to replace M1 with the entire package including P1, P2, and P3. We
> must
> consider the case where one of the parents is already in the mempool (in
> this
> case, P2), which means we must allow P3 to have new unconfirmed inputs.
> However,
> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not replace
> M1
> with this package.
>
> Thus, the package RBF rule regarding new unconfirmed inputs is less strict
> than
> BIP125#2. However, we still achieve the same goal of requiring the
> replacement
> transactions to have a ancestor score at least as high as the original
> ones. As
> a result, the entire package is required to be a higher feerate mining
> candidate
> than each of the replaced transactions.
>
> Another note: the [comment][13] above the BIP125#2 code in the original RBF
> implementation suggests that the rule was intended to be temporary.
>
> ##### Absolute Fee (Rule #3)
>
> The package must increase the absolute fee of the mempool, i.e. the total
> fees
> of the package must be higher than the absolute fees of the mempool
> transactions
> it replaces. Combined with the CPFP rule above, this differs from BIP125
> Rule #3
> - an individual transaction in the package may have lower fees than the
>   transaction(s) it is replacing. In fact, it may have 0 fees, and the
> child
> pays for RBF.
>
> ##### Feerate (Rule #4)
>
> The package must pay for its own bandwidth; the package feerate must be
> higher
> than the replaced transactions by at least minimum relay feerate
> (`incrementalRelayFee`). Combined with the CPFP rule above, this differs
> from
> BIP125 Rule #4 - an individual transaction in the package can have a lower
> feerate than the transaction(s) it is replacing. In fact, it may have 0
> fees,
> and the child pays for RBF.
>
> ##### Total Number of Replaced Transactions (Rule #5)
>
> The package cannot replace more than 100 mempool transactions. This is
> identical
> to BIP125 Rule #5.
>
> ### Expected FAQs
>
> 1. Is it possible for only some of the package to make it into the mempool?
>
>    Yes, it is. However, since we evict transactions from the mempool by
> descendant score and the package child is supposed to be sponsoring the
> fees of
> its parents, the most common scenario would be all-or-nothing. This is
> incentive-compatible. In fact, to be conservative, package validation
> should
> begin by trying to submit all of the transactions individually, and only
> use the
> package mempool acceptance logic if the parents fail due to low feerate.
>
> 2. Should we allow packages to contain already-confirmed transactions?
>
>     No, for practical reasons. In mempool validation, we actually aren't
> able to
> tell with 100% confidence if we are looking at a transaction that has
> already
> confirmed, because we look up inputs using a UTXO set. If we have
> historical
> block data, it's possible to look for it, but this is inefficient, not
> always
> possible for pruning nodes, and unnecessary because we're not going to do
> anything with the transaction anyway. As such, we already have the
> expectation
> that transaction relay is somewhat "stateful" i.e. nobody should be
> relaying
> transactions that have already been confirmed. Similarly, we shouldn't be
> relaying packages that contain already-confirmed transactions.
>
> [1]: https://github.com/bitcoin/bitcoin/pull/22290
> [2]:
> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations
> [3]:
> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282
> [4]: https://github.com/bitcoin/bitcoin/pull/16400
> [5]: https://github.com/bitcoin/bitcoin/pull/21062
> [6]: https://github.com/bitcoin/bitcoin/pull/22675
> [7]: https://github.com/bitcoin/bitcoin/pull/22796
> [8]: https://github.com/bitcoin/bitcoin/pull/20833
> [9]: https://github.com/bitcoin/bitcoin/pull/21800
> [10]: https://github.com/bitcoin/bitcoin/pull/16401
> [11]: https://github.com/bitcoin/bitcoin/pull/19621
> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki
> [13]:
> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104
> [14]:
> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png
> [15]:
> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png
> [16]:
> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png
> [17]:
> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png
> [18]:
> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png
> [19]:
> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png
> [20]:
> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210920/adec3501/attachment-0001.html>

From aj at erisian.com.au  Mon Sep 20 14:52:46 2021
From: aj at erisian.com.au (Anthony Towns)
Date: Tue, 21 Sep 2021 00:52:46 +1000
Subject: [bitcoin-dev] TAPLEAF_UPDATE_VERIFY covenant opcode
In-Reply-To: <CALZpt+Hczvy1Fxu40cCKKC8bR9fouQ+sAiqV65-Z4VuLp+Bi7w@mail.gmail.com>
References: <20210909064138.GA22496@erisian.com.au>
 <CALZpt+FnnbGJC4=KO_OPiKxt0Ey9Bzh1gxP1dQSDz2aBi9WyOA@mail.gmail.com>
 <20210911032644.GB23578@erisian.com.au>
 <CALZpt+HzM__OJntegOhDqkg5zU=PQXtKgQoB518A2qP9=foovw@mail.gmail.com>
 <20210915065051.GA26119@erisian.com.au>
 <CALZpt+Hczvy1Fxu40cCKKC8bR9fouQ+sAiqV65-Z4VuLp+Bi7w@mail.gmail.com>
Message-ID: <20210920145246.GB21263@erisian.com.au>

On Sat, Sep 18, 2021 at 10:11:10AM -0400, Antoine Riard wrote:
> I think one design advantage of combining scope-minimal opcodes like MERKLESUB
> with sighash malleability is the ability to update a subset of the off-chain
> contract transactions fields after the funding phase.

Note that it's not "update" so much as "add to"; and I mostly think
graftroot (and friends), or just updating the utxo onchain, are a better
general purpose way of doing that. It's definitely a tradeoff though.

> Yes this is a different contract policy that I would like to set up.
> Let's say you would like to express the following set of capabilities.
> C0="Split the 4 BTC funds between Alice/Bob and Caroll/Dave"
> C1="Alice can withdraw 1 BTC after 2 weeks"
> C2="Bob can withdraw 1 BTC after 2 weeks"
> C3="Caroll can withdraw 1 BTC after 2 weeks"
> C4="Dave can withdraw 1 BTC after 2 weeks"
> C5="If USDT price=X, Alice can withdraw 2 BTC or Caroll can withdraw 2 BTC"

Hmm, I'm reading C5 as "If an oracle says X, and Alice and Carol agree,
they can distribute all the remaining funds as they see fit".

> If C4 is exercised, to avoid trust in the remaining counterparty, both Alice or
> Caroll should be able to conserve the C5 option, without relying on the updated
> key path.

> As you're saying, as we know the group in advance, one way to setup the tree
> could be:
> ? ? ? ?(A, (((((B, C), BC), D), BCD), ((((E, F), EF), G), EFG)))

Make it:

  (((AB, (A,B)), (CD, (C,D))), ACO)

AB = DROP <alice+bob> DUP 0 6 TLUV CHECKSIGVERIFY IN_OUT_AMOUNT SUB 2BTC LESSTHAN
CD = same but for carol+dave
A = <alice> DUP <B'> 10 TLUV CHECKSIGVERIFY IN_OUT_AMOUNT SUB 1BTC LESSTHAN
B' = <bob> DUP 0 2 TLUV CHECKSIGVERIFY IN_OUT_AMOUNT SUB 1BTC LESSTHAN
B,C,D = same as A but for bob, etc
A',C',D' = same as B' but for alice, etc
ACO = <alice+carol> CHECKSIGVERIFY <oracle> CHECKSIG

Probably AB, CD, A..D, A'..D' all want a CLTV delay in there as well.
(Relative timelocks would probably be annoying for everyone who wasn't
the first to exit the pool)

> Note, this solution isn't really satisfying as the G path isn't neutralized on
> the Caroll/Dave fork and could be replayed by Alice or Bob...

I think the above fixes that -- when AB is spent it deletes itself and
the (A,B) pair; when A is spent, it deletes (A, B and AB) and replaces
them with B'; when B' is spent it just deletes itself.

Cheers,
aj

From gloriajzhao at gmail.com  Mon Sep 20 15:10:14 2021
From: gloriajzhao at gmail.com (Gloria Zhao)
Date: Mon, 20 Sep 2021 16:10:14 +0100
Subject: [bitcoin-dev] Proposal: Package Mempool Accept and Package RBF
In-Reply-To: <CALZpt+HpvmEHUEOgye34T6pVQ+wnKKn-_8cTJTQXYQb9t1jOTA@mail.gmail.com>
References: <CAFXO6=+cHyQKM2n9yn4EhwLZO+AUB0ZD81qWPxmpN27rjUoU3w@mail.gmail.com>
 <CALZpt+HpvmEHUEOgye34T6pVQ+wnKKn-_8cTJTQXYQb9t1jOTA@mail.gmail.com>
Message-ID: <CAFXO6=JzsYgiXJE2geSKMpfgPo+GGNX_+Pw0JQx1QQxAfhCdBQ@mail.gmail.com>

Hi Antoine,

First of all, thank you for the thorough review. I appreciate your insight
on LN requirements.

> IIUC, you have a package A+B+C submitted for acceptance and A is already
in your mempool. You trim out A from the package and then evaluate B+C.

> I think this might be an issue if A is the higher-fee element of the ABC
package. B+C package fees might be under the mempool min fee and will be
rejected, potentially breaking the acceptance expectations of the package
issuer ?

Correct, if B+C is too low feerate to be accepted, we will reject it. I
prefer this because it is incentive compatible: A can be mined by itself,
so there's no reason to prefer A+B+C instead of A.
As another way of looking at this, consider the case where we do accept
A+B+C and it sits at the "bottom" of our mempool. If our mempool reaches
capacity, we evict the lowest descendant feerate transactions, which are
B+C in this case. This gives us the same resulting mempool, with A and not
B+C.


> Further, I think the dedup should be done on wtxid, as you might have
multiple valid witnesses. Though with varying vsizes and as such offering
different feerates.

I agree that variations of the same package with different witnesses is a
case that must be handled. I consider witness replacement to be a project
that can be done in parallel to package mempool acceptance because being
able to accept packages does not worsen the problem of a
same-txid-different-witness "pinning" attack.

If or when we have witness replacement, the logic is: if the individual
transaction is enough to replace the mempool one, the replacement will
happen during the preceding individual transaction acceptance, and
deduplication logic will work. Otherwise, we will try to deduplicate by
wtxid, see that we need a package witness replacement, and use the package
feerate to evaluate whether this is economically rational.

See the #22290 "handle package transactions already in mempool" commit (
https://github.com/bitcoin/bitcoin/pull/22290/commits/fea75a2237b46cf76145242fecad7e274bfcb5ff),
which handles the case of same-txid-different-witness by simply using the
transaction in the mempool for now, with TODOs for what I just described.


> I'm not clearly understanding the accepted topologies. By "parent and
child to share a parent", do you mean the set of transactions A, B, C,
where B is spending A and C is spending A and B would be correct ?

Yes, that is what I meant. Yes, that would a valid package under these
rules.

> If yes, is there a width-limit introduced or we fallback on
MAX_PACKAGE_COUNT=25 ?

No, there is no limit on connectivity other than "child with all
unconfirmed parents." We will enforce MAX_PACKAGE_COUNT=25 and child's
in-mempool + in-package ancestor limits.


> Considering the current Core's mempool acceptance rules, I think CPFP
batching is unsafe for LN time-sensitive closure. A malicious tx-relay
jamming successful on one channel commitment transaction would contamine
the remaining commitments sharing the same package.

> E.g, you broadcast the package A+B+C+D+E where A,B,C,D are commitment
transactions and E a shared CPFP. If a malicious A' transaction has a
better feerate than A, the whole package acceptance will fail. Even if A'
confirms in the following block,
the propagation and confirmation of B+C+D have been delayed. This could
carry on a loss of funds.

Please note that A may replace A' even if A' has higher fees than A
individually, because the proposed package RBF utilizes the fees and size
of the entire package. This just requires E to pay enough fees, although
this can be pretty high if there are also potential B' and C' competing
commitment transactions that we don't know about.


> IMHO, I'm leaning towards deploying during a first phase
1-parent/1-child. I think it's the most conservative step still improving
second-layer safety.

So far, my understanding is that multi-parent-1-child is desired for
batched fee-bumping (
https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289) and
I've also seen your response which I have less context on (
https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202). That
being said, I am happy to create a new proposal for 1 parent + 1 child
(which would be slightly simpler) and plan for moving to
multi-parent-1-child later if that is preferred. I am very interested in
hearing feedback on that approach.


> If A+B is submitted to replace A', where A pays 0 sats, B pays 200 sats
and A' pays 100 sats. If we apply the individual RBF on A, A+B acceptance
fails. For this reason I think the individual RBF should be bypassed and
only the package RBF apply ?

I think there is a misunderstanding here - let me describe what I'm
proposing we'd do in this situation: we'll try individual submission for A,
see that it fails due to "insufficient fees." Then, we'll try package
validation for A+B and use package RBF. If A+B pays enough, it can still
replace A'. If A fails for a bad signature, we won't look at B or A+B. Does
this meet your expectations?


> What problem are you trying to solve by the package feerate *after* dedup
rule ?
> My understanding is that an in-package transaction might be already in
the mempool. Therefore, to compute a correct RBF penalty replacement, the
vsize of this transaction could be discarded lowering the cost of package
RBF.

I'm proposing that, when a transaction has already been submitted to
mempool, we would ignore both its fees and vsize when calculating package
feerate. In example G2, we shouldn't count M1 fees after its submission to
mempool, since M1's fees have already been used to pay for its individual
bandwidth, and it shouldn't be used again to pay for P2 and P3's bandwidth.
We also shouldn't count its vsize, since it has already been paid for.


> I think this is a footgunish API, as if a package issuer send the
multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.
Then try to broadcast the higher-feerate C'+D' package, it should be
rejected. So it's breaking the naive broadcaster assumption that a
higher-feerate/higher-fee package always replaces ?

Note that, if C' conflicts with C, it also conflicts with D, since D is a
descendant of C and would thus need to be evicted along with it.
Implicitly, D' would not be in conflict with D.
More generally, this example is surprising to me because I didn't think
packages would be used to fee-bump replaceable transactions. Do we want the
child to be able to replace mempool transactions as well? This can be
implemented with a bit of additional logic.

> I think this is unsafe for L2s if counterparties have malleability of the
child transaction. They can block your package replacement by opting-out
from RBF signaling. IIRC, LN's "anchor output" presents such an ability.

I'm not sure what you mean? Let's say we have a package of parent A + child
B, where A is supposed to replace a mempool transaction A'. Are you saying
that counterparties are able to malleate the package child B, or a child of
A'? If they can malleate a child of A', that shouldn't matter as long as A'
is signaling replacement. This would be handled identically with full RBF
and what Core currently implements.

> I think this is an issue brought by the trimming during the dedup phase.
If we preserve the package integrity, only re-using the tx-level checks
results of already in-mempool transactions to gain in CPU time we won't
have this issue. Package childs can add unconfirmed inputs as long as
they're in-package, the bip125 rule2 is only evaluated against parents ?

Sorry, I don't understand what you mean by "preserve the package
integrity?" Could you elaborate?

> Let's say you have in-mempool A, B where A pays 10 sat/vb for 100 vbytes
and B pays 10 sat/vb for 100 vbytes. You have the candidate replacement D
spending both A and C where D pays 15sat/vb for 100 vbytes and C pays 1
sat/vb for 1000 vbytes.

> Package A + B ancestor score is 10 sat/vb.

> D has a higher feerate/absolute fee than B.

> Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's 1000
sats + D's 1500 sats) / A's 100 vb + C's 1000 vb + D's 100 vb)

I am in agreement with your calculations but unsure if we disagree on the
expected outcome. Yes, B has an ancestor score of 10sat/vb and D has an
ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than B's,
it fails the proposed package RBF Rule #2, so this package would be
rejected. Does this meet your expectations?

Thank you for linking to projects that might be interested in package relay
:)

Thanks,
Gloria

On Mon, Sep 20, 2021 at 12:16 AM Antoine Riard <antoine.riard at gmail.com>
wrote:

> Hi Gloria,
>
> > A package may contain transactions that are already in the mempool. We
> > remove
> > ("deduplicate") those transactions from the package for the purposes of
> > package
> > mempool acceptance. If a package is empty after deduplication, we do
> > nothing.
>
> IIUC, you have a package A+B+C submitted for acceptance and A is already
> in your mempool. You trim out A from the package and then evaluate B+C.
>
> I think this might be an issue if A is the higher-fee element of the ABC
> package. B+C package fees might be under the mempool min fee and will be
> rejected, potentially breaking the acceptance expectations of the package
> issuer ?
>
> Further, I think the dedup should be done on wtxid, as you might have
> multiple valid witnesses. Though with varying vsizes and as such offering
> different feerates.
>
> E.g you're going to evaluate the package A+B and A' is already in your
> mempool with a bigger valid witness. You trim A based on txid, then you
> evaluate A'+B, which fails the fee checks. However, evaluating A+B would
> have been a success.
>
> AFAICT, the dedup rationale would be to save on CPU time/IO disk, to avoid
> repeated signatures verification and parent UTXOs fetches ? Can we achieve
> the same goal by bypassing tx-level checks for already-in txn while
> conserving the package integrity for package-level checks ?
>
> > Note that it's possible for the parents to be
> > indirect
> > descendants/ancestors of one another, or for parent and child to share a
> > parent,
> > so we cannot make any other topology assumptions.
>
> I'm not clearly understanding the accepted topologies. By "parent and
> child to share a parent", do you mean the set of transactions A, B, C,
> where B is spending A and C is spending A and B would be correct ?
>
> If yes, is there a width-limit introduced or we fallback on
> MAX_PACKAGE_COUNT=25 ?
>
> IIRC, one rationale to come with this topology limitation was to lower the
> DoS risks when potentially deploying p2p packages.
>
> Considering the current Core's mempool acceptance rules, I think CPFP
> batching is unsafe for LN time-sensitive closure. A malicious tx-relay
> jamming successful on one channel commitment transaction would contamine
> the remaining commitments sharing the same package.
>
> E.g, you broadcast the package A+B+C+D+E where A,B,C,D are commitment
> transactions and E a shared CPFP. If a malicious A' transaction has a
> better feerate than A, the whole package acceptance will fail. Even if A'
> confirms in the following block,
> the propagation and confirmation of B+C+D have been delayed. This could
> carry on a loss of funds.
>
> That said, if you're broadcasting commitment transactions without
> time-sensitive HTLC outputs, I think the batching is effectively a fee
> saving as you don't have to duplicate the CPFP.
>
> IMHO, I'm leaning towards deploying during a first phase 1-parent/1-child.
> I think it's the most conservative step still improving second-layer safety.
>
> > *Rationale*:  It would be incorrect to use the fees of transactions that
> are
> > already in the mempool, as we do not want a transaction's fees to be
> > double-counted for both its individual RBF and package RBF.
>
> I'm unsure about the logical order of the checks proposed.
>
> If A+B is submitted to replace A', where A pays 0 sats, B pays 200 sats
> and A' pays 100 sats. If we apply the individual RBF on A, A+B acceptance
> fails. For this reason I think the individual RBF should be bypassed and
> only the package RBF apply ?
>
> Note this situation is plausible, with current LN design, your
> counterparty can have a commitment transaction with a better fee just by
> selecting a higher `dust_limit_satoshis` than yours.
>
> > Examples F and G [14] show the same package, but P1 is submitted
> > individually before
> > the package in example G. In example F, we can see that the 300vB package
> > pays
> > an additional 200sat in fees, which is not enough to pay for its own
> > bandwidth
> > (BIP125#4). In example G, we can see that P1 pays enough to replace M1,
> but
> > using P1's fees again during package submission would make it look like a
> > 300sat
> > increase for a 200vB package. Even including its fees and size would not
> be
> > sufficient in this example, since the 300sat looks like enough for the
> 300vB
> > package. The calculcation after deduplication is 100sat increase for a
> > package
> > of size 200vB, which correctly fails BIP125#4. Assume all transactions
> have
> > a
> > size of 100vB.
>
> What problem are you trying to solve by the package feerate *after* dedup
> rule ?
>
> My understanding is that an in-package transaction might be already in the
> mempool. Therefore, to compute a correct RBF penalty replacement, the vsize
> of this transaction could be discarded lowering the cost of package RBF.
>
> If we keep a "safe" dedup mechanism (see my point above), I think this
> discount is justified, as the validation cost of node operators is paid for
> ?
>
> > The child cannot replace mempool transactions.
>
> Let's say you issue package A+B, then package C+B', where B' is a child of
> both A and C. This rule fails the acceptance of C+B' ?
>
> I think this is a footgunish API, as if a package issuer send the
> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.
> Then try to broadcast the higher-feerate C'+D' package, it should be
> rejected. So it's breaking the naive broadcaster assumption that a
> higher-feerate/higher-fee package always replaces ? And it might be unsafe
> in protocols where states are symmetric. E.g a malicious counterparty
> broadcasts first S+A, then you honestly broadcast S+B, where B pays better
> fees.
>
> > All mempool transactions to be replaced must signal replaceability.
>
> I think this is unsafe for L2s if counterparties have malleability of the
> child transaction. They can block your package replacement by opting-out
> from RBF signaling. IIRC, LN's "anchor output" presents such an ability.
>
> I think it's better to either fix inherited signaling or move towards
> full-rbf.
>
> > if a package parent has already been submitted, it would
> > look
> >like the child is spending a "new" unconfirmed input.
>
> I think this is an issue brought by the trimming during the dedup phase.
> If we preserve the package integrity, only re-using the tx-level checks
> results of already in-mempool transactions to gain in CPU time we won't
> have this issue. Package childs can add unconfirmed inputs as long as
> they're in-package, the bip125 rule2 is only evaluated against parents ?
>
> > However, we still achieve the same goal of requiring the
> > replacement
> > transactions to have a ancestor score at least as high as the original
> > ones.
>
> I'm not sure if this holds...
>
> Let's say you have in-mempool A, B where A pays 10 sat/vb for 100 vbytes
> and B pays 10 sat/vb for 100 vbytes. You have the candidate replacement D
> spending both A and C where D pays 15sat/vb for 100 vbytes and C pays 1
> sat/vb for 1000 vbytes.
>
> Package A + B ancestor score is 10 sat/vb.
>
> D has a higher feerate/absolute fee than B.
>
> Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's 1000
> sats + D's 1500 sats) /
> A's 100 vb + C's 1000 vb + D's 100 vb)
>
> Overall, this is a review through the lenses of LN requirements. I think
> other L2 protocols/applications
> could be candidates to using package accept/relay such as:
> * https://github.com/lightninglabs/pool
> * https://github.com/discreetlogcontracts/dlcspecs
> * https://github.com/bitcoin-teleport/teleport-transactions/
> * https://github.com/sapio-lang/sapio
> * https://github.com/commerceblock/mercury/blob/master/doc/statechains.md
> * https://github.com/revault/practical-revault
>
> Thanks for rolling forward the ball on this subject.
>
> Antoine
>
> Le jeu. 16 sept. 2021 ? 03:55, Gloria Zhao via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> a ?crit :
>
>> Hi there,
>>
>> I'm writing to propose a set of mempool policy changes to enable package
>> validation (in preparation for package relay) in Bitcoin Core. These
>> would not
>> be consensus or P2P protocol changes. However, since mempool policy
>> significantly affects transaction propagation, I believe this is relevant
>> for
>> the mailing list.
>>
>> My proposal enables packages consisting of multiple parents and 1 child.
>> If you
>> develop software that relies on specific transaction relay assumptions
>> and/or
>> are interested in using package relay in the future, I'm very interested
>> to hear
>> your feedback on the utility or restrictiveness of these package policies
>> for
>> your use cases.
>>
>> A draft implementation of this proposal can be found in [Bitcoin Core
>> PR#22290][1].
>>
>> An illustrated version of this post can be found at
>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.
>> I have also linked the images below.
>>
>> ## Background
>>
>> Feel free to skip this section if you are already familiar with mempool
>> policy
>> and package relay terminology.
>>
>> ### Terminology Clarifications
>>
>> * Package = an ordered list of related transactions, representable by a
>> Directed
>>   Acyclic Graph.
>> * Package Feerate = the total modified fees divided by the total virtual
>> size of
>>   all transactions in the package.
>>     - Modified fees = a transaction's base fees + fee delta applied by
>> the user
>>       with `prioritisetransaction`. As such, we expect this to vary across
>> mempools.
>>     - Virtual Size = the maximum of virtual sizes calculated using [BIP141
>>       virtual size][2] and sigop weight. [Implemented here in Bitcoin
>> Core][3].
>>     - Note that feerate is not necessarily based on the base fees and
>> serialized
>>       size.
>>
>> * Fee-Bumping = user/wallet actions that take advantage of miner
>> incentives to
>>   boost a transaction's candidacy for inclusion in a block, including
>> Child Pays
>> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our intention in
>> mempool policy is to recognize when the new transaction is more
>> economical to
>> mine than the original one(s) but not open DoS vectors, so there are some
>> limitations.
>>
>> ### Policy
>>
>> The purpose of the mempool is to store the best (to be most
>> incentive-compatible
>> with miners, highest feerate) candidates for inclusion in a block. Miners
>> use
>> the mempool to build block templates. The mempool is also useful as a
>> cache for
>> boosting block relay and validation performance, aiding transaction
>> relay, and
>> generating feerate estimations.
>>
>> Ideally, all consensus-valid transactions paying reasonable fees should
>> make it
>> to miners through normal transaction relay, without any special
>> connectivity or
>> relationships with miners. On the other hand, nodes do not have unlimited
>> resources, and a P2P network designed to let any honest node broadcast
>> their
>> transactions also exposes the transaction validation engine to DoS
>> attacks from
>> malicious peers.
>>
>> As such, for unconfirmed transactions we are considering for our mempool,
>> we
>> apply a set of validation rules in addition to consensus, primarily to
>> protect
>> us from resource exhaustion and aid our efforts to keep the highest fee
>> transactions. We call this mempool _policy_: a set of (configurable,
>> node-specific) rules that transactions must abide by in order to be
>> accepted
>> into our mempool. Transaction "Standardness" rules and mempool
>> restrictions such
>> as "too-long-mempool-chain" are both examples of policy.
>>
>> ### Package Relay and Package Mempool Accept
>>
>> In transaction relay, we currently consider transactions one at a time for
>> submission to the mempool. This creates a limitation in the node's
>> ability to
>> determine which transactions have the highest feerates, since we cannot
>> take
>> into account descendants (i.e. cannot use CPFP) until all the
>> transactions are
>> in the mempool. Similarly, we cannot use a transaction's descendants when
>> considering it for RBF. When an individual transaction does not meet the
>> mempool
>> minimum feerate and the user isn't able to create a replacement
>> transaction
>> directly, it will not be accepted by mempools.
>>
>> This limitation presents a security issue for applications and users
>> relying on
>> time-sensitive transactions. For example, Lightning and other protocols
>> create
>> UTXOs with multiple spending paths, where one counterparty's spending
>> path opens
>> up after a timelock, and users are protected from cheating scenarios as
>> long as
>> they redeem on-chain in time. A key security assumption is that all
>> parties'
>> transactions will propagate and confirm in a timely manner. This
>> assumption can
>> be broken if fee-bumping does not work as intended.
>>
>> The end goal for Package Relay is to consider multiple transactions at
>> the same
>> time, e.g. a transaction with its high-fee child. This may help us better
>> determine whether transactions should be accepted to our mempool,
>> especially if
>> they don't meet fee requirements individually or are better RBF
>> candidates as a
>> package. A combination of changes to mempool validation logic, policy, and
>> transaction relay allows us to better propagate the transactions with the
>> highest package feerates to miners, and makes fee-bumping tools more
>> powerful
>> for users.
>>
>> The "relay" part of Package Relay suggests P2P messaging changes, but a
>> large
>> part of the changes are in the mempool's package validation logic. We
>> call this
>> *Package Mempool Accept*.
>>
>> ### Previous Work
>>
>> * Given that mempool validation is DoS-sensitive and complex, it would be
>>   dangerous to haphazardly tack on package validation logic. Many efforts
>> have
>> been made to make mempool validation less opaque (see [#16400][4],
>> [#21062][5],
>> [#22675][6], [#22796][7]).
>> * [#20833][8] Added basic capabilities for package validation, test
>> accepts only
>>   (no submission to mempool).
>> * [#21800][9] Implemented package ancestor/descendant limit checks for
>> arbitrary
>>   packages. Still test accepts only.
>> * Previous package relay proposals (see [#16401][10], [#19621][11]).
>>
>> ### Existing Package Rules
>>
>> These are in master as introduced in [#20833][8] and [#21800][9]. I'll
>> consider
>> them as "given" in the rest of this document, though they can be changed,
>> since
>> package validation is test-accept only right now.
>>
>> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and
>> `MAX_PACKAGE_SIZE=101KvB` total size [8]
>>
>>    *Rationale*: This is already enforced as mempool ancestor/descendant
>> limits.
>> Presumably, transactions in a package are all related, so exceeding this
>> limit
>> would mean that the package can either be split up or it wouldn't pass
>> this
>> mempool policy.
>>
>> 2. Packages must be topologically sorted: if any dependencies exist
>> between
>> transactions, parents must appear somewhere before children. [8]
>>
>> 3. A package cannot have conflicting transactions, i.e. none of them can
>> spend
>> the same inputs. This also means there cannot be duplicate transactions.
>> [8]
>>
>> 4. When packages are evaluated against ancestor/descendant limits in a
>> test
>> accept, the union of all of their descendants and ancestors is
>> considered. This
>> is essentially a "worst case" heuristic where every transaction in the
>> package
>> is treated as each other's ancestor and descendant. [8]
>> Packages for which ancestor/descendant limits are accurately captured by
>> this
>> heuristic: [19]
>>
>> There are also limitations such as the fact that CPFP carve out is not
>> applied
>> to package transactions. #20833 also disables RBF in package validation;
>> this
>> proposal overrides that to allow packages to use RBF.
>>
>> ## Proposed Changes
>>
>> The next step in the Package Mempool Accept project is to implement
>> submission
>> to mempool, initially through RPC only. This allows us to test the
>> submission
>> logic before exposing it on P2P.
>>
>> ### Summary
>>
>> - Packages may contain already-in-mempool transactions.
>> - Packages are 2 generations, Multi-Parent-1-Child.
>> - Fee-related checks use the package feerate. This means that wallets can
>> create a package that utilizes CPFP.
>> - Parents are allowed to RBF mempool transactions with a set of rules
>> similar
>>   to BIP125. This enables a combination of CPFP and RBF, where a
>> transaction's descendant fees pay for replacing mempool conflicts.
>>
>> There is a draft implementation in [#22290][1]. It is WIP, but feedback is
>> always welcome.
>>
>> ### Details
>>
>> #### Packages May Contain Already-in-Mempool Transactions
>>
>> A package may contain transactions that are already in the mempool. We
>> remove
>> ("deduplicate") those transactions from the package for the purposes of
>> package
>> mempool acceptance. If a package is empty after deduplication, we do
>> nothing.
>>
>> *Rationale*: Mempools vary across the network. It's possible for a parent
>> to be
>> accepted to the mempool of a peer on its own due to differences in policy
>> and
>> fee market fluctuations. We should not reject or penalize the entire
>> package for
>> an individual transaction as that could be a censorship vector.
>>
>> #### Packages Are Multi-Parent-1-Child
>>
>> Only packages of a specific topology are permitted. Namely, a package is
>> exactly
>> 1 child with all of its unconfirmed parents. After deduplication, the
>> package
>> may be exactly the same, empty, 1 child, 1 child with just some of its
>> unconfirmed parents, etc. Note that it's possible for the parents to be
>> indirect
>> descendants/ancestors of one another, or for parent and child to share a
>> parent,
>> so we cannot make any other topology assumptions.
>>
>> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple
>> parents
>> makes it possible to fee-bump a batch of transactions. Restricting
>> packages to a
>> defined topology is also easier to reason about and simplifies the
>> validation
>> logic greatly. Multi-parent-1-child allows us to think of the package as
>> one big
>> transaction, where:
>>
>> - Inputs = all the inputs of parents + inputs of the child that come from
>>   confirmed UTXOs
>> - Outputs = all the outputs of the child + all outputs of the parents that
>>   aren't spent by other transactions in the package
>>
>> Examples of packages that follow this rule (variations of example A show
>> some
>> possibilities after deduplication): ![image][15]
>>
>> #### Fee-Related Checks Use Package Feerate
>>
>> Package Feerate = the total modified fees divided by the total virtual
>> size of
>> all transactions in the package.
>>
>> To meet the two feerate requirements of a mempool, i.e., the
>> pre-configured
>> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum
>> feerate, the
>> total package feerate is used instead of the individual feerate. The
>> individual
>> transactions are allowed to be below feerate requirements if the package
>> meets
>> the feerate requirements. For example, the parent(s) in the package can
>> have 0
>> fees but be paid for by the child.
>>
>> *Rationale*: This can be thought of as "CPFP within a package," solving
>> the
>> issue of a parent not meeting minimum fees on its own. This allows L2
>> applications to adjust their fees at broadcast time instead of
>> overshooting or
>> risking getting stuck/pinned.
>>
>> We use the package feerate of the package *after deduplication*.
>>
>> *Rationale*:  It would be incorrect to use the fees of transactions that
>> are
>> already in the mempool, as we do not want a transaction's fees to be
>> double-counted for both its individual RBF and package RBF.
>>
>> Examples F and G [14] show the same package, but P1 is submitted
>> individually before
>> the package in example G. In example F, we can see that the 300vB package
>> pays
>> an additional 200sat in fees, which is not enough to pay for its own
>> bandwidth
>> (BIP125#4). In example G, we can see that P1 pays enough to replace M1,
>> but
>> using P1's fees again during package submission would make it look like a
>> 300sat
>> increase for a 200vB package. Even including its fees and size would not
>> be
>> sufficient in this example, since the 300sat looks like enough for the
>> 300vB
>> package. The calculcation after deduplication is 100sat increase for a
>> package
>> of size 200vB, which correctly fails BIP125#4. Assume all transactions
>> have a
>> size of 100vB.
>>
>> #### Package RBF
>>
>> If a package meets feerate requirements as a package, the parents in the
>> transaction are allowed to replace-by-fee mempool transactions. The child
>> cannot
>> replace mempool transactions. Multiple transactions can replace the same
>> transaction, but in order to be valid, none of the transactions can try to
>> replace an ancestor of another transaction in the same package (which
>> would thus
>> make its inputs unavailable).
>>
>> *Rationale*: Even if we are using package feerate, a package will not
>> propagate
>> as intended if RBF still requires each individual transaction to meet the
>> feerate requirements.
>>
>> We use a set of rules slightly modified from BIP125 as follows:
>>
>> ##### Signaling (Rule #1)
>>
>> All mempool transactions to be replaced must signal replaceability.
>>
>> *Rationale*: Package RBF signaling logic should be the same for package
>> RBF and
>> single transaction acceptance. This would be updated if single transaction
>> validation moves to full RBF.
>>
>> ##### New Unconfirmed Inputs (Rule #2)
>>
>> A package may include new unconfirmed inputs, but the ancestor feerate of
>> the
>> child must be at least as high as the ancestor feerates of every
>> transaction
>> being replaced. This is contrary to BIP125#2, which states "The
>> replacement
>> transaction may only include an unconfirmed input if that input was
>> included in
>> one of the original transactions. (An unconfirmed input spends an output
>> from a
>> currently-unconfirmed transaction.)"
>>
>> *Rationale*: The purpose of BIP125#2 is to ensure that the replacement
>> transaction has a higher ancestor score than the original transaction(s)
>> (see
>> [comment][13]). Example H [16] shows how adding a new unconfirmed input
>> can lower the
>> ancestor score of the replacement transaction. P1 is trying to replace
>> M1, and
>> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat, and
>> M2 pays
>> 100sat. Assume all transactions have a size of 100vB. While, in
>> isolation, P1
>> looks like a better mining candidate than M1, it must be mined with M2,
>> so its
>> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's ancestor
>> feerate, which is 6sat/vB.
>>
>> In package RBF, the rule analogous to BIP125#2 would be "none of the
>> transactions in the package can spend new unconfirmed inputs." Example J
>> [17] shows
>> why, if any of the package transactions have ancestors, package feerate
>> is no
>> longer accurate. Even though M2 and M3 are not ancestors of P1 (which is
>> the
>> replacement transaction in an RBF), we're actually interested in the
>> entire
>> package. A miner should mine M1 which is 5sat/vB instead of M2, M3, P1,
>> P2, and
>> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened to
>> only allow
>> the child to have new unconfirmed inputs, either, because it can still
>> cause us
>> to overestimate the package's ancestor score.
>>
>> However, enforcing a rule analogous to BIP125#2 would not only make
>> Package RBF
>> less useful, but would also break Package RBF for packages with parents
>> already
>> in the mempool: if a package parent has already been submitted, it would
>> look
>> like the child is spending a "new" unconfirmed input. In example K [18],
>> we're
>> looking to replace M1 with the entire package including P1, P2, and P3.
>> We must
>> consider the case where one of the parents is already in the mempool (in
>> this
>> case, P2), which means we must allow P3 to have new unconfirmed inputs.
>> However,
>> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not replace
>> M1
>> with this package.
>>
>> Thus, the package RBF rule regarding new unconfirmed inputs is less
>> strict than
>> BIP125#2. However, we still achieve the same goal of requiring the
>> replacement
>> transactions to have a ancestor score at least as high as the original
>> ones. As
>> a result, the entire package is required to be a higher feerate mining
>> candidate
>> than each of the replaced transactions.
>>
>> Another note: the [comment][13] above the BIP125#2 code in the original
>> RBF
>> implementation suggests that the rule was intended to be temporary.
>>
>> ##### Absolute Fee (Rule #3)
>>
>> The package must increase the absolute fee of the mempool, i.e. the total
>> fees
>> of the package must be higher than the absolute fees of the mempool
>> transactions
>> it replaces. Combined with the CPFP rule above, this differs from BIP125
>> Rule #3
>> - an individual transaction in the package may have lower fees than the
>>   transaction(s) it is replacing. In fact, it may have 0 fees, and the
>> child
>> pays for RBF.
>>
>> ##### Feerate (Rule #4)
>>
>> The package must pay for its own bandwidth; the package feerate must be
>> higher
>> than the replaced transactions by at least minimum relay feerate
>> (`incrementalRelayFee`). Combined with the CPFP rule above, this differs
>> from
>> BIP125 Rule #4 - an individual transaction in the package can have a lower
>> feerate than the transaction(s) it is replacing. In fact, it may have 0
>> fees,
>> and the child pays for RBF.
>>
>> ##### Total Number of Replaced Transactions (Rule #5)
>>
>> The package cannot replace more than 100 mempool transactions. This is
>> identical
>> to BIP125 Rule #5.
>>
>> ### Expected FAQs
>>
>> 1. Is it possible for only some of the package to make it into the
>> mempool?
>>
>>    Yes, it is. However, since we evict transactions from the mempool by
>> descendant score and the package child is supposed to be sponsoring the
>> fees of
>> its parents, the most common scenario would be all-or-nothing. This is
>> incentive-compatible. In fact, to be conservative, package validation
>> should
>> begin by trying to submit all of the transactions individually, and only
>> use the
>> package mempool acceptance logic if the parents fail due to low feerate.
>>
>> 2. Should we allow packages to contain already-confirmed transactions?
>>
>>     No, for practical reasons. In mempool validation, we actually aren't
>> able to
>> tell with 100% confidence if we are looking at a transaction that has
>> already
>> confirmed, because we look up inputs using a UTXO set. If we have
>> historical
>> block data, it's possible to look for it, but this is inefficient, not
>> always
>> possible for pruning nodes, and unnecessary because we're not going to do
>> anything with the transaction anyway. As such, we already have the
>> expectation
>> that transaction relay is somewhat "stateful" i.e. nobody should be
>> relaying
>> transactions that have already been confirmed. Similarly, we shouldn't be
>> relaying packages that contain already-confirmed transactions.
>>
>> [1]: https://github.com/bitcoin/bitcoin/pull/22290
>> [2]:
>> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations
>> [3]:
>> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282
>> [4]: https://github.com/bitcoin/bitcoin/pull/16400
>> [5]: https://github.com/bitcoin/bitcoin/pull/21062
>> [6]: https://github.com/bitcoin/bitcoin/pull/22675
>> [7]: https://github.com/bitcoin/bitcoin/pull/22796
>> [8]: https://github.com/bitcoin/bitcoin/pull/20833
>> [9]: https://github.com/bitcoin/bitcoin/pull/21800
>> [10]: https://github.com/bitcoin/bitcoin/pull/16401
>> [11]: https://github.com/bitcoin/bitcoin/pull/19621
>> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki
>> [13]:
>> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104
>> [14]:
>> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png
>> [15]:
>> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png
>> [16]:
>> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png
>> [17]:
>> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png
>> [18]:
>> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png
>> [19]:
>> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png
>> [20]:
>> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210920/09a51edf/attachment-0001.html>

From ZmnSCPxj at protonmail.com  Tue Sep 21 02:11:42 2021
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Tue, 21 Sep 2021 02:11:42 +0000
Subject: [bitcoin-dev] [Lightning-dev] Inherited IDs - A safer,
	more powerful alternative to BIP-118 (ANYPREVOUT) for scaling
	Bitcoin
In-Reply-To: <CAD5xwhh-1zUbPgYW6hE8q3CmhFZFdEqjx5pB7+VFM4mV=1FfaQ@mail.gmail.com>
References: <CAD5xwhh-1zUbPgYW6hE8q3CmhFZFdEqjx5pB7+VFM4mV=1FfaQ@mail.gmail.com>
Message-ID: <wDdSrhoOhFv2L7-IsDteE5PoDaZJk3aFZSvNgYd_PbuoVwLQ3qwheW-00wV52utUrpDhzsbAKvzYRhm5WUkXBtC-y0YPe9t17TaWodK1WsY=@protonmail.com>

Good morning John Law,


> (at the expense of requiring an on-chain transaction to update
> the set of channels created by the factory).

Hmmm this kind of loses the point of a factory?
By my understanding, the point is that the set of channels can be changed *without* an onchain transaction.

Otherwise, it seems to me that factories with this "expense of requiring an on-chain transaction" can be created, today, without even Taproot:

* The funding transaction output pays to a simple n-of-n.
* The above n-of-n is spent by an *offchain* transaction that splits the funds to the current set of channels.
* To change the set of channels, the participants perform this ritual:
  * Create, but do not sign, an alternate transaction that spends the above n-of-n to a new n-of-n with the same participants (possibly with tweaked keys).
  * Create and sign, but do not broadcast, a transaction that spends the above alternate n-of-n output and splits it to the new set of channels.
  * Sign the alternate transaction and broadcast it, this is the on-chain transaction needed to update the set of channels.

The above works today without changes to Bitcoin, and even without Taproot (though for large N the witness size does become fairly large without Taproot).

The above is really just a "no updates" factory that cuts through its closing transaction with the opening of a new factory.

Regards,
ZmnSCPxj

From gloriajzhao at gmail.com  Tue Sep 21 11:18:31 2021
From: gloriajzhao at gmail.com (Gloria Zhao)
Date: Tue, 21 Sep 2021 12:18:31 +0100
Subject: [bitcoin-dev] Proposal: Package Mempool Accept and Package RBF
In-Reply-To: <CACdvm3NdqYFvJ9t4ocXjdLT09fPu40YYwdvpvOnyYCmk5QXyrQ@mail.gmail.com>
References: <CAFXO6=+cHyQKM2n9yn4EhwLZO+AUB0ZD81qWPxmpN27rjUoU3w@mail.gmail.com>
 <CACdvm3NdqYFvJ9t4ocXjdLT09fPu40YYwdvpvOnyYCmk5QXyrQ@mail.gmail.com>
Message-ID: <CAFXO6=+auN_X=C52WB8fBqREUYahnFr1dzYotys7k7x+PjO1DQ@mail.gmail.com>

Hi Bastien,

Thank you for your feedback!

> In your example we have a parent transaction A already in the mempool
> and an unrelated child B. We submit a package C + D where C spends
> another of A's inputs. You're highlighting that this package may be
> rejected because of the unrelated transaction(s) B.

> The way I see this, an attacker can abuse this rule to ensure
> transaction A stays pinned in the mempool without confirming by
> broadcasting a set of child transactions that reach these limits
> and pay low fees (where A would be a commit tx in LN).

I believe you are describing a pinning attack in which your adversarial
counterparty attempts to monopolize the mempool descendant limit of the
shared  transaction A in order to prevent you from submitting a fee-bumping
child C; I've tried to illustrate this as diagram A here:
https://user-images.githubusercontent.com/25183001/134159860-068080d0-bbb6-4356-ae74-00df00644c74.png
(please let me know if I'm misunderstanding).

I believe this attack is mitigated as long as we attempt to submit
transactions individually (and thus take advantage of CPFP carve out)
before attempting package validation. So, in scenario A2, even if the
mempool receives a package with A+C, it would deduplicate A, submit C as an
individual transaction, and allow it due to the CPFP carve out exemption. A
more general goal is: if a transaction would propagate successfully on its
own now, it should still propagate regardless of whether it is included in
a package. The best way to ensure this, as far as I can tell, is to always
try to submit them individually first.

I would note that this proposal doesn't accommodate something like diagram
B, where C is getting CPFP carve out and wants to bring a +1 (e.g. C has
very low fees and is bumped by D). I don't think this is a use case since C
should be the one fee-bumping A, but since we're talking about limitations
around the CPFP carve out, this is it.

Let me know if this addresses your concerns?

Thanks,
Gloria

On Mon, Sep 20, 2021 at 10:19 AM Bastien TEINTURIER <bastien at acinq.fr>
wrote:

> Hi Gloria,
>
> Thanks for this detailed post!
>
> The illustrations you provided are very useful for this kind of graph
> topology problems.
>
> The rules you lay out for package RBF look good to me at first glance
> as there are some subtle improvements compared to BIP 125.
>
> > 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and
> > `MAX_PACKAGE_SIZE=101KvB` total size [8]
>
> I have a question regarding this rule, as your example 2C could be
> concerning for LN (unless I didn't understand it correctly).
>
> This also touches on the package RBF rule 5 ("The package cannot
> replace more than 100 mempool transactions.")
>
> In your example we have a parent transaction A already in the mempool
> and an unrelated child B. We submit a package C + D where C spends
> another of A's inputs. You're highlighting that this package may be
> rejected because of the unrelated transaction(s) B.
>
> The way I see this, an attacker can abuse this rule to ensure
> transaction A stays pinned in the mempool without confirming by
> broadcasting a set of child transactions that reach these limits
> and pay low fees (where A would be a commit tx in LN).
>
> We had to create the CPFP carve-out rule explicitly to work around
> this limitation, and I think it would be necessary for package RBF
> as well, because in such cases we do want to be able to submit a
> package A + C where C pays high fees to speed up A's confirmation,
> regardless of unrelated unconfirmed children of A...
>
> We could submit only C to benefit from the existing CPFP carve-out
> rule, but that wouldn't work if our local mempool doesn't have A yet,
> but other remote mempools do.
>
> Is my concern justified? Is this something that we should dig into a
> bit deeper?
>
> Thanks,
> Bastien
>
> Le jeu. 16 sept. 2021 ? 09:55, Gloria Zhao via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> a ?crit :
>
>> Hi there,
>>
>> I'm writing to propose a set of mempool policy changes to enable package
>> validation (in preparation for package relay) in Bitcoin Core. These
>> would not
>> be consensus or P2P protocol changes. However, since mempool policy
>> significantly affects transaction propagation, I believe this is relevant
>> for
>> the mailing list.
>>
>> My proposal enables packages consisting of multiple parents and 1 child.
>> If you
>> develop software that relies on specific transaction relay assumptions
>> and/or
>> are interested in using package relay in the future, I'm very interested
>> to hear
>> your feedback on the utility or restrictiveness of these package policies
>> for
>> your use cases.
>>
>> A draft implementation of this proposal can be found in [Bitcoin Core
>> PR#22290][1].
>>
>> An illustrated version of this post can be found at
>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.
>> I have also linked the images below.
>>
>> ## Background
>>
>> Feel free to skip this section if you are already familiar with mempool
>> policy
>> and package relay terminology.
>>
>> ### Terminology Clarifications
>>
>> * Package = an ordered list of related transactions, representable by a
>> Directed
>>   Acyclic Graph.
>> * Package Feerate = the total modified fees divided by the total virtual
>> size of
>>   all transactions in the package.
>>     - Modified fees = a transaction's base fees + fee delta applied by
>> the user
>>       with `prioritisetransaction`. As such, we expect this to vary across
>> mempools.
>>     - Virtual Size = the maximum of virtual sizes calculated using [BIP141
>>       virtual size][2] and sigop weight. [Implemented here in Bitcoin
>> Core][3].
>>     - Note that feerate is not necessarily based on the base fees and
>> serialized
>>       size.
>>
>> * Fee-Bumping = user/wallet actions that take advantage of miner
>> incentives to
>>   boost a transaction's candidacy for inclusion in a block, including
>> Child Pays
>> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our intention in
>> mempool policy is to recognize when the new transaction is more
>> economical to
>> mine than the original one(s) but not open DoS vectors, so there are some
>> limitations.
>>
>> ### Policy
>>
>> The purpose of the mempool is to store the best (to be most
>> incentive-compatible
>> with miners, highest feerate) candidates for inclusion in a block. Miners
>> use
>> the mempool to build block templates. The mempool is also useful as a
>> cache for
>> boosting block relay and validation performance, aiding transaction
>> relay, and
>> generating feerate estimations.
>>
>> Ideally, all consensus-valid transactions paying reasonable fees should
>> make it
>> to miners through normal transaction relay, without any special
>> connectivity or
>> relationships with miners. On the other hand, nodes do not have unlimited
>> resources, and a P2P network designed to let any honest node broadcast
>> their
>> transactions also exposes the transaction validation engine to DoS
>> attacks from
>> malicious peers.
>>
>> As such, for unconfirmed transactions we are considering for our mempool,
>> we
>> apply a set of validation rules in addition to consensus, primarily to
>> protect
>> us from resource exhaustion and aid our efforts to keep the highest fee
>> transactions. We call this mempool _policy_: a set of (configurable,
>> node-specific) rules that transactions must abide by in order to be
>> accepted
>> into our mempool. Transaction "Standardness" rules and mempool
>> restrictions such
>> as "too-long-mempool-chain" are both examples of policy.
>>
>> ### Package Relay and Package Mempool Accept
>>
>> In transaction relay, we currently consider transactions one at a time for
>> submission to the mempool. This creates a limitation in the node's
>> ability to
>> determine which transactions have the highest feerates, since we cannot
>> take
>> into account descendants (i.e. cannot use CPFP) until all the
>> transactions are
>> in the mempool. Similarly, we cannot use a transaction's descendants when
>> considering it for RBF. When an individual transaction does not meet the
>> mempool
>> minimum feerate and the user isn't able to create a replacement
>> transaction
>> directly, it will not be accepted by mempools.
>>
>> This limitation presents a security issue for applications and users
>> relying on
>> time-sensitive transactions. For example, Lightning and other protocols
>> create
>> UTXOs with multiple spending paths, where one counterparty's spending
>> path opens
>> up after a timelock, and users are protected from cheating scenarios as
>> long as
>> they redeem on-chain in time. A key security assumption is that all
>> parties'
>> transactions will propagate and confirm in a timely manner. This
>> assumption can
>> be broken if fee-bumping does not work as intended.
>>
>> The end goal for Package Relay is to consider multiple transactions at
>> the same
>> time, e.g. a transaction with its high-fee child. This may help us better
>> determine whether transactions should be accepted to our mempool,
>> especially if
>> they don't meet fee requirements individually or are better RBF
>> candidates as a
>> package. A combination of changes to mempool validation logic, policy, and
>> transaction relay allows us to better propagate the transactions with the
>> highest package feerates to miners, and makes fee-bumping tools more
>> powerful
>> for users.
>>
>> The "relay" part of Package Relay suggests P2P messaging changes, but a
>> large
>> part of the changes are in the mempool's package validation logic. We
>> call this
>> *Package Mempool Accept*.
>>
>> ### Previous Work
>>
>> * Given that mempool validation is DoS-sensitive and complex, it would be
>>   dangerous to haphazardly tack on package validation logic. Many efforts
>> have
>> been made to make mempool validation less opaque (see [#16400][4],
>> [#21062][5],
>> [#22675][6], [#22796][7]).
>> * [#20833][8] Added basic capabilities for package validation, test
>> accepts only
>>   (no submission to mempool).
>> * [#21800][9] Implemented package ancestor/descendant limit checks for
>> arbitrary
>>   packages. Still test accepts only.
>> * Previous package relay proposals (see [#16401][10], [#19621][11]).
>>
>> ### Existing Package Rules
>>
>> These are in master as introduced in [#20833][8] and [#21800][9]. I'll
>> consider
>> them as "given" in the rest of this document, though they can be changed,
>> since
>> package validation is test-accept only right now.
>>
>> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and
>> `MAX_PACKAGE_SIZE=101KvB` total size [8]
>>
>>    *Rationale*: This is already enforced as mempool ancestor/descendant
>> limits.
>> Presumably, transactions in a package are all related, so exceeding this
>> limit
>> would mean that the package can either be split up or it wouldn't pass
>> this
>> mempool policy.
>>
>> 2. Packages must be topologically sorted: if any dependencies exist
>> between
>> transactions, parents must appear somewhere before children. [8]
>>
>> 3. A package cannot have conflicting transactions, i.e. none of them can
>> spend
>> the same inputs. This also means there cannot be duplicate transactions.
>> [8]
>>
>> 4. When packages are evaluated against ancestor/descendant limits in a
>> test
>> accept, the union of all of their descendants and ancestors is
>> considered. This
>> is essentially a "worst case" heuristic where every transaction in the
>> package
>> is treated as each other's ancestor and descendant. [8]
>> Packages for which ancestor/descendant limits are accurately captured by
>> this
>> heuristic: [19]
>>
>> There are also limitations such as the fact that CPFP carve out is not
>> applied
>> to package transactions. #20833 also disables RBF in package validation;
>> this
>> proposal overrides that to allow packages to use RBF.
>>
>> ## Proposed Changes
>>
>> The next step in the Package Mempool Accept project is to implement
>> submission
>> to mempool, initially through RPC only. This allows us to test the
>> submission
>> logic before exposing it on P2P.
>>
>> ### Summary
>>
>> - Packages may contain already-in-mempool transactions.
>> - Packages are 2 generations, Multi-Parent-1-Child.
>> - Fee-related checks use the package feerate. This means that wallets can
>> create a package that utilizes CPFP.
>> - Parents are allowed to RBF mempool transactions with a set of rules
>> similar
>>   to BIP125. This enables a combination of CPFP and RBF, where a
>> transaction's descendant fees pay for replacing mempool conflicts.
>>
>> There is a draft implementation in [#22290][1]. It is WIP, but feedback is
>> always welcome.
>>
>> ### Details
>>
>> #### Packages May Contain Already-in-Mempool Transactions
>>
>> A package may contain transactions that are already in the mempool. We
>> remove
>> ("deduplicate") those transactions from the package for the purposes of
>> package
>> mempool acceptance. If a package is empty after deduplication, we do
>> nothing.
>>
>> *Rationale*: Mempools vary across the network. It's possible for a parent
>> to be
>> accepted to the mempool of a peer on its own due to differences in policy
>> and
>> fee market fluctuations. We should not reject or penalize the entire
>> package for
>> an individual transaction as that could be a censorship vector.
>>
>> #### Packages Are Multi-Parent-1-Child
>>
>> Only packages of a specific topology are permitted. Namely, a package is
>> exactly
>> 1 child with all of its unconfirmed parents. After deduplication, the
>> package
>> may be exactly the same, empty, 1 child, 1 child with just some of its
>> unconfirmed parents, etc. Note that it's possible for the parents to be
>> indirect
>> descendants/ancestors of one another, or for parent and child to share a
>> parent,
>> so we cannot make any other topology assumptions.
>>
>> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple
>> parents
>> makes it possible to fee-bump a batch of transactions. Restricting
>> packages to a
>> defined topology is also easier to reason about and simplifies the
>> validation
>> logic greatly. Multi-parent-1-child allows us to think of the package as
>> one big
>> transaction, where:
>>
>> - Inputs = all the inputs of parents + inputs of the child that come from
>>   confirmed UTXOs
>> - Outputs = all the outputs of the child + all outputs of the parents that
>>   aren't spent by other transactions in the package
>>
>> Examples of packages that follow this rule (variations of example A show
>> some
>> possibilities after deduplication): ![image][15]
>>
>> #### Fee-Related Checks Use Package Feerate
>>
>> Package Feerate = the total modified fees divided by the total virtual
>> size of
>> all transactions in the package.
>>
>> To meet the two feerate requirements of a mempool, i.e., the
>> pre-configured
>> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum
>> feerate, the
>> total package feerate is used instead of the individual feerate. The
>> individual
>> transactions are allowed to be below feerate requirements if the package
>> meets
>> the feerate requirements. For example, the parent(s) in the package can
>> have 0
>> fees but be paid for by the child.
>>
>> *Rationale*: This can be thought of as "CPFP within a package," solving
>> the
>> issue of a parent not meeting minimum fees on its own. This allows L2
>> applications to adjust their fees at broadcast time instead of
>> overshooting or
>> risking getting stuck/pinned.
>>
>> We use the package feerate of the package *after deduplication*.
>>
>> *Rationale*:  It would be incorrect to use the fees of transactions that
>> are
>> already in the mempool, as we do not want a transaction's fees to be
>> double-counted for both its individual RBF and package RBF.
>>
>> Examples F and G [14] show the same package, but P1 is submitted
>> individually before
>> the package in example G. In example F, we can see that the 300vB package
>> pays
>> an additional 200sat in fees, which is not enough to pay for its own
>> bandwidth
>> (BIP125#4). In example G, we can see that P1 pays enough to replace M1,
>> but
>> using P1's fees again during package submission would make it look like a
>> 300sat
>> increase for a 200vB package. Even including its fees and size would not
>> be
>> sufficient in this example, since the 300sat looks like enough for the
>> 300vB
>> package. The calculcation after deduplication is 100sat increase for a
>> package
>> of size 200vB, which correctly fails BIP125#4. Assume all transactions
>> have a
>> size of 100vB.
>>
>> #### Package RBF
>>
>> If a package meets feerate requirements as a package, the parents in the
>> transaction are allowed to replace-by-fee mempool transactions. The child
>> cannot
>> replace mempool transactions. Multiple transactions can replace the same
>> transaction, but in order to be valid, none of the transactions can try to
>> replace an ancestor of another transaction in the same package (which
>> would thus
>> make its inputs unavailable).
>>
>> *Rationale*: Even if we are using package feerate, a package will not
>> propagate
>> as intended if RBF still requires each individual transaction to meet the
>> feerate requirements.
>>
>> We use a set of rules slightly modified from BIP125 as follows:
>>
>> ##### Signaling (Rule #1)
>>
>> All mempool transactions to be replaced must signal replaceability.
>>
>> *Rationale*: Package RBF signaling logic should be the same for package
>> RBF and
>> single transaction acceptance. This would be updated if single transaction
>> validation moves to full RBF.
>>
>> ##### New Unconfirmed Inputs (Rule #2)
>>
>> A package may include new unconfirmed inputs, but the ancestor feerate of
>> the
>> child must be at least as high as the ancestor feerates of every
>> transaction
>> being replaced. This is contrary to BIP125#2, which states "The
>> replacement
>> transaction may only include an unconfirmed input if that input was
>> included in
>> one of the original transactions. (An unconfirmed input spends an output
>> from a
>> currently-unconfirmed transaction.)"
>>
>> *Rationale*: The purpose of BIP125#2 is to ensure that the replacement
>> transaction has a higher ancestor score than the original transaction(s)
>> (see
>> [comment][13]). Example H [16] shows how adding a new unconfirmed input
>> can lower the
>> ancestor score of the replacement transaction. P1 is trying to replace
>> M1, and
>> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat, and
>> M2 pays
>> 100sat. Assume all transactions have a size of 100vB. While, in
>> isolation, P1
>> looks like a better mining candidate than M1, it must be mined with M2,
>> so its
>> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's ancestor
>> feerate, which is 6sat/vB.
>>
>> In package RBF, the rule analogous to BIP125#2 would be "none of the
>> transactions in the package can spend new unconfirmed inputs." Example J
>> [17] shows
>> why, if any of the package transactions have ancestors, package feerate
>> is no
>> longer accurate. Even though M2 and M3 are not ancestors of P1 (which is
>> the
>> replacement transaction in an RBF), we're actually interested in the
>> entire
>> package. A miner should mine M1 which is 5sat/vB instead of M2, M3, P1,
>> P2, and
>> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened to
>> only allow
>> the child to have new unconfirmed inputs, either, because it can still
>> cause us
>> to overestimate the package's ancestor score.
>>
>> However, enforcing a rule analogous to BIP125#2 would not only make
>> Package RBF
>> less useful, but would also break Package RBF for packages with parents
>> already
>> in the mempool: if a package parent has already been submitted, it would
>> look
>> like the child is spending a "new" unconfirmed input. In example K [18],
>> we're
>> looking to replace M1 with the entire package including P1, P2, and P3.
>> We must
>> consider the case where one of the parents is already in the mempool (in
>> this
>> case, P2), which means we must allow P3 to have new unconfirmed inputs.
>> However,
>> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not replace
>> M1
>> with this package.
>>
>> Thus, the package RBF rule regarding new unconfirmed inputs is less
>> strict than
>> BIP125#2. However, we still achieve the same goal of requiring the
>> replacement
>> transactions to have a ancestor score at least as high as the original
>> ones. As
>> a result, the entire package is required to be a higher feerate mining
>> candidate
>> than each of the replaced transactions.
>>
>> Another note: the [comment][13] above the BIP125#2 code in the original
>> RBF
>> implementation suggests that the rule was intended to be temporary.
>>
>> ##### Absolute Fee (Rule #3)
>>
>> The package must increase the absolute fee of the mempool, i.e. the total
>> fees
>> of the package must be higher than the absolute fees of the mempool
>> transactions
>> it replaces. Combined with the CPFP rule above, this differs from BIP125
>> Rule #3
>> - an individual transaction in the package may have lower fees than the
>>   transaction(s) it is replacing. In fact, it may have 0 fees, and the
>> child
>> pays for RBF.
>>
>> ##### Feerate (Rule #4)
>>
>> The package must pay for its own bandwidth; the package feerate must be
>> higher
>> than the replaced transactions by at least minimum relay feerate
>> (`incrementalRelayFee`). Combined with the CPFP rule above, this differs
>> from
>> BIP125 Rule #4 - an individual transaction in the package can have a lower
>> feerate than the transaction(s) it is replacing. In fact, it may have 0
>> fees,
>> and the child pays for RBF.
>>
>> ##### Total Number of Replaced Transactions (Rule #5)
>>
>> The package cannot replace more than 100 mempool transactions. This is
>> identical
>> to BIP125 Rule #5.
>>
>> ### Expected FAQs
>>
>> 1. Is it possible for only some of the package to make it into the
>> mempool?
>>
>>    Yes, it is. However, since we evict transactions from the mempool by
>> descendant score and the package child is supposed to be sponsoring the
>> fees of
>> its parents, the most common scenario would be all-or-nothing. This is
>> incentive-compatible. In fact, to be conservative, package validation
>> should
>> begin by trying to submit all of the transactions individually, and only
>> use the
>> package mempool acceptance logic if the parents fail due to low feerate.
>>
>> 2. Should we allow packages to contain already-confirmed transactions?
>>
>>     No, for practical reasons. In mempool validation, we actually aren't
>> able to
>> tell with 100% confidence if we are looking at a transaction that has
>> already
>> confirmed, because we look up inputs using a UTXO set. If we have
>> historical
>> block data, it's possible to look for it, but this is inefficient, not
>> always
>> possible for pruning nodes, and unnecessary because we're not going to do
>> anything with the transaction anyway. As such, we already have the
>> expectation
>> that transaction relay is somewhat "stateful" i.e. nobody should be
>> relaying
>> transactions that have already been confirmed. Similarly, we shouldn't be
>> relaying packages that contain already-confirmed transactions.
>>
>> [1]: https://github.com/bitcoin/bitcoin/pull/22290
>> [2]:
>> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations
>> [3]:
>> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282
>> [4]: https://github.com/bitcoin/bitcoin/pull/16400
>> [5]: https://github.com/bitcoin/bitcoin/pull/21062
>> [6]: https://github.com/bitcoin/bitcoin/pull/22675
>> [7]: https://github.com/bitcoin/bitcoin/pull/22796
>> [8]: https://github.com/bitcoin/bitcoin/pull/20833
>> [9]: https://github.com/bitcoin/bitcoin/pull/21800
>> [10]: https://github.com/bitcoin/bitcoin/pull/16401
>> [11]: https://github.com/bitcoin/bitcoin/pull/19621
>> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki
>> [13]:
>> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104
>> [14]:
>> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png
>> [15]:
>> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png
>> [16]:
>> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png
>> [17]:
>> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png
>> [18]:
>> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png
>> [19]:
>> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png
>> [20]:
>> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210921/505919a9/attachment-0001.html>

From bastien at acinq.fr  Tue Sep 21 15:18:28 2021
From: bastien at acinq.fr (Bastien TEINTURIER)
Date: Tue, 21 Sep 2021 17:18:28 +0200
Subject: [bitcoin-dev] Proposal: Package Mempool Accept and Package RBF
In-Reply-To: <CAFXO6=+auN_X=C52WB8fBqREUYahnFr1dzYotys7k7x+PjO1DQ@mail.gmail.com>
References: <CAFXO6=+cHyQKM2n9yn4EhwLZO+AUB0ZD81qWPxmpN27rjUoU3w@mail.gmail.com>
 <CACdvm3NdqYFvJ9t4ocXjdLT09fPu40YYwdvpvOnyYCmk5QXyrQ@mail.gmail.com>
 <CAFXO6=+auN_X=C52WB8fBqREUYahnFr1dzYotys7k7x+PjO1DQ@mail.gmail.com>
Message-ID: <CACdvm3Oo6ABbK79FvuAnZyJ+v4EU3nY=Zi1yzWmv45j5LjWSOg@mail.gmail.com>

Hi Gloria,

> I believe this attack is mitigated as long as we attempt to submit
transactions individually

Unfortunately not, as there exists a pinning scenario in LN where a
different commit tx is pinned, but you actually can't know which one.

Since I really like your diagrams, I made one as well to illustrate:
https://user-images.githubusercontent.com/31281497/134198114-5e9c6857-e8fc-405a-be57-18181d5e54cb.jpg

Here the issue is that a revoked commitment tx A' is pinned in other
mempools, with a long chain of descendants (or descendants that reach
the maximum replaceable size).

We would really like A + C to be able to replace this pinned A'.
We can't submit individually because A on its own won't replace A'...

> I would note that this proposal doesn't accommodate something like
diagram B, where C is getting CPFP carve out and wants to bring a +1

No worries, that case shouldn't be a concern.
I believe any L2 protocol can always ensure it confirms such tx trees
"one depth after the other" without impacting funds safety, so it
only needs to ensure A + C can get into mempools.

Thanks,
Bastien

Le mar. 21 sept. 2021 ? 13:18, Gloria Zhao <gloriajzhao at gmail.com> a ?crit :

> Hi Bastien,
>
> Thank you for your feedback!
>
> > In your example we have a parent transaction A already in the mempool
> > and an unrelated child B. We submit a package C + D where C spends
> > another of A's inputs. You're highlighting that this package may be
> > rejected because of the unrelated transaction(s) B.
>
> > The way I see this, an attacker can abuse this rule to ensure
> > transaction A stays pinned in the mempool without confirming by
> > broadcasting a set of child transactions that reach these limits
> > and pay low fees (where A would be a commit tx in LN).
>
> I believe you are describing a pinning attack in which your adversarial
> counterparty attempts to monopolize the mempool descendant limit of the
> shared  transaction A in order to prevent you from submitting a fee-bumping
> child C; I've tried to illustrate this as diagram A here:
> https://user-images.githubusercontent.com/25183001/134159860-068080d0-bbb6-4356-ae74-00df00644c74.png
> (please let me know if I'm misunderstanding).
>
> I believe this attack is mitigated as long as we attempt to submit
> transactions individually (and thus take advantage of CPFP carve out)
> before attempting package validation. So, in scenario A2, even if the
> mempool receives a package with A+C, it would deduplicate A, submit C as an
> individual transaction, and allow it due to the CPFP carve out exemption. A
> more general goal is: if a transaction would propagate successfully on its
> own now, it should still propagate regardless of whether it is included in
> a package. The best way to ensure this, as far as I can tell, is to always
> try to submit them individually first.
>
> I would note that this proposal doesn't accommodate something like diagram
> B, where C is getting CPFP carve out and wants to bring a +1 (e.g. C has
> very low fees and is bumped by D). I don't think this is a use case since C
> should be the one fee-bumping A, but since we're talking about limitations
> around the CPFP carve out, this is it.
>
> Let me know if this addresses your concerns?
>
> Thanks,
> Gloria
>
> On Mon, Sep 20, 2021 at 10:19 AM Bastien TEINTURIER <bastien at acinq.fr>
> wrote:
>
>> Hi Gloria,
>>
>> Thanks for this detailed post!
>>
>> The illustrations you provided are very useful for this kind of graph
>> topology problems.
>>
>> The rules you lay out for package RBF look good to me at first glance
>> as there are some subtle improvements compared to BIP 125.
>>
>> > 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and
>> > `MAX_PACKAGE_SIZE=101KvB` total size [8]
>>
>> I have a question regarding this rule, as your example 2C could be
>> concerning for LN (unless I didn't understand it correctly).
>>
>> This also touches on the package RBF rule 5 ("The package cannot
>> replace more than 100 mempool transactions.")
>>
>> In your example we have a parent transaction A already in the mempool
>> and an unrelated child B. We submit a package C + D where C spends
>> another of A's inputs. You're highlighting that this package may be
>> rejected because of the unrelated transaction(s) B.
>>
>> The way I see this, an attacker can abuse this rule to ensure
>> transaction A stays pinned in the mempool without confirming by
>> broadcasting a set of child transactions that reach these limits
>> and pay low fees (where A would be a commit tx in LN).
>>
>> We had to create the CPFP carve-out rule explicitly to work around
>> this limitation, and I think it would be necessary for package RBF
>> as well, because in such cases we do want to be able to submit a
>> package A + C where C pays high fees to speed up A's confirmation,
>> regardless of unrelated unconfirmed children of A...
>>
>> We could submit only C to benefit from the existing CPFP carve-out
>> rule, but that wouldn't work if our local mempool doesn't have A yet,
>> but other remote mempools do.
>>
>> Is my concern justified? Is this something that we should dig into a
>> bit deeper?
>>
>> Thanks,
>> Bastien
>>
>> Le jeu. 16 sept. 2021 ? 09:55, Gloria Zhao via bitcoin-dev <
>> bitcoin-dev at lists.linuxfoundation.org> a ?crit :
>>
>>> Hi there,
>>>
>>> I'm writing to propose a set of mempool policy changes to enable package
>>> validation (in preparation for package relay) in Bitcoin Core. These
>>> would not
>>> be consensus or P2P protocol changes. However, since mempool policy
>>> significantly affects transaction propagation, I believe this is
>>> relevant for
>>> the mailing list.
>>>
>>> My proposal enables packages consisting of multiple parents and 1 child.
>>> If you
>>> develop software that relies on specific transaction relay assumptions
>>> and/or
>>> are interested in using package relay in the future, I'm very interested
>>> to hear
>>> your feedback on the utility or restrictiveness of these package
>>> policies for
>>> your use cases.
>>>
>>> A draft implementation of this proposal can be found in [Bitcoin Core
>>> PR#22290][1].
>>>
>>> An illustrated version of this post can be found at
>>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.
>>> I have also linked the images below.
>>>
>>> ## Background
>>>
>>> Feel free to skip this section if you are already familiar with mempool
>>> policy
>>> and package relay terminology.
>>>
>>> ### Terminology Clarifications
>>>
>>> * Package = an ordered list of related transactions, representable by a
>>> Directed
>>>   Acyclic Graph.
>>> * Package Feerate = the total modified fees divided by the total virtual
>>> size of
>>>   all transactions in the package.
>>>     - Modified fees = a transaction's base fees + fee delta applied by
>>> the user
>>>       with `prioritisetransaction`. As such, we expect this to vary
>>> across
>>> mempools.
>>>     - Virtual Size = the maximum of virtual sizes calculated using
>>> [BIP141
>>>       virtual size][2] and sigop weight. [Implemented here in Bitcoin
>>> Core][3].
>>>     - Note that feerate is not necessarily based on the base fees and
>>> serialized
>>>       size.
>>>
>>> * Fee-Bumping = user/wallet actions that take advantage of miner
>>> incentives to
>>>   boost a transaction's candidacy for inclusion in a block, including
>>> Child Pays
>>> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our intention in
>>> mempool policy is to recognize when the new transaction is more
>>> economical to
>>> mine than the original one(s) but not open DoS vectors, so there are some
>>> limitations.
>>>
>>> ### Policy
>>>
>>> The purpose of the mempool is to store the best (to be most
>>> incentive-compatible
>>> with miners, highest feerate) candidates for inclusion in a block.
>>> Miners use
>>> the mempool to build block templates. The mempool is also useful as a
>>> cache for
>>> boosting block relay and validation performance, aiding transaction
>>> relay, and
>>> generating feerate estimations.
>>>
>>> Ideally, all consensus-valid transactions paying reasonable fees should
>>> make it
>>> to miners through normal transaction relay, without any special
>>> connectivity or
>>> relationships with miners. On the other hand, nodes do not have unlimited
>>> resources, and a P2P network designed to let any honest node broadcast
>>> their
>>> transactions also exposes the transaction validation engine to DoS
>>> attacks from
>>> malicious peers.
>>>
>>> As such, for unconfirmed transactions we are considering for our
>>> mempool, we
>>> apply a set of validation rules in addition to consensus, primarily to
>>> protect
>>> us from resource exhaustion and aid our efforts to keep the highest fee
>>> transactions. We call this mempool _policy_: a set of (configurable,
>>> node-specific) rules that transactions must abide by in order to be
>>> accepted
>>> into our mempool. Transaction "Standardness" rules and mempool
>>> restrictions such
>>> as "too-long-mempool-chain" are both examples of policy.
>>>
>>> ### Package Relay and Package Mempool Accept
>>>
>>> In transaction relay, we currently consider transactions one at a time
>>> for
>>> submission to the mempool. This creates a limitation in the node's
>>> ability to
>>> determine which transactions have the highest feerates, since we cannot
>>> take
>>> into account descendants (i.e. cannot use CPFP) until all the
>>> transactions are
>>> in the mempool. Similarly, we cannot use a transaction's descendants when
>>> considering it for RBF. When an individual transaction does not meet the
>>> mempool
>>> minimum feerate and the user isn't able to create a replacement
>>> transaction
>>> directly, it will not be accepted by mempools.
>>>
>>> This limitation presents a security issue for applications and users
>>> relying on
>>> time-sensitive transactions. For example, Lightning and other protocols
>>> create
>>> UTXOs with multiple spending paths, where one counterparty's spending
>>> path opens
>>> up after a timelock, and users are protected from cheating scenarios as
>>> long as
>>> they redeem on-chain in time. A key security assumption is that all
>>> parties'
>>> transactions will propagate and confirm in a timely manner. This
>>> assumption can
>>> be broken if fee-bumping does not work as intended.
>>>
>>> The end goal for Package Relay is to consider multiple transactions at
>>> the same
>>> time, e.g. a transaction with its high-fee child. This may help us better
>>> determine whether transactions should be accepted to our mempool,
>>> especially if
>>> they don't meet fee requirements individually or are better RBF
>>> candidates as a
>>> package. A combination of changes to mempool validation logic, policy,
>>> and
>>> transaction relay allows us to better propagate the transactions with the
>>> highest package feerates to miners, and makes fee-bumping tools more
>>> powerful
>>> for users.
>>>
>>> The "relay" part of Package Relay suggests P2P messaging changes, but a
>>> large
>>> part of the changes are in the mempool's package validation logic. We
>>> call this
>>> *Package Mempool Accept*.
>>>
>>> ### Previous Work
>>>
>>> * Given that mempool validation is DoS-sensitive and complex, it would be
>>>   dangerous to haphazardly tack on package validation logic. Many
>>> efforts have
>>> been made to make mempool validation less opaque (see [#16400][4],
>>> [#21062][5],
>>> [#22675][6], [#22796][7]).
>>> * [#20833][8] Added basic capabilities for package validation, test
>>> accepts only
>>>   (no submission to mempool).
>>> * [#21800][9] Implemented package ancestor/descendant limit checks for
>>> arbitrary
>>>   packages. Still test accepts only.
>>> * Previous package relay proposals (see [#16401][10], [#19621][11]).
>>>
>>> ### Existing Package Rules
>>>
>>> These are in master as introduced in [#20833][8] and [#21800][9]. I'll
>>> consider
>>> them as "given" in the rest of this document, though they can be
>>> changed, since
>>> package validation is test-accept only right now.
>>>
>>> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and
>>> `MAX_PACKAGE_SIZE=101KvB` total size [8]
>>>
>>>    *Rationale*: This is already enforced as mempool ancestor/descendant
>>> limits.
>>> Presumably, transactions in a package are all related, so exceeding this
>>> limit
>>> would mean that the package can either be split up or it wouldn't pass
>>> this
>>> mempool policy.
>>>
>>> 2. Packages must be topologically sorted: if any dependencies exist
>>> between
>>> transactions, parents must appear somewhere before children. [8]
>>>
>>> 3. A package cannot have conflicting transactions, i.e. none of them can
>>> spend
>>> the same inputs. This also means there cannot be duplicate transactions.
>>> [8]
>>>
>>> 4. When packages are evaluated against ancestor/descendant limits in a
>>> test
>>> accept, the union of all of their descendants and ancestors is
>>> considered. This
>>> is essentially a "worst case" heuristic where every transaction in the
>>> package
>>> is treated as each other's ancestor and descendant. [8]
>>> Packages for which ancestor/descendant limits are accurately captured by
>>> this
>>> heuristic: [19]
>>>
>>> There are also limitations such as the fact that CPFP carve out is not
>>> applied
>>> to package transactions. #20833 also disables RBF in package validation;
>>> this
>>> proposal overrides that to allow packages to use RBF.
>>>
>>> ## Proposed Changes
>>>
>>> The next step in the Package Mempool Accept project is to implement
>>> submission
>>> to mempool, initially through RPC only. This allows us to test the
>>> submission
>>> logic before exposing it on P2P.
>>>
>>> ### Summary
>>>
>>> - Packages may contain already-in-mempool transactions.
>>> - Packages are 2 generations, Multi-Parent-1-Child.
>>> - Fee-related checks use the package feerate. This means that wallets can
>>> create a package that utilizes CPFP.
>>> - Parents are allowed to RBF mempool transactions with a set of rules
>>> similar
>>>   to BIP125. This enables a combination of CPFP and RBF, where a
>>> transaction's descendant fees pay for replacing mempool conflicts.
>>>
>>> There is a draft implementation in [#22290][1]. It is WIP, but feedback
>>> is
>>> always welcome.
>>>
>>> ### Details
>>>
>>> #### Packages May Contain Already-in-Mempool Transactions
>>>
>>> A package may contain transactions that are already in the mempool. We
>>> remove
>>> ("deduplicate") those transactions from the package for the purposes of
>>> package
>>> mempool acceptance. If a package is empty after deduplication, we do
>>> nothing.
>>>
>>> *Rationale*: Mempools vary across the network. It's possible for a
>>> parent to be
>>> accepted to the mempool of a peer on its own due to differences in
>>> policy and
>>> fee market fluctuations. We should not reject or penalize the entire
>>> package for
>>> an individual transaction as that could be a censorship vector.
>>>
>>> #### Packages Are Multi-Parent-1-Child
>>>
>>> Only packages of a specific topology are permitted. Namely, a package is
>>> exactly
>>> 1 child with all of its unconfirmed parents. After deduplication, the
>>> package
>>> may be exactly the same, empty, 1 child, 1 child with just some of its
>>> unconfirmed parents, etc. Note that it's possible for the parents to be
>>> indirect
>>> descendants/ancestors of one another, or for parent and child to share a
>>> parent,
>>> so we cannot make any other topology assumptions.
>>>
>>> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple
>>> parents
>>> makes it possible to fee-bump a batch of transactions. Restricting
>>> packages to a
>>> defined topology is also easier to reason about and simplifies the
>>> validation
>>> logic greatly. Multi-parent-1-child allows us to think of the package as
>>> one big
>>> transaction, where:
>>>
>>> - Inputs = all the inputs of parents + inputs of the child that come from
>>>   confirmed UTXOs
>>> - Outputs = all the outputs of the child + all outputs of the parents
>>> that
>>>   aren't spent by other transactions in the package
>>>
>>> Examples of packages that follow this rule (variations of example A show
>>> some
>>> possibilities after deduplication): ![image][15]
>>>
>>> #### Fee-Related Checks Use Package Feerate
>>>
>>> Package Feerate = the total modified fees divided by the total virtual
>>> size of
>>> all transactions in the package.
>>>
>>> To meet the two feerate requirements of a mempool, i.e., the
>>> pre-configured
>>> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum
>>> feerate, the
>>> total package feerate is used instead of the individual feerate. The
>>> individual
>>> transactions are allowed to be below feerate requirements if the package
>>> meets
>>> the feerate requirements. For example, the parent(s) in the package can
>>> have 0
>>> fees but be paid for by the child.
>>>
>>> *Rationale*: This can be thought of as "CPFP within a package," solving
>>> the
>>> issue of a parent not meeting minimum fees on its own. This allows L2
>>> applications to adjust their fees at broadcast time instead of
>>> overshooting or
>>> risking getting stuck/pinned.
>>>
>>> We use the package feerate of the package *after deduplication*.
>>>
>>> *Rationale*:  It would be incorrect to use the fees of transactions that
>>> are
>>> already in the mempool, as we do not want a transaction's fees to be
>>> double-counted for both its individual RBF and package RBF.
>>>
>>> Examples F and G [14] show the same package, but P1 is submitted
>>> individually before
>>> the package in example G. In example F, we can see that the 300vB
>>> package pays
>>> an additional 200sat in fees, which is not enough to pay for its own
>>> bandwidth
>>> (BIP125#4). In example G, we can see that P1 pays enough to replace M1,
>>> but
>>> using P1's fees again during package submission would make it look like
>>> a 300sat
>>> increase for a 200vB package. Even including its fees and size would not
>>> be
>>> sufficient in this example, since the 300sat looks like enough for the
>>> 300vB
>>> package. The calculcation after deduplication is 100sat increase for a
>>> package
>>> of size 200vB, which correctly fails BIP125#4. Assume all transactions
>>> have a
>>> size of 100vB.
>>>
>>> #### Package RBF
>>>
>>> If a package meets feerate requirements as a package, the parents in the
>>> transaction are allowed to replace-by-fee mempool transactions. The
>>> child cannot
>>> replace mempool transactions. Multiple transactions can replace the same
>>> transaction, but in order to be valid, none of the transactions can try
>>> to
>>> replace an ancestor of another transaction in the same package (which
>>> would thus
>>> make its inputs unavailable).
>>>
>>> *Rationale*: Even if we are using package feerate, a package will not
>>> propagate
>>> as intended if RBF still requires each individual transaction to meet the
>>> feerate requirements.
>>>
>>> We use a set of rules slightly modified from BIP125 as follows:
>>>
>>> ##### Signaling (Rule #1)
>>>
>>> All mempool transactions to be replaced must signal replaceability.
>>>
>>> *Rationale*: Package RBF signaling logic should be the same for package
>>> RBF and
>>> single transaction acceptance. This would be updated if single
>>> transaction
>>> validation moves to full RBF.
>>>
>>> ##### New Unconfirmed Inputs (Rule #2)
>>>
>>> A package may include new unconfirmed inputs, but the ancestor feerate
>>> of the
>>> child must be at least as high as the ancestor feerates of every
>>> transaction
>>> being replaced. This is contrary to BIP125#2, which states "The
>>> replacement
>>> transaction may only include an unconfirmed input if that input was
>>> included in
>>> one of the original transactions. (An unconfirmed input spends an output
>>> from a
>>> currently-unconfirmed transaction.)"
>>>
>>> *Rationale*: The purpose of BIP125#2 is to ensure that the replacement
>>> transaction has a higher ancestor score than the original transaction(s)
>>> (see
>>> [comment][13]). Example H [16] shows how adding a new unconfirmed input
>>> can lower the
>>> ancestor score of the replacement transaction. P1 is trying to replace
>>> M1, and
>>> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat, and
>>> M2 pays
>>> 100sat. Assume all transactions have a size of 100vB. While, in
>>> isolation, P1
>>> looks like a better mining candidate than M1, it must be mined with M2,
>>> so its
>>> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's ancestor
>>> feerate, which is 6sat/vB.
>>>
>>> In package RBF, the rule analogous to BIP125#2 would be "none of the
>>> transactions in the package can spend new unconfirmed inputs." Example J
>>> [17] shows
>>> why, if any of the package transactions have ancestors, package feerate
>>> is no
>>> longer accurate. Even though M2 and M3 are not ancestors of P1 (which is
>>> the
>>> replacement transaction in an RBF), we're actually interested in the
>>> entire
>>> package. A miner should mine M1 which is 5sat/vB instead of M2, M3, P1,
>>> P2, and
>>> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened to
>>> only allow
>>> the child to have new unconfirmed inputs, either, because it can still
>>> cause us
>>> to overestimate the package's ancestor score.
>>>
>>> However, enforcing a rule analogous to BIP125#2 would not only make
>>> Package RBF
>>> less useful, but would also break Package RBF for packages with parents
>>> already
>>> in the mempool: if a package parent has already been submitted, it would
>>> look
>>> like the child is spending a "new" unconfirmed input. In example K [18],
>>> we're
>>> looking to replace M1 with the entire package including P1, P2, and P3.
>>> We must
>>> consider the case where one of the parents is already in the mempool (in
>>> this
>>> case, P2), which means we must allow P3 to have new unconfirmed inputs.
>>> However,
>>> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not
>>> replace M1
>>> with this package.
>>>
>>> Thus, the package RBF rule regarding new unconfirmed inputs is less
>>> strict than
>>> BIP125#2. However, we still achieve the same goal of requiring the
>>> replacement
>>> transactions to have a ancestor score at least as high as the original
>>> ones. As
>>> a result, the entire package is required to be a higher feerate mining
>>> candidate
>>> than each of the replaced transactions.
>>>
>>> Another note: the [comment][13] above the BIP125#2 code in the original
>>> RBF
>>> implementation suggests that the rule was intended to be temporary.
>>>
>>> ##### Absolute Fee (Rule #3)
>>>
>>> The package must increase the absolute fee of the mempool, i.e. the
>>> total fees
>>> of the package must be higher than the absolute fees of the mempool
>>> transactions
>>> it replaces. Combined with the CPFP rule above, this differs from BIP125
>>> Rule #3
>>> - an individual transaction in the package may have lower fees than the
>>>   transaction(s) it is replacing. In fact, it may have 0 fees, and the
>>> child
>>> pays for RBF.
>>>
>>> ##### Feerate (Rule #4)
>>>
>>> The package must pay for its own bandwidth; the package feerate must be
>>> higher
>>> than the replaced transactions by at least minimum relay feerate
>>> (`incrementalRelayFee`). Combined with the CPFP rule above, this differs
>>> from
>>> BIP125 Rule #4 - an individual transaction in the package can have a
>>> lower
>>> feerate than the transaction(s) it is replacing. In fact, it may have 0
>>> fees,
>>> and the child pays for RBF.
>>>
>>> ##### Total Number of Replaced Transactions (Rule #5)
>>>
>>> The package cannot replace more than 100 mempool transactions. This is
>>> identical
>>> to BIP125 Rule #5.
>>>
>>> ### Expected FAQs
>>>
>>> 1. Is it possible for only some of the package to make it into the
>>> mempool?
>>>
>>>    Yes, it is. However, since we evict transactions from the mempool by
>>> descendant score and the package child is supposed to be sponsoring the
>>> fees of
>>> its parents, the most common scenario would be all-or-nothing. This is
>>> incentive-compatible. In fact, to be conservative, package validation
>>> should
>>> begin by trying to submit all of the transactions individually, and only
>>> use the
>>> package mempool acceptance logic if the parents fail due to low feerate.
>>>
>>> 2. Should we allow packages to contain already-confirmed transactions?
>>>
>>>     No, for practical reasons. In mempool validation, we actually aren't
>>> able to
>>> tell with 100% confidence if we are looking at a transaction that has
>>> already
>>> confirmed, because we look up inputs using a UTXO set. If we have
>>> historical
>>> block data, it's possible to look for it, but this is inefficient, not
>>> always
>>> possible for pruning nodes, and unnecessary because we're not going to do
>>> anything with the transaction anyway. As such, we already have the
>>> expectation
>>> that transaction relay is somewhat "stateful" i.e. nobody should be
>>> relaying
>>> transactions that have already been confirmed. Similarly, we shouldn't be
>>> relaying packages that contain already-confirmed transactions.
>>>
>>> [1]: https://github.com/bitcoin/bitcoin/pull/22290
>>> [2]:
>>> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations
>>> [3]:
>>> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282
>>> [4]: https://github.com/bitcoin/bitcoin/pull/16400
>>> [5]: https://github.com/bitcoin/bitcoin/pull/21062
>>> [6]: https://github.com/bitcoin/bitcoin/pull/22675
>>> [7]: https://github.com/bitcoin/bitcoin/pull/22796
>>> [8]: https://github.com/bitcoin/bitcoin/pull/20833
>>> [9]: https://github.com/bitcoin/bitcoin/pull/21800
>>> [10]: https://github.com/bitcoin/bitcoin/pull/16401
>>> [11]: https://github.com/bitcoin/bitcoin/pull/19621
>>> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki
>>> [13]:
>>> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104
>>> [14]:
>>> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png
>>> [15]:
>>> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png
>>> [16]:
>>> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png
>>> [17]:
>>> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png
>>> [18]:
>>> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png
>>> [19]:
>>> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png
>>> [20]:
>>> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev at lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210921/464a6299/attachment-0001.html>

From gloriajzhao at gmail.com  Tue Sep 21 16:42:33 2021
From: gloriajzhao at gmail.com (Gloria Zhao)
Date: Tue, 21 Sep 2021 17:42:33 +0100
Subject: [bitcoin-dev] Proposal: Package Mempool Accept and Package RBF
In-Reply-To: <CACdvm3Oo6ABbK79FvuAnZyJ+v4EU3nY=Zi1yzWmv45j5LjWSOg@mail.gmail.com>
References: <CAFXO6=+cHyQKM2n9yn4EhwLZO+AUB0ZD81qWPxmpN27rjUoU3w@mail.gmail.com>
 <CACdvm3NdqYFvJ9t4ocXjdLT09fPu40YYwdvpvOnyYCmk5QXyrQ@mail.gmail.com>
 <CAFXO6=+auN_X=C52WB8fBqREUYahnFr1dzYotys7k7x+PjO1DQ@mail.gmail.com>
 <CACdvm3Oo6ABbK79FvuAnZyJ+v4EU3nY=Zi1yzWmv45j5LjWSOg@mail.gmail.com>
Message-ID: <CAFXO6=LrHWsddH7VP_AgK93Thw8mdeqmCjjkdXAZy1mymyBefA@mail.gmail.com>

Hi Bastien,

Excellent diagram :D

> Here the issue is that a revoked commitment tx A' is pinned in other
> mempools, with a long chain of descendants (or descendants that reach
> the maximum replaceable size).
> We would really like A + C to be able to replace this pinned A'.
> We can't submit individually because A on its own won't replace A'...

Right, this is a key motivation for having Package RBF. In this case, A+C
can replace A' + B1...B24.

Due to the descendant limit (each node operator can increase it on their
own node, but the default is 25), A' should have no more than 25
descendants, even including CPFP carve out. As long as A only conflicts
with A', it won't be trying to replace more than 100 transactions. The
proposed package RBF will allow C to pay for A's conflicts, since their
package feerate is used in the fee comparisons. A is not a descendant of
A', so the existence of B1...B24 does not prevent the replacement.

Best,
Gloria

On Tue, Sep 21, 2021 at 4:18 PM Bastien TEINTURIER <bastien at acinq.fr> wrote:

> Hi Gloria,
>
> > I believe this attack is mitigated as long as we attempt to submit
> transactions individually
>
> Unfortunately not, as there exists a pinning scenario in LN where a
> different commit tx is pinned, but you actually can't know which one.
>
> Since I really like your diagrams, I made one as well to illustrate:
>
> https://user-images.githubusercontent.com/31281497/134198114-5e9c6857-e8fc-405a-be57-18181d5e54cb.jpg
>
> Here the issue is that a revoked commitment tx A' is pinned in other
> mempools, with a long chain of descendants (or descendants that reach
> the maximum replaceable size).
>
> We would really like A + C to be able to replace this pinned A'.
> We can't submit individually because A on its own won't replace A'...
>
> > I would note that this proposal doesn't accommodate something like
> diagram B, where C is getting CPFP carve out and wants to bring a +1
>
> No worries, that case shouldn't be a concern.
> I believe any L2 protocol can always ensure it confirms such tx trees
> "one depth after the other" without impacting funds safety, so it
> only needs to ensure A + C can get into mempools.
>
> Thanks,
> Bastien
>
> Le mar. 21 sept. 2021 ? 13:18, Gloria Zhao <gloriajzhao at gmail.com> a
> ?crit :
>
>> Hi Bastien,
>>
>> Thank you for your feedback!
>>
>> > In your example we have a parent transaction A already in the mempool
>> > and an unrelated child B. We submit a package C + D where C spends
>> > another of A's inputs. You're highlighting that this package may be
>> > rejected because of the unrelated transaction(s) B.
>>
>> > The way I see this, an attacker can abuse this rule to ensure
>> > transaction A stays pinned in the mempool without confirming by
>> > broadcasting a set of child transactions that reach these limits
>> > and pay low fees (where A would be a commit tx in LN).
>>
>> I believe you are describing a pinning attack in which your adversarial
>> counterparty attempts to monopolize the mempool descendant limit of the
>> shared  transaction A in order to prevent you from submitting a fee-bumping
>> child C; I've tried to illustrate this as diagram A here:
>> https://user-images.githubusercontent.com/25183001/134159860-068080d0-bbb6-4356-ae74-00df00644c74.png
>> (please let me know if I'm misunderstanding).
>>
>> I believe this attack is mitigated as long as we attempt to submit
>> transactions individually (and thus take advantage of CPFP carve out)
>> before attempting package validation. So, in scenario A2, even if the
>> mempool receives a package with A+C, it would deduplicate A, submit C as an
>> individual transaction, and allow it due to the CPFP carve out exemption. A
>> more general goal is: if a transaction would propagate successfully on its
>> own now, it should still propagate regardless of whether it is included in
>> a package. The best way to ensure this, as far as I can tell, is to always
>> try to submit them individually first.
>>
>> I would note that this proposal doesn't accommodate something like
>> diagram B, where C is getting CPFP carve out and wants to bring a +1 (e.g.
>> C has very low fees and is bumped by D). I don't think this is a use case
>> since C should be the one fee-bumping A, but since we're talking about
>> limitations around the CPFP carve out, this is it.
>>
>> Let me know if this addresses your concerns?
>>
>> Thanks,
>> Gloria
>>
>> On Mon, Sep 20, 2021 at 10:19 AM Bastien TEINTURIER <bastien at acinq.fr>
>> wrote:
>>
>>> Hi Gloria,
>>>
>>> Thanks for this detailed post!
>>>
>>> The illustrations you provided are very useful for this kind of graph
>>> topology problems.
>>>
>>> The rules you lay out for package RBF look good to me at first glance
>>> as there are some subtle improvements compared to BIP 125.
>>>
>>> > 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and
>>> > `MAX_PACKAGE_SIZE=101KvB` total size [8]
>>>
>>> I have a question regarding this rule, as your example 2C could be
>>> concerning for LN (unless I didn't understand it correctly).
>>>
>>> This also touches on the package RBF rule 5 ("The package cannot
>>> replace more than 100 mempool transactions.")
>>>
>>> In your example we have a parent transaction A already in the mempool
>>> and an unrelated child B. We submit a package C + D where C spends
>>> another of A's inputs. You're highlighting that this package may be
>>> rejected because of the unrelated transaction(s) B.
>>>
>>> The way I see this, an attacker can abuse this rule to ensure
>>> transaction A stays pinned in the mempool without confirming by
>>> broadcasting a set of child transactions that reach these limits
>>> and pay low fees (where A would be a commit tx in LN).
>>>
>>> We had to create the CPFP carve-out rule explicitly to work around
>>> this limitation, and I think it would be necessary for package RBF
>>> as well, because in such cases we do want to be able to submit a
>>> package A + C where C pays high fees to speed up A's confirmation,
>>> regardless of unrelated unconfirmed children of A...
>>>
>>> We could submit only C to benefit from the existing CPFP carve-out
>>> rule, but that wouldn't work if our local mempool doesn't have A yet,
>>> but other remote mempools do.
>>>
>>> Is my concern justified? Is this something that we should dig into a
>>> bit deeper?
>>>
>>> Thanks,
>>> Bastien
>>>
>>> Le jeu. 16 sept. 2021 ? 09:55, Gloria Zhao via bitcoin-dev <
>>> bitcoin-dev at lists.linuxfoundation.org> a ?crit :
>>>
>>>> Hi there,
>>>>
>>>> I'm writing to propose a set of mempool policy changes to enable package
>>>> validation (in preparation for package relay) in Bitcoin Core. These
>>>> would not
>>>> be consensus or P2P protocol changes. However, since mempool policy
>>>> significantly affects transaction propagation, I believe this is
>>>> relevant for
>>>> the mailing list.
>>>>
>>>> My proposal enables packages consisting of multiple parents and 1
>>>> child. If you
>>>> develop software that relies on specific transaction relay assumptions
>>>> and/or
>>>> are interested in using package relay in the future, I'm very
>>>> interested to hear
>>>> your feedback on the utility or restrictiveness of these package
>>>> policies for
>>>> your use cases.
>>>>
>>>> A draft implementation of this proposal can be found in [Bitcoin Core
>>>> PR#22290][1].
>>>>
>>>> An illustrated version of this post can be found at
>>>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.
>>>> I have also linked the images below.
>>>>
>>>> ## Background
>>>>
>>>> Feel free to skip this section if you are already familiar with mempool
>>>> policy
>>>> and package relay terminology.
>>>>
>>>> ### Terminology Clarifications
>>>>
>>>> * Package = an ordered list of related transactions, representable by a
>>>> Directed
>>>>   Acyclic Graph.
>>>> * Package Feerate = the total modified fees divided by the total
>>>> virtual size of
>>>>   all transactions in the package.
>>>>     - Modified fees = a transaction's base fees + fee delta applied by
>>>> the user
>>>>       with `prioritisetransaction`. As such, we expect this to vary
>>>> across
>>>> mempools.
>>>>     - Virtual Size = the maximum of virtual sizes calculated using
>>>> [BIP141
>>>>       virtual size][2] and sigop weight. [Implemented here in Bitcoin
>>>> Core][3].
>>>>     - Note that feerate is not necessarily based on the base fees and
>>>> serialized
>>>>       size.
>>>>
>>>> * Fee-Bumping = user/wallet actions that take advantage of miner
>>>> incentives to
>>>>   boost a transaction's candidacy for inclusion in a block, including
>>>> Child Pays
>>>> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our intention
>>>> in
>>>> mempool policy is to recognize when the new transaction is more
>>>> economical to
>>>> mine than the original one(s) but not open DoS vectors, so there are
>>>> some
>>>> limitations.
>>>>
>>>> ### Policy
>>>>
>>>> The purpose of the mempool is to store the best (to be most
>>>> incentive-compatible
>>>> with miners, highest feerate) candidates for inclusion in a block.
>>>> Miners use
>>>> the mempool to build block templates. The mempool is also useful as a
>>>> cache for
>>>> boosting block relay and validation performance, aiding transaction
>>>> relay, and
>>>> generating feerate estimations.
>>>>
>>>> Ideally, all consensus-valid transactions paying reasonable fees should
>>>> make it
>>>> to miners through normal transaction relay, without any special
>>>> connectivity or
>>>> relationships with miners. On the other hand, nodes do not have
>>>> unlimited
>>>> resources, and a P2P network designed to let any honest node broadcast
>>>> their
>>>> transactions also exposes the transaction validation engine to DoS
>>>> attacks from
>>>> malicious peers.
>>>>
>>>> As such, for unconfirmed transactions we are considering for our
>>>> mempool, we
>>>> apply a set of validation rules in addition to consensus, primarily to
>>>> protect
>>>> us from resource exhaustion and aid our efforts to keep the highest fee
>>>> transactions. We call this mempool _policy_: a set of (configurable,
>>>> node-specific) rules that transactions must abide by in order to be
>>>> accepted
>>>> into our mempool. Transaction "Standardness" rules and mempool
>>>> restrictions such
>>>> as "too-long-mempool-chain" are both examples of policy.
>>>>
>>>> ### Package Relay and Package Mempool Accept
>>>>
>>>> In transaction relay, we currently consider transactions one at a time
>>>> for
>>>> submission to the mempool. This creates a limitation in the node's
>>>> ability to
>>>> determine which transactions have the highest feerates, since we cannot
>>>> take
>>>> into account descendants (i.e. cannot use CPFP) until all the
>>>> transactions are
>>>> in the mempool. Similarly, we cannot use a transaction's descendants
>>>> when
>>>> considering it for RBF. When an individual transaction does not meet
>>>> the mempool
>>>> minimum feerate and the user isn't able to create a replacement
>>>> transaction
>>>> directly, it will not be accepted by mempools.
>>>>
>>>> This limitation presents a security issue for applications and users
>>>> relying on
>>>> time-sensitive transactions. For example, Lightning and other protocols
>>>> create
>>>> UTXOs with multiple spending paths, where one counterparty's spending
>>>> path opens
>>>> up after a timelock, and users are protected from cheating scenarios as
>>>> long as
>>>> they redeem on-chain in time. A key security assumption is that all
>>>> parties'
>>>> transactions will propagate and confirm in a timely manner. This
>>>> assumption can
>>>> be broken if fee-bumping does not work as intended.
>>>>
>>>> The end goal for Package Relay is to consider multiple transactions at
>>>> the same
>>>> time, e.g. a transaction with its high-fee child. This may help us
>>>> better
>>>> determine whether transactions should be accepted to our mempool,
>>>> especially if
>>>> they don't meet fee requirements individually or are better RBF
>>>> candidates as a
>>>> package. A combination of changes to mempool validation logic, policy,
>>>> and
>>>> transaction relay allows us to better propagate the transactions with
>>>> the
>>>> highest package feerates to miners, and makes fee-bumping tools more
>>>> powerful
>>>> for users.
>>>>
>>>> The "relay" part of Package Relay suggests P2P messaging changes, but a
>>>> large
>>>> part of the changes are in the mempool's package validation logic. We
>>>> call this
>>>> *Package Mempool Accept*.
>>>>
>>>> ### Previous Work
>>>>
>>>> * Given that mempool validation is DoS-sensitive and complex, it would
>>>> be
>>>>   dangerous to haphazardly tack on package validation logic. Many
>>>> efforts have
>>>> been made to make mempool validation less opaque (see [#16400][4],
>>>> [#21062][5],
>>>> [#22675][6], [#22796][7]).
>>>> * [#20833][8] Added basic capabilities for package validation, test
>>>> accepts only
>>>>   (no submission to mempool).
>>>> * [#21800][9] Implemented package ancestor/descendant limit checks for
>>>> arbitrary
>>>>   packages. Still test accepts only.
>>>> * Previous package relay proposals (see [#16401][10], [#19621][11]).
>>>>
>>>> ### Existing Package Rules
>>>>
>>>> These are in master as introduced in [#20833][8] and [#21800][9]. I'll
>>>> consider
>>>> them as "given" in the rest of this document, though they can be
>>>> changed, since
>>>> package validation is test-accept only right now.
>>>>
>>>> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and
>>>> `MAX_PACKAGE_SIZE=101KvB` total size [8]
>>>>
>>>>    *Rationale*: This is already enforced as mempool ancestor/descendant
>>>> limits.
>>>> Presumably, transactions in a package are all related, so exceeding
>>>> this limit
>>>> would mean that the package can either be split up or it wouldn't pass
>>>> this
>>>> mempool policy.
>>>>
>>>> 2. Packages must be topologically sorted: if any dependencies exist
>>>> between
>>>> transactions, parents must appear somewhere before children. [8]
>>>>
>>>> 3. A package cannot have conflicting transactions, i.e. none of them
>>>> can spend
>>>> the same inputs. This also means there cannot be duplicate
>>>> transactions. [8]
>>>>
>>>> 4. When packages are evaluated against ancestor/descendant limits in a
>>>> test
>>>> accept, the union of all of their descendants and ancestors is
>>>> considered. This
>>>> is essentially a "worst case" heuristic where every transaction in the
>>>> package
>>>> is treated as each other's ancestor and descendant. [8]
>>>> Packages for which ancestor/descendant limits are accurately captured
>>>> by this
>>>> heuristic: [19]
>>>>
>>>> There are also limitations such as the fact that CPFP carve out is not
>>>> applied
>>>> to package transactions. #20833 also disables RBF in package
>>>> validation; this
>>>> proposal overrides that to allow packages to use RBF.
>>>>
>>>> ## Proposed Changes
>>>>
>>>> The next step in the Package Mempool Accept project is to implement
>>>> submission
>>>> to mempool, initially through RPC only. This allows us to test the
>>>> submission
>>>> logic before exposing it on P2P.
>>>>
>>>> ### Summary
>>>>
>>>> - Packages may contain already-in-mempool transactions.
>>>> - Packages are 2 generations, Multi-Parent-1-Child.
>>>> - Fee-related checks use the package feerate. This means that wallets
>>>> can
>>>> create a package that utilizes CPFP.
>>>> - Parents are allowed to RBF mempool transactions with a set of rules
>>>> similar
>>>>   to BIP125. This enables a combination of CPFP and RBF, where a
>>>> transaction's descendant fees pay for replacing mempool conflicts.
>>>>
>>>> There is a draft implementation in [#22290][1]. It is WIP, but feedback
>>>> is
>>>> always welcome.
>>>>
>>>> ### Details
>>>>
>>>> #### Packages May Contain Already-in-Mempool Transactions
>>>>
>>>> A package may contain transactions that are already in the mempool. We
>>>> remove
>>>> ("deduplicate") those transactions from the package for the purposes of
>>>> package
>>>> mempool acceptance. If a package is empty after deduplication, we do
>>>> nothing.
>>>>
>>>> *Rationale*: Mempools vary across the network. It's possible for a
>>>> parent to be
>>>> accepted to the mempool of a peer on its own due to differences in
>>>> policy and
>>>> fee market fluctuations. We should not reject or penalize the entire
>>>> package for
>>>> an individual transaction as that could be a censorship vector.
>>>>
>>>> #### Packages Are Multi-Parent-1-Child
>>>>
>>>> Only packages of a specific topology are permitted. Namely, a package
>>>> is exactly
>>>> 1 child with all of its unconfirmed parents. After deduplication, the
>>>> package
>>>> may be exactly the same, empty, 1 child, 1 child with just some of its
>>>> unconfirmed parents, etc. Note that it's possible for the parents to be
>>>> indirect
>>>> descendants/ancestors of one another, or for parent and child to share
>>>> a parent,
>>>> so we cannot make any other topology assumptions.
>>>>
>>>> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple
>>>> parents
>>>> makes it possible to fee-bump a batch of transactions. Restricting
>>>> packages to a
>>>> defined topology is also easier to reason about and simplifies the
>>>> validation
>>>> logic greatly. Multi-parent-1-child allows us to think of the package
>>>> as one big
>>>> transaction, where:
>>>>
>>>> - Inputs = all the inputs of parents + inputs of the child that come
>>>> from
>>>>   confirmed UTXOs
>>>> - Outputs = all the outputs of the child + all outputs of the parents
>>>> that
>>>>   aren't spent by other transactions in the package
>>>>
>>>> Examples of packages that follow this rule (variations of example A
>>>> show some
>>>> possibilities after deduplication): ![image][15]
>>>>
>>>> #### Fee-Related Checks Use Package Feerate
>>>>
>>>> Package Feerate = the total modified fees divided by the total virtual
>>>> size of
>>>> all transactions in the package.
>>>>
>>>> To meet the two feerate requirements of a mempool, i.e., the
>>>> pre-configured
>>>> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum
>>>> feerate, the
>>>> total package feerate is used instead of the individual feerate. The
>>>> individual
>>>> transactions are allowed to be below feerate requirements if the
>>>> package meets
>>>> the feerate requirements. For example, the parent(s) in the package can
>>>> have 0
>>>> fees but be paid for by the child.
>>>>
>>>> *Rationale*: This can be thought of as "CPFP within a package," solving
>>>> the
>>>> issue of a parent not meeting minimum fees on its own. This allows L2
>>>> applications to adjust their fees at broadcast time instead of
>>>> overshooting or
>>>> risking getting stuck/pinned.
>>>>
>>>> We use the package feerate of the package *after deduplication*.
>>>>
>>>> *Rationale*:  It would be incorrect to use the fees of transactions
>>>> that are
>>>> already in the mempool, as we do not want a transaction's fees to be
>>>> double-counted for both its individual RBF and package RBF.
>>>>
>>>> Examples F and G [14] show the same package, but P1 is submitted
>>>> individually before
>>>> the package in example G. In example F, we can see that the 300vB
>>>> package pays
>>>> an additional 200sat in fees, which is not enough to pay for its own
>>>> bandwidth
>>>> (BIP125#4). In example G, we can see that P1 pays enough to replace M1,
>>>> but
>>>> using P1's fees again during package submission would make it look like
>>>> a 300sat
>>>> increase for a 200vB package. Even including its fees and size would
>>>> not be
>>>> sufficient in this example, since the 300sat looks like enough for the
>>>> 300vB
>>>> package. The calculcation after deduplication is 100sat increase for a
>>>> package
>>>> of size 200vB, which correctly fails BIP125#4. Assume all transactions
>>>> have a
>>>> size of 100vB.
>>>>
>>>> #### Package RBF
>>>>
>>>> If a package meets feerate requirements as a package, the parents in the
>>>> transaction are allowed to replace-by-fee mempool transactions. The
>>>> child cannot
>>>> replace mempool transactions. Multiple transactions can replace the same
>>>> transaction, but in order to be valid, none of the transactions can try
>>>> to
>>>> replace an ancestor of another transaction in the same package (which
>>>> would thus
>>>> make its inputs unavailable).
>>>>
>>>> *Rationale*: Even if we are using package feerate, a package will not
>>>> propagate
>>>> as intended if RBF still requires each individual transaction to meet
>>>> the
>>>> feerate requirements.
>>>>
>>>> We use a set of rules slightly modified from BIP125 as follows:
>>>>
>>>> ##### Signaling (Rule #1)
>>>>
>>>> All mempool transactions to be replaced must signal replaceability.
>>>>
>>>> *Rationale*: Package RBF signaling logic should be the same for package
>>>> RBF and
>>>> single transaction acceptance. This would be updated if single
>>>> transaction
>>>> validation moves to full RBF.
>>>>
>>>> ##### New Unconfirmed Inputs (Rule #2)
>>>>
>>>> A package may include new unconfirmed inputs, but the ancestor feerate
>>>> of the
>>>> child must be at least as high as the ancestor feerates of every
>>>> transaction
>>>> being replaced. This is contrary to BIP125#2, which states "The
>>>> replacement
>>>> transaction may only include an unconfirmed input if that input was
>>>> included in
>>>> one of the original transactions. (An unconfirmed input spends an
>>>> output from a
>>>> currently-unconfirmed transaction.)"
>>>>
>>>> *Rationale*: The purpose of BIP125#2 is to ensure that the replacement
>>>> transaction has a higher ancestor score than the original
>>>> transaction(s) (see
>>>> [comment][13]). Example H [16] shows how adding a new unconfirmed input
>>>> can lower the
>>>> ancestor score of the replacement transaction. P1 is trying to replace
>>>> M1, and
>>>> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat, and
>>>> M2 pays
>>>> 100sat. Assume all transactions have a size of 100vB. While, in
>>>> isolation, P1
>>>> looks like a better mining candidate than M1, it must be mined with M2,
>>>> so its
>>>> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's
>>>> ancestor
>>>> feerate, which is 6sat/vB.
>>>>
>>>> In package RBF, the rule analogous to BIP125#2 would be "none of the
>>>> transactions in the package can spend new unconfirmed inputs." Example
>>>> J [17] shows
>>>> why, if any of the package transactions have ancestors, package feerate
>>>> is no
>>>> longer accurate. Even though M2 and M3 are not ancestors of P1 (which
>>>> is the
>>>> replacement transaction in an RBF), we're actually interested in the
>>>> entire
>>>> package. A miner should mine M1 which is 5sat/vB instead of M2, M3, P1,
>>>> P2, and
>>>> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened to
>>>> only allow
>>>> the child to have new unconfirmed inputs, either, because it can still
>>>> cause us
>>>> to overestimate the package's ancestor score.
>>>>
>>>> However, enforcing a rule analogous to BIP125#2 would not only make
>>>> Package RBF
>>>> less useful, but would also break Package RBF for packages with parents
>>>> already
>>>> in the mempool: if a package parent has already been submitted, it
>>>> would look
>>>> like the child is spending a "new" unconfirmed input. In example K
>>>> [18], we're
>>>> looking to replace M1 with the entire package including P1, P2, and P3.
>>>> We must
>>>> consider the case where one of the parents is already in the mempool
>>>> (in this
>>>> case, P2), which means we must allow P3 to have new unconfirmed inputs.
>>>> However,
>>>> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not
>>>> replace M1
>>>> with this package.
>>>>
>>>> Thus, the package RBF rule regarding new unconfirmed inputs is less
>>>> strict than
>>>> BIP125#2. However, we still achieve the same goal of requiring the
>>>> replacement
>>>> transactions to have a ancestor score at least as high as the original
>>>> ones. As
>>>> a result, the entire package is required to be a higher feerate mining
>>>> candidate
>>>> than each of the replaced transactions.
>>>>
>>>> Another note: the [comment][13] above the BIP125#2 code in the original
>>>> RBF
>>>> implementation suggests that the rule was intended to be temporary.
>>>>
>>>> ##### Absolute Fee (Rule #3)
>>>>
>>>> The package must increase the absolute fee of the mempool, i.e. the
>>>> total fees
>>>> of the package must be higher than the absolute fees of the mempool
>>>> transactions
>>>> it replaces. Combined with the CPFP rule above, this differs from
>>>> BIP125 Rule #3
>>>> - an individual transaction in the package may have lower fees than the
>>>>   transaction(s) it is replacing. In fact, it may have 0 fees, and the
>>>> child
>>>> pays for RBF.
>>>>
>>>> ##### Feerate (Rule #4)
>>>>
>>>> The package must pay for its own bandwidth; the package feerate must be
>>>> higher
>>>> than the replaced transactions by at least minimum relay feerate
>>>> (`incrementalRelayFee`). Combined with the CPFP rule above, this
>>>> differs from
>>>> BIP125 Rule #4 - an individual transaction in the package can have a
>>>> lower
>>>> feerate than the transaction(s) it is replacing. In fact, it may have 0
>>>> fees,
>>>> and the child pays for RBF.
>>>>
>>>> ##### Total Number of Replaced Transactions (Rule #5)
>>>>
>>>> The package cannot replace more than 100 mempool transactions. This is
>>>> identical
>>>> to BIP125 Rule #5.
>>>>
>>>> ### Expected FAQs
>>>>
>>>> 1. Is it possible for only some of the package to make it into the
>>>> mempool?
>>>>
>>>>    Yes, it is. However, since we evict transactions from the mempool by
>>>> descendant score and the package child is supposed to be sponsoring the
>>>> fees of
>>>> its parents, the most common scenario would be all-or-nothing. This is
>>>> incentive-compatible. In fact, to be conservative, package validation
>>>> should
>>>> begin by trying to submit all of the transactions individually, and
>>>> only use the
>>>> package mempool acceptance logic if the parents fail due to low feerate.
>>>>
>>>> 2. Should we allow packages to contain already-confirmed transactions?
>>>>
>>>>     No, for practical reasons. In mempool validation, we actually
>>>> aren't able to
>>>> tell with 100% confidence if we are looking at a transaction that has
>>>> already
>>>> confirmed, because we look up inputs using a UTXO set. If we have
>>>> historical
>>>> block data, it's possible to look for it, but this is inefficient, not
>>>> always
>>>> possible for pruning nodes, and unnecessary because we're not going to
>>>> do
>>>> anything with the transaction anyway. As such, we already have the
>>>> expectation
>>>> that transaction relay is somewhat "stateful" i.e. nobody should be
>>>> relaying
>>>> transactions that have already been confirmed. Similarly, we shouldn't
>>>> be
>>>> relaying packages that contain already-confirmed transactions.
>>>>
>>>> [1]: https://github.com/bitcoin/bitcoin/pull/22290
>>>> [2]:
>>>> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations
>>>> [3]:
>>>> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282
>>>> [4]: https://github.com/bitcoin/bitcoin/pull/16400
>>>> [5]: https://github.com/bitcoin/bitcoin/pull/21062
>>>> [6]: https://github.com/bitcoin/bitcoin/pull/22675
>>>> [7]: https://github.com/bitcoin/bitcoin/pull/22796
>>>> [8]: https://github.com/bitcoin/bitcoin/pull/20833
>>>> [9]: https://github.com/bitcoin/bitcoin/pull/21800
>>>> [10]: https://github.com/bitcoin/bitcoin/pull/16401
>>>> [11]: https://github.com/bitcoin/bitcoin/pull/19621
>>>> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki
>>>> [13]:
>>>> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104
>>>> [14]:
>>>> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png
>>>> [15]:
>>>> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png
>>>> [16]:
>>>> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png
>>>> [17]:
>>>> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png
>>>> [18]:
>>>> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png
>>>> [19]:
>>>> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png
>>>> [20]:
>>>> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png
>>>> _______________________________________________
>>>> bitcoin-dev mailing list
>>>> bitcoin-dev at lists.linuxfoundation.org
>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>>
>>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210921/2519ec71/attachment-0001.html>

From antoine.riard at gmail.com  Wed Sep 22 01:40:16 2021
From: antoine.riard at gmail.com (Antoine Riard)
Date: Tue, 21 Sep 2021 21:40:16 -0400
Subject: [bitcoin-dev] TAPLEAF_UPDATE_VERIFY covenant opcode
In-Reply-To: <20210920145246.GB21263@erisian.com.au>
References: <20210909064138.GA22496@erisian.com.au>
 <CALZpt+FnnbGJC4=KO_OPiKxt0Ey9Bzh1gxP1dQSDz2aBi9WyOA@mail.gmail.com>
 <20210911032644.GB23578@erisian.com.au>
 <CALZpt+HzM__OJntegOhDqkg5zU=PQXtKgQoB518A2qP9=foovw@mail.gmail.com>
 <20210915065051.GA26119@erisian.com.au>
 <CALZpt+Hczvy1Fxu40cCKKC8bR9fouQ+sAiqV65-Z4VuLp+Bi7w@mail.gmail.com>
 <20210920145246.GB21263@erisian.com.au>
Message-ID: <CALZpt+HTnrpcXE-kd3_WMZyrYH6-g3Y2H_gOfc5YAn=qFHxo+w@mail.gmail.com>

> Hmm, I'm reading C5 as "If an oracle says X, and Alice and Carol agree,
> they can distribute all the remaining funds as they see fit".

Should be read as an OR:

        IF 2 <oracle_sig> <alice_sig> 2 CHECKMULTISIG
        ELSE 2 <oracle_sig> <bob_sig> 2 CHECKMULTISIG
        ENDIF
        <> 2 IN_OUT_AMOUNT

The empty vector is a wildcard on the spent amount, as this tapscript may
be executed before/
after the split or any withdraw option.

> (Relative timelocks would probably be annoying for everyone who wasn't
> the first to exit the pool)

And I think unsafe, if you're wrapping a time-sensitive output in your
withdraw scriptPubkey.

> I think the above fixes that -- when AB is spent it deletes itself and
> the (A,B) pair; when A is spent, it deletes (A, B and AB) and replaces
> them with B'; when B' is spent it just deletes itself.

Right, here the subtlety in reading the scripts is about the B'
substitution tapscript in the
A one. And it sounds correct to me that AB exercise deletes the withdraw
pair (A, B).

Le lun. 20 sept. 2021 ? 10:52, Anthony Towns <aj at erisian.com.au> a ?crit :

> On Sat, Sep 18, 2021 at 10:11:10AM -0400, Antoine Riard wrote:
> > I think one design advantage of combining scope-minimal opcodes like
> MERKLESUB
> > with sighash malleability is the ability to update a subset of the
> off-chain
> > contract transactions fields after the funding phase.
>
> Note that it's not "update" so much as "add to"; and I mostly think
> graftroot (and friends), or just updating the utxo onchain, are a better
> general purpose way of doing that. It's definitely a tradeoff though.
>
> > Yes this is a different contract policy that I would like to set up.
> > Let's say you would like to express the following set of capabilities.
> > C0="Split the 4 BTC funds between Alice/Bob and Caroll/Dave"
> > C1="Alice can withdraw 1 BTC after 2 weeks"
> > C2="Bob can withdraw 1 BTC after 2 weeks"
> > C3="Caroll can withdraw 1 BTC after 2 weeks"
> > C4="Dave can withdraw 1 BTC after 2 weeks"
> > C5="If USDT price=X, Alice can withdraw 2 BTC or Caroll can withdraw 2
> BTC"
>
> Hmm, I'm reading C5 as "If an oracle says X, and Alice and Carol agree,
> they can distribute all the remaining funds as they see fit".
>
> > If C4 is exercised, to avoid trust in the remaining counterparty, both
> Alice or
> > Caroll should be able to conserve the C5 option, without relying on the
> updated
> > key path.
>
> > As you're saying, as we know the group in advance, one way to setup the
> tree
> > could be:
> >        (A, (((((B, C), BC), D), BCD), ((((E, F), EF), G), EFG)))
>
> Make it:
>
>   (((AB, (A,B)), (CD, (C,D))), ACO)
>
> AB = DROP <alice+bob> DUP 0 6 TLUV CHECKSIGVERIFY IN_OUT_AMOUNT SUB 2BTC
> LESSTHAN
> CD = same but for carol+dave
> A = <alice> DUP <B'> 10 TLUV CHECKSIGVERIFY IN_OUT_AMOUNT SUB 1BTC LESSTHAN
> B' = <bob> DUP 0 2 TLUV CHECKSIGVERIFY IN_OUT_AMOUNT SUB 1BTC LESSTHAN
> B,C,D = same as A but for bob, etc
> A',C',D' = same as B' but for alice, etc
> ACO = <alice+carol> CHECKSIGVERIFY <oracle> CHECKSIG
>
> Probably AB, CD, A..D, A'..D' all want a CLTV delay in there as well.
> (Relative timelocks would probably be annoying for everyone who wasn't
> the first to exit the pool)
>
> > Note, this solution isn't really satisfying as the G path isn't
> neutralized on
> > the Caroll/Dave fork and could be replayed by Alice or Bob...
>
> I think the above fixes that -- when AB is spent it deletes itself and
> the (A,B) pair; when A is spent, it deletes (A, B and AB) and replaces
> them with B'; when B' is spent it just deletes itself.
>
> Cheers,
> aj
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210921/a563fe77/attachment-0001.html>

From bastien at acinq.fr  Wed Sep 22 07:10:38 2021
From: bastien at acinq.fr (Bastien TEINTURIER)
Date: Wed, 22 Sep 2021 09:10:38 +0200
Subject: [bitcoin-dev] Proposal: Package Mempool Accept and Package RBF
In-Reply-To: <CAFXO6=LrHWsddH7VP_AgK93Thw8mdeqmCjjkdXAZy1mymyBefA@mail.gmail.com>
References: <CAFXO6=+cHyQKM2n9yn4EhwLZO+AUB0ZD81qWPxmpN27rjUoU3w@mail.gmail.com>
 <CACdvm3NdqYFvJ9t4ocXjdLT09fPu40YYwdvpvOnyYCmk5QXyrQ@mail.gmail.com>
 <CAFXO6=+auN_X=C52WB8fBqREUYahnFr1dzYotys7k7x+PjO1DQ@mail.gmail.com>
 <CACdvm3Oo6ABbK79FvuAnZyJ+v4EU3nY=Zi1yzWmv45j5LjWSOg@mail.gmail.com>
 <CAFXO6=LrHWsddH7VP_AgK93Thw8mdeqmCjjkdXAZy1mymyBefA@mail.gmail.com>
Message-ID: <CACdvm3Ou_o31m1bbn6wkpxvfCiOUDYLF-4s5MVK9Z3i6=w=LzA@mail.gmail.com>

Great, thanks for this clarification!

Can you confirm that this won't be an issue either with your
example 2C (in your first set of diagrams)? If I understand it
correctly it shouldn't, but I'd rather be 100% sure.

A package A + C will be able to replace A' + B regardless of
the weight of A' + B?

Thanks,
Bastien

Le mar. 21 sept. 2021 ? 18:42, Gloria Zhao <gloriajzhao at gmail.com> a ?crit :

> Hi Bastien,
>
> Excellent diagram :D
>
> > Here the issue is that a revoked commitment tx A' is pinned in other
> > mempools, with a long chain of descendants (or descendants that reach
> > the maximum replaceable size).
> > We would really like A + C to be able to replace this pinned A'.
> > We can't submit individually because A on its own won't replace A'...
>
> Right, this is a key motivation for having Package RBF. In this case, A+C
> can replace A' + B1...B24.
>
> Due to the descendant limit (each node operator can increase it on their
> own node, but the default is 25), A' should have no more than 25
> descendants, even including CPFP carve out. As long as A only conflicts
> with A', it won't be trying to replace more than 100 transactions. The
> proposed package RBF will allow C to pay for A's conflicts, since their
> package feerate is used in the fee comparisons. A is not a descendant of
> A', so the existence of B1...B24 does not prevent the replacement.
>
> Best,
> Gloria
>
> On Tue, Sep 21, 2021 at 4:18 PM Bastien TEINTURIER <bastien at acinq.fr>
> wrote:
>
>> Hi Gloria,
>>
>> > I believe this attack is mitigated as long as we attempt to submit
>> transactions individually
>>
>> Unfortunately not, as there exists a pinning scenario in LN where a
>> different commit tx is pinned, but you actually can't know which one.
>>
>> Since I really like your diagrams, I made one as well to illustrate:
>>
>> https://user-images.githubusercontent.com/31281497/134198114-5e9c6857-e8fc-405a-be57-18181d5e54cb.jpg
>>
>> Here the issue is that a revoked commitment tx A' is pinned in other
>> mempools, with a long chain of descendants (or descendants that reach
>> the maximum replaceable size).
>>
>> We would really like A + C to be able to replace this pinned A'.
>> We can't submit individually because A on its own won't replace A'...
>>
>> > I would note that this proposal doesn't accommodate something like
>> diagram B, where C is getting CPFP carve out and wants to bring a +1
>>
>> No worries, that case shouldn't be a concern.
>> I believe any L2 protocol can always ensure it confirms such tx trees
>> "one depth after the other" without impacting funds safety, so it
>> only needs to ensure A + C can get into mempools.
>>
>> Thanks,
>> Bastien
>>
>> Le mar. 21 sept. 2021 ? 13:18, Gloria Zhao <gloriajzhao at gmail.com> a
>> ?crit :
>>
>>> Hi Bastien,
>>>
>>> Thank you for your feedback!
>>>
>>> > In your example we have a parent transaction A already in the mempool
>>> > and an unrelated child B. We submit a package C + D where C spends
>>> > another of A's inputs. You're highlighting that this package may be
>>> > rejected because of the unrelated transaction(s) B.
>>>
>>> > The way I see this, an attacker can abuse this rule to ensure
>>> > transaction A stays pinned in the mempool without confirming by
>>> > broadcasting a set of child transactions that reach these limits
>>> > and pay low fees (where A would be a commit tx in LN).
>>>
>>> I believe you are describing a pinning attack in which your adversarial
>>> counterparty attempts to monopolize the mempool descendant limit of the
>>> shared  transaction A in order to prevent you from submitting a fee-bumping
>>> child C; I've tried to illustrate this as diagram A here:
>>> https://user-images.githubusercontent.com/25183001/134159860-068080d0-bbb6-4356-ae74-00df00644c74.png
>>> (please let me know if I'm misunderstanding).
>>>
>>> I believe this attack is mitigated as long as we attempt to submit
>>> transactions individually (and thus take advantage of CPFP carve out)
>>> before attempting package validation. So, in scenario A2, even if the
>>> mempool receives a package with A+C, it would deduplicate A, submit C as an
>>> individual transaction, and allow it due to the CPFP carve out exemption. A
>>> more general goal is: if a transaction would propagate successfully on its
>>> own now, it should still propagate regardless of whether it is included in
>>> a package. The best way to ensure this, as far as I can tell, is to always
>>> try to submit them individually first.
>>>
>>> I would note that this proposal doesn't accommodate something like
>>> diagram B, where C is getting CPFP carve out and wants to bring a +1 (e.g.
>>> C has very low fees and is bumped by D). I don't think this is a use case
>>> since C should be the one fee-bumping A, but since we're talking about
>>> limitations around the CPFP carve out, this is it.
>>>
>>> Let me know if this addresses your concerns?
>>>
>>> Thanks,
>>> Gloria
>>>
>>> On Mon, Sep 20, 2021 at 10:19 AM Bastien TEINTURIER <bastien at acinq.fr>
>>> wrote:
>>>
>>>> Hi Gloria,
>>>>
>>>> Thanks for this detailed post!
>>>>
>>>> The illustrations you provided are very useful for this kind of graph
>>>> topology problems.
>>>>
>>>> The rules you lay out for package RBF look good to me at first glance
>>>> as there are some subtle improvements compared to BIP 125.
>>>>
>>>> > 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and
>>>> > `MAX_PACKAGE_SIZE=101KvB` total size [8]
>>>>
>>>> I have a question regarding this rule, as your example 2C could be
>>>> concerning for LN (unless I didn't understand it correctly).
>>>>
>>>> This also touches on the package RBF rule 5 ("The package cannot
>>>> replace more than 100 mempool transactions.")
>>>>
>>>> In your example we have a parent transaction A already in the mempool
>>>> and an unrelated child B. We submit a package C + D where C spends
>>>> another of A's inputs. You're highlighting that this package may be
>>>> rejected because of the unrelated transaction(s) B.
>>>>
>>>> The way I see this, an attacker can abuse this rule to ensure
>>>> transaction A stays pinned in the mempool without confirming by
>>>> broadcasting a set of child transactions that reach these limits
>>>> and pay low fees (where A would be a commit tx in LN).
>>>>
>>>> We had to create the CPFP carve-out rule explicitly to work around
>>>> this limitation, and I think it would be necessary for package RBF
>>>> as well, because in such cases we do want to be able to submit a
>>>> package A + C where C pays high fees to speed up A's confirmation,
>>>> regardless of unrelated unconfirmed children of A...
>>>>
>>>> We could submit only C to benefit from the existing CPFP carve-out
>>>> rule, but that wouldn't work if our local mempool doesn't have A yet,
>>>> but other remote mempools do.
>>>>
>>>> Is my concern justified? Is this something that we should dig into a
>>>> bit deeper?
>>>>
>>>> Thanks,
>>>> Bastien
>>>>
>>>> Le jeu. 16 sept. 2021 ? 09:55, Gloria Zhao via bitcoin-dev <
>>>> bitcoin-dev at lists.linuxfoundation.org> a ?crit :
>>>>
>>>>> Hi there,
>>>>>
>>>>> I'm writing to propose a set of mempool policy changes to enable
>>>>> package
>>>>> validation (in preparation for package relay) in Bitcoin Core. These
>>>>> would not
>>>>> be consensus or P2P protocol changes. However, since mempool policy
>>>>> significantly affects transaction propagation, I believe this is
>>>>> relevant for
>>>>> the mailing list.
>>>>>
>>>>> My proposal enables packages consisting of multiple parents and 1
>>>>> child. If you
>>>>> develop software that relies on specific transaction relay assumptions
>>>>> and/or
>>>>> are interested in using package relay in the future, I'm very
>>>>> interested to hear
>>>>> your feedback on the utility or restrictiveness of these package
>>>>> policies for
>>>>> your use cases.
>>>>>
>>>>> A draft implementation of this proposal can be found in [Bitcoin Core
>>>>> PR#22290][1].
>>>>>
>>>>> An illustrated version of this post can be found at
>>>>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.
>>>>> I have also linked the images below.
>>>>>
>>>>> ## Background
>>>>>
>>>>> Feel free to skip this section if you are already familiar with
>>>>> mempool policy
>>>>> and package relay terminology.
>>>>>
>>>>> ### Terminology Clarifications
>>>>>
>>>>> * Package = an ordered list of related transactions, representable by
>>>>> a Directed
>>>>>   Acyclic Graph.
>>>>> * Package Feerate = the total modified fees divided by the total
>>>>> virtual size of
>>>>>   all transactions in the package.
>>>>>     - Modified fees = a transaction's base fees + fee delta applied by
>>>>> the user
>>>>>       with `prioritisetransaction`. As such, we expect this to vary
>>>>> across
>>>>> mempools.
>>>>>     - Virtual Size = the maximum of virtual sizes calculated using
>>>>> [BIP141
>>>>>       virtual size][2] and sigop weight. [Implemented here in Bitcoin
>>>>> Core][3].
>>>>>     - Note that feerate is not necessarily based on the base fees and
>>>>> serialized
>>>>>       size.
>>>>>
>>>>> * Fee-Bumping = user/wallet actions that take advantage of miner
>>>>> incentives to
>>>>>   boost a transaction's candidacy for inclusion in a block, including
>>>>> Child Pays
>>>>> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our intention
>>>>> in
>>>>> mempool policy is to recognize when the new transaction is more
>>>>> economical to
>>>>> mine than the original one(s) but not open DoS vectors, so there are
>>>>> some
>>>>> limitations.
>>>>>
>>>>> ### Policy
>>>>>
>>>>> The purpose of the mempool is to store the best (to be most
>>>>> incentive-compatible
>>>>> with miners, highest feerate) candidates for inclusion in a block.
>>>>> Miners use
>>>>> the mempool to build block templates. The mempool is also useful as a
>>>>> cache for
>>>>> boosting block relay and validation performance, aiding transaction
>>>>> relay, and
>>>>> generating feerate estimations.
>>>>>
>>>>> Ideally, all consensus-valid transactions paying reasonable fees
>>>>> should make it
>>>>> to miners through normal transaction relay, without any special
>>>>> connectivity or
>>>>> relationships with miners. On the other hand, nodes do not have
>>>>> unlimited
>>>>> resources, and a P2P network designed to let any honest node broadcast
>>>>> their
>>>>> transactions also exposes the transaction validation engine to DoS
>>>>> attacks from
>>>>> malicious peers.
>>>>>
>>>>> As such, for unconfirmed transactions we are considering for our
>>>>> mempool, we
>>>>> apply a set of validation rules in addition to consensus, primarily to
>>>>> protect
>>>>> us from resource exhaustion and aid our efforts to keep the highest fee
>>>>> transactions. We call this mempool _policy_: a set of (configurable,
>>>>> node-specific) rules that transactions must abide by in order to be
>>>>> accepted
>>>>> into our mempool. Transaction "Standardness" rules and mempool
>>>>> restrictions such
>>>>> as "too-long-mempool-chain" are both examples of policy.
>>>>>
>>>>> ### Package Relay and Package Mempool Accept
>>>>>
>>>>> In transaction relay, we currently consider transactions one at a time
>>>>> for
>>>>> submission to the mempool. This creates a limitation in the node's
>>>>> ability to
>>>>> determine which transactions have the highest feerates, since we
>>>>> cannot take
>>>>> into account descendants (i.e. cannot use CPFP) until all the
>>>>> transactions are
>>>>> in the mempool. Similarly, we cannot use a transaction's descendants
>>>>> when
>>>>> considering it for RBF. When an individual transaction does not meet
>>>>> the mempool
>>>>> minimum feerate and the user isn't able to create a replacement
>>>>> transaction
>>>>> directly, it will not be accepted by mempools.
>>>>>
>>>>> This limitation presents a security issue for applications and users
>>>>> relying on
>>>>> time-sensitive transactions. For example, Lightning and other
>>>>> protocols create
>>>>> UTXOs with multiple spending paths, where one counterparty's spending
>>>>> path opens
>>>>> up after a timelock, and users are protected from cheating scenarios
>>>>> as long as
>>>>> they redeem on-chain in time. A key security assumption is that all
>>>>> parties'
>>>>> transactions will propagate and confirm in a timely manner. This
>>>>> assumption can
>>>>> be broken if fee-bumping does not work as intended.
>>>>>
>>>>> The end goal for Package Relay is to consider multiple transactions at
>>>>> the same
>>>>> time, e.g. a transaction with its high-fee child. This may help us
>>>>> better
>>>>> determine whether transactions should be accepted to our mempool,
>>>>> especially if
>>>>> they don't meet fee requirements individually or are better RBF
>>>>> candidates as a
>>>>> package. A combination of changes to mempool validation logic, policy,
>>>>> and
>>>>> transaction relay allows us to better propagate the transactions with
>>>>> the
>>>>> highest package feerates to miners, and makes fee-bumping tools more
>>>>> powerful
>>>>> for users.
>>>>>
>>>>> The "relay" part of Package Relay suggests P2P messaging changes, but
>>>>> a large
>>>>> part of the changes are in the mempool's package validation logic. We
>>>>> call this
>>>>> *Package Mempool Accept*.
>>>>>
>>>>> ### Previous Work
>>>>>
>>>>> * Given that mempool validation is DoS-sensitive and complex, it would
>>>>> be
>>>>>   dangerous to haphazardly tack on package validation logic. Many
>>>>> efforts have
>>>>> been made to make mempool validation less opaque (see [#16400][4],
>>>>> [#21062][5],
>>>>> [#22675][6], [#22796][7]).
>>>>> * [#20833][8] Added basic capabilities for package validation, test
>>>>> accepts only
>>>>>   (no submission to mempool).
>>>>> * [#21800][9] Implemented package ancestor/descendant limit checks for
>>>>> arbitrary
>>>>>   packages. Still test accepts only.
>>>>> * Previous package relay proposals (see [#16401][10], [#19621][11]).
>>>>>
>>>>> ### Existing Package Rules
>>>>>
>>>>> These are in master as introduced in [#20833][8] and [#21800][9]. I'll
>>>>> consider
>>>>> them as "given" in the rest of this document, though they can be
>>>>> changed, since
>>>>> package validation is test-accept only right now.
>>>>>
>>>>> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and
>>>>> `MAX_PACKAGE_SIZE=101KvB` total size [8]
>>>>>
>>>>>    *Rationale*: This is already enforced as mempool
>>>>> ancestor/descendant limits.
>>>>> Presumably, transactions in a package are all related, so exceeding
>>>>> this limit
>>>>> would mean that the package can either be split up or it wouldn't pass
>>>>> this
>>>>> mempool policy.
>>>>>
>>>>> 2. Packages must be topologically sorted: if any dependencies exist
>>>>> between
>>>>> transactions, parents must appear somewhere before children. [8]
>>>>>
>>>>> 3. A package cannot have conflicting transactions, i.e. none of them
>>>>> can spend
>>>>> the same inputs. This also means there cannot be duplicate
>>>>> transactions. [8]
>>>>>
>>>>> 4. When packages are evaluated against ancestor/descendant limits in a
>>>>> test
>>>>> accept, the union of all of their descendants and ancestors is
>>>>> considered. This
>>>>> is essentially a "worst case" heuristic where every transaction in the
>>>>> package
>>>>> is treated as each other's ancestor and descendant. [8]
>>>>> Packages for which ancestor/descendant limits are accurately captured
>>>>> by this
>>>>> heuristic: [19]
>>>>>
>>>>> There are also limitations such as the fact that CPFP carve out is not
>>>>> applied
>>>>> to package transactions. #20833 also disables RBF in package
>>>>> validation; this
>>>>> proposal overrides that to allow packages to use RBF.
>>>>>
>>>>> ## Proposed Changes
>>>>>
>>>>> The next step in the Package Mempool Accept project is to implement
>>>>> submission
>>>>> to mempool, initially through RPC only. This allows us to test the
>>>>> submission
>>>>> logic before exposing it on P2P.
>>>>>
>>>>> ### Summary
>>>>>
>>>>> - Packages may contain already-in-mempool transactions.
>>>>> - Packages are 2 generations, Multi-Parent-1-Child.
>>>>> - Fee-related checks use the package feerate. This means that wallets
>>>>> can
>>>>> create a package that utilizes CPFP.
>>>>> - Parents are allowed to RBF mempool transactions with a set of rules
>>>>> similar
>>>>>   to BIP125. This enables a combination of CPFP and RBF, where a
>>>>> transaction's descendant fees pay for replacing mempool conflicts.
>>>>>
>>>>> There is a draft implementation in [#22290][1]. It is WIP, but
>>>>> feedback is
>>>>> always welcome.
>>>>>
>>>>> ### Details
>>>>>
>>>>> #### Packages May Contain Already-in-Mempool Transactions
>>>>>
>>>>> A package may contain transactions that are already in the mempool. We
>>>>> remove
>>>>> ("deduplicate") those transactions from the package for the purposes
>>>>> of package
>>>>> mempool acceptance. If a package is empty after deduplication, we do
>>>>> nothing.
>>>>>
>>>>> *Rationale*: Mempools vary across the network. It's possible for a
>>>>> parent to be
>>>>> accepted to the mempool of a peer on its own due to differences in
>>>>> policy and
>>>>> fee market fluctuations. We should not reject or penalize the entire
>>>>> package for
>>>>> an individual transaction as that could be a censorship vector.
>>>>>
>>>>> #### Packages Are Multi-Parent-1-Child
>>>>>
>>>>> Only packages of a specific topology are permitted. Namely, a package
>>>>> is exactly
>>>>> 1 child with all of its unconfirmed parents. After deduplication, the
>>>>> package
>>>>> may be exactly the same, empty, 1 child, 1 child with just some of its
>>>>> unconfirmed parents, etc. Note that it's possible for the parents to
>>>>> be indirect
>>>>> descendants/ancestors of one another, or for parent and child to share
>>>>> a parent,
>>>>> so we cannot make any other topology assumptions.
>>>>>
>>>>> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple
>>>>> parents
>>>>> makes it possible to fee-bump a batch of transactions. Restricting
>>>>> packages to a
>>>>> defined topology is also easier to reason about and simplifies the
>>>>> validation
>>>>> logic greatly. Multi-parent-1-child allows us to think of the package
>>>>> as one big
>>>>> transaction, where:
>>>>>
>>>>> - Inputs = all the inputs of parents + inputs of the child that come
>>>>> from
>>>>>   confirmed UTXOs
>>>>> - Outputs = all the outputs of the child + all outputs of the parents
>>>>> that
>>>>>   aren't spent by other transactions in the package
>>>>>
>>>>> Examples of packages that follow this rule (variations of example A
>>>>> show some
>>>>> possibilities after deduplication): ![image][15]
>>>>>
>>>>> #### Fee-Related Checks Use Package Feerate
>>>>>
>>>>> Package Feerate = the total modified fees divided by the total virtual
>>>>> size of
>>>>> all transactions in the package.
>>>>>
>>>>> To meet the two feerate requirements of a mempool, i.e., the
>>>>> pre-configured
>>>>> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum
>>>>> feerate, the
>>>>> total package feerate is used instead of the individual feerate. The
>>>>> individual
>>>>> transactions are allowed to be below feerate requirements if the
>>>>> package meets
>>>>> the feerate requirements. For example, the parent(s) in the package
>>>>> can have 0
>>>>> fees but be paid for by the child.
>>>>>
>>>>> *Rationale*: This can be thought of as "CPFP within a package,"
>>>>> solving the
>>>>> issue of a parent not meeting minimum fees on its own. This allows L2
>>>>> applications to adjust their fees at broadcast time instead of
>>>>> overshooting or
>>>>> risking getting stuck/pinned.
>>>>>
>>>>> We use the package feerate of the package *after deduplication*.
>>>>>
>>>>> *Rationale*:  It would be incorrect to use the fees of transactions
>>>>> that are
>>>>> already in the mempool, as we do not want a transaction's fees to be
>>>>> double-counted for both its individual RBF and package RBF.
>>>>>
>>>>> Examples F and G [14] show the same package, but P1 is submitted
>>>>> individually before
>>>>> the package in example G. In example F, we can see that the 300vB
>>>>> package pays
>>>>> an additional 200sat in fees, which is not enough to pay for its own
>>>>> bandwidth
>>>>> (BIP125#4). In example G, we can see that P1 pays enough to replace
>>>>> M1, but
>>>>> using P1's fees again during package submission would make it look
>>>>> like a 300sat
>>>>> increase for a 200vB package. Even including its fees and size would
>>>>> not be
>>>>> sufficient in this example, since the 300sat looks like enough for the
>>>>> 300vB
>>>>> package. The calculcation after deduplication is 100sat increase for a
>>>>> package
>>>>> of size 200vB, which correctly fails BIP125#4. Assume all transactions
>>>>> have a
>>>>> size of 100vB.
>>>>>
>>>>> #### Package RBF
>>>>>
>>>>> If a package meets feerate requirements as a package, the parents in
>>>>> the
>>>>> transaction are allowed to replace-by-fee mempool transactions. The
>>>>> child cannot
>>>>> replace mempool transactions. Multiple transactions can replace the
>>>>> same
>>>>> transaction, but in order to be valid, none of the transactions can
>>>>> try to
>>>>> replace an ancestor of another transaction in the same package (which
>>>>> would thus
>>>>> make its inputs unavailable).
>>>>>
>>>>> *Rationale*: Even if we are using package feerate, a package will not
>>>>> propagate
>>>>> as intended if RBF still requires each individual transaction to meet
>>>>> the
>>>>> feerate requirements.
>>>>>
>>>>> We use a set of rules slightly modified from BIP125 as follows:
>>>>>
>>>>> ##### Signaling (Rule #1)
>>>>>
>>>>> All mempool transactions to be replaced must signal replaceability.
>>>>>
>>>>> *Rationale*: Package RBF signaling logic should be the same for
>>>>> package RBF and
>>>>> single transaction acceptance. This would be updated if single
>>>>> transaction
>>>>> validation moves to full RBF.
>>>>>
>>>>> ##### New Unconfirmed Inputs (Rule #2)
>>>>>
>>>>> A package may include new unconfirmed inputs, but the ancestor feerate
>>>>> of the
>>>>> child must be at least as high as the ancestor feerates of every
>>>>> transaction
>>>>> being replaced. This is contrary to BIP125#2, which states "The
>>>>> replacement
>>>>> transaction may only include an unconfirmed input if that input was
>>>>> included in
>>>>> one of the original transactions. (An unconfirmed input spends an
>>>>> output from a
>>>>> currently-unconfirmed transaction.)"
>>>>>
>>>>> *Rationale*: The purpose of BIP125#2 is to ensure that the replacement
>>>>> transaction has a higher ancestor score than the original
>>>>> transaction(s) (see
>>>>> [comment][13]). Example H [16] shows how adding a new unconfirmed
>>>>> input can lower the
>>>>> ancestor score of the replacement transaction. P1 is trying to replace
>>>>> M1, and
>>>>> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat,
>>>>> and M2 pays
>>>>> 100sat. Assume all transactions have a size of 100vB. While, in
>>>>> isolation, P1
>>>>> looks like a better mining candidate than M1, it must be mined with
>>>>> M2, so its
>>>>> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's
>>>>> ancestor
>>>>> feerate, which is 6sat/vB.
>>>>>
>>>>> In package RBF, the rule analogous to BIP125#2 would be "none of the
>>>>> transactions in the package can spend new unconfirmed inputs." Example
>>>>> J [17] shows
>>>>> why, if any of the package transactions have ancestors, package
>>>>> feerate is no
>>>>> longer accurate. Even though M2 and M3 are not ancestors of P1 (which
>>>>> is the
>>>>> replacement transaction in an RBF), we're actually interested in the
>>>>> entire
>>>>> package. A miner should mine M1 which is 5sat/vB instead of M2, M3,
>>>>> P1, P2, and
>>>>> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened to
>>>>> only allow
>>>>> the child to have new unconfirmed inputs, either, because it can still
>>>>> cause us
>>>>> to overestimate the package's ancestor score.
>>>>>
>>>>> However, enforcing a rule analogous to BIP125#2 would not only make
>>>>> Package RBF
>>>>> less useful, but would also break Package RBF for packages with
>>>>> parents already
>>>>> in the mempool: if a package parent has already been submitted, it
>>>>> would look
>>>>> like the child is spending a "new" unconfirmed input. In example K
>>>>> [18], we're
>>>>> looking to replace M1 with the entire package including P1, P2, and
>>>>> P3. We must
>>>>> consider the case where one of the parents is already in the mempool
>>>>> (in this
>>>>> case, P2), which means we must allow P3 to have new unconfirmed
>>>>> inputs. However,
>>>>> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not
>>>>> replace M1
>>>>> with this package.
>>>>>
>>>>> Thus, the package RBF rule regarding new unconfirmed inputs is less
>>>>> strict than
>>>>> BIP125#2. However, we still achieve the same goal of requiring the
>>>>> replacement
>>>>> transactions to have a ancestor score at least as high as the original
>>>>> ones. As
>>>>> a result, the entire package is required to be a higher feerate mining
>>>>> candidate
>>>>> than each of the replaced transactions.
>>>>>
>>>>> Another note: the [comment][13] above the BIP125#2 code in the
>>>>> original RBF
>>>>> implementation suggests that the rule was intended to be temporary.
>>>>>
>>>>> ##### Absolute Fee (Rule #3)
>>>>>
>>>>> The package must increase the absolute fee of the mempool, i.e. the
>>>>> total fees
>>>>> of the package must be higher than the absolute fees of the mempool
>>>>> transactions
>>>>> it replaces. Combined with the CPFP rule above, this differs from
>>>>> BIP125 Rule #3
>>>>> - an individual transaction in the package may have lower fees than the
>>>>>   transaction(s) it is replacing. In fact, it may have 0 fees, and the
>>>>> child
>>>>> pays for RBF.
>>>>>
>>>>> ##### Feerate (Rule #4)
>>>>>
>>>>> The package must pay for its own bandwidth; the package feerate must
>>>>> be higher
>>>>> than the replaced transactions by at least minimum relay feerate
>>>>> (`incrementalRelayFee`). Combined with the CPFP rule above, this
>>>>> differs from
>>>>> BIP125 Rule #4 - an individual transaction in the package can have a
>>>>> lower
>>>>> feerate than the transaction(s) it is replacing. In fact, it may have
>>>>> 0 fees,
>>>>> and the child pays for RBF.
>>>>>
>>>>> ##### Total Number of Replaced Transactions (Rule #5)
>>>>>
>>>>> The package cannot replace more than 100 mempool transactions. This is
>>>>> identical
>>>>> to BIP125 Rule #5.
>>>>>
>>>>> ### Expected FAQs
>>>>>
>>>>> 1. Is it possible for only some of the package to make it into the
>>>>> mempool?
>>>>>
>>>>>    Yes, it is. However, since we evict transactions from the mempool by
>>>>> descendant score and the package child is supposed to be sponsoring
>>>>> the fees of
>>>>> its parents, the most common scenario would be all-or-nothing. This is
>>>>> incentive-compatible. In fact, to be conservative, package validation
>>>>> should
>>>>> begin by trying to submit all of the transactions individually, and
>>>>> only use the
>>>>> package mempool acceptance logic if the parents fail due to low
>>>>> feerate.
>>>>>
>>>>> 2. Should we allow packages to contain already-confirmed transactions?
>>>>>
>>>>>     No, for practical reasons. In mempool validation, we actually
>>>>> aren't able to
>>>>> tell with 100% confidence if we are looking at a transaction that has
>>>>> already
>>>>> confirmed, because we look up inputs using a UTXO set. If we have
>>>>> historical
>>>>> block data, it's possible to look for it, but this is inefficient, not
>>>>> always
>>>>> possible for pruning nodes, and unnecessary because we're not going to
>>>>> do
>>>>> anything with the transaction anyway. As such, we already have the
>>>>> expectation
>>>>> that transaction relay is somewhat "stateful" i.e. nobody should be
>>>>> relaying
>>>>> transactions that have already been confirmed. Similarly, we shouldn't
>>>>> be
>>>>> relaying packages that contain already-confirmed transactions.
>>>>>
>>>>> [1]: https://github.com/bitcoin/bitcoin/pull/22290
>>>>> [2]:
>>>>> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations
>>>>> [3]:
>>>>> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282
>>>>> [4]: https://github.com/bitcoin/bitcoin/pull/16400
>>>>> [5]: https://github.com/bitcoin/bitcoin/pull/21062
>>>>> [6]: https://github.com/bitcoin/bitcoin/pull/22675
>>>>> [7]: https://github.com/bitcoin/bitcoin/pull/22796
>>>>> [8]: https://github.com/bitcoin/bitcoin/pull/20833
>>>>> [9]: https://github.com/bitcoin/bitcoin/pull/21800
>>>>> [10]: https://github.com/bitcoin/bitcoin/pull/16401
>>>>> [11]: https://github.com/bitcoin/bitcoin/pull/19621
>>>>> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki
>>>>> [13]:
>>>>> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104
>>>>> [14]:
>>>>> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png
>>>>> [15]:
>>>>> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png
>>>>> [16]:
>>>>> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png
>>>>> [17]:
>>>>> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png
>>>>> [18]:
>>>>> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png
>>>>> [19]:
>>>>> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png
>>>>> [20]:
>>>>> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png
>>>>> _______________________________________________
>>>>> bitcoin-dev mailing list
>>>>> bitcoin-dev at lists.linuxfoundation.org
>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>>>
>>>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210922/5adc42ff/attachment-0001.html>

From gloriajzhao at gmail.com  Wed Sep 22 13:26:14 2021
From: gloriajzhao at gmail.com (Gloria Zhao)
Date: Wed, 22 Sep 2021 14:26:14 +0100
Subject: [bitcoin-dev] Proposal: Package Mempool Accept and Package RBF
In-Reply-To: <CACdvm3Ou_o31m1bbn6wkpxvfCiOUDYLF-4s5MVK9Z3i6=w=LzA@mail.gmail.com>
References: <CAFXO6=+cHyQKM2n9yn4EhwLZO+AUB0ZD81qWPxmpN27rjUoU3w@mail.gmail.com>
 <CACdvm3NdqYFvJ9t4ocXjdLT09fPu40YYwdvpvOnyYCmk5QXyrQ@mail.gmail.com>
 <CAFXO6=+auN_X=C52WB8fBqREUYahnFr1dzYotys7k7x+PjO1DQ@mail.gmail.com>
 <CACdvm3Oo6ABbK79FvuAnZyJ+v4EU3nY=Zi1yzWmv45j5LjWSOg@mail.gmail.com>
 <CAFXO6=LrHWsddH7VP_AgK93Thw8mdeqmCjjkdXAZy1mymyBefA@mail.gmail.com>
 <CACdvm3Ou_o31m1bbn6wkpxvfCiOUDYLF-4s5MVK9Z3i6=w=LzA@mail.gmail.com>
Message-ID: <CAFXO6=KYYEB9gYMYsHPyxNLbzGB12+XHrX_NbfNoUuFtJjpXgw@mail.gmail.com>

Hi Bastien,

> A package A + C will be able to replace A' + B regardless of
> the weight of A' + B?

Correct, the weight of A' + B will not prevent A+C from replacing it (as
long as A+C pays enough fees). In example 2C, we would be able to replace A
with a package.

Best,
Gloria

On Wed, Sep 22, 2021 at 8:10 AM Bastien TEINTURIER <bastien at acinq.fr> wrote:

> Great, thanks for this clarification!
>
> Can you confirm that this won't be an issue either with your
> example 2C (in your first set of diagrams)? If I understand it
> correctly it shouldn't, but I'd rather be 100% sure.
>
> A package A + C will be able to replace A' + B regardless of
> the weight of A' + B?
>
> Thanks,
> Bastien
>
> Le mar. 21 sept. 2021 ? 18:42, Gloria Zhao <gloriajzhao at gmail.com> a
> ?crit :
>
>> Hi Bastien,
>>
>> Excellent diagram :D
>>
>> > Here the issue is that a revoked commitment tx A' is pinned in other
>> > mempools, with a long chain of descendants (or descendants that reach
>> > the maximum replaceable size).
>> > We would really like A + C to be able to replace this pinned A'.
>> > We can't submit individually because A on its own won't replace A'...
>>
>> Right, this is a key motivation for having Package RBF. In this case, A+C
>> can replace A' + B1...B24.
>>
>> Due to the descendant limit (each node operator can increase it on their
>> own node, but the default is 25), A' should have no more than 25
>> descendants, even including CPFP carve out. As long as A only conflicts
>> with A', it won't be trying to replace more than 100 transactions. The
>> proposed package RBF will allow C to pay for A's conflicts, since their
>> package feerate is used in the fee comparisons. A is not a descendant of
>> A', so the existence of B1...B24 does not prevent the replacement.
>>
>> Best,
>> Gloria
>>
>> On Tue, Sep 21, 2021 at 4:18 PM Bastien TEINTURIER <bastien at acinq.fr>
>> wrote:
>>
>>> Hi Gloria,
>>>
>>> > I believe this attack is mitigated as long as we attempt to submit
>>> transactions individually
>>>
>>> Unfortunately not, as there exists a pinning scenario in LN where a
>>> different commit tx is pinned, but you actually can't know which one.
>>>
>>> Since I really like your diagrams, I made one as well to illustrate:
>>>
>>> https://user-images.githubusercontent.com/31281497/134198114-5e9c6857-e8fc-405a-be57-18181d5e54cb.jpg
>>>
>>> Here the issue is that a revoked commitment tx A' is pinned in other
>>> mempools, with a long chain of descendants (or descendants that reach
>>> the maximum replaceable size).
>>>
>>> We would really like A + C to be able to replace this pinned A'.
>>> We can't submit individually because A on its own won't replace A'...
>>>
>>> > I would note that this proposal doesn't accommodate something like
>>> diagram B, where C is getting CPFP carve out and wants to bring a +1
>>>
>>> No worries, that case shouldn't be a concern.
>>> I believe any L2 protocol can always ensure it confirms such tx trees
>>> "one depth after the other" without impacting funds safety, so it
>>> only needs to ensure A + C can get into mempools.
>>>
>>> Thanks,
>>> Bastien
>>>
>>> Le mar. 21 sept. 2021 ? 13:18, Gloria Zhao <gloriajzhao at gmail.com> a
>>> ?crit :
>>>
>>>> Hi Bastien,
>>>>
>>>> Thank you for your feedback!
>>>>
>>>> > In your example we have a parent transaction A already in the mempool
>>>> > and an unrelated child B. We submit a package C + D where C spends
>>>> > another of A's inputs. You're highlighting that this package may be
>>>> > rejected because of the unrelated transaction(s) B.
>>>>
>>>> > The way I see this, an attacker can abuse this rule to ensure
>>>> > transaction A stays pinned in the mempool without confirming by
>>>> > broadcasting a set of child transactions that reach these limits
>>>> > and pay low fees (where A would be a commit tx in LN).
>>>>
>>>> I believe you are describing a pinning attack in which your adversarial
>>>> counterparty attempts to monopolize the mempool descendant limit of the
>>>> shared  transaction A in order to prevent you from submitting a fee-bumping
>>>> child C; I've tried to illustrate this as diagram A here:
>>>> https://user-images.githubusercontent.com/25183001/134159860-068080d0-bbb6-4356-ae74-00df00644c74.png
>>>> (please let me know if I'm misunderstanding).
>>>>
>>>> I believe this attack is mitigated as long as we attempt to submit
>>>> transactions individually (and thus take advantage of CPFP carve out)
>>>> before attempting package validation. So, in scenario A2, even if the
>>>> mempool receives a package with A+C, it would deduplicate A, submit C as an
>>>> individual transaction, and allow it due to the CPFP carve out exemption. A
>>>> more general goal is: if a transaction would propagate successfully on its
>>>> own now, it should still propagate regardless of whether it is included in
>>>> a package. The best way to ensure this, as far as I can tell, is to always
>>>> try to submit them individually first.
>>>>
>>>> I would note that this proposal doesn't accommodate something like
>>>> diagram B, where C is getting CPFP carve out and wants to bring a +1 (e.g.
>>>> C has very low fees and is bumped by D). I don't think this is a use case
>>>> since C should be the one fee-bumping A, but since we're talking about
>>>> limitations around the CPFP carve out, this is it.
>>>>
>>>> Let me know if this addresses your concerns?
>>>>
>>>> Thanks,
>>>> Gloria
>>>>
>>>> On Mon, Sep 20, 2021 at 10:19 AM Bastien TEINTURIER <bastien at acinq.fr>
>>>> wrote:
>>>>
>>>>> Hi Gloria,
>>>>>
>>>>> Thanks for this detailed post!
>>>>>
>>>>> The illustrations you provided are very useful for this kind of graph
>>>>> topology problems.
>>>>>
>>>>> The rules you lay out for package RBF look good to me at first glance
>>>>> as there are some subtle improvements compared to BIP 125.
>>>>>
>>>>> > 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and
>>>>> > `MAX_PACKAGE_SIZE=101KvB` total size [8]
>>>>>
>>>>> I have a question regarding this rule, as your example 2C could be
>>>>> concerning for LN (unless I didn't understand it correctly).
>>>>>
>>>>> This also touches on the package RBF rule 5 ("The package cannot
>>>>> replace more than 100 mempool transactions.")
>>>>>
>>>>> In your example we have a parent transaction A already in the mempool
>>>>> and an unrelated child B. We submit a package C + D where C spends
>>>>> another of A's inputs. You're highlighting that this package may be
>>>>> rejected because of the unrelated transaction(s) B.
>>>>>
>>>>> The way I see this, an attacker can abuse this rule to ensure
>>>>> transaction A stays pinned in the mempool without confirming by
>>>>> broadcasting a set of child transactions that reach these limits
>>>>> and pay low fees (where A would be a commit tx in LN).
>>>>>
>>>>> We had to create the CPFP carve-out rule explicitly to work around
>>>>> this limitation, and I think it would be necessary for package RBF
>>>>> as well, because in such cases we do want to be able to submit a
>>>>> package A + C where C pays high fees to speed up A's confirmation,
>>>>> regardless of unrelated unconfirmed children of A...
>>>>>
>>>>> We could submit only C to benefit from the existing CPFP carve-out
>>>>> rule, but that wouldn't work if our local mempool doesn't have A yet,
>>>>> but other remote mempools do.
>>>>>
>>>>> Is my concern justified? Is this something that we should dig into a
>>>>> bit deeper?
>>>>>
>>>>> Thanks,
>>>>> Bastien
>>>>>
>>>>> Le jeu. 16 sept. 2021 ? 09:55, Gloria Zhao via bitcoin-dev <
>>>>> bitcoin-dev at lists.linuxfoundation.org> a ?crit :
>>>>>
>>>>>> Hi there,
>>>>>>
>>>>>> I'm writing to propose a set of mempool policy changes to enable
>>>>>> package
>>>>>> validation (in preparation for package relay) in Bitcoin Core. These
>>>>>> would not
>>>>>> be consensus or P2P protocol changes. However, since mempool policy
>>>>>> significantly affects transaction propagation, I believe this is
>>>>>> relevant for
>>>>>> the mailing list.
>>>>>>
>>>>>> My proposal enables packages consisting of multiple parents and 1
>>>>>> child. If you
>>>>>> develop software that relies on specific transaction relay
>>>>>> assumptions and/or
>>>>>> are interested in using package relay in the future, I'm very
>>>>>> interested to hear
>>>>>> your feedback on the utility or restrictiveness of these package
>>>>>> policies for
>>>>>> your use cases.
>>>>>>
>>>>>> A draft implementation of this proposal can be found in [Bitcoin Core
>>>>>> PR#22290][1].
>>>>>>
>>>>>> An illustrated version of this post can be found at
>>>>>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.
>>>>>> I have also linked the images below.
>>>>>>
>>>>>> ## Background
>>>>>>
>>>>>> Feel free to skip this section if you are already familiar with
>>>>>> mempool policy
>>>>>> and package relay terminology.
>>>>>>
>>>>>> ### Terminology Clarifications
>>>>>>
>>>>>> * Package = an ordered list of related transactions, representable by
>>>>>> a Directed
>>>>>>   Acyclic Graph.
>>>>>> * Package Feerate = the total modified fees divided by the total
>>>>>> virtual size of
>>>>>>   all transactions in the package.
>>>>>>     - Modified fees = a transaction's base fees + fee delta applied
>>>>>> by the user
>>>>>>       with `prioritisetransaction`. As such, we expect this to vary
>>>>>> across
>>>>>> mempools.
>>>>>>     - Virtual Size = the maximum of virtual sizes calculated using
>>>>>> [BIP141
>>>>>>       virtual size][2] and sigop weight. [Implemented here in Bitcoin
>>>>>> Core][3].
>>>>>>     - Note that feerate is not necessarily based on the base fees and
>>>>>> serialized
>>>>>>       size.
>>>>>>
>>>>>> * Fee-Bumping = user/wallet actions that take advantage of miner
>>>>>> incentives to
>>>>>>   boost a transaction's candidacy for inclusion in a block, including
>>>>>> Child Pays
>>>>>> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our
>>>>>> intention in
>>>>>> mempool policy is to recognize when the new transaction is more
>>>>>> economical to
>>>>>> mine than the original one(s) but not open DoS vectors, so there are
>>>>>> some
>>>>>> limitations.
>>>>>>
>>>>>> ### Policy
>>>>>>
>>>>>> The purpose of the mempool is to store the best (to be most
>>>>>> incentive-compatible
>>>>>> with miners, highest feerate) candidates for inclusion in a block.
>>>>>> Miners use
>>>>>> the mempool to build block templates. The mempool is also useful as a
>>>>>> cache for
>>>>>> boosting block relay and validation performance, aiding transaction
>>>>>> relay, and
>>>>>> generating feerate estimations.
>>>>>>
>>>>>> Ideally, all consensus-valid transactions paying reasonable fees
>>>>>> should make it
>>>>>> to miners through normal transaction relay, without any special
>>>>>> connectivity or
>>>>>> relationships with miners. On the other hand, nodes do not have
>>>>>> unlimited
>>>>>> resources, and a P2P network designed to let any honest node
>>>>>> broadcast their
>>>>>> transactions also exposes the transaction validation engine to DoS
>>>>>> attacks from
>>>>>> malicious peers.
>>>>>>
>>>>>> As such, for unconfirmed transactions we are considering for our
>>>>>> mempool, we
>>>>>> apply a set of validation rules in addition to consensus, primarily
>>>>>> to protect
>>>>>> us from resource exhaustion and aid our efforts to keep the highest
>>>>>> fee
>>>>>> transactions. We call this mempool _policy_: a set of (configurable,
>>>>>> node-specific) rules that transactions must abide by in order to be
>>>>>> accepted
>>>>>> into our mempool. Transaction "Standardness" rules and mempool
>>>>>> restrictions such
>>>>>> as "too-long-mempool-chain" are both examples of policy.
>>>>>>
>>>>>> ### Package Relay and Package Mempool Accept
>>>>>>
>>>>>> In transaction relay, we currently consider transactions one at a
>>>>>> time for
>>>>>> submission to the mempool. This creates a limitation in the node's
>>>>>> ability to
>>>>>> determine which transactions have the highest feerates, since we
>>>>>> cannot take
>>>>>> into account descendants (i.e. cannot use CPFP) until all the
>>>>>> transactions are
>>>>>> in the mempool. Similarly, we cannot use a transaction's descendants
>>>>>> when
>>>>>> considering it for RBF. When an individual transaction does not meet
>>>>>> the mempool
>>>>>> minimum feerate and the user isn't able to create a replacement
>>>>>> transaction
>>>>>> directly, it will not be accepted by mempools.
>>>>>>
>>>>>> This limitation presents a security issue for applications and users
>>>>>> relying on
>>>>>> time-sensitive transactions. For example, Lightning and other
>>>>>> protocols create
>>>>>> UTXOs with multiple spending paths, where one counterparty's spending
>>>>>> path opens
>>>>>> up after a timelock, and users are protected from cheating scenarios
>>>>>> as long as
>>>>>> they redeem on-chain in time. A key security assumption is that all
>>>>>> parties'
>>>>>> transactions will propagate and confirm in a timely manner. This
>>>>>> assumption can
>>>>>> be broken if fee-bumping does not work as intended.
>>>>>>
>>>>>> The end goal for Package Relay is to consider multiple transactions
>>>>>> at the same
>>>>>> time, e.g. a transaction with its high-fee child. This may help us
>>>>>> better
>>>>>> determine whether transactions should be accepted to our mempool,
>>>>>> especially if
>>>>>> they don't meet fee requirements individually or are better RBF
>>>>>> candidates as a
>>>>>> package. A combination of changes to mempool validation logic,
>>>>>> policy, and
>>>>>> transaction relay allows us to better propagate the transactions with
>>>>>> the
>>>>>> highest package feerates to miners, and makes fee-bumping tools more
>>>>>> powerful
>>>>>> for users.
>>>>>>
>>>>>> The "relay" part of Package Relay suggests P2P messaging changes, but
>>>>>> a large
>>>>>> part of the changes are in the mempool's package validation logic. We
>>>>>> call this
>>>>>> *Package Mempool Accept*.
>>>>>>
>>>>>> ### Previous Work
>>>>>>
>>>>>> * Given that mempool validation is DoS-sensitive and complex, it
>>>>>> would be
>>>>>>   dangerous to haphazardly tack on package validation logic. Many
>>>>>> efforts have
>>>>>> been made to make mempool validation less opaque (see [#16400][4],
>>>>>> [#21062][5],
>>>>>> [#22675][6], [#22796][7]).
>>>>>> * [#20833][8] Added basic capabilities for package validation, test
>>>>>> accepts only
>>>>>>   (no submission to mempool).
>>>>>> * [#21800][9] Implemented package ancestor/descendant limit checks
>>>>>> for arbitrary
>>>>>>   packages. Still test accepts only.
>>>>>> * Previous package relay proposals (see [#16401][10], [#19621][11]).
>>>>>>
>>>>>> ### Existing Package Rules
>>>>>>
>>>>>> These are in master as introduced in [#20833][8] and [#21800][9].
>>>>>> I'll consider
>>>>>> them as "given" in the rest of this document, though they can be
>>>>>> changed, since
>>>>>> package validation is test-accept only right now.
>>>>>>
>>>>>> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and
>>>>>> `MAX_PACKAGE_SIZE=101KvB` total size [8]
>>>>>>
>>>>>>    *Rationale*: This is already enforced as mempool
>>>>>> ancestor/descendant limits.
>>>>>> Presumably, transactions in a package are all related, so exceeding
>>>>>> this limit
>>>>>> would mean that the package can either be split up or it wouldn't
>>>>>> pass this
>>>>>> mempool policy.
>>>>>>
>>>>>> 2. Packages must be topologically sorted: if any dependencies exist
>>>>>> between
>>>>>> transactions, parents must appear somewhere before children. [8]
>>>>>>
>>>>>> 3. A package cannot have conflicting transactions, i.e. none of them
>>>>>> can spend
>>>>>> the same inputs. This also means there cannot be duplicate
>>>>>> transactions. [8]
>>>>>>
>>>>>> 4. When packages are evaluated against ancestor/descendant limits in
>>>>>> a test
>>>>>> accept, the union of all of their descendants and ancestors is
>>>>>> considered. This
>>>>>> is essentially a "worst case" heuristic where every transaction in
>>>>>> the package
>>>>>> is treated as each other's ancestor and descendant. [8]
>>>>>> Packages for which ancestor/descendant limits are accurately captured
>>>>>> by this
>>>>>> heuristic: [19]
>>>>>>
>>>>>> There are also limitations such as the fact that CPFP carve out is
>>>>>> not applied
>>>>>> to package transactions. #20833 also disables RBF in package
>>>>>> validation; this
>>>>>> proposal overrides that to allow packages to use RBF.
>>>>>>
>>>>>> ## Proposed Changes
>>>>>>
>>>>>> The next step in the Package Mempool Accept project is to implement
>>>>>> submission
>>>>>> to mempool, initially through RPC only. This allows us to test the
>>>>>> submission
>>>>>> logic before exposing it on P2P.
>>>>>>
>>>>>> ### Summary
>>>>>>
>>>>>> - Packages may contain already-in-mempool transactions.
>>>>>> - Packages are 2 generations, Multi-Parent-1-Child.
>>>>>> - Fee-related checks use the package feerate. This means that wallets
>>>>>> can
>>>>>> create a package that utilizes CPFP.
>>>>>> - Parents are allowed to RBF mempool transactions with a set of rules
>>>>>> similar
>>>>>>   to BIP125. This enables a combination of CPFP and RBF, where a
>>>>>> transaction's descendant fees pay for replacing mempool conflicts.
>>>>>>
>>>>>> There is a draft implementation in [#22290][1]. It is WIP, but
>>>>>> feedback is
>>>>>> always welcome.
>>>>>>
>>>>>> ### Details
>>>>>>
>>>>>> #### Packages May Contain Already-in-Mempool Transactions
>>>>>>
>>>>>> A package may contain transactions that are already in the mempool.
>>>>>> We remove
>>>>>> ("deduplicate") those transactions from the package for the purposes
>>>>>> of package
>>>>>> mempool acceptance. If a package is empty after deduplication, we do
>>>>>> nothing.
>>>>>>
>>>>>> *Rationale*: Mempools vary across the network. It's possible for a
>>>>>> parent to be
>>>>>> accepted to the mempool of a peer on its own due to differences in
>>>>>> policy and
>>>>>> fee market fluctuations. We should not reject or penalize the entire
>>>>>> package for
>>>>>> an individual transaction as that could be a censorship vector.
>>>>>>
>>>>>> #### Packages Are Multi-Parent-1-Child
>>>>>>
>>>>>> Only packages of a specific topology are permitted. Namely, a package
>>>>>> is exactly
>>>>>> 1 child with all of its unconfirmed parents. After deduplication, the
>>>>>> package
>>>>>> may be exactly the same, empty, 1 child, 1 child with just some of its
>>>>>> unconfirmed parents, etc. Note that it's possible for the parents to
>>>>>> be indirect
>>>>>> descendants/ancestors of one another, or for parent and child to
>>>>>> share a parent,
>>>>>> so we cannot make any other topology assumptions.
>>>>>>
>>>>>> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple
>>>>>> parents
>>>>>> makes it possible to fee-bump a batch of transactions. Restricting
>>>>>> packages to a
>>>>>> defined topology is also easier to reason about and simplifies the
>>>>>> validation
>>>>>> logic greatly. Multi-parent-1-child allows us to think of the package
>>>>>> as one big
>>>>>> transaction, where:
>>>>>>
>>>>>> - Inputs = all the inputs of parents + inputs of the child that come
>>>>>> from
>>>>>>   confirmed UTXOs
>>>>>> - Outputs = all the outputs of the child + all outputs of the parents
>>>>>> that
>>>>>>   aren't spent by other transactions in the package
>>>>>>
>>>>>> Examples of packages that follow this rule (variations of example A
>>>>>> show some
>>>>>> possibilities after deduplication): ![image][15]
>>>>>>
>>>>>> #### Fee-Related Checks Use Package Feerate
>>>>>>
>>>>>> Package Feerate = the total modified fees divided by the total
>>>>>> virtual size of
>>>>>> all transactions in the package.
>>>>>>
>>>>>> To meet the two feerate requirements of a mempool, i.e., the
>>>>>> pre-configured
>>>>>> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum
>>>>>> feerate, the
>>>>>> total package feerate is used instead of the individual feerate. The
>>>>>> individual
>>>>>> transactions are allowed to be below feerate requirements if the
>>>>>> package meets
>>>>>> the feerate requirements. For example, the parent(s) in the package
>>>>>> can have 0
>>>>>> fees but be paid for by the child.
>>>>>>
>>>>>> *Rationale*: This can be thought of as "CPFP within a package,"
>>>>>> solving the
>>>>>> issue of a parent not meeting minimum fees on its own. This allows L2
>>>>>> applications to adjust their fees at broadcast time instead of
>>>>>> overshooting or
>>>>>> risking getting stuck/pinned.
>>>>>>
>>>>>> We use the package feerate of the package *after deduplication*.
>>>>>>
>>>>>> *Rationale*:  It would be incorrect to use the fees of transactions
>>>>>> that are
>>>>>> already in the mempool, as we do not want a transaction's fees to be
>>>>>> double-counted for both its individual RBF and package RBF.
>>>>>>
>>>>>> Examples F and G [14] show the same package, but P1 is submitted
>>>>>> individually before
>>>>>> the package in example G. In example F, we can see that the 300vB
>>>>>> package pays
>>>>>> an additional 200sat in fees, which is not enough to pay for its own
>>>>>> bandwidth
>>>>>> (BIP125#4). In example G, we can see that P1 pays enough to replace
>>>>>> M1, but
>>>>>> using P1's fees again during package submission would make it look
>>>>>> like a 300sat
>>>>>> increase for a 200vB package. Even including its fees and size would
>>>>>> not be
>>>>>> sufficient in this example, since the 300sat looks like enough for
>>>>>> the 300vB
>>>>>> package. The calculcation after deduplication is 100sat increase for
>>>>>> a package
>>>>>> of size 200vB, which correctly fails BIP125#4. Assume all
>>>>>> transactions have a
>>>>>> size of 100vB.
>>>>>>
>>>>>> #### Package RBF
>>>>>>
>>>>>> If a package meets feerate requirements as a package, the parents in
>>>>>> the
>>>>>> transaction are allowed to replace-by-fee mempool transactions. The
>>>>>> child cannot
>>>>>> replace mempool transactions. Multiple transactions can replace the
>>>>>> same
>>>>>> transaction, but in order to be valid, none of the transactions can
>>>>>> try to
>>>>>> replace an ancestor of another transaction in the same package (which
>>>>>> would thus
>>>>>> make its inputs unavailable).
>>>>>>
>>>>>> *Rationale*: Even if we are using package feerate, a package will not
>>>>>> propagate
>>>>>> as intended if RBF still requires each individual transaction to meet
>>>>>> the
>>>>>> feerate requirements.
>>>>>>
>>>>>> We use a set of rules slightly modified from BIP125 as follows:
>>>>>>
>>>>>> ##### Signaling (Rule #1)
>>>>>>
>>>>>> All mempool transactions to be replaced must signal replaceability.
>>>>>>
>>>>>> *Rationale*: Package RBF signaling logic should be the same for
>>>>>> package RBF and
>>>>>> single transaction acceptance. This would be updated if single
>>>>>> transaction
>>>>>> validation moves to full RBF.
>>>>>>
>>>>>> ##### New Unconfirmed Inputs (Rule #2)
>>>>>>
>>>>>> A package may include new unconfirmed inputs, but the ancestor
>>>>>> feerate of the
>>>>>> child must be at least as high as the ancestor feerates of every
>>>>>> transaction
>>>>>> being replaced. This is contrary to BIP125#2, which states "The
>>>>>> replacement
>>>>>> transaction may only include an unconfirmed input if that input was
>>>>>> included in
>>>>>> one of the original transactions. (An unconfirmed input spends an
>>>>>> output from a
>>>>>> currently-unconfirmed transaction.)"
>>>>>>
>>>>>> *Rationale*: The purpose of BIP125#2 is to ensure that the replacement
>>>>>> transaction has a higher ancestor score than the original
>>>>>> transaction(s) (see
>>>>>> [comment][13]). Example H [16] shows how adding a new unconfirmed
>>>>>> input can lower the
>>>>>> ancestor score of the replacement transaction. P1 is trying to
>>>>>> replace M1, and
>>>>>> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat,
>>>>>> and M2 pays
>>>>>> 100sat. Assume all transactions have a size of 100vB. While, in
>>>>>> isolation, P1
>>>>>> looks like a better mining candidate than M1, it must be mined with
>>>>>> M2, so its
>>>>>> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's
>>>>>> ancestor
>>>>>> feerate, which is 6sat/vB.
>>>>>>
>>>>>> In package RBF, the rule analogous to BIP125#2 would be "none of the
>>>>>> transactions in the package can spend new unconfirmed inputs."
>>>>>> Example J [17] shows
>>>>>> why, if any of the package transactions have ancestors, package
>>>>>> feerate is no
>>>>>> longer accurate. Even though M2 and M3 are not ancestors of P1 (which
>>>>>> is the
>>>>>> replacement transaction in an RBF), we're actually interested in the
>>>>>> entire
>>>>>> package. A miner should mine M1 which is 5sat/vB instead of M2, M3,
>>>>>> P1, P2, and
>>>>>> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened to
>>>>>> only allow
>>>>>> the child to have new unconfirmed inputs, either, because it can
>>>>>> still cause us
>>>>>> to overestimate the package's ancestor score.
>>>>>>
>>>>>> However, enforcing a rule analogous to BIP125#2 would not only make
>>>>>> Package RBF
>>>>>> less useful, but would also break Package RBF for packages with
>>>>>> parents already
>>>>>> in the mempool: if a package parent has already been submitted, it
>>>>>> would look
>>>>>> like the child is spending a "new" unconfirmed input. In example K
>>>>>> [18], we're
>>>>>> looking to replace M1 with the entire package including P1, P2, and
>>>>>> P3. We must
>>>>>> consider the case where one of the parents is already in the mempool
>>>>>> (in this
>>>>>> case, P2), which means we must allow P3 to have new unconfirmed
>>>>>> inputs. However,
>>>>>> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not
>>>>>> replace M1
>>>>>> with this package.
>>>>>>
>>>>>> Thus, the package RBF rule regarding new unconfirmed inputs is less
>>>>>> strict than
>>>>>> BIP125#2. However, we still achieve the same goal of requiring the
>>>>>> replacement
>>>>>> transactions to have a ancestor score at least as high as the
>>>>>> original ones. As
>>>>>> a result, the entire package is required to be a higher feerate
>>>>>> mining candidate
>>>>>> than each of the replaced transactions.
>>>>>>
>>>>>> Another note: the [comment][13] above the BIP125#2 code in the
>>>>>> original RBF
>>>>>> implementation suggests that the rule was intended to be temporary.
>>>>>>
>>>>>> ##### Absolute Fee (Rule #3)
>>>>>>
>>>>>> The package must increase the absolute fee of the mempool, i.e. the
>>>>>> total fees
>>>>>> of the package must be higher than the absolute fees of the mempool
>>>>>> transactions
>>>>>> it replaces. Combined with the CPFP rule above, this differs from
>>>>>> BIP125 Rule #3
>>>>>> - an individual transaction in the package may have lower fees than
>>>>>> the
>>>>>>   transaction(s) it is replacing. In fact, it may have 0 fees, and
>>>>>> the child
>>>>>> pays for RBF.
>>>>>>
>>>>>> ##### Feerate (Rule #4)
>>>>>>
>>>>>> The package must pay for its own bandwidth; the package feerate must
>>>>>> be higher
>>>>>> than the replaced transactions by at least minimum relay feerate
>>>>>> (`incrementalRelayFee`). Combined with the CPFP rule above, this
>>>>>> differs from
>>>>>> BIP125 Rule #4 - an individual transaction in the package can have a
>>>>>> lower
>>>>>> feerate than the transaction(s) it is replacing. In fact, it may have
>>>>>> 0 fees,
>>>>>> and the child pays for RBF.
>>>>>>
>>>>>> ##### Total Number of Replaced Transactions (Rule #5)
>>>>>>
>>>>>> The package cannot replace more than 100 mempool transactions. This
>>>>>> is identical
>>>>>> to BIP125 Rule #5.
>>>>>>
>>>>>> ### Expected FAQs
>>>>>>
>>>>>> 1. Is it possible for only some of the package to make it into the
>>>>>> mempool?
>>>>>>
>>>>>>    Yes, it is. However, since we evict transactions from the mempool
>>>>>> by
>>>>>> descendant score and the package child is supposed to be sponsoring
>>>>>> the fees of
>>>>>> its parents, the most common scenario would be all-or-nothing. This is
>>>>>> incentive-compatible. In fact, to be conservative, package validation
>>>>>> should
>>>>>> begin by trying to submit all of the transactions individually, and
>>>>>> only use the
>>>>>> package mempool acceptance logic if the parents fail due to low
>>>>>> feerate.
>>>>>>
>>>>>> 2. Should we allow packages to contain already-confirmed transactions?
>>>>>>
>>>>>>     No, for practical reasons. In mempool validation, we actually
>>>>>> aren't able to
>>>>>> tell with 100% confidence if we are looking at a transaction that has
>>>>>> already
>>>>>> confirmed, because we look up inputs using a UTXO set. If we have
>>>>>> historical
>>>>>> block data, it's possible to look for it, but this is inefficient,
>>>>>> not always
>>>>>> possible for pruning nodes, and unnecessary because we're not going
>>>>>> to do
>>>>>> anything with the transaction anyway. As such, we already have the
>>>>>> expectation
>>>>>> that transaction relay is somewhat "stateful" i.e. nobody should be
>>>>>> relaying
>>>>>> transactions that have already been confirmed. Similarly, we
>>>>>> shouldn't be
>>>>>> relaying packages that contain already-confirmed transactions.
>>>>>>
>>>>>> [1]: https://github.com/bitcoin/bitcoin/pull/22290
>>>>>> [2]:
>>>>>> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations
>>>>>> [3]:
>>>>>> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282
>>>>>> [4]: https://github.com/bitcoin/bitcoin/pull/16400
>>>>>> [5]: https://github.com/bitcoin/bitcoin/pull/21062
>>>>>> [6]: https://github.com/bitcoin/bitcoin/pull/22675
>>>>>> [7]: https://github.com/bitcoin/bitcoin/pull/22796
>>>>>> [8]: https://github.com/bitcoin/bitcoin/pull/20833
>>>>>> [9]: https://github.com/bitcoin/bitcoin/pull/21800
>>>>>> [10]: https://github.com/bitcoin/bitcoin/pull/16401
>>>>>> [11]: https://github.com/bitcoin/bitcoin/pull/19621
>>>>>> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki
>>>>>> [13]:
>>>>>> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104
>>>>>> [14]:
>>>>>> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png
>>>>>> [15]:
>>>>>> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png
>>>>>> [16]:
>>>>>> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png
>>>>>> [17]:
>>>>>> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png
>>>>>> [18]:
>>>>>> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png
>>>>>> [19]:
>>>>>> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png
>>>>>> [20]:
>>>>>> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png
>>>>>> _______________________________________________
>>>>>> bitcoin-dev mailing list
>>>>>> bitcoin-dev at lists.linuxfoundation.org
>>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>>>>
>>>>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210922/6d265bbb/attachment-0001.html>

From laolu32 at gmail.com  Thu Sep 23 00:29:18 2021
From: laolu32 at gmail.com (Olaoluwa Osuntokun)
Date: Wed, 22 Sep 2021 17:29:18 -0700
Subject: [bitcoin-dev] TAPLEAF_UPDATE_VERIFY covenant opcode
In-Reply-To: <20210909064138.GA22496@erisian.com.au>
References: <20210909064138.GA22496@erisian.com.au>
Message-ID: <CAO3Pvs-OviFkDJOsOWOt3Ea9t0g6xXb79hYdEe_L1=nB5ZjSQw@mail.gmail.com>

Hi AJ,

Happy to see that this proposal has finally seen the light of day! I've been
hearing about it in hinted background convos over the past few months, so
happy I can finally dig into the specifics of its operation.

> So the idea is to do just that via a new opcode "TAPLEAF_UPDATE_VERIFY"
> (TLUV) that takes three inputs: one that specifies how to update the
> internal public key (X), one that specifies a new step for the merkle path
> (F), and one that specifies whether to remove the current script and/or
> how many merkle path steps to remove

What if instead, it obtained the script from the _annex_? I think this small
modification would make the op code even _more_ powerful. Consider that this
allows a new script to be passed _dynamically_ after the output has been
created, possibly by a threshold of parties that control the output, or them
all (mu sig, etc, etc). This serves to create a generic "upgrade" mechanism
for any tapscript output (covenant or not). Functionally, this is similar to
the existence of "admin keys" or voted DAO upgrades that exists in chains
that utilize an account based systems. This is really useful as it allows a
script any given output to optional add in graftroot like behavior (leaf in
tree that accepts script updates), and also allows contract developers to
progressively upgrade or fix issues in prior versions of their deployed
contracts.

This little trick is secure since unlike the witness itself, the annex is
actually _signed_ within the sighash like everything else. Incorporating
this proposal would require the addition of an OP_PUSH_ANNEX op code, which
by itself seems expertly useful. If one views the annex as a sort of
authenticated associated data that can be passed into the script execution
context, then this actually serves to absorb _some_ uses cases of a
hypothetical OP_CHECKSIG_FROM_STACK opcode. A push annex op code also makes
doing things like output delegation to a given key passed into the witness
secure since the prior "owner" of the output commits to the key within the
sighash.

Even assuming a more powerful type of covenant that allows partial
application of binding logic, something like this is still super useful
since the action of re-creating a new tapscript tree based in dynamic input
data would generate a rather large witness if only something like OP_CAT was
available. The unique "update" nature of this appears to augment any other
type of covenant, which is pretty cool. Consider that it would allow you
(with the annex addition above), take something like a CTV congestion tree,
and add in _new_ users at the tree is already being unrolled (just a toy
example).

It would also allow an individual to _join_ the payment pool construct
described earlier which makes it 1000x more useful (vs just supporting
unrolling). I haven't written it all down yet, but I think this along with
something like CTV or CSFS makes it possible to implement a Plasma Cash [4]
like Commit Chain [5], which is super exciting (assume a counter is embedded
in the main script that tracks the next free leaf slot(s). With this model
an "operator" is able to include a single transaction in the chain that
stamps a batch of updates in the payment tree.  Users then get a
contestation period where they can refute a modification to the tree in
order to withdraw their funds.

> And second, it doesn't provide a way for utxos to "interact",

This is due to the fact that the op code doesn't allow any sort of late
binding or pattern matching then constraining _where_ (or whence?) the coins
can Be sent to. There's a group of developers that are attempting to make an
AMM-like system on Liquid [1] using more generic stack based covenants [2]
(see the `OP_INSPECTINPUT` op code, which seems very much inspired by
jl2012's old proposal). However one challenge that still need to be tackled
in the UTXO model is allowing multiple participants to easily interact w/
the
contract in a single block w/o a coordination layer to synchronize the
access.

One solution to this concurrency issue, that I believe is already employed
by Chia is to allow "contracts" to be identified via a fixed ID (as long as
their active in the chain) [3]. This lets transactions spend/interact with a
contract, without always needing to know the set of active UTXOs where that
contract lives. Transactions then specify their contract and "regular"
inputs, with the requirement that every transaction spends at least a single
regular input.

The trade-off here is that nodes need to maintain this extra index into the
UTXO set. However, this can be alleviated by applying a utreexo like
solution: nodes maintain some merklized data structure over the index and
require that spending transactions provide an _inclusion_ proof of the
active contract. Nodes then only need to maintain root hashes of the UTXO
and contract set.

I'm super happy w.r.t how the covenant space has been processing over the
past few years. IMO its the single most important (along with the utreexo
type stateless stuff mentioned above) missing component to allow the
creation of more decentralized self-custodial applications built on top of
Bitcoin.

-- Laolu

[1]: https://medium.com/bit-matrix
[2]:
https://github.com/sanket1729/elements/blob/84339ba5e5dc65328d98afe2b1b33dcb69ba4311/doc/tapscript_opcodes.md
[3]: https://forum.celestia.org/t/accounts-strict-access-lists-and-utxos/37
[4]: https://www.learnplasma.org/en/learn/cash.html
[5]: https://eprint.iacr.org/2018/642
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210922/4ccd6220/attachment.html>

From antoine.riard at gmail.com  Thu Sep 23 04:29:39 2021
From: antoine.riard at gmail.com (Antoine Riard)
Date: Thu, 23 Sep 2021 00:29:39 -0400
Subject: [bitcoin-dev] Proposal: Package Mempool Accept and Package RBF
In-Reply-To: <CAFXO6=JzsYgiXJE2geSKMpfgPo+GGNX_+Pw0JQx1QQxAfhCdBQ@mail.gmail.com>
References: <CAFXO6=+cHyQKM2n9yn4EhwLZO+AUB0ZD81qWPxmpN27rjUoU3w@mail.gmail.com>
 <CALZpt+HpvmEHUEOgye34T6pVQ+wnKKn-_8cTJTQXYQb9t1jOTA@mail.gmail.com>
 <CAFXO6=JzsYgiXJE2geSKMpfgPo+GGNX_+Pw0JQx1QQxAfhCdBQ@mail.gmail.com>
Message-ID: <CALZpt+HQpdrebhWGXv_yLqiSCB5Ur71L1K13bd7w5TZb9DwJEQ@mail.gmail.com>

> Correct, if B+C is too low feerate to be accepted, we will reject it. I
> prefer this because it is incentive compatible: A can be mined by itself,
> so there's no reason to prefer A+B+C instead of A.
> As another way of looking at this, consider the case where we do accept
> A+B+C and it sits at the "bottom" of our mempool. If our mempool reaches
> capacity, we evict the lowest descendant feerate transactions, which are
> B+C in this case. This gives us the same resulting mempool, with A and not
> B+C.

I agree here. Doing otherwise, we might evict other transactions mempool in
`MempoolAccept::Finalize` with a higher-feerate than B+C while those
evicted transactions are the most compelling for block construction.

I thought at first missing this acceptance requirement would break a
fee-bumping scheme like Parent-Pay-For-Child where a high-fee parent is
attached to a child signed with SIGHASH_ANYONECANPAY but in this case the
child fee is capturing the parent value. I can't think of other fee-bumping
schemes potentially affected. If they do exist I would say they're wrong in
their design assumptions.

> If or when we have witness replacement, the logic is: if the individual
> transaction is enough to replace the mempool one, the replacement will
> happen during the preceding individual transaction acceptance, and
> deduplication logic will work. Otherwise, we will try to deduplicate by
> wtxid, see that we need a package witness replacement, and use the package
> feerate to evaluate whether this is economically rational.

IIUC, you have package A+B, during the dedup phase early in
`AcceptMultipleTransactions` if you observe same-txid-different-wtixd A'
and A' is higher feerate than A, you trim A and replace by A' ?

I think this approach is safe, the one who appears unsafe to me is when A'
has a _lower_ feerate, even if A' is already accepted by our mempool ? In
that case iirc that would be a pinning.

Good to see progress on witness replacement before we see usage of Taproot
tree in the context of multi-party, where a malicious counterparty inflates
its witness to jam a honest spending.

(Note, the commit linked currently points nowhere :))


> Please note that A may replace A' even if A' has higher fees than A
> individually, because the proposed package RBF utilizes the fees and size
> of the entire package. This just requires E to pay enough fees, although
> this can be pretty high if there are also potential B' and C' competing
> commitment transactions that we don't know about.

Ah right, if the package acceptance waives `PaysMoreThanConflicts` for the
individual check on A, the honest package should replace the pinning
attempt. I've not fully parsed the proposed implementation yet.

Though note, I think it's still unsafe for a Lightning
multi-commitment-broadcast-as-one-package as a malicious A' might have an
absolute fee higher than E. It sounds uneconomical for
an attacker but I think it's not when you consider than you can "batch"
attack against multiple honest counterparties. E.g, Mallory broadcast A' +
B' + C' + D' where A' conflicts with Alice's honest package P1, B'
conflicts with Bob's honest package P2, C' conflicts with Caroll's honest
package P3. And D' is a high-fee child of A' + B' + C'.

If D' is higher-fee than P1 or P2 or P3 but inferior to the sum of HTLCs
confirmed by P1+P2+P3, I think it's lucrative for the attacker ?

> So far, my understanding is that multi-parent-1-child is desired for
> batched fee-bumping (
> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289) and
> I've also seen your response which I have less context on (
> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202).
That
> being said, I am happy to create a new proposal for 1 parent + 1 child
> (which would be slightly simpler) and plan for moving to
> multi-parent-1-child later if that is preferred. I am very interested in
> hearing feedback on that approach.

I think batched fee-bumping is okay as long as you don't have
time-sensitive outputs encumbering your commitment transactions. For the
reasons mentioned above, I think that's unsafe.

What I'm worried about is  L2 developers, potentially not aware about all
the mempool subtleties blurring the difference and always batching their
broadcast by default.

IMO, a good thing by restraining to 1-parent + 1 child,  we artificially
constraint L2 design space for now and minimize risks of unsafe usage of
the package API :)

I think that's a point where it would be relevant to have the opinion of
more L2 devs.

> I think there is a misunderstanding here - let me describe what I'm
> proposing we'd do in this situation: we'll try individual submission for
A,
> see that it fails due to "insufficient fees." Then, we'll try package
> validation for A+B and use package RBF. If A+B pays enough, it can still
> replace A'. If A fails for a bad signature, we won't look at B or A+B.
Does
> this meet your expectations?

Yes there was a misunderstanding, I think this approach is correct, it's
more a question of performance. Do we assume that broadcasted packages are
"honest" by default and that the parent(s) always need the child to pass
the fee checks, that way saving the processing of individual transactions
which are expected to fail in 99% of cases or more ad hoc composition of
packages at relay ?

I think this point is quite dependent on the p2p packages format/logic
we'll end up on and that we should feel free to revisit it later ?


> What problem are you trying to solve by the package feerate *after* dedup
rule ?
> My understanding is that an in-package transaction might be already in
the mempool. Therefore, to compute a correct RBF penalty replacement, the
vsize of this transaction could be discarded lowering the cost of package
RBF.

> I'm proposing that, when a transaction has already been submitted to
> mempool, we would ignore both its fees and vsize when calculating package
> feerate.

Yes, if you receive A+B, and A is already in-mempoo, I agree you can
discard its feerate as B should pay for all fees checked on its own. Where
I'm unclear is when you have in-mempool A+B and receive A+B'. Should B'
have a fee high enough to cover the bandwidth penalty replacement
(`PaysForRBF`, 2nd check) of both A+B' or only B' ?

If you have a second-layer like current Lightning, you might have a
counterparty commitment to replace and should always expect to have to pay
for parent replacement bandwidth.

Where a potential discount sounds interesting is when you have an univoque
state on the first-stage of transactions. E.g DLC's funding transaction
which might be CPFP by any participant iirc.

> Note that, if C' conflicts with C, it also conflicts with D, since D is a
> descendant of C and would thus need to be evicted along with it.

Ah once again I think it's a misunderstanding without the code under my
eyes! If we do C' `PreChecks`, solve the conflicts provoked by it, i.e mark
for potential eviction D and don't consider it for future conflicts in the
rest of the package, I think D' `PreChecks` should be good ?

> More generally, this example is surprising to me because I didn't think
> packages would be used to fee-bump replaceable transactions. Do we want
the
> child to be able to replace mempool transactions as well?

If we mean when you have replaceable A+B then A'+B' try to replace with a
higher-feerate ? I think that's exactly the case we need for Lightning as
A+B is coming from Alice and A'+B' is coming from Bob :/

> I'm not sure what you mean? Let's say we have a package of parent A +
child
> B, where A is supposed to replace a mempool transaction A'. Are you saying
> that counterparties are able to malleate the package child B, or a child
of
> A'?

The second option, a child of A', In the LN case I think the CPFP is
attached on one's anchor output.

I think it's good if we assume the
solve-conflicts-after-parent's`'PreChecks` mentioned above or fixing
inherited signaling or full-rbf ?

> Sorry, I don't understand what you mean by "preserve the package
> integrity?" Could you elaborate?

After thinking the relaxation about the "new" unconfirmed input is not
linked to trimming but I would say more to the multi-parent support.

Let's say you have A+B trying to replace C+D where B is also spending
already in-mempool E. To succeed, you need to waive the no-new-unconfirmed
input as D isn't spending E.

So good, I think we agree on the problem description here.

> I am in agreement with your calculations but unsure if we disagree on the
> expected outcome. Yes, B has an ancestor score of 10sat/vb and D has an
> ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than B's,
> it fails the proposed package RBF Rule #2, so this package would be
> rejected. Does this meet your expectations?

Well what sounds odd to me, in my example, we fail D even if it has a
higher-fee than B. Like A+B absolute fees are 2000 sats and A+C+D absolute
fees are 4500 sats ?

Is this compatible with a model where a miner prioritizes absolute fees
over ancestor score, in the case that mempools aren't full-enough to
fulfill a block ?

Let me know if I can clarify a point.

Antoine

Le lun. 20 sept. 2021 ? 11:10, Gloria Zhao <gloriajzhao at gmail.com> a ?crit :

>
> Hi Antoine,
>
> First of all, thank you for the thorough review. I appreciate your insight
> on LN requirements.
>
> > IIUC, you have a package A+B+C submitted for acceptance and A is already
> in your mempool. You trim out A from the package and then evaluate B+C.
>
> > I think this might be an issue if A is the higher-fee element of the ABC
> package. B+C package fees might be under the mempool min fee and will be
> rejected, potentially breaking the acceptance expectations of the package
> issuer ?
>
> Correct, if B+C is too low feerate to be accepted, we will reject it. I
> prefer this because it is incentive compatible: A can be mined by itself,
> so there's no reason to prefer A+B+C instead of A.
> As another way of looking at this, consider the case where we do accept
> A+B+C and it sits at the "bottom" of our mempool. If our mempool reaches
> capacity, we evict the lowest descendant feerate transactions, which are
> B+C in this case. This gives us the same resulting mempool, with A and not
> B+C.
>
>
> > Further, I think the dedup should be done on wtxid, as you might have
> multiple valid witnesses. Though with varying vsizes and as such offering
> different feerates.
>
> I agree that variations of the same package with different witnesses is a
> case that must be handled. I consider witness replacement to be a project
> that can be done in parallel to package mempool acceptance because being
> able to accept packages does not worsen the problem of a
> same-txid-different-witness "pinning" attack.
>
> If or when we have witness replacement, the logic is: if the individual
> transaction is enough to replace the mempool one, the replacement will
> happen during the preceding individual transaction acceptance, and
> deduplication logic will work. Otherwise, we will try to deduplicate by
> wtxid, see that we need a package witness replacement, and use the package
> feerate to evaluate whether this is economically rational.
>
> See the #22290 "handle package transactions already in mempool" commit (
> https://github.com/bitcoin/bitcoin/pull/22290/commits/fea75a2237b46cf76145242fecad7e274bfcb5ff),
> which handles the case of same-txid-different-witness by simply using the
> transaction in the mempool for now, with TODOs for what I just described.
>
>
> > I'm not clearly understanding the accepted topologies. By "parent and
> child to share a parent", do you mean the set of transactions A, B, C,
> where B is spending A and C is spending A and B would be correct ?
>
> Yes, that is what I meant. Yes, that would a valid package under these
> rules.
>
> > If yes, is there a width-limit introduced or we fallback on
> MAX_PACKAGE_COUNT=25 ?
>
> No, there is no limit on connectivity other than "child with all
> unconfirmed parents." We will enforce MAX_PACKAGE_COUNT=25 and child's
> in-mempool + in-package ancestor limits.
>
>
> > Considering the current Core's mempool acceptance rules, I think CPFP
> batching is unsafe for LN time-sensitive closure. A malicious tx-relay
> jamming successful on one channel commitment transaction would contamine
> the remaining commitments sharing the same package.
>
> > E.g, you broadcast the package A+B+C+D+E where A,B,C,D are commitment
> transactions and E a shared CPFP. If a malicious A' transaction has a
> better feerate than A, the whole package acceptance will fail. Even if A'
> confirms in the following block,
> the propagation and confirmation of B+C+D have been delayed. This could
> carry on a loss of funds.
>
> Please note that A may replace A' even if A' has higher fees than A
> individually, because the proposed package RBF utilizes the fees and size
> of the entire package. This just requires E to pay enough fees, although
> this can be pretty high if there are also potential B' and C' competing
> commitment transactions that we don't know about.
>
>
> > IMHO, I'm leaning towards deploying during a first phase
> 1-parent/1-child. I think it's the most conservative step still improving
> second-layer safety.
>
> So far, my understanding is that multi-parent-1-child is desired for
> batched fee-bumping (
> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289) and
> I've also seen your response which I have less context on (
> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202).
> That being said, I am happy to create a new proposal for 1 parent + 1 child
> (which would be slightly simpler) and plan for moving to
> multi-parent-1-child later if that is preferred. I am very interested in
> hearing feedback on that approach.
>
>
> > If A+B is submitted to replace A', where A pays 0 sats, B pays 200 sats
> and A' pays 100 sats. If we apply the individual RBF on A, A+B acceptance
> fails. For this reason I think the individual RBF should be bypassed and
> only the package RBF apply ?
>
> I think there is a misunderstanding here - let me describe what I'm
> proposing we'd do in this situation: we'll try individual submission for A,
> see that it fails due to "insufficient fees." Then, we'll try package
> validation for A+B and use package RBF. If A+B pays enough, it can still
> replace A'. If A fails for a bad signature, we won't look at B or A+B. Does
> this meet your expectations?
>
>
> > What problem are you trying to solve by the package feerate *after*
> dedup rule ?
> > My understanding is that an in-package transaction might be already in
> the mempool. Therefore, to compute a correct RBF penalty replacement, the
> vsize of this transaction could be discarded lowering the cost of package
> RBF.
>
> I'm proposing that, when a transaction has already been submitted to
> mempool, we would ignore both its fees and vsize when calculating package
> feerate. In example G2, we shouldn't count M1 fees after its submission to
> mempool, since M1's fees have already been used to pay for its individual
> bandwidth, and it shouldn't be used again to pay for P2 and P3's bandwidth.
> We also shouldn't count its vsize, since it has already been paid for.
>
>
> > I think this is a footgunish API, as if a package issuer send the
> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.
> Then try to broadcast the higher-feerate C'+D' package, it should be
> rejected. So it's breaking the naive broadcaster assumption that a
> higher-feerate/higher-fee package always replaces ?
>
> Note that, if C' conflicts with C, it also conflicts with D, since D is a
> descendant of C and would thus need to be evicted along with it.
> Implicitly, D' would not be in conflict with D.
> More generally, this example is surprising to me because I didn't think
> packages would be used to fee-bump replaceable transactions. Do we want the
> child to be able to replace mempool transactions as well? This can be
> implemented with a bit of additional logic.
>
> > I think this is unsafe for L2s if counterparties have malleability of
> the child transaction. They can block your package replacement by
> opting-out from RBF signaling. IIRC, LN's "anchor output" presents such an
> ability.
>
> I'm not sure what you mean? Let's say we have a package of parent A +
> child B, where A is supposed to replace a mempool transaction A'. Are you
> saying that counterparties are able to malleate the package child B, or a
> child of A'? If they can malleate a child of A', that shouldn't matter as
> long as A' is signaling replacement. This would be handled identically with
> full RBF and what Core currently implements.
>
> > I think this is an issue brought by the trimming during the dedup phase.
> If we preserve the package integrity, only re-using the tx-level checks
> results of already in-mempool transactions to gain in CPU time we won't
> have this issue. Package childs can add unconfirmed inputs as long as
> they're in-package, the bip125 rule2 is only evaluated against parents ?
>
> Sorry, I don't understand what you mean by "preserve the package
> integrity?" Could you elaborate?
>
> > Let's say you have in-mempool A, B where A pays 10 sat/vb for 100 vbytes
> and B pays 10 sat/vb for 100 vbytes. You have the candidate replacement D
> spending both A and C where D pays 15sat/vb for 100 vbytes and C pays 1
> sat/vb for 1000 vbytes.
>
> > Package A + B ancestor score is 10 sat/vb.
>
> > D has a higher feerate/absolute fee than B.
>
> > Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's
> 1000 sats + D's 1500 sats) / A's 100 vb + C's 1000 vb + D's 100 vb)
>
> I am in agreement with your calculations but unsure if we disagree on the
> expected outcome. Yes, B has an ancestor score of 10sat/vb and D has an
> ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than B's,
> it fails the proposed package RBF Rule #2, so this package would be
> rejected. Does this meet your expectations?
>
> Thank you for linking to projects that might be interested in package
> relay :)
>
> Thanks,
> Gloria
>
> On Mon, Sep 20, 2021 at 12:16 AM Antoine Riard <antoine.riard at gmail.com>
> wrote:
>
>> Hi Gloria,
>>
>> > A package may contain transactions that are already in the mempool. We
>> > remove
>> > ("deduplicate") those transactions from the package for the purposes of
>> > package
>> > mempool acceptance. If a package is empty after deduplication, we do
>> > nothing.
>>
>> IIUC, you have a package A+B+C submitted for acceptance and A is already
>> in your mempool. You trim out A from the package and then evaluate B+C.
>>
>> I think this might be an issue if A is the higher-fee element of the ABC
>> package. B+C package fees might be under the mempool min fee and will be
>> rejected, potentially breaking the acceptance expectations of the package
>> issuer ?
>>
>> Further, I think the dedup should be done on wtxid, as you might have
>> multiple valid witnesses. Though with varying vsizes and as such offering
>> different feerates.
>>
>> E.g you're going to evaluate the package A+B and A' is already in your
>> mempool with a bigger valid witness. You trim A based on txid, then you
>> evaluate A'+B, which fails the fee checks. However, evaluating A+B would
>> have been a success.
>>
>> AFAICT, the dedup rationale would be to save on CPU time/IO disk, to
>> avoid repeated signatures verification and parent UTXOs fetches ? Can we
>> achieve the same goal by bypassing tx-level checks for already-in txn while
>> conserving the package integrity for package-level checks ?
>>
>> > Note that it's possible for the parents to be
>> > indirect
>> > descendants/ancestors of one another, or for parent and child to share a
>> > parent,
>> > so we cannot make any other topology assumptions.
>>
>> I'm not clearly understanding the accepted topologies. By "parent and
>> child to share a parent", do you mean the set of transactions A, B, C,
>> where B is spending A and C is spending A and B would be correct ?
>>
>> If yes, is there a width-limit introduced or we fallback on
>> MAX_PACKAGE_COUNT=25 ?
>>
>> IIRC, one rationale to come with this topology limitation was to lower
>> the DoS risks when potentially deploying p2p packages.
>>
>> Considering the current Core's mempool acceptance rules, I think CPFP
>> batching is unsafe for LN time-sensitive closure. A malicious tx-relay
>> jamming successful on one channel commitment transaction would contamine
>> the remaining commitments sharing the same package.
>>
>> E.g, you broadcast the package A+B+C+D+E where A,B,C,D are commitment
>> transactions and E a shared CPFP. If a malicious A' transaction has a
>> better feerate than A, the whole package acceptance will fail. Even if A'
>> confirms in the following block,
>> the propagation and confirmation of B+C+D have been delayed. This could
>> carry on a loss of funds.
>>
>> That said, if you're broadcasting commitment transactions without
>> time-sensitive HTLC outputs, I think the batching is effectively a fee
>> saving as you don't have to duplicate the CPFP.
>>
>> IMHO, I'm leaning towards deploying during a first phase
>> 1-parent/1-child. I think it's the most conservative step still improving
>> second-layer safety.
>>
>> > *Rationale*:  It would be incorrect to use the fees of transactions
>> that are
>> > already in the mempool, as we do not want a transaction's fees to be
>> > double-counted for both its individual RBF and package RBF.
>>
>> I'm unsure about the logical order of the checks proposed.
>>
>> If A+B is submitted to replace A', where A pays 0 sats, B pays 200 sats
>> and A' pays 100 sats. If we apply the individual RBF on A, A+B acceptance
>> fails. For this reason I think the individual RBF should be bypassed and
>> only the package RBF apply ?
>>
>> Note this situation is plausible, with current LN design, your
>> counterparty can have a commitment transaction with a better fee just by
>> selecting a higher `dust_limit_satoshis` than yours.
>>
>> > Examples F and G [14] show the same package, but P1 is submitted
>> > individually before
>> > the package in example G. In example F, we can see that the 300vB
>> package
>> > pays
>> > an additional 200sat in fees, which is not enough to pay for its own
>> > bandwidth
>> > (BIP125#4). In example G, we can see that P1 pays enough to replace M1,
>> but
>> > using P1's fees again during package submission would make it look like
>> a
>> > 300sat
>> > increase for a 200vB package. Even including its fees and size would
>> not be
>> > sufficient in this example, since the 300sat looks like enough for the
>> 300vB
>> > package. The calculcation after deduplication is 100sat increase for a
>> > package
>> > of size 200vB, which correctly fails BIP125#4. Assume all transactions
>> have
>> > a
>> > size of 100vB.
>>
>> What problem are you trying to solve by the package feerate *after* dedup
>> rule ?
>>
>> My understanding is that an in-package transaction might be already in
>> the mempool. Therefore, to compute a correct RBF penalty replacement, the
>> vsize of this transaction could be discarded lowering the cost of package
>> RBF.
>>
>> If we keep a "safe" dedup mechanism (see my point above), I think this
>> discount is justified, as the validation cost of node operators is paid for
>> ?
>>
>> > The child cannot replace mempool transactions.
>>
>> Let's say you issue package A+B, then package C+B', where B' is a child
>> of both A and C. This rule fails the acceptance of C+B' ?
>>
>> I think this is a footgunish API, as if a package issuer send the
>> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.
>> Then try to broadcast the higher-feerate C'+D' package, it should be
>> rejected. So it's breaking the naive broadcaster assumption that a
>> higher-feerate/higher-fee package always replaces ? And it might be unsafe
>> in protocols where states are symmetric. E.g a malicious counterparty
>> broadcasts first S+A, then you honestly broadcast S+B, where B pays better
>> fees.
>>
>> > All mempool transactions to be replaced must signal replaceability.
>>
>> I think this is unsafe for L2s if counterparties have malleability of the
>> child transaction. They can block your package replacement by opting-out
>> from RBF signaling. IIRC, LN's "anchor output" presents such an ability.
>>
>> I think it's better to either fix inherited signaling or move towards
>> full-rbf.
>>
>> > if a package parent has already been submitted, it would
>> > look
>> >like the child is spending a "new" unconfirmed input.
>>
>> I think this is an issue brought by the trimming during the dedup phase.
>> If we preserve the package integrity, only re-using the tx-level checks
>> results of already in-mempool transactions to gain in CPU time we won't
>> have this issue. Package childs can add unconfirmed inputs as long as
>> they're in-package, the bip125 rule2 is only evaluated against parents ?
>>
>> > However, we still achieve the same goal of requiring the
>> > replacement
>> > transactions to have a ancestor score at least as high as the original
>> > ones.
>>
>> I'm not sure if this holds...
>>
>> Let's say you have in-mempool A, B where A pays 10 sat/vb for 100 vbytes
>> and B pays 10 sat/vb for 100 vbytes. You have the candidate replacement D
>> spending both A and C where D pays 15sat/vb for 100 vbytes and C pays 1
>> sat/vb for 1000 vbytes.
>>
>> Package A + B ancestor score is 10 sat/vb.
>>
>> D has a higher feerate/absolute fee than B.
>>
>> Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's 1000
>> sats + D's 1500 sats) /
>> A's 100 vb + C's 1000 vb + D's 100 vb)
>>
>> Overall, this is a review through the lenses of LN requirements. I think
>> other L2 protocols/applications
>> could be candidates to using package accept/relay such as:
>> * https://github.com/lightninglabs/pool
>> * https://github.com/discreetlogcontracts/dlcspecs
>> * https://github.com/bitcoin-teleport/teleport-transactions/
>> * https://github.com/sapio-lang/sapio
>> * https://github.com/commerceblock/mercury/blob/master/doc/statechains.md
>> * https://github.com/revault/practical-revault
>>
>> Thanks for rolling forward the ball on this subject.
>>
>> Antoine
>>
>> Le jeu. 16 sept. 2021 ? 03:55, Gloria Zhao via bitcoin-dev <
>> bitcoin-dev at lists.linuxfoundation.org> a ?crit :
>>
>>> Hi there,
>>>
>>> I'm writing to propose a set of mempool policy changes to enable package
>>> validation (in preparation for package relay) in Bitcoin Core. These
>>> would not
>>> be consensus or P2P protocol changes. However, since mempool policy
>>> significantly affects transaction propagation, I believe this is
>>> relevant for
>>> the mailing list.
>>>
>>> My proposal enables packages consisting of multiple parents and 1 child.
>>> If you
>>> develop software that relies on specific transaction relay assumptions
>>> and/or
>>> are interested in using package relay in the future, I'm very interested
>>> to hear
>>> your feedback on the utility or restrictiveness of these package
>>> policies for
>>> your use cases.
>>>
>>> A draft implementation of this proposal can be found in [Bitcoin Core
>>> PR#22290][1].
>>>
>>> An illustrated version of this post can be found at
>>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.
>>> I have also linked the images below.
>>>
>>> ## Background
>>>
>>> Feel free to skip this section if you are already familiar with mempool
>>> policy
>>> and package relay terminology.
>>>
>>> ### Terminology Clarifications
>>>
>>> * Package = an ordered list of related transactions, representable by a
>>> Directed
>>>   Acyclic Graph.
>>> * Package Feerate = the total modified fees divided by the total virtual
>>> size of
>>>   all transactions in the package.
>>>     - Modified fees = a transaction's base fees + fee delta applied by
>>> the user
>>>       with `prioritisetransaction`. As such, we expect this to vary
>>> across
>>> mempools.
>>>     - Virtual Size = the maximum of virtual sizes calculated using
>>> [BIP141
>>>       virtual size][2] and sigop weight. [Implemented here in Bitcoin
>>> Core][3].
>>>     - Note that feerate is not necessarily based on the base fees and
>>> serialized
>>>       size.
>>>
>>> * Fee-Bumping = user/wallet actions that take advantage of miner
>>> incentives to
>>>   boost a transaction's candidacy for inclusion in a block, including
>>> Child Pays
>>> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our intention in
>>> mempool policy is to recognize when the new transaction is more
>>> economical to
>>> mine than the original one(s) but not open DoS vectors, so there are some
>>> limitations.
>>>
>>> ### Policy
>>>
>>> The purpose of the mempool is to store the best (to be most
>>> incentive-compatible
>>> with miners, highest feerate) candidates for inclusion in a block.
>>> Miners use
>>> the mempool to build block templates. The mempool is also useful as a
>>> cache for
>>> boosting block relay and validation performance, aiding transaction
>>> relay, and
>>> generating feerate estimations.
>>>
>>> Ideally, all consensus-valid transactions paying reasonable fees should
>>> make it
>>> to miners through normal transaction relay, without any special
>>> connectivity or
>>> relationships with miners. On the other hand, nodes do not have unlimited
>>> resources, and a P2P network designed to let any honest node broadcast
>>> their
>>> transactions also exposes the transaction validation engine to DoS
>>> attacks from
>>> malicious peers.
>>>
>>> As such, for unconfirmed transactions we are considering for our
>>> mempool, we
>>> apply a set of validation rules in addition to consensus, primarily to
>>> protect
>>> us from resource exhaustion and aid our efforts to keep the highest fee
>>> transactions. We call this mempool _policy_: a set of (configurable,
>>> node-specific) rules that transactions must abide by in order to be
>>> accepted
>>> into our mempool. Transaction "Standardness" rules and mempool
>>> restrictions such
>>> as "too-long-mempool-chain" are both examples of policy.
>>>
>>> ### Package Relay and Package Mempool Accept
>>>
>>> In transaction relay, we currently consider transactions one at a time
>>> for
>>> submission to the mempool. This creates a limitation in the node's
>>> ability to
>>> determine which transactions have the highest feerates, since we cannot
>>> take
>>> into account descendants (i.e. cannot use CPFP) until all the
>>> transactions are
>>> in the mempool. Similarly, we cannot use a transaction's descendants when
>>> considering it for RBF. When an individual transaction does not meet the
>>> mempool
>>> minimum feerate and the user isn't able to create a replacement
>>> transaction
>>> directly, it will not be accepted by mempools.
>>>
>>> This limitation presents a security issue for applications and users
>>> relying on
>>> time-sensitive transactions. For example, Lightning and other protocols
>>> create
>>> UTXOs with multiple spending paths, where one counterparty's spending
>>> path opens
>>> up after a timelock, and users are protected from cheating scenarios as
>>> long as
>>> they redeem on-chain in time. A key security assumption is that all
>>> parties'
>>> transactions will propagate and confirm in a timely manner. This
>>> assumption can
>>> be broken if fee-bumping does not work as intended.
>>>
>>> The end goal for Package Relay is to consider multiple transactions at
>>> the same
>>> time, e.g. a transaction with its high-fee child. This may help us better
>>> determine whether transactions should be accepted to our mempool,
>>> especially if
>>> they don't meet fee requirements individually or are better RBF
>>> candidates as a
>>> package. A combination of changes to mempool validation logic, policy,
>>> and
>>> transaction relay allows us to better propagate the transactions with the
>>> highest package feerates to miners, and makes fee-bumping tools more
>>> powerful
>>> for users.
>>>
>>> The "relay" part of Package Relay suggests P2P messaging changes, but a
>>> large
>>> part of the changes are in the mempool's package validation logic. We
>>> call this
>>> *Package Mempool Accept*.
>>>
>>> ### Previous Work
>>>
>>> * Given that mempool validation is DoS-sensitive and complex, it would be
>>>   dangerous to haphazardly tack on package validation logic. Many
>>> efforts have
>>> been made to make mempool validation less opaque (see [#16400][4],
>>> [#21062][5],
>>> [#22675][6], [#22796][7]).
>>> * [#20833][8] Added basic capabilities for package validation, test
>>> accepts only
>>>   (no submission to mempool).
>>> * [#21800][9] Implemented package ancestor/descendant limit checks for
>>> arbitrary
>>>   packages. Still test accepts only.
>>> * Previous package relay proposals (see [#16401][10], [#19621][11]).
>>>
>>> ### Existing Package Rules
>>>
>>> These are in master as introduced in [#20833][8] and [#21800][9]. I'll
>>> consider
>>> them as "given" in the rest of this document, though they can be
>>> changed, since
>>> package validation is test-accept only right now.
>>>
>>> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and
>>> `MAX_PACKAGE_SIZE=101KvB` total size [8]
>>>
>>>    *Rationale*: This is already enforced as mempool ancestor/descendant
>>> limits.
>>> Presumably, transactions in a package are all related, so exceeding this
>>> limit
>>> would mean that the package can either be split up or it wouldn't pass
>>> this
>>> mempool policy.
>>>
>>> 2. Packages must be topologically sorted: if any dependencies exist
>>> between
>>> transactions, parents must appear somewhere before children. [8]
>>>
>>> 3. A package cannot have conflicting transactions, i.e. none of them can
>>> spend
>>> the same inputs. This also means there cannot be duplicate transactions.
>>> [8]
>>>
>>> 4. When packages are evaluated against ancestor/descendant limits in a
>>> test
>>> accept, the union of all of their descendants and ancestors is
>>> considered. This
>>> is essentially a "worst case" heuristic where every transaction in the
>>> package
>>> is treated as each other's ancestor and descendant. [8]
>>> Packages for which ancestor/descendant limits are accurately captured by
>>> this
>>> heuristic: [19]
>>>
>>> There are also limitations such as the fact that CPFP carve out is not
>>> applied
>>> to package transactions. #20833 also disables RBF in package validation;
>>> this
>>> proposal overrides that to allow packages to use RBF.
>>>
>>> ## Proposed Changes
>>>
>>> The next step in the Package Mempool Accept project is to implement
>>> submission
>>> to mempool, initially through RPC only. This allows us to test the
>>> submission
>>> logic before exposing it on P2P.
>>>
>>> ### Summary
>>>
>>> - Packages may contain already-in-mempool transactions.
>>> - Packages are 2 generations, Multi-Parent-1-Child.
>>> - Fee-related checks use the package feerate. This means that wallets can
>>> create a package that utilizes CPFP.
>>> - Parents are allowed to RBF mempool transactions with a set of rules
>>> similar
>>>   to BIP125. This enables a combination of CPFP and RBF, where a
>>> transaction's descendant fees pay for replacing mempool conflicts.
>>>
>>> There is a draft implementation in [#22290][1]. It is WIP, but feedback
>>> is
>>> always welcome.
>>>
>>> ### Details
>>>
>>> #### Packages May Contain Already-in-Mempool Transactions
>>>
>>> A package may contain transactions that are already in the mempool. We
>>> remove
>>> ("deduplicate") those transactions from the package for the purposes of
>>> package
>>> mempool acceptance. If a package is empty after deduplication, we do
>>> nothing.
>>>
>>> *Rationale*: Mempools vary across the network. It's possible for a
>>> parent to be
>>> accepted to the mempool of a peer on its own due to differences in
>>> policy and
>>> fee market fluctuations. We should not reject or penalize the entire
>>> package for
>>> an individual transaction as that could be a censorship vector.
>>>
>>> #### Packages Are Multi-Parent-1-Child
>>>
>>> Only packages of a specific topology are permitted. Namely, a package is
>>> exactly
>>> 1 child with all of its unconfirmed parents. After deduplication, the
>>> package
>>> may be exactly the same, empty, 1 child, 1 child with just some of its
>>> unconfirmed parents, etc. Note that it's possible for the parents to be
>>> indirect
>>> descendants/ancestors of one another, or for parent and child to share a
>>> parent,
>>> so we cannot make any other topology assumptions.
>>>
>>> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple
>>> parents
>>> makes it possible to fee-bump a batch of transactions. Restricting
>>> packages to a
>>> defined topology is also easier to reason about and simplifies the
>>> validation
>>> logic greatly. Multi-parent-1-child allows us to think of the package as
>>> one big
>>> transaction, where:
>>>
>>> - Inputs = all the inputs of parents + inputs of the child that come from
>>>   confirmed UTXOs
>>> - Outputs = all the outputs of the child + all outputs of the parents
>>> that
>>>   aren't spent by other transactions in the package
>>>
>>> Examples of packages that follow this rule (variations of example A show
>>> some
>>> possibilities after deduplication): ![image][15]
>>>
>>> #### Fee-Related Checks Use Package Feerate
>>>
>>> Package Feerate = the total modified fees divided by the total virtual
>>> size of
>>> all transactions in the package.
>>>
>>> To meet the two feerate requirements of a mempool, i.e., the
>>> pre-configured
>>> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum
>>> feerate, the
>>> total package feerate is used instead of the individual feerate. The
>>> individual
>>> transactions are allowed to be below feerate requirements if the package
>>> meets
>>> the feerate requirements. For example, the parent(s) in the package can
>>> have 0
>>> fees but be paid for by the child.
>>>
>>> *Rationale*: This can be thought of as "CPFP within a package," solving
>>> the
>>> issue of a parent not meeting minimum fees on its own. This allows L2
>>> applications to adjust their fees at broadcast time instead of
>>> overshooting or
>>> risking getting stuck/pinned.
>>>
>>> We use the package feerate of the package *after deduplication*.
>>>
>>> *Rationale*:  It would be incorrect to use the fees of transactions that
>>> are
>>> already in the mempool, as we do not want a transaction's fees to be
>>> double-counted for both its individual RBF and package RBF.
>>>
>>> Examples F and G [14] show the same package, but P1 is submitted
>>> individually before
>>> the package in example G. In example F, we can see that the 300vB
>>> package pays
>>> an additional 200sat in fees, which is not enough to pay for its own
>>> bandwidth
>>> (BIP125#4). In example G, we can see that P1 pays enough to replace M1,
>>> but
>>> using P1's fees again during package submission would make it look like
>>> a 300sat
>>> increase for a 200vB package. Even including its fees and size would not
>>> be
>>> sufficient in this example, since the 300sat looks like enough for the
>>> 300vB
>>> package. The calculcation after deduplication is 100sat increase for a
>>> package
>>> of size 200vB, which correctly fails BIP125#4. Assume all transactions
>>> have a
>>> size of 100vB.
>>>
>>> #### Package RBF
>>>
>>> If a package meets feerate requirements as a package, the parents in the
>>> transaction are allowed to replace-by-fee mempool transactions. The
>>> child cannot
>>> replace mempool transactions. Multiple transactions can replace the same
>>> transaction, but in order to be valid, none of the transactions can try
>>> to
>>> replace an ancestor of another transaction in the same package (which
>>> would thus
>>> make its inputs unavailable).
>>>
>>> *Rationale*: Even if we are using package feerate, a package will not
>>> propagate
>>> as intended if RBF still requires each individual transaction to meet the
>>> feerate requirements.
>>>
>>> We use a set of rules slightly modified from BIP125 as follows:
>>>
>>> ##### Signaling (Rule #1)
>>>
>>> All mempool transactions to be replaced must signal replaceability.
>>>
>>> *Rationale*: Package RBF signaling logic should be the same for package
>>> RBF and
>>> single transaction acceptance. This would be updated if single
>>> transaction
>>> validation moves to full RBF.
>>>
>>> ##### New Unconfirmed Inputs (Rule #2)
>>>
>>> A package may include new unconfirmed inputs, but the ancestor feerate
>>> of the
>>> child must be at least as high as the ancestor feerates of every
>>> transaction
>>> being replaced. This is contrary to BIP125#2, which states "The
>>> replacement
>>> transaction may only include an unconfirmed input if that input was
>>> included in
>>> one of the original transactions. (An unconfirmed input spends an output
>>> from a
>>> currently-unconfirmed transaction.)"
>>>
>>> *Rationale*: The purpose of BIP125#2 is to ensure that the replacement
>>> transaction has a higher ancestor score than the original transaction(s)
>>> (see
>>> [comment][13]). Example H [16] shows how adding a new unconfirmed input
>>> can lower the
>>> ancestor score of the replacement transaction. P1 is trying to replace
>>> M1, and
>>> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat, and
>>> M2 pays
>>> 100sat. Assume all transactions have a size of 100vB. While, in
>>> isolation, P1
>>> looks like a better mining candidate than M1, it must be mined with M2,
>>> so its
>>> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's ancestor
>>> feerate, which is 6sat/vB.
>>>
>>> In package RBF, the rule analogous to BIP125#2 would be "none of the
>>> transactions in the package can spend new unconfirmed inputs." Example J
>>> [17] shows
>>> why, if any of the package transactions have ancestors, package feerate
>>> is no
>>> longer accurate. Even though M2 and M3 are not ancestors of P1 (which is
>>> the
>>> replacement transaction in an RBF), we're actually interested in the
>>> entire
>>> package. A miner should mine M1 which is 5sat/vB instead of M2, M3, P1,
>>> P2, and
>>> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened to
>>> only allow
>>> the child to have new unconfirmed inputs, either, because it can still
>>> cause us
>>> to overestimate the package's ancestor score.
>>>
>>> However, enforcing a rule analogous to BIP125#2 would not only make
>>> Package RBF
>>> less useful, but would also break Package RBF for packages with parents
>>> already
>>> in the mempool: if a package parent has already been submitted, it would
>>> look
>>> like the child is spending a "new" unconfirmed input. In example K [18],
>>> we're
>>> looking to replace M1 with the entire package including P1, P2, and P3.
>>> We must
>>> consider the case where one of the parents is already in the mempool (in
>>> this
>>> case, P2), which means we must allow P3 to have new unconfirmed inputs.
>>> However,
>>> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not
>>> replace M1
>>> with this package.
>>>
>>> Thus, the package RBF rule regarding new unconfirmed inputs is less
>>> strict than
>>> BIP125#2. However, we still achieve the same goal of requiring the
>>> replacement
>>> transactions to have a ancestor score at least as high as the original
>>> ones. As
>>> a result, the entire package is required to be a higher feerate mining
>>> candidate
>>> than each of the replaced transactions.
>>>
>>> Another note: the [comment][13] above the BIP125#2 code in the original
>>> RBF
>>> implementation suggests that the rule was intended to be temporary.
>>>
>>> ##### Absolute Fee (Rule #3)
>>>
>>> The package must increase the absolute fee of the mempool, i.e. the
>>> total fees
>>> of the package must be higher than the absolute fees of the mempool
>>> transactions
>>> it replaces. Combined with the CPFP rule above, this differs from BIP125
>>> Rule #3
>>> - an individual transaction in the package may have lower fees than the
>>>   transaction(s) it is replacing. In fact, it may have 0 fees, and the
>>> child
>>> pays for RBF.
>>>
>>> ##### Feerate (Rule #4)
>>>
>>> The package must pay for its own bandwidth; the package feerate must be
>>> higher
>>> than the replaced transactions by at least minimum relay feerate
>>> (`incrementalRelayFee`). Combined with the CPFP rule above, this differs
>>> from
>>> BIP125 Rule #4 - an individual transaction in the package can have a
>>> lower
>>> feerate than the transaction(s) it is replacing. In fact, it may have 0
>>> fees,
>>> and the child pays for RBF.
>>>
>>> ##### Total Number of Replaced Transactions (Rule #5)
>>>
>>> The package cannot replace more than 100 mempool transactions. This is
>>> identical
>>> to BIP125 Rule #5.
>>>
>>> ### Expected FAQs
>>>
>>> 1. Is it possible for only some of the package to make it into the
>>> mempool?
>>>
>>>    Yes, it is. However, since we evict transactions from the mempool by
>>> descendant score and the package child is supposed to be sponsoring the
>>> fees of
>>> its parents, the most common scenario would be all-or-nothing. This is
>>> incentive-compatible. In fact, to be conservative, package validation
>>> should
>>> begin by trying to submit all of the transactions individually, and only
>>> use the
>>> package mempool acceptance logic if the parents fail due to low feerate.
>>>
>>> 2. Should we allow packages to contain already-confirmed transactions?
>>>
>>>     No, for practical reasons. In mempool validation, we actually aren't
>>> able to
>>> tell with 100% confidence if we are looking at a transaction that has
>>> already
>>> confirmed, because we look up inputs using a UTXO set. If we have
>>> historical
>>> block data, it's possible to look for it, but this is inefficient, not
>>> always
>>> possible for pruning nodes, and unnecessary because we're not going to do
>>> anything with the transaction anyway. As such, we already have the
>>> expectation
>>> that transaction relay is somewhat "stateful" i.e. nobody should be
>>> relaying
>>> transactions that have already been confirmed. Similarly, we shouldn't be
>>> relaying packages that contain already-confirmed transactions.
>>>
>>> [1]: https://github.com/bitcoin/bitcoin/pull/22290
>>> [2]:
>>> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations
>>> [3]:
>>> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282
>>> [4]: https://github.com/bitcoin/bitcoin/pull/16400
>>> [5]: https://github.com/bitcoin/bitcoin/pull/21062
>>> [6]: https://github.com/bitcoin/bitcoin/pull/22675
>>> [7]: https://github.com/bitcoin/bitcoin/pull/22796
>>> [8]: https://github.com/bitcoin/bitcoin/pull/20833
>>> [9]: https://github.com/bitcoin/bitcoin/pull/21800
>>> [10]: https://github.com/bitcoin/bitcoin/pull/16401
>>> [11]: https://github.com/bitcoin/bitcoin/pull/19621
>>> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki
>>> [13]:
>>> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104
>>> [14]:
>>> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png
>>> [15]:
>>> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png
>>> [16]:
>>> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png
>>> [17]:
>>> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png
>>> [18]:
>>> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png
>>> [19]:
>>> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png
>>> [20]:
>>> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev at lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210923/f8611b91/attachment-0001.html>

From gloriajzhao at gmail.com  Thu Sep 23 15:36:02 2021
From: gloriajzhao at gmail.com (Gloria Zhao)
Date: Thu, 23 Sep 2021 16:36:02 +0100
Subject: [bitcoin-dev] Proposal: Package Mempool Accept and Package RBF
In-Reply-To: <CALZpt+HQpdrebhWGXv_yLqiSCB5Ur71L1K13bd7w5TZb9DwJEQ@mail.gmail.com>
References: <CAFXO6=+cHyQKM2n9yn4EhwLZO+AUB0ZD81qWPxmpN27rjUoU3w@mail.gmail.com>
 <CALZpt+HpvmEHUEOgye34T6pVQ+wnKKn-_8cTJTQXYQb9t1jOTA@mail.gmail.com>
 <CAFXO6=JzsYgiXJE2geSKMpfgPo+GGNX_+Pw0JQx1QQxAfhCdBQ@mail.gmail.com>
 <CALZpt+HQpdrebhWGXv_yLqiSCB5Ur71L1K13bd7w5TZb9DwJEQ@mail.gmail.com>
Message-ID: <CAFXO6=Lvcr7Pwn_ZD1CZohYUFKY-cC5sGRxdTOiP2MgnTvFnYA@mail.gmail.com>

Hi Antoine,

Thanks as always for your input. I'm glad we agree on so much!

In summary, it seems that the decisions that might still need
attention/input from devs on this mailing list are:
1. Whether we should start with multiple-parent-1-child or 1-parent-1-child.
2. Whether it's ok to require that the child not have conflicts with
mempool transactions.

Responding to your comments...

> IIUC, you have package A+B, during the dedup phase early in
`AcceptMultipleTransactions` if you observe same-txid-different-wtixd A'
and A' is higher feerate than A, you trim A and replace by A' ?

> I think this approach is safe, the one who appears unsafe to me is when
A' has a _lower_ feerate, even if A' is already accepted by our mempool ?
In that case iirc that would be a pinning.

Right, the fact that we essentially always choose the first-seen witness is
an unfortunate limitation that exists already. Adding package mempool
accept doesn't worsen this, but the procedure in the future is to replace
the witness when it makes sense economically. We can also add logic to
allow package feerate to pay for witness replacements as well. This is
pretty far into the future, though.

> It sounds uneconomical for an attacker but I think it's not when you
consider than you can "batch" attack against multiple honest
counterparties. E.g, Mallory broadcast A' + B' + C' + D' where A' conflicts
with Alice's honest package P1, B' conflicts with Bob's honest package P2,
C' conflicts with Caroll's honest package P3. And D' is a high-fee child of
A' + B' + C'.

> If D' is higher-fee than P1 or P2 or P3 but inferior to the sum of HTLCs
confirmed by P1+P2+P3, I think it's lucrative for the attacker ?

I could be misunderstanding, but an attacker wouldn't be able to
batch-attack like this. Alice's package only conflicts with A' + D', not A'
+ B' + C' + D'. She only needs to pay for evicting 2 transactions.

> Do we assume that broadcasted packages are "honest" by default and that
the parent(s) always need the child to pass the fee checks, that way saving
the processing of individual transactions which are expected to fail in 99%
of cases or more ad hoc composition of packages at relay ?
> I think this point is quite dependent on the p2p packages format/logic
we'll end up on and that we should feel free to revisit it later ?

I think it's the opposite; there's no way for us to assume that p2p
packages will be "honest." I'd like to have two things before we expose on
P2P: (1) ensure that the amount of resources potentially allocated for
package validation isn't disproportionately higher than that of single
transaction validation and (2) only use package validation when we're
unsatisifed with the single validation result, e.g. we might get better
fees.
Yes, let's revisit this later :)

 > Yes, if you receive A+B, and A is already in-mempoo, I agree you can
discard its feerate as B should pay for all fees checked on its own. Where
I'm unclear is when you have in-mempool A+B and receive A+B'. Should B'
have a fee high enough to cover the bandwidth penalty replacement
(`PaysForRBF`, 2nd check) of both A+B' or only B' ?

 B' only needs to pay for itself in this case.

> > Do we want the child to be able to replace mempool transactions as well?

> If we mean when you have replaceable A+B then A'+B' try to replace with a
higher-feerate ? I think that's exactly the case we need for Lightning as
A+B is coming from Alice and A'+B' is coming from Bob :/

Let me clarify this because I can see that my wording was ambiguous, and
then please let me know if it fits Lightning's needs?

In my proposal, I wrote "If a package meets feerate requirements as a
package, the parents in the transaction are allowed to replace-by-fee
mempool transactions. The child cannot replace mempool transactions." What
I meant was: the package can replace mempool transactions if any of the
parents conflict with mempool transactions. The child cannot not conflict
with any mempool transactions.
The Lightning use case this attempts to address is: Alice and Mallory are
LN counterparties, and have packages A+B and A'+B', respectively. A and A'
are their commitment transactions and conflict with each other; they have
shared inputs and different txids.
B spends Alice's anchor output from A. B' spends Mallory's anchor output
from A'. Thus, B and B' do not conflict with each other.
Alice can broadcast her package, A+B, to replace Mallory's package, A'+B',
since B doesn't conflict with the mempool.

Would this be ok?

> The second option, a child of A', In the LN case I think the CPFP is
attached on one's anchor output.

While it would be nice to have full RBF, malleability of the child won't
block RBF here. If we're trying to replace A', we only require that A'
signals replaceability, and don't mind if its child doesn't.

> > B has an ancestor score of 10sat/vb and D has an
> > ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than
B's,
> > it fails the proposed package RBF Rule #2, so this package would be
> > rejected. Does this meet your expectations?

> Well what sounds odd to me, in my example, we fail D even if it has a
higher-fee than B. Like A+B absolute fees are 2000 sats and A+C+D absolute
fees are 4500 sats ?

Yes, A+C+D pays 2500sat more in fees, but it is also 1000vB larger. A miner
should prefer to utilize their block space more effectively.

> Is this compatible with a model where a miner prioritizes absolute fees
over ancestor score, in the case that mempools aren't full-enough to
fulfill a block ?

No, because we don't use that model.

Thanks,
Gloria

On Thu, Sep 23, 2021 at 5:29 AM Antoine Riard <antoine.riard at gmail.com>
wrote:

> > Correct, if B+C is too low feerate to be accepted, we will reject it. I
> > prefer this because it is incentive compatible: A can be mined by itself,
> > so there's no reason to prefer A+B+C instead of A.
> > As another way of looking at this, consider the case where we do accept
> > A+B+C and it sits at the "bottom" of our mempool. If our mempool reaches
> > capacity, we evict the lowest descendant feerate transactions, which are
> > B+C in this case. This gives us the same resulting mempool, with A and
> not
> > B+C.
>
> I agree here. Doing otherwise, we might evict other transactions mempool
> in `MempoolAccept::Finalize` with a higher-feerate than B+C while those
> evicted transactions are the most compelling for block construction.
>
> I thought at first missing this acceptance requirement would break a
> fee-bumping scheme like Parent-Pay-For-Child where a high-fee parent is
> attached to a child signed with SIGHASH_ANYONECANPAY but in this case the
> child fee is capturing the parent value. I can't think of other fee-bumping
> schemes potentially affected. If they do exist I would say they're wrong in
> their design assumptions.
>
> > If or when we have witness replacement, the logic is: if the individual
> > transaction is enough to replace the mempool one, the replacement will
> > happen during the preceding individual transaction acceptance, and
> > deduplication logic will work. Otherwise, we will try to deduplicate by
> > wtxid, see that we need a package witness replacement, and use the
> package
> > feerate to evaluate whether this is economically rational.
>
> IIUC, you have package A+B, during the dedup phase early in
> `AcceptMultipleTransactions` if you observe same-txid-different-wtixd A'
> and A' is higher feerate than A, you trim A and replace by A' ?
>
> I think this approach is safe, the one who appears unsafe to me is when A'
> has a _lower_ feerate, even if A' is already accepted by our mempool ? In
> that case iirc that would be a pinning.
>
> Good to see progress on witness replacement before we see usage of Taproot
> tree in the context of multi-party, where a malicious counterparty inflates
> its witness to jam a honest spending.
>
> (Note, the commit linked currently points nowhere :))
>
>
> > Please note that A may replace A' even if A' has higher fees than A
> > individually, because the proposed package RBF utilizes the fees and size
> > of the entire package. This just requires E to pay enough fees, although
> > this can be pretty high if there are also potential B' and C' competing
> > commitment transactions that we don't know about.
>
> Ah right, if the package acceptance waives `PaysMoreThanConflicts` for the
> individual check on A, the honest package should replace the pinning
> attempt. I've not fully parsed the proposed implementation yet.
>
> Though note, I think it's still unsafe for a Lightning
> multi-commitment-broadcast-as-one-package as a malicious A' might have an
> absolute fee higher than E. It sounds uneconomical for
> an attacker but I think it's not when you consider than you can "batch"
> attack against multiple honest counterparties. E.g, Mallory broadcast A' +
> B' + C' + D' where A' conflicts with Alice's honest package P1, B'
> conflicts with Bob's honest package P2, C' conflicts with Caroll's honest
> package P3. And D' is a high-fee child of A' + B' + C'.
>
> If D' is higher-fee than P1 or P2 or P3 but inferior to the sum of HTLCs
> confirmed by P1+P2+P3, I think it's lucrative for the attacker ?
>
> > So far, my understanding is that multi-parent-1-child is desired for
> > batched fee-bumping (
> > https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289)
> and
> > I've also seen your response which I have less context on (
> > https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202).
> That
> > being said, I am happy to create a new proposal for 1 parent + 1 child
> > (which would be slightly simpler) and plan for moving to
> > multi-parent-1-child later if that is preferred. I am very interested in
> > hearing feedback on that approach.
>
> I think batched fee-bumping is okay as long as you don't have
> time-sensitive outputs encumbering your commitment transactions. For the
> reasons mentioned above, I think that's unsafe.
>
> What I'm worried about is  L2 developers, potentially not aware about all
> the mempool subtleties blurring the difference and always batching their
> broadcast by default.
>
> IMO, a good thing by restraining to 1-parent + 1 child,  we artificially
> constraint L2 design space for now and minimize risks of unsafe usage of
> the package API :)
>
> I think that's a point where it would be relevant to have the opinion of
> more L2 devs.
>
> > I think there is a misunderstanding here - let me describe what I'm
> > proposing we'd do in this situation: we'll try individual submission for
> A,
> > see that it fails due to "insufficient fees." Then, we'll try package
> > validation for A+B and use package RBF. If A+B pays enough, it can still
> > replace A'. If A fails for a bad signature, we won't look at B or A+B.
> Does
> > this meet your expectations?
>
> Yes there was a misunderstanding, I think this approach is correct, it's
> more a question of performance. Do we assume that broadcasted packages are
> "honest" by default and that the parent(s) always need the child to pass
> the fee checks, that way saving the processing of individual transactions
> which are expected to fail in 99% of cases or more ad hoc composition of
> packages at relay ?
>
> I think this point is quite dependent on the p2p packages format/logic
> we'll end up on and that we should feel free to revisit it later ?
>
>
> > What problem are you trying to solve by the package feerate *after* dedup
> rule ?
> > My understanding is that an in-package transaction might be already in
> the mempool. Therefore, to compute a correct RBF penalty replacement, the
> vsize of this transaction could be discarded lowering the cost of package
> RBF.
>
> > I'm proposing that, when a transaction has already been submitted to
> > mempool, we would ignore both its fees and vsize when calculating package
> > feerate.
>
> Yes, if you receive A+B, and A is already in-mempoo, I agree you can
> discard its feerate as B should pay for all fees checked on its own. Where
> I'm unclear is when you have in-mempool A+B and receive A+B'. Should B'
> have a fee high enough to cover the bandwidth penalty replacement
> (`PaysForRBF`, 2nd check) of both A+B' or only B' ?
>
> If you have a second-layer like current Lightning, you might have a
> counterparty commitment to replace and should always expect to have to pay
> for parent replacement bandwidth.
>
> Where a potential discount sounds interesting is when you have an univoque
> state on the first-stage of transactions. E.g DLC's funding transaction
> which might be CPFP by any participant iirc.
>
> > Note that, if C' conflicts with C, it also conflicts with D, since D is a
> > descendant of C and would thus need to be evicted along with it.
>
> Ah once again I think it's a misunderstanding without the code under my
> eyes! If we do C' `PreChecks`, solve the conflicts provoked by it, i.e mark
> for potential eviction D and don't consider it for future conflicts in the
> rest of the package, I think D' `PreChecks` should be good ?
>
> > More generally, this example is surprising to me because I didn't think
> > packages would be used to fee-bump replaceable transactions. Do we want
> the
> > child to be able to replace mempool transactions as well?
>
> If we mean when you have replaceable A+B then A'+B' try to replace with a
> higher-feerate ? I think that's exactly the case we need for Lightning as
> A+B is coming from Alice and A'+B' is coming from Bob :/
>
> > I'm not sure what you mean? Let's say we have a package of parent A +
> child
> > B, where A is supposed to replace a mempool transaction A'. Are you
> saying
> > that counterparties are able to malleate the package child B, or a child
> of
> > A'?
>
> The second option, a child of A', In the LN case I think the CPFP is
> attached on one's anchor output.
>
> I think it's good if we assume the
> solve-conflicts-after-parent's`'PreChecks` mentioned above or fixing
> inherited signaling or full-rbf ?
>
> > Sorry, I don't understand what you mean by "preserve the package
> > integrity?" Could you elaborate?
>
> After thinking the relaxation about the "new" unconfirmed input is not
> linked to trimming but I would say more to the multi-parent support.
>
> Let's say you have A+B trying to replace C+D where B is also spending
> already in-mempool E. To succeed, you need to waive the no-new-unconfirmed
> input as D isn't spending E.
>
> So good, I think we agree on the problem description here.
>
> > I am in agreement with your calculations but unsure if we disagree on the
> > expected outcome. Yes, B has an ancestor score of 10sat/vb and D has an
> > ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than B's,
> > it fails the proposed package RBF Rule #2, so this package would be
> > rejected. Does this meet your expectations?
>
> Well what sounds odd to me, in my example, we fail D even if it has a
> higher-fee than B. Like A+B absolute fees are 2000 sats and A+C+D absolute
> fees are 4500 sats ?
>
> Is this compatible with a model where a miner prioritizes absolute fees
> over ancestor score, in the case that mempools aren't full-enough to
> fulfill a block ?
>
> Let me know if I can clarify a point.
>
> Antoine
>
> Le lun. 20 sept. 2021 ? 11:10, Gloria Zhao <gloriajzhao at gmail.com> a
> ?crit :
>
>>
>> Hi Antoine,
>>
>> First of all, thank you for the thorough review. I appreciate your
>> insight on LN requirements.
>>
>> > IIUC, you have a package A+B+C submitted for acceptance and A is
>> already in your mempool. You trim out A from the package and then evaluate
>> B+C.
>>
>> > I think this might be an issue if A is the higher-fee element of the
>> ABC package. B+C package fees might be under the mempool min fee and will
>> be rejected, potentially breaking the acceptance expectations of the
>> package issuer ?
>>
>> Correct, if B+C is too low feerate to be accepted, we will reject it. I
>> prefer this because it is incentive compatible: A can be mined by itself,
>> so there's no reason to prefer A+B+C instead of A.
>> As another way of looking at this, consider the case where we do accept
>> A+B+C and it sits at the "bottom" of our mempool. If our mempool reaches
>> capacity, we evict the lowest descendant feerate transactions, which are
>> B+C in this case. This gives us the same resulting mempool, with A and not
>> B+C.
>>
>>
>> > Further, I think the dedup should be done on wtxid, as you might have
>> multiple valid witnesses. Though with varying vsizes and as such offering
>> different feerates.
>>
>> I agree that variations of the same package with different witnesses is a
>> case that must be handled. I consider witness replacement to be a project
>> that can be done in parallel to package mempool acceptance because being
>> able to accept packages does not worsen the problem of a
>> same-txid-different-witness "pinning" attack.
>>
>> If or when we have witness replacement, the logic is: if the individual
>> transaction is enough to replace the mempool one, the replacement will
>> happen during the preceding individual transaction acceptance, and
>> deduplication logic will work. Otherwise, we will try to deduplicate by
>> wtxid, see that we need a package witness replacement, and use the package
>> feerate to evaluate whether this is economically rational.
>>
>> See the #22290 "handle package transactions already in mempool" commit (
>> https://github.com/bitcoin/bitcoin/pull/22290/commits/fea75a2237b46cf76145242fecad7e274bfcb5ff),
>> which handles the case of same-txid-different-witness by simply using the
>> transaction in the mempool for now, with TODOs for what I just described.
>>
>>
>> > I'm not clearly understanding the accepted topologies. By "parent and
>> child to share a parent", do you mean the set of transactions A, B, C,
>> where B is spending A and C is spending A and B would be correct ?
>>
>> Yes, that is what I meant. Yes, that would a valid package under these
>> rules.
>>
>> > If yes, is there a width-limit introduced or we fallback on
>> MAX_PACKAGE_COUNT=25 ?
>>
>> No, there is no limit on connectivity other than "child with all
>> unconfirmed parents." We will enforce MAX_PACKAGE_COUNT=25 and child's
>> in-mempool + in-package ancestor limits.
>>
>>
>> > Considering the current Core's mempool acceptance rules, I think CPFP
>> batching is unsafe for LN time-sensitive closure. A malicious tx-relay
>> jamming successful on one channel commitment transaction would contamine
>> the remaining commitments sharing the same package.
>>
>> > E.g, you broadcast the package A+B+C+D+E where A,B,C,D are commitment
>> transactions and E a shared CPFP. If a malicious A' transaction has a
>> better feerate than A, the whole package acceptance will fail. Even if A'
>> confirms in the following block,
>> the propagation and confirmation of B+C+D have been delayed. This could
>> carry on a loss of funds.
>>
>> Please note that A may replace A' even if A' has higher fees than A
>> individually, because the proposed package RBF utilizes the fees and size
>> of the entire package. This just requires E to pay enough fees, although
>> this can be pretty high if there are also potential B' and C' competing
>> commitment transactions that we don't know about.
>>
>>
>> > IMHO, I'm leaning towards deploying during a first phase
>> 1-parent/1-child. I think it's the most conservative step still improving
>> second-layer safety.
>>
>> So far, my understanding is that multi-parent-1-child is desired for
>> batched fee-bumping (
>> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289)
>> and I've also seen your response which I have less context on (
>> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202).
>> That being said, I am happy to create a new proposal for 1 parent + 1 child
>> (which would be slightly simpler) and plan for moving to
>> multi-parent-1-child later if that is preferred. I am very interested in
>> hearing feedback on that approach.
>>
>>
>> > If A+B is submitted to replace A', where A pays 0 sats, B pays 200 sats
>> and A' pays 100 sats. If we apply the individual RBF on A, A+B acceptance
>> fails. For this reason I think the individual RBF should be bypassed and
>> only the package RBF apply ?
>>
>> I think there is a misunderstanding here - let me describe what I'm
>> proposing we'd do in this situation: we'll try individual submission for A,
>> see that it fails due to "insufficient fees." Then, we'll try package
>> validation for A+B and use package RBF. If A+B pays enough, it can still
>> replace A'. If A fails for a bad signature, we won't look at B or A+B. Does
>> this meet your expectations?
>>
>>
>> > What problem are you trying to solve by the package feerate *after*
>> dedup rule ?
>> > My understanding is that an in-package transaction might be already in
>> the mempool. Therefore, to compute a correct RBF penalty replacement, the
>> vsize of this transaction could be discarded lowering the cost of package
>> RBF.
>>
>> I'm proposing that, when a transaction has already been submitted to
>> mempool, we would ignore both its fees and vsize when calculating package
>> feerate. In example G2, we shouldn't count M1 fees after its submission to
>> mempool, since M1's fees have already been used to pay for its individual
>> bandwidth, and it shouldn't be used again to pay for P2 and P3's bandwidth.
>> We also shouldn't count its vsize, since it has already been paid for.
>>
>>
>> > I think this is a footgunish API, as if a package issuer send the
>> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.
>> Then try to broadcast the higher-feerate C'+D' package, it should be
>> rejected. So it's breaking the naive broadcaster assumption that a
>> higher-feerate/higher-fee package always replaces ?
>>
>> Note that, if C' conflicts with C, it also conflicts with D, since D is a
>> descendant of C and would thus need to be evicted along with it.
>> Implicitly, D' would not be in conflict with D.
>> More generally, this example is surprising to me because I didn't think
>> packages would be used to fee-bump replaceable transactions. Do we want the
>> child to be able to replace mempool transactions as well? This can be
>> implemented with a bit of additional logic.
>>
>> > I think this is unsafe for L2s if counterparties have malleability of
>> the child transaction. They can block your package replacement by
>> opting-out from RBF signaling. IIRC, LN's "anchor output" presents such an
>> ability.
>>
>> I'm not sure what you mean? Let's say we have a package of parent A +
>> child B, where A is supposed to replace a mempool transaction A'. Are you
>> saying that counterparties are able to malleate the package child B, or a
>> child of A'? If they can malleate a child of A', that shouldn't matter as
>> long as A' is signaling replacement. This would be handled identically with
>> full RBF and what Core currently implements.
>>
>> > I think this is an issue brought by the trimming during the dedup
>> phase. If we preserve the package integrity, only re-using the tx-level
>> checks results of already in-mempool transactions to gain in CPU time we
>> won't have this issue. Package childs can add unconfirmed inputs as long as
>> they're in-package, the bip125 rule2 is only evaluated against parents ?
>>
>> Sorry, I don't understand what you mean by "preserve the package
>> integrity?" Could you elaborate?
>>
>> > Let's say you have in-mempool A, B where A pays 10 sat/vb for 100
>> vbytes and B pays 10 sat/vb for 100 vbytes. You have the candidate
>> replacement D spending both A and C where D pays 15sat/vb for 100 vbytes
>> and C pays 1 sat/vb for 1000 vbytes.
>>
>> > Package A + B ancestor score is 10 sat/vb.
>>
>> > D has a higher feerate/absolute fee than B.
>>
>> > Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's
>> 1000 sats + D's 1500 sats) / A's 100 vb + C's 1000 vb + D's 100 vb)
>>
>> I am in agreement with your calculations but unsure if we disagree on the
>> expected outcome. Yes, B has an ancestor score of 10sat/vb and D has an
>> ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than B's,
>> it fails the proposed package RBF Rule #2, so this package would be
>> rejected. Does this meet your expectations?
>>
>> Thank you for linking to projects that might be interested in package
>> relay :)
>>
>> Thanks,
>> Gloria
>>
>> On Mon, Sep 20, 2021 at 12:16 AM Antoine Riard <antoine.riard at gmail.com>
>> wrote:
>>
>>> Hi Gloria,
>>>
>>> > A package may contain transactions that are already in the mempool. We
>>> > remove
>>> > ("deduplicate") those transactions from the package for the purposes of
>>> > package
>>> > mempool acceptance. If a package is empty after deduplication, we do
>>> > nothing.
>>>
>>> IIUC, you have a package A+B+C submitted for acceptance and A is already
>>> in your mempool. You trim out A from the package and then evaluate B+C.
>>>
>>> I think this might be an issue if A is the higher-fee element of the ABC
>>> package. B+C package fees might be under the mempool min fee and will be
>>> rejected, potentially breaking the acceptance expectations of the package
>>> issuer ?
>>>
>>> Further, I think the dedup should be done on wtxid, as you might have
>>> multiple valid witnesses. Though with varying vsizes and as such offering
>>> different feerates.
>>>
>>> E.g you're going to evaluate the package A+B and A' is already in your
>>> mempool with a bigger valid witness. You trim A based on txid, then you
>>> evaluate A'+B, which fails the fee checks. However, evaluating A+B would
>>> have been a success.
>>>
>>> AFAICT, the dedup rationale would be to save on CPU time/IO disk, to
>>> avoid repeated signatures verification and parent UTXOs fetches ? Can we
>>> achieve the same goal by bypassing tx-level checks for already-in txn while
>>> conserving the package integrity for package-level checks ?
>>>
>>> > Note that it's possible for the parents to be
>>> > indirect
>>> > descendants/ancestors of one another, or for parent and child to share
>>> a
>>> > parent,
>>> > so we cannot make any other topology assumptions.
>>>
>>> I'm not clearly understanding the accepted topologies. By "parent and
>>> child to share a parent", do you mean the set of transactions A, B, C,
>>> where B is spending A and C is spending A and B would be correct ?
>>>
>>> If yes, is there a width-limit introduced or we fallback on
>>> MAX_PACKAGE_COUNT=25 ?
>>>
>>> IIRC, one rationale to come with this topology limitation was to lower
>>> the DoS risks when potentially deploying p2p packages.
>>>
>>> Considering the current Core's mempool acceptance rules, I think CPFP
>>> batching is unsafe for LN time-sensitive closure. A malicious tx-relay
>>> jamming successful on one channel commitment transaction would contamine
>>> the remaining commitments sharing the same package.
>>>
>>> E.g, you broadcast the package A+B+C+D+E where A,B,C,D are commitment
>>> transactions and E a shared CPFP. If a malicious A' transaction has a
>>> better feerate than A, the whole package acceptance will fail. Even if A'
>>> confirms in the following block,
>>> the propagation and confirmation of B+C+D have been delayed. This could
>>> carry on a loss of funds.
>>>
>>> That said, if you're broadcasting commitment transactions without
>>> time-sensitive HTLC outputs, I think the batching is effectively a fee
>>> saving as you don't have to duplicate the CPFP.
>>>
>>> IMHO, I'm leaning towards deploying during a first phase
>>> 1-parent/1-child. I think it's the most conservative step still improving
>>> second-layer safety.
>>>
>>> > *Rationale*:  It would be incorrect to use the fees of transactions
>>> that are
>>> > already in the mempool, as we do not want a transaction's fees to be
>>> > double-counted for both its individual RBF and package RBF.
>>>
>>> I'm unsure about the logical order of the checks proposed.
>>>
>>> If A+B is submitted to replace A', where A pays 0 sats, B pays 200 sats
>>> and A' pays 100 sats. If we apply the individual RBF on A, A+B acceptance
>>> fails. For this reason I think the individual RBF should be bypassed and
>>> only the package RBF apply ?
>>>
>>> Note this situation is plausible, with current LN design, your
>>> counterparty can have a commitment transaction with a better fee just by
>>> selecting a higher `dust_limit_satoshis` than yours.
>>>
>>> > Examples F and G [14] show the same package, but P1 is submitted
>>> > individually before
>>> > the package in example G. In example F, we can see that the 300vB
>>> package
>>> > pays
>>> > an additional 200sat in fees, which is not enough to pay for its own
>>> > bandwidth
>>> > (BIP125#4). In example G, we can see that P1 pays enough to replace
>>> M1, but
>>> > using P1's fees again during package submission would make it look
>>> like a
>>> > 300sat
>>> > increase for a 200vB package. Even including its fees and size would
>>> not be
>>> > sufficient in this example, since the 300sat looks like enough for the
>>> 300vB
>>> > package. The calculcation after deduplication is 100sat increase for a
>>> > package
>>> > of size 200vB, which correctly fails BIP125#4. Assume all transactions
>>> have
>>> > a
>>> > size of 100vB.
>>>
>>> What problem are you trying to solve by the package feerate *after*
>>> dedup rule ?
>>>
>>> My understanding is that an in-package transaction might be already in
>>> the mempool. Therefore, to compute a correct RBF penalty replacement, the
>>> vsize of this transaction could be discarded lowering the cost of package
>>> RBF.
>>>
>>> If we keep a "safe" dedup mechanism (see my point above), I think this
>>> discount is justified, as the validation cost of node operators is paid for
>>> ?
>>>
>>> > The child cannot replace mempool transactions.
>>>
>>> Let's say you issue package A+B, then package C+B', where B' is a child
>>> of both A and C. This rule fails the acceptance of C+B' ?
>>>
>>> I think this is a footgunish API, as if a package issuer send the
>>> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.
>>> Then try to broadcast the higher-feerate C'+D' package, it should be
>>> rejected. So it's breaking the naive broadcaster assumption that a
>>> higher-feerate/higher-fee package always replaces ? And it might be unsafe
>>> in protocols where states are symmetric. E.g a malicious counterparty
>>> broadcasts first S+A, then you honestly broadcast S+B, where B pays better
>>> fees.
>>>
>>> > All mempool transactions to be replaced must signal replaceability.
>>>
>>> I think this is unsafe for L2s if counterparties have malleability of
>>> the child transaction. They can block your package replacement by
>>> opting-out from RBF signaling. IIRC, LN's "anchor output" presents such an
>>> ability.
>>>
>>> I think it's better to either fix inherited signaling or move towards
>>> full-rbf.
>>>
>>> > if a package parent has already been submitted, it would
>>> > look
>>> >like the child is spending a "new" unconfirmed input.
>>>
>>> I think this is an issue brought by the trimming during the dedup phase.
>>> If we preserve the package integrity, only re-using the tx-level checks
>>> results of already in-mempool transactions to gain in CPU time we won't
>>> have this issue. Package childs can add unconfirmed inputs as long as
>>> they're in-package, the bip125 rule2 is only evaluated against parents ?
>>>
>>> > However, we still achieve the same goal of requiring the
>>> > replacement
>>> > transactions to have a ancestor score at least as high as the original
>>> > ones.
>>>
>>> I'm not sure if this holds...
>>>
>>> Let's say you have in-mempool A, B where A pays 10 sat/vb for 100 vbytes
>>> and B pays 10 sat/vb for 100 vbytes. You have the candidate replacement D
>>> spending both A and C where D pays 15sat/vb for 100 vbytes and C pays 1
>>> sat/vb for 1000 vbytes.
>>>
>>> Package A + B ancestor score is 10 sat/vb.
>>>
>>> D has a higher feerate/absolute fee than B.
>>>
>>> Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's
>>> 1000 sats + D's 1500 sats) /
>>> A's 100 vb + C's 1000 vb + D's 100 vb)
>>>
>>> Overall, this is a review through the lenses of LN requirements. I think
>>> other L2 protocols/applications
>>> could be candidates to using package accept/relay such as:
>>> * https://github.com/lightninglabs/pool
>>> * https://github.com/discreetlogcontracts/dlcspecs
>>> * https://github.com/bitcoin-teleport/teleport-transactions/
>>> * https://github.com/sapio-lang/sapio
>>> *
>>> https://github.com/commerceblock/mercury/blob/master/doc/statechains.md
>>> * https://github.com/revault/practical-revault
>>>
>>> Thanks for rolling forward the ball on this subject.
>>>
>>> Antoine
>>>
>>> Le jeu. 16 sept. 2021 ? 03:55, Gloria Zhao via bitcoin-dev <
>>> bitcoin-dev at lists.linuxfoundation.org> a ?crit :
>>>
>>>> Hi there,
>>>>
>>>> I'm writing to propose a set of mempool policy changes to enable package
>>>> validation (in preparation for package relay) in Bitcoin Core. These
>>>> would not
>>>> be consensus or P2P protocol changes. However, since mempool policy
>>>> significantly affects transaction propagation, I believe this is
>>>> relevant for
>>>> the mailing list.
>>>>
>>>> My proposal enables packages consisting of multiple parents and 1
>>>> child. If you
>>>> develop software that relies on specific transaction relay assumptions
>>>> and/or
>>>> are interested in using package relay in the future, I'm very
>>>> interested to hear
>>>> your feedback on the utility or restrictiveness of these package
>>>> policies for
>>>> your use cases.
>>>>
>>>> A draft implementation of this proposal can be found in [Bitcoin Core
>>>> PR#22290][1].
>>>>
>>>> An illustrated version of this post can be found at
>>>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.
>>>> I have also linked the images below.
>>>>
>>>> ## Background
>>>>
>>>> Feel free to skip this section if you are already familiar with mempool
>>>> policy
>>>> and package relay terminology.
>>>>
>>>> ### Terminology Clarifications
>>>>
>>>> * Package = an ordered list of related transactions, representable by a
>>>> Directed
>>>>   Acyclic Graph.
>>>> * Package Feerate = the total modified fees divided by the total
>>>> virtual size of
>>>>   all transactions in the package.
>>>>     - Modified fees = a transaction's base fees + fee delta applied by
>>>> the user
>>>>       with `prioritisetransaction`. As such, we expect this to vary
>>>> across
>>>> mempools.
>>>>     - Virtual Size = the maximum of virtual sizes calculated using
>>>> [BIP141
>>>>       virtual size][2] and sigop weight. [Implemented here in Bitcoin
>>>> Core][3].
>>>>     - Note that feerate is not necessarily based on the base fees and
>>>> serialized
>>>>       size.
>>>>
>>>> * Fee-Bumping = user/wallet actions that take advantage of miner
>>>> incentives to
>>>>   boost a transaction's candidacy for inclusion in a block, including
>>>> Child Pays
>>>> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our intention
>>>> in
>>>> mempool policy is to recognize when the new transaction is more
>>>> economical to
>>>> mine than the original one(s) but not open DoS vectors, so there are
>>>> some
>>>> limitations.
>>>>
>>>> ### Policy
>>>>
>>>> The purpose of the mempool is to store the best (to be most
>>>> incentive-compatible
>>>> with miners, highest feerate) candidates for inclusion in a block.
>>>> Miners use
>>>> the mempool to build block templates. The mempool is also useful as a
>>>> cache for
>>>> boosting block relay and validation performance, aiding transaction
>>>> relay, and
>>>> generating feerate estimations.
>>>>
>>>> Ideally, all consensus-valid transactions paying reasonable fees should
>>>> make it
>>>> to miners through normal transaction relay, without any special
>>>> connectivity or
>>>> relationships with miners. On the other hand, nodes do not have
>>>> unlimited
>>>> resources, and a P2P network designed to let any honest node broadcast
>>>> their
>>>> transactions also exposes the transaction validation engine to DoS
>>>> attacks from
>>>> malicious peers.
>>>>
>>>> As such, for unconfirmed transactions we are considering for our
>>>> mempool, we
>>>> apply a set of validation rules in addition to consensus, primarily to
>>>> protect
>>>> us from resource exhaustion and aid our efforts to keep the highest fee
>>>> transactions. We call this mempool _policy_: a set of (configurable,
>>>> node-specific) rules that transactions must abide by in order to be
>>>> accepted
>>>> into our mempool. Transaction "Standardness" rules and mempool
>>>> restrictions such
>>>> as "too-long-mempool-chain" are both examples of policy.
>>>>
>>>> ### Package Relay and Package Mempool Accept
>>>>
>>>> In transaction relay, we currently consider transactions one at a time
>>>> for
>>>> submission to the mempool. This creates a limitation in the node's
>>>> ability to
>>>> determine which transactions have the highest feerates, since we cannot
>>>> take
>>>> into account descendants (i.e. cannot use CPFP) until all the
>>>> transactions are
>>>> in the mempool. Similarly, we cannot use a transaction's descendants
>>>> when
>>>> considering it for RBF. When an individual transaction does not meet
>>>> the mempool
>>>> minimum feerate and the user isn't able to create a replacement
>>>> transaction
>>>> directly, it will not be accepted by mempools.
>>>>
>>>> This limitation presents a security issue for applications and users
>>>> relying on
>>>> time-sensitive transactions. For example, Lightning and other protocols
>>>> create
>>>> UTXOs with multiple spending paths, where one counterparty's spending
>>>> path opens
>>>> up after a timelock, and users are protected from cheating scenarios as
>>>> long as
>>>> they redeem on-chain in time. A key security assumption is that all
>>>> parties'
>>>> transactions will propagate and confirm in a timely manner. This
>>>> assumption can
>>>> be broken if fee-bumping does not work as intended.
>>>>
>>>> The end goal for Package Relay is to consider multiple transactions at
>>>> the same
>>>> time, e.g. a transaction with its high-fee child. This may help us
>>>> better
>>>> determine whether transactions should be accepted to our mempool,
>>>> especially if
>>>> they don't meet fee requirements individually or are better RBF
>>>> candidates as a
>>>> package. A combination of changes to mempool validation logic, policy,
>>>> and
>>>> transaction relay allows us to better propagate the transactions with
>>>> the
>>>> highest package feerates to miners, and makes fee-bumping tools more
>>>> powerful
>>>> for users.
>>>>
>>>> The "relay" part of Package Relay suggests P2P messaging changes, but a
>>>> large
>>>> part of the changes are in the mempool's package validation logic. We
>>>> call this
>>>> *Package Mempool Accept*.
>>>>
>>>> ### Previous Work
>>>>
>>>> * Given that mempool validation is DoS-sensitive and complex, it would
>>>> be
>>>>   dangerous to haphazardly tack on package validation logic. Many
>>>> efforts have
>>>> been made to make mempool validation less opaque (see [#16400][4],
>>>> [#21062][5],
>>>> [#22675][6], [#22796][7]).
>>>> * [#20833][8] Added basic capabilities for package validation, test
>>>> accepts only
>>>>   (no submission to mempool).
>>>> * [#21800][9] Implemented package ancestor/descendant limit checks for
>>>> arbitrary
>>>>   packages. Still test accepts only.
>>>> * Previous package relay proposals (see [#16401][10], [#19621][11]).
>>>>
>>>> ### Existing Package Rules
>>>>
>>>> These are in master as introduced in [#20833][8] and [#21800][9]. I'll
>>>> consider
>>>> them as "given" in the rest of this document, though they can be
>>>> changed, since
>>>> package validation is test-accept only right now.
>>>>
>>>> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and
>>>> `MAX_PACKAGE_SIZE=101KvB` total size [8]
>>>>
>>>>    *Rationale*: This is already enforced as mempool ancestor/descendant
>>>> limits.
>>>> Presumably, transactions in a package are all related, so exceeding
>>>> this limit
>>>> would mean that the package can either be split up or it wouldn't pass
>>>> this
>>>> mempool policy.
>>>>
>>>> 2. Packages must be topologically sorted: if any dependencies exist
>>>> between
>>>> transactions, parents must appear somewhere before children. [8]
>>>>
>>>> 3. A package cannot have conflicting transactions, i.e. none of them
>>>> can spend
>>>> the same inputs. This also means there cannot be duplicate
>>>> transactions. [8]
>>>>
>>>> 4. When packages are evaluated against ancestor/descendant limits in a
>>>> test
>>>> accept, the union of all of their descendants and ancestors is
>>>> considered. This
>>>> is essentially a "worst case" heuristic where every transaction in the
>>>> package
>>>> is treated as each other's ancestor and descendant. [8]
>>>> Packages for which ancestor/descendant limits are accurately captured
>>>> by this
>>>> heuristic: [19]
>>>>
>>>> There are also limitations such as the fact that CPFP carve out is not
>>>> applied
>>>> to package transactions. #20833 also disables RBF in package
>>>> validation; this
>>>> proposal overrides that to allow packages to use RBF.
>>>>
>>>> ## Proposed Changes
>>>>
>>>> The next step in the Package Mempool Accept project is to implement
>>>> submission
>>>> to mempool, initially through RPC only. This allows us to test the
>>>> submission
>>>> logic before exposing it on P2P.
>>>>
>>>> ### Summary
>>>>
>>>> - Packages may contain already-in-mempool transactions.
>>>> - Packages are 2 generations, Multi-Parent-1-Child.
>>>> - Fee-related checks use the package feerate. This means that wallets
>>>> can
>>>> create a package that utilizes CPFP.
>>>> - Parents are allowed to RBF mempool transactions with a set of rules
>>>> similar
>>>>   to BIP125. This enables a combination of CPFP and RBF, where a
>>>> transaction's descendant fees pay for replacing mempool conflicts.
>>>>
>>>> There is a draft implementation in [#22290][1]. It is WIP, but feedback
>>>> is
>>>> always welcome.
>>>>
>>>> ### Details
>>>>
>>>> #### Packages May Contain Already-in-Mempool Transactions
>>>>
>>>> A package may contain transactions that are already in the mempool. We
>>>> remove
>>>> ("deduplicate") those transactions from the package for the purposes of
>>>> package
>>>> mempool acceptance. If a package is empty after deduplication, we do
>>>> nothing.
>>>>
>>>> *Rationale*: Mempools vary across the network. It's possible for a
>>>> parent to be
>>>> accepted to the mempool of a peer on its own due to differences in
>>>> policy and
>>>> fee market fluctuations. We should not reject or penalize the entire
>>>> package for
>>>> an individual transaction as that could be a censorship vector.
>>>>
>>>> #### Packages Are Multi-Parent-1-Child
>>>>
>>>> Only packages of a specific topology are permitted. Namely, a package
>>>> is exactly
>>>> 1 child with all of its unconfirmed parents. After deduplication, the
>>>> package
>>>> may be exactly the same, empty, 1 child, 1 child with just some of its
>>>> unconfirmed parents, etc. Note that it's possible for the parents to be
>>>> indirect
>>>> descendants/ancestors of one another, or for parent and child to share
>>>> a parent,
>>>> so we cannot make any other topology assumptions.
>>>>
>>>> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple
>>>> parents
>>>> makes it possible to fee-bump a batch of transactions. Restricting
>>>> packages to a
>>>> defined topology is also easier to reason about and simplifies the
>>>> validation
>>>> logic greatly. Multi-parent-1-child allows us to think of the package
>>>> as one big
>>>> transaction, where:
>>>>
>>>> - Inputs = all the inputs of parents + inputs of the child that come
>>>> from
>>>>   confirmed UTXOs
>>>> - Outputs = all the outputs of the child + all outputs of the parents
>>>> that
>>>>   aren't spent by other transactions in the package
>>>>
>>>> Examples of packages that follow this rule (variations of example A
>>>> show some
>>>> possibilities after deduplication): ![image][15]
>>>>
>>>> #### Fee-Related Checks Use Package Feerate
>>>>
>>>> Package Feerate = the total modified fees divided by the total virtual
>>>> size of
>>>> all transactions in the package.
>>>>
>>>> To meet the two feerate requirements of a mempool, i.e., the
>>>> pre-configured
>>>> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum
>>>> feerate, the
>>>> total package feerate is used instead of the individual feerate. The
>>>> individual
>>>> transactions are allowed to be below feerate requirements if the
>>>> package meets
>>>> the feerate requirements. For example, the parent(s) in the package can
>>>> have 0
>>>> fees but be paid for by the child.
>>>>
>>>> *Rationale*: This can be thought of as "CPFP within a package," solving
>>>> the
>>>> issue of a parent not meeting minimum fees on its own. This allows L2
>>>> applications to adjust their fees at broadcast time instead of
>>>> overshooting or
>>>> risking getting stuck/pinned.
>>>>
>>>> We use the package feerate of the package *after deduplication*.
>>>>
>>>> *Rationale*:  It would be incorrect to use the fees of transactions
>>>> that are
>>>> already in the mempool, as we do not want a transaction's fees to be
>>>> double-counted for both its individual RBF and package RBF.
>>>>
>>>> Examples F and G [14] show the same package, but P1 is submitted
>>>> individually before
>>>> the package in example G. In example F, we can see that the 300vB
>>>> package pays
>>>> an additional 200sat in fees, which is not enough to pay for its own
>>>> bandwidth
>>>> (BIP125#4). In example G, we can see that P1 pays enough to replace M1,
>>>> but
>>>> using P1's fees again during package submission would make it look like
>>>> a 300sat
>>>> increase for a 200vB package. Even including its fees and size would
>>>> not be
>>>> sufficient in this example, since the 300sat looks like enough for the
>>>> 300vB
>>>> package. The calculcation after deduplication is 100sat increase for a
>>>> package
>>>> of size 200vB, which correctly fails BIP125#4. Assume all transactions
>>>> have a
>>>> size of 100vB.
>>>>
>>>> #### Package RBF
>>>>
>>>> If a package meets feerate requirements as a package, the parents in the
>>>> transaction are allowed to replace-by-fee mempool transactions. The
>>>> child cannot
>>>> replace mempool transactions. Multiple transactions can replace the same
>>>> transaction, but in order to be valid, none of the transactions can try
>>>> to
>>>> replace an ancestor of another transaction in the same package (which
>>>> would thus
>>>> make its inputs unavailable).
>>>>
>>>> *Rationale*: Even if we are using package feerate, a package will not
>>>> propagate
>>>> as intended if RBF still requires each individual transaction to meet
>>>> the
>>>> feerate requirements.
>>>>
>>>> We use a set of rules slightly modified from BIP125 as follows:
>>>>
>>>> ##### Signaling (Rule #1)
>>>>
>>>> All mempool transactions to be replaced must signal replaceability.
>>>>
>>>> *Rationale*: Package RBF signaling logic should be the same for package
>>>> RBF and
>>>> single transaction acceptance. This would be updated if single
>>>> transaction
>>>> validation moves to full RBF.
>>>>
>>>> ##### New Unconfirmed Inputs (Rule #2)
>>>>
>>>> A package may include new unconfirmed inputs, but the ancestor feerate
>>>> of the
>>>> child must be at least as high as the ancestor feerates of every
>>>> transaction
>>>> being replaced. This is contrary to BIP125#2, which states "The
>>>> replacement
>>>> transaction may only include an unconfirmed input if that input was
>>>> included in
>>>> one of the original transactions. (An unconfirmed input spends an
>>>> output from a
>>>> currently-unconfirmed transaction.)"
>>>>
>>>> *Rationale*: The purpose of BIP125#2 is to ensure that the replacement
>>>> transaction has a higher ancestor score than the original
>>>> transaction(s) (see
>>>> [comment][13]). Example H [16] shows how adding a new unconfirmed input
>>>> can lower the
>>>> ancestor score of the replacement transaction. P1 is trying to replace
>>>> M1, and
>>>> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat, and
>>>> M2 pays
>>>> 100sat. Assume all transactions have a size of 100vB. While, in
>>>> isolation, P1
>>>> looks like a better mining candidate than M1, it must be mined with M2,
>>>> so its
>>>> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's
>>>> ancestor
>>>> feerate, which is 6sat/vB.
>>>>
>>>> In package RBF, the rule analogous to BIP125#2 would be "none of the
>>>> transactions in the package can spend new unconfirmed inputs." Example
>>>> J [17] shows
>>>> why, if any of the package transactions have ancestors, package feerate
>>>> is no
>>>> longer accurate. Even though M2 and M3 are not ancestors of P1 (which
>>>> is the
>>>> replacement transaction in an RBF), we're actually interested in the
>>>> entire
>>>> package. A miner should mine M1 which is 5sat/vB instead of M2, M3, P1,
>>>> P2, and
>>>> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened to
>>>> only allow
>>>> the child to have new unconfirmed inputs, either, because it can still
>>>> cause us
>>>> to overestimate the package's ancestor score.
>>>>
>>>> However, enforcing a rule analogous to BIP125#2 would not only make
>>>> Package RBF
>>>> less useful, but would also break Package RBF for packages with parents
>>>> already
>>>> in the mempool: if a package parent has already been submitted, it
>>>> would look
>>>> like the child is spending a "new" unconfirmed input. In example K
>>>> [18], we're
>>>> looking to replace M1 with the entire package including P1, P2, and P3.
>>>> We must
>>>> consider the case where one of the parents is already in the mempool
>>>> (in this
>>>> case, P2), which means we must allow P3 to have new unconfirmed inputs.
>>>> However,
>>>> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not
>>>> replace M1
>>>> with this package.
>>>>
>>>> Thus, the package RBF rule regarding new unconfirmed inputs is less
>>>> strict than
>>>> BIP125#2. However, we still achieve the same goal of requiring the
>>>> replacement
>>>> transactions to have a ancestor score at least as high as the original
>>>> ones. As
>>>> a result, the entire package is required to be a higher feerate mining
>>>> candidate
>>>> than each of the replaced transactions.
>>>>
>>>> Another note: the [comment][13] above the BIP125#2 code in the original
>>>> RBF
>>>> implementation suggests that the rule was intended to be temporary.
>>>>
>>>> ##### Absolute Fee (Rule #3)
>>>>
>>>> The package must increase the absolute fee of the mempool, i.e. the
>>>> total fees
>>>> of the package must be higher than the absolute fees of the mempool
>>>> transactions
>>>> it replaces. Combined with the CPFP rule above, this differs from
>>>> BIP125 Rule #3
>>>> - an individual transaction in the package may have lower fees than the
>>>>   transaction(s) it is replacing. In fact, it may have 0 fees, and the
>>>> child
>>>> pays for RBF.
>>>>
>>>> ##### Feerate (Rule #4)
>>>>
>>>> The package must pay for its own bandwidth; the package feerate must be
>>>> higher
>>>> than the replaced transactions by at least minimum relay feerate
>>>> (`incrementalRelayFee`). Combined with the CPFP rule above, this
>>>> differs from
>>>> BIP125 Rule #4 - an individual transaction in the package can have a
>>>> lower
>>>> feerate than the transaction(s) it is replacing. In fact, it may have 0
>>>> fees,
>>>> and the child pays for RBF.
>>>>
>>>> ##### Total Number of Replaced Transactions (Rule #5)
>>>>
>>>> The package cannot replace more than 100 mempool transactions. This is
>>>> identical
>>>> to BIP125 Rule #5.
>>>>
>>>> ### Expected FAQs
>>>>
>>>> 1. Is it possible for only some of the package to make it into the
>>>> mempool?
>>>>
>>>>    Yes, it is. However, since we evict transactions from the mempool by
>>>> descendant score and the package child is supposed to be sponsoring the
>>>> fees of
>>>> its parents, the most common scenario would be all-or-nothing. This is
>>>> incentive-compatible. In fact, to be conservative, package validation
>>>> should
>>>> begin by trying to submit all of the transactions individually, and
>>>> only use the
>>>> package mempool acceptance logic if the parents fail due to low feerate.
>>>>
>>>> 2. Should we allow packages to contain already-confirmed transactions?
>>>>
>>>>     No, for practical reasons. In mempool validation, we actually
>>>> aren't able to
>>>> tell with 100% confidence if we are looking at a transaction that has
>>>> already
>>>> confirmed, because we look up inputs using a UTXO set. If we have
>>>> historical
>>>> block data, it's possible to look for it, but this is inefficient, not
>>>> always
>>>> possible for pruning nodes, and unnecessary because we're not going to
>>>> do
>>>> anything with the transaction anyway. As such, we already have the
>>>> expectation
>>>> that transaction relay is somewhat "stateful" i.e. nobody should be
>>>> relaying
>>>> transactions that have already been confirmed. Similarly, we shouldn't
>>>> be
>>>> relaying packages that contain already-confirmed transactions.
>>>>
>>>> [1]: https://github.com/bitcoin/bitcoin/pull/22290
>>>> [2]:
>>>> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations
>>>> [3]:
>>>> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282
>>>> [4]: https://github.com/bitcoin/bitcoin/pull/16400
>>>> [5]: https://github.com/bitcoin/bitcoin/pull/21062
>>>> [6]: https://github.com/bitcoin/bitcoin/pull/22675
>>>> [7]: https://github.com/bitcoin/bitcoin/pull/22796
>>>> [8]: https://github.com/bitcoin/bitcoin/pull/20833
>>>> [9]: https://github.com/bitcoin/bitcoin/pull/21800
>>>> [10]: https://github.com/bitcoin/bitcoin/pull/16401
>>>> [11]: https://github.com/bitcoin/bitcoin/pull/19621
>>>> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki
>>>> [13]:
>>>> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104
>>>> [14]:
>>>> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png
>>>> [15]:
>>>> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png
>>>> [16]:
>>>> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png
>>>> [17]:
>>>> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png
>>>> [18]:
>>>> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png
>>>> [19]:
>>>> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png
>>>> [20]:
>>>> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png
>>>> _______________________________________________
>>>> bitcoin-dev mailing list
>>>> bitcoin-dev at lists.linuxfoundation.org
>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>>
>>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210923/d1576a8f/attachment-0001.html>

From jlrubin at mit.edu  Fri Sep 24 07:27:03 2021
From: jlrubin at mit.edu (Jeremy)
Date: Fri, 24 Sep 2021 00:27:03 -0700
Subject: [bitcoin-dev] [Lightning-dev] Inherited IDs - A safer,
 more powerful alternative to BIP-118 (ANYPREVOUT) for scaling
 Bitcoin
In-Reply-To: <wDdSrhoOhFv2L7-IsDteE5PoDaZJk3aFZSvNgYd_PbuoVwLQ3qwheW-00wV52utUrpDhzsbAKvzYRhm5WUkXBtC-y0YPe9t17TaWodK1WsY=@protonmail.com>
References: <CAD5xwhh-1zUbPgYW6hE8q3CmhFZFdEqjx5pB7+VFM4mV=1FfaQ@mail.gmail.com>
 <wDdSrhoOhFv2L7-IsDteE5PoDaZJk3aFZSvNgYd_PbuoVwLQ3qwheW-00wV52utUrpDhzsbAKvzYRhm5WUkXBtC-y0YPe9t17TaWodK1WsY=@protonmail.com>
Message-ID: <CAD5xwhhZd2Mr17bsPk0gszitROWF13i2hTg_7KvRnR948_zQpw@mail.gmail.com>

John let me know that he's posted some responses in his Github repo
https://github.com/JohnLaw2/btc-iids

probably easiest to respond to him via e.g. a github issue or something.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210924/f1ecc8d3/attachment.html>

From antoine.riard at gmail.com  Sun Sep 26 21:10:14 2021
From: antoine.riard at gmail.com (Antoine Riard)
Date: Sun, 26 Sep 2021 17:10:14 -0400
Subject: [bitcoin-dev] Proposal: Package Mempool Accept and Package RBF
In-Reply-To: <CAFXO6=Lvcr7Pwn_ZD1CZohYUFKY-cC5sGRxdTOiP2MgnTvFnYA@mail.gmail.com>
References: <CAFXO6=+cHyQKM2n9yn4EhwLZO+AUB0ZD81qWPxmpN27rjUoU3w@mail.gmail.com>
 <CALZpt+HpvmEHUEOgye34T6pVQ+wnKKn-_8cTJTQXYQb9t1jOTA@mail.gmail.com>
 <CAFXO6=JzsYgiXJE2geSKMpfgPo+GGNX_+Pw0JQx1QQxAfhCdBQ@mail.gmail.com>
 <CALZpt+HQpdrebhWGXv_yLqiSCB5Ur71L1K13bd7w5TZb9DwJEQ@mail.gmail.com>
 <CAFXO6=Lvcr7Pwn_ZD1CZohYUFKY-cC5sGRxdTOiP2MgnTvFnYA@mail.gmail.com>
Message-ID: <CALZpt+FFSk-+BBxu7SSdjw580UCFfkdo1DTa1Yj9K81M4E1vPg@mail.gmail.com>

Hi Gloria,

Thanks for your answers,

> In summary, it seems that the decisions that might still need
> attention/input from devs on this mailing list are:
> 1. Whether we should start with multiple-parent-1-child or
1-parent-1-child.
> 2. Whether it's ok to require that the child not have conflicts with
> mempool transactions.

Yes 1) it would be good to have inputs of more potential users of package
acceptance . And 2) I think it's more a matter of clearer wording of the
proposal.

However, see my final point on the relaxation around "unconfirmed inputs"
which might in fact alter our current block construction strategy.

> Right, the fact that we essentially always choose the first-seen witness
is
> an unfortunate limitation that exists already. Adding package mempool
> accept doesn't worsen this, but the procedure in the future is to replace
> the witness when it makes sense economically. We can also add logic to
> allow package feerate to pay for witness replacements as well. This is
> pretty far into the future, though.

Yes I agree package mempool doesn't worsen this. And it's not an issue for
current LN as you can't significantly inflate a spending witness for the
2-of-2 funding output.
However, it might be an issue for multi-party protocol where the spending
script has alternative branches with asymmetric valid witness weights.
Taproot should ease that kind of script so hopefully we would deploy
wtxid-replacement not too far in the future.

> I could be misunderstanding, but an attacker wouldn't be able to
> batch-attack like this. Alice's package only conflicts with A' + D', not
A'
> + B' + C' + D'. She only needs to pay for evicting 2 transactions.

Yeah I can be clearer, I think you have 2 pinning attacks scenarios to
consider.

In LN, if you're trying to confirm a commitment transaction to time-out or
claim on-chain a HTLC and the timelock is near-expiration, you should be
ready to pay in commitment+2nd-stage HTLC transaction fees as much as the
value offered by the HTLC.

Following this security assumption, an attacker can exploit it by targeting
together commitment transactions from different channels by blocking them
under a high-fee child, of which the fee value
is equal to the top-value HTLC + 1. Victims's fee-bumping logics won't
overbid as it's not worthy to offer fees beyond their competed HTLCs. Apart
from observing mempools state, victims can't learn they're targeted by the
same attacker.

To draw from the aforementioned topology, Mallory broadcasts A' + B' + C' +
D', where A' conflicts with Alice's P1, B' conflicts with Bob's P2, C'
conflicts with Caroll's P3. Let's assume P1 is confirming the top-value
HTLC of the set. If D' fees is higher than P1 + 1, it won't be rational for
Alice or Bob or Caroll to keep offering competing feerates. Mallory will be
at loss on stealing P1, as she has paid more in fees but will realize a
gain on P2+P3.

In this model, Alice is allowed to evict those 2 transactions (A' + D') but
as she is economically-bounded she won't succeed.

Mallory is maliciously exploiting RBF rule 3 on absolute fee. I think this
1st pinning scenario is correct and "lucractive" when you sum the global
gain/loss.

There is a 2nd attack scenario where A + B + C + D, where D is the child of
A,B,C. All those transactions are honestly issued by Alice. Once A + B + C
+ D are propagated in network mempools, Mallory is able to replace A + D
with  A' + D' where D' is paying a higher fee. This package A' + D' will
confirm soon if D feerate was compelling but Mallory succeeds in delaying
the confirmation
of B + C for one or more blocks. As B + C are pre-signed commitments with a
low-fee rate they won't confirm without Alice issuing a new child E.
Mallory can repeat the same trick by broadcasting
B' + E' and delay again the confirmation of C.

If the remaining package pending HTLC has a higher-value than all the
malicious fees over-bid, Mallory should realize a gain. With this 2nd
pinning attack, the malicious entity buys confirmation delay of your
packaged-together commitments.

Assuming those attacks are correct, I'm leaning towards being conservative
with the LDK broadcast backend. Though once again, other L2 devs have
likely other use-cases and opinions :)

>  B' only needs to pay for itself in this case.

Yes I think it's a nice discount when UTXO is single-owned. In the context
of shared-owned UTXO (e.g LN), you might not if there is an in-mempool
package already spending the UTXO and have to assume the worst-case
scenario. I.e have B' committing enough fee to pay for A' replacement
bandwidth. I think we can't do that much for this case...

> If a package meets feerate requirements as a
package, the parents in the transaction are allowed to replace-by-fee
mempool transactions. The child cannot replace mempool transactions."

I agree with the Mallory-vs-Alice case. Though if Alice broadcasts A+B' to
replace A+B because the first broadcast isn't satisfying anymore due to
mempool spikes ? Assuming B' fees is enough, I think that case as child B'
replacing in-mempool transaction B. Which I understand going against  "The
child cannot replace mempool transactions".

Maybe wording could be a bit clearer ?

> While it would be nice to have full RBF, malleability of the child won't
> block RBF here. If we're trying to replace A', we only require that A'
> signals replaceability, and don't mind if its child doesn't.

Yes, it sounds good.

> Yes, A+C+D pays 2500sat more in fees, but it is also 1000vB larger. A
miner
> should prefer to utilize their block space more effectively.

If your mempool is empty and only composed of A+C+D or A+B, I think taking
A+C+D is the most efficient block construction you can come up with as a
miner ?

> No, because we don't use that model.

Can you describe what miner model we are using ? Like the block
construction strategy implemented by `addPackagesTxs` or also encompassing
our current mempool acceptance policy, which I think rely on absolute fee
over ancestor score in case of replacement ?

I think this point is worthy to discuss as otherwise we might downgrade the
efficiency of our current block construction strategy in periods of
near-empty mempools. A knowledge which could be discreetly leveraged by a
miner to gain an advantage on the rest of the mining ecosystem.

Note, I think we *might* have to go in this direction if we want to replace
replace-by-fee by replace-by-feerate or replace-by-ancestor and solve
in-depth pinning attacks. Though if we do so,
IMO we would need more thoughts.

I think we could restrain package acceptance to only confirmed inputs for
now and revisit later this point ? For LN-anchor, you can assume that the
fee-bumping UTXO feeding the CPFP is already
confirmed. Or are there currently-deployed use-cases which would benefit
from your proposed Rule #2 ?

Antoine

Le jeu. 23 sept. 2021 ? 11:36, Gloria Zhao <gloriajzhao at gmail.com> a ?crit :

> Hi Antoine,
>
> Thanks as always for your input. I'm glad we agree on so much!
>
> In summary, it seems that the decisions that might still need
> attention/input from devs on this mailing list are:
> 1. Whether we should start with multiple-parent-1-child or
> 1-parent-1-child.
> 2. Whether it's ok to require that the child not have conflicts with
> mempool transactions.
>
> Responding to your comments...
>
> > IIUC, you have package A+B, during the dedup phase early in
> `AcceptMultipleTransactions` if you observe same-txid-different-wtixd A'
> and A' is higher feerate than A, you trim A and replace by A' ?
>
> > I think this approach is safe, the one who appears unsafe to me is when
> A' has a _lower_ feerate, even if A' is already accepted by our mempool ?
> In that case iirc that would be a pinning.
>
> Right, the fact that we essentially always choose the first-seen witness
> is an unfortunate limitation that exists already. Adding package mempool
> accept doesn't worsen this, but the procedure in the future is to replace
> the witness when it makes sense economically. We can also add logic to
> allow package feerate to pay for witness replacements as well. This is
> pretty far into the future, though.
>
> > It sounds uneconomical for an attacker but I think it's not when you
> consider than you can "batch" attack against multiple honest
> counterparties. E.g, Mallory broadcast A' + B' + C' + D' where A' conflicts
> with Alice's honest package P1, B' conflicts with Bob's honest package P2,
> C' conflicts with Caroll's honest package P3. And D' is a high-fee child of
> A' + B' + C'.
>
> > If D' is higher-fee than P1 or P2 or P3 but inferior to the sum of HTLCs
> confirmed by P1+P2+P3, I think it's lucrative for the attacker ?
>
> I could be misunderstanding, but an attacker wouldn't be able to
> batch-attack like this. Alice's package only conflicts with A' + D', not A'
> + B' + C' + D'. She only needs to pay for evicting 2 transactions.
>
> > Do we assume that broadcasted packages are "honest" by default and that
> the parent(s) always need the child to pass the fee checks, that way saving
> the processing of individual transactions which are expected to fail in 99%
> of cases or more ad hoc composition of packages at relay ?
> > I think this point is quite dependent on the p2p packages format/logic
> we'll end up on and that we should feel free to revisit it later ?
>
> I think it's the opposite; there's no way for us to assume that p2p
> packages will be "honest." I'd like to have two things before we expose on
> P2P: (1) ensure that the amount of resources potentially allocated for
> package validation isn't disproportionately higher than that of single
> transaction validation and (2) only use package validation when we're
> unsatisifed with the single validation result, e.g. we might get better
> fees.
> Yes, let's revisit this later :)
>
>  > Yes, if you receive A+B, and A is already in-mempoo, I agree you can
> discard its feerate as B should pay for all fees checked on its own. Where
> I'm unclear is when you have in-mempool A+B and receive A+B'. Should B'
> have a fee high enough to cover the bandwidth penalty replacement
> (`PaysForRBF`, 2nd check) of both A+B' or only B' ?
>
>  B' only needs to pay for itself in this case.
>
> > > Do we want the child to be able to replace mempool transactions as
> well?
>
> > If we mean when you have replaceable A+B then A'+B' try to replace with
> a higher-feerate ? I think that's exactly the case we need for Lightning as
> A+B is coming from Alice and A'+B' is coming from Bob :/
>
> Let me clarify this because I can see that my wording was ambiguous, and
> then please let me know if it fits Lightning's needs?
>
> In my proposal, I wrote "If a package meets feerate requirements as a
> package, the parents in the transaction are allowed to replace-by-fee
> mempool transactions. The child cannot replace mempool transactions." What
> I meant was: the package can replace mempool transactions if any of the
> parents conflict with mempool transactions. The child cannot not conflict
> with any mempool transactions.
> The Lightning use case this attempts to address is: Alice and Mallory are
> LN counterparties, and have packages A+B and A'+B', respectively. A and A'
> are their commitment transactions and conflict with each other; they have
> shared inputs and different txids.
> B spends Alice's anchor output from A. B' spends Mallory's anchor output
> from A'. Thus, B and B' do not conflict with each other.
> Alice can broadcast her package, A+B, to replace Mallory's package, A'+B',
> since B doesn't conflict with the mempool.
>
> Would this be ok?
>
> > The second option, a child of A', In the LN case I think the CPFP is
> attached on one's anchor output.
>
> While it would be nice to have full RBF, malleability of the child won't
> block RBF here. If we're trying to replace A', we only require that A'
> signals replaceability, and don't mind if its child doesn't.
>
> > > B has an ancestor score of 10sat/vb and D has an
> > > ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than
> B's,
> > > it fails the proposed package RBF Rule #2, so this package would be
> > > rejected. Does this meet your expectations?
>
> > Well what sounds odd to me, in my example, we fail D even if it has a
> higher-fee than B. Like A+B absolute fees are 2000 sats and A+C+D absolute
> fees are 4500 sats ?
>
> Yes, A+C+D pays 2500sat more in fees, but it is also 1000vB larger. A
> miner should prefer to utilize their block space more effectively.
>
> > Is this compatible with a model where a miner prioritizes absolute fees
> over ancestor score, in the case that mempools aren't full-enough to
> fulfill a block ?
>
> No, because we don't use that model.
>
> Thanks,
> Gloria
>
> On Thu, Sep 23, 2021 at 5:29 AM Antoine Riard <antoine.riard at gmail.com>
> wrote:
>
>> > Correct, if B+C is too low feerate to be accepted, we will reject it. I
>> > prefer this because it is incentive compatible: A can be mined by
>> itself,
>> > so there's no reason to prefer A+B+C instead of A.
>> > As another way of looking at this, consider the case where we do accept
>> > A+B+C and it sits at the "bottom" of our mempool. If our mempool reaches
>> > capacity, we evict the lowest descendant feerate transactions, which are
>> > B+C in this case. This gives us the same resulting mempool, with A and
>> not
>> > B+C.
>>
>> I agree here. Doing otherwise, we might evict other transactions mempool
>> in `MempoolAccept::Finalize` with a higher-feerate than B+C while those
>> evicted transactions are the most compelling for block construction.
>>
>> I thought at first missing this acceptance requirement would break a
>> fee-bumping scheme like Parent-Pay-For-Child where a high-fee parent is
>> attached to a child signed with SIGHASH_ANYONECANPAY but in this case the
>> child fee is capturing the parent value. I can't think of other fee-bumping
>> schemes potentially affected. If they do exist I would say they're wrong in
>> their design assumptions.
>>
>> > If or when we have witness replacement, the logic is: if the individual
>> > transaction is enough to replace the mempool one, the replacement will
>> > happen during the preceding individual transaction acceptance, and
>> > deduplication logic will work. Otherwise, we will try to deduplicate by
>> > wtxid, see that we need a package witness replacement, and use the
>> package
>> > feerate to evaluate whether this is economically rational.
>>
>> IIUC, you have package A+B, during the dedup phase early in
>> `AcceptMultipleTransactions` if you observe same-txid-different-wtixd A'
>> and A' is higher feerate than A, you trim A and replace by A' ?
>>
>> I think this approach is safe, the one who appears unsafe to me is when
>> A' has a _lower_ feerate, even if A' is already accepted by our mempool ?
>> In that case iirc that would be a pinning.
>>
>> Good to see progress on witness replacement before we see usage of
>> Taproot tree in the context of multi-party, where a malicious counterparty
>> inflates its witness to jam a honest spending.
>>
>> (Note, the commit linked currently points nowhere :))
>>
>>
>> > Please note that A may replace A' even if A' has higher fees than A
>> > individually, because the proposed package RBF utilizes the fees and
>> size
>> > of the entire package. This just requires E to pay enough fees, although
>> > this can be pretty high if there are also potential B' and C' competing
>> > commitment transactions that we don't know about.
>>
>> Ah right, if the package acceptance waives `PaysMoreThanConflicts` for
>> the individual check on A, the honest package should replace the pinning
>> attempt. I've not fully parsed the proposed implementation yet.
>>
>> Though note, I think it's still unsafe for a Lightning
>> multi-commitment-broadcast-as-one-package as a malicious A' might have an
>> absolute fee higher than E. It sounds uneconomical for
>> an attacker but I think it's not when you consider than you can "batch"
>> attack against multiple honest counterparties. E.g, Mallory broadcast A' +
>> B' + C' + D' where A' conflicts with Alice's honest package P1, B'
>> conflicts with Bob's honest package P2, C' conflicts with Caroll's honest
>> package P3. And D' is a high-fee child of A' + B' + C'.
>>
>> If D' is higher-fee than P1 or P2 or P3 but inferior to the sum of HTLCs
>> confirmed by P1+P2+P3, I think it's lucrative for the attacker ?
>>
>> > So far, my understanding is that multi-parent-1-child is desired for
>> > batched fee-bumping (
>> > https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289)
>> and
>> > I've also seen your response which I have less context on (
>> > https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202).
>> That
>> > being said, I am happy to create a new proposal for 1 parent + 1 child
>> > (which would be slightly simpler) and plan for moving to
>> > multi-parent-1-child later if that is preferred. I am very interested in
>> > hearing feedback on that approach.
>>
>> I think batched fee-bumping is okay as long as you don't have
>> time-sensitive outputs encumbering your commitment transactions. For the
>> reasons mentioned above, I think that's unsafe.
>>
>> What I'm worried about is  L2 developers, potentially not aware about all
>> the mempool subtleties blurring the difference and always batching their
>> broadcast by default.
>>
>> IMO, a good thing by restraining to 1-parent + 1 child,  we artificially
>> constraint L2 design space for now and minimize risks of unsafe usage of
>> the package API :)
>>
>> I think that's a point where it would be relevant to have the opinion of
>> more L2 devs.
>>
>> > I think there is a misunderstanding here - let me describe what I'm
>> > proposing we'd do in this situation: we'll try individual submission
>> for A,
>> > see that it fails due to "insufficient fees." Then, we'll try package
>> > validation for A+B and use package RBF. If A+B pays enough, it can still
>> > replace A'. If A fails for a bad signature, we won't look at B or A+B.
>> Does
>> > this meet your expectations?
>>
>> Yes there was a misunderstanding, I think this approach is correct, it's
>> more a question of performance. Do we assume that broadcasted packages are
>> "honest" by default and that the parent(s) always need the child to pass
>> the fee checks, that way saving the processing of individual transactions
>> which are expected to fail in 99% of cases or more ad hoc composition of
>> packages at relay ?
>>
>> I think this point is quite dependent on the p2p packages format/logic
>> we'll end up on and that we should feel free to revisit it later ?
>>
>>
>> > What problem are you trying to solve by the package feerate *after*
>> dedup
>> rule ?
>> > My understanding is that an in-package transaction might be already in
>> the mempool. Therefore, to compute a correct RBF penalty replacement, the
>> vsize of this transaction could be discarded lowering the cost of package
>> RBF.
>>
>> > I'm proposing that, when a transaction has already been submitted to
>> > mempool, we would ignore both its fees and vsize when calculating
>> package
>> > feerate.
>>
>> Yes, if you receive A+B, and A is already in-mempoo, I agree you can
>> discard its feerate as B should pay for all fees checked on its own. Where
>> I'm unclear is when you have in-mempool A+B and receive A+B'. Should B'
>> have a fee high enough to cover the bandwidth penalty replacement
>> (`PaysForRBF`, 2nd check) of both A+B' or only B' ?
>>
>> If you have a second-layer like current Lightning, you might have a
>> counterparty commitment to replace and should always expect to have to pay
>> for parent replacement bandwidth.
>>
>> Where a potential discount sounds interesting is when you have an
>> univoque state on the first-stage of transactions. E.g DLC's funding
>> transaction which might be CPFP by any participant iirc.
>>
>> > Note that, if C' conflicts with C, it also conflicts with D, since D is
>> a
>> > descendant of C and would thus need to be evicted along with it.
>>
>> Ah once again I think it's a misunderstanding without the code under my
>> eyes! If we do C' `PreChecks`, solve the conflicts provoked by it, i.e mark
>> for potential eviction D and don't consider it for future conflicts in the
>> rest of the package, I think D' `PreChecks` should be good ?
>>
>> > More generally, this example is surprising to me because I didn't think
>> > packages would be used to fee-bump replaceable transactions. Do we want
>> the
>> > child to be able to replace mempool transactions as well?
>>
>> If we mean when you have replaceable A+B then A'+B' try to replace with a
>> higher-feerate ? I think that's exactly the case we need for Lightning as
>> A+B is coming from Alice and A'+B' is coming from Bob :/
>>
>> > I'm not sure what you mean? Let's say we have a package of parent A +
>> child
>> > B, where A is supposed to replace a mempool transaction A'. Are you
>> saying
>> > that counterparties are able to malleate the package child B, or a
>> child of
>> > A'?
>>
>> The second option, a child of A', In the LN case I think the CPFP is
>> attached on one's anchor output.
>>
>> I think it's good if we assume the
>> solve-conflicts-after-parent's`'PreChecks` mentioned above or fixing
>> inherited signaling or full-rbf ?
>>
>> > Sorry, I don't understand what you mean by "preserve the package
>> > integrity?" Could you elaborate?
>>
>> After thinking the relaxation about the "new" unconfirmed input is not
>> linked to trimming but I would say more to the multi-parent support.
>>
>> Let's say you have A+B trying to replace C+D where B is also spending
>> already in-mempool E. To succeed, you need to waive the no-new-unconfirmed
>> input as D isn't spending E.
>>
>> So good, I think we agree on the problem description here.
>>
>> > I am in agreement with your calculations but unsure if we disagree on
>> the
>> > expected outcome. Yes, B has an ancestor score of 10sat/vb and D has an
>> > ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than
>> B's,
>> > it fails the proposed package RBF Rule #2, so this package would be
>> > rejected. Does this meet your expectations?
>>
>> Well what sounds odd to me, in my example, we fail D even if it has a
>> higher-fee than B. Like A+B absolute fees are 2000 sats and A+C+D absolute
>> fees are 4500 sats ?
>>
>> Is this compatible with a model where a miner prioritizes absolute fees
>> over ancestor score, in the case that mempools aren't full-enough to
>> fulfill a block ?
>>
>> Let me know if I can clarify a point.
>>
>> Antoine
>>
>> Le lun. 20 sept. 2021 ? 11:10, Gloria Zhao <gloriajzhao at gmail.com> a
>> ?crit :
>>
>>>
>>> Hi Antoine,
>>>
>>> First of all, thank you for the thorough review. I appreciate your
>>> insight on LN requirements.
>>>
>>> > IIUC, you have a package A+B+C submitted for acceptance and A is
>>> already in your mempool. You trim out A from the package and then evaluate
>>> B+C.
>>>
>>> > I think this might be an issue if A is the higher-fee element of the
>>> ABC package. B+C package fees might be under the mempool min fee and will
>>> be rejected, potentially breaking the acceptance expectations of the
>>> package issuer ?
>>>
>>> Correct, if B+C is too low feerate to be accepted, we will reject it. I
>>> prefer this because it is incentive compatible: A can be mined by itself,
>>> so there's no reason to prefer A+B+C instead of A.
>>> As another way of looking at this, consider the case where we do accept
>>> A+B+C and it sits at the "bottom" of our mempool. If our mempool reaches
>>> capacity, we evict the lowest descendant feerate transactions, which are
>>> B+C in this case. This gives us the same resulting mempool, with A and not
>>> B+C.
>>>
>>>
>>> > Further, I think the dedup should be done on wtxid, as you might have
>>> multiple valid witnesses. Though with varying vsizes and as such offering
>>> different feerates.
>>>
>>> I agree that variations of the same package with different witnesses is
>>> a case that must be handled. I consider witness replacement to be a project
>>> that can be done in parallel to package mempool acceptance because being
>>> able to accept packages does not worsen the problem of a
>>> same-txid-different-witness "pinning" attack.
>>>
>>> If or when we have witness replacement, the logic is: if the individual
>>> transaction is enough to replace the mempool one, the replacement will
>>> happen during the preceding individual transaction acceptance, and
>>> deduplication logic will work. Otherwise, we will try to deduplicate by
>>> wtxid, see that we need a package witness replacement, and use the package
>>> feerate to evaluate whether this is economically rational.
>>>
>>> See the #22290 "handle package transactions already in mempool" commit (
>>> https://github.com/bitcoin/bitcoin/pull/22290/commits/fea75a2237b46cf76145242fecad7e274bfcb5ff),
>>> which handles the case of same-txid-different-witness by simply using the
>>> transaction in the mempool for now, with TODOs for what I just described.
>>>
>>>
>>> > I'm not clearly understanding the accepted topologies. By "parent and
>>> child to share a parent", do you mean the set of transactions A, B, C,
>>> where B is spending A and C is spending A and B would be correct ?
>>>
>>> Yes, that is what I meant. Yes, that would a valid package under these
>>> rules.
>>>
>>> > If yes, is there a width-limit introduced or we fallback on
>>> MAX_PACKAGE_COUNT=25 ?
>>>
>>> No, there is no limit on connectivity other than "child with all
>>> unconfirmed parents." We will enforce MAX_PACKAGE_COUNT=25 and child's
>>> in-mempool + in-package ancestor limits.
>>>
>>>
>>> > Considering the current Core's mempool acceptance rules, I think CPFP
>>> batching is unsafe for LN time-sensitive closure. A malicious tx-relay
>>> jamming successful on one channel commitment transaction would contamine
>>> the remaining commitments sharing the same package.
>>>
>>> > E.g, you broadcast the package A+B+C+D+E where A,B,C,D are commitment
>>> transactions and E a shared CPFP. If a malicious A' transaction has a
>>> better feerate than A, the whole package acceptance will fail. Even if A'
>>> confirms in the following block,
>>> the propagation and confirmation of B+C+D have been delayed. This could
>>> carry on a loss of funds.
>>>
>>> Please note that A may replace A' even if A' has higher fees than A
>>> individually, because the proposed package RBF utilizes the fees and size
>>> of the entire package. This just requires E to pay enough fees, although
>>> this can be pretty high if there are also potential B' and C' competing
>>> commitment transactions that we don't know about.
>>>
>>>
>>> > IMHO, I'm leaning towards deploying during a first phase
>>> 1-parent/1-child. I think it's the most conservative step still improving
>>> second-layer safety.
>>>
>>> So far, my understanding is that multi-parent-1-child is desired for
>>> batched fee-bumping (
>>> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289)
>>> and I've also seen your response which I have less context on (
>>> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202).
>>> That being said, I am happy to create a new proposal for 1 parent + 1 child
>>> (which would be slightly simpler) and plan for moving to
>>> multi-parent-1-child later if that is preferred. I am very interested in
>>> hearing feedback on that approach.
>>>
>>>
>>> > If A+B is submitted to replace A', where A pays 0 sats, B pays 200
>>> sats and A' pays 100 sats. If we apply the individual RBF on A, A+B
>>> acceptance fails. For this reason I think the individual RBF should be
>>> bypassed and only the package RBF apply ?
>>>
>>> I think there is a misunderstanding here - let me describe what I'm
>>> proposing we'd do in this situation: we'll try individual submission for A,
>>> see that it fails due to "insufficient fees." Then, we'll try package
>>> validation for A+B and use package RBF. If A+B pays enough, it can still
>>> replace A'. If A fails for a bad signature, we won't look at B or A+B. Does
>>> this meet your expectations?
>>>
>>>
>>> > What problem are you trying to solve by the package feerate *after*
>>> dedup rule ?
>>> > My understanding is that an in-package transaction might be already in
>>> the mempool. Therefore, to compute a correct RBF penalty replacement, the
>>> vsize of this transaction could be discarded lowering the cost of package
>>> RBF.
>>>
>>> I'm proposing that, when a transaction has already been submitted to
>>> mempool, we would ignore both its fees and vsize when calculating package
>>> feerate. In example G2, we shouldn't count M1 fees after its submission to
>>> mempool, since M1's fees have already been used to pay for its individual
>>> bandwidth, and it shouldn't be used again to pay for P2 and P3's bandwidth.
>>> We also shouldn't count its vsize, since it has already been paid for.
>>>
>>>
>>> > I think this is a footgunish API, as if a package issuer send the
>>> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.
>>> Then try to broadcast the higher-feerate C'+D' package, it should be
>>> rejected. So it's breaking the naive broadcaster assumption that a
>>> higher-feerate/higher-fee package always replaces ?
>>>
>>> Note that, if C' conflicts with C, it also conflicts with D, since D is
>>> a descendant of C and would thus need to be evicted along with it.
>>> Implicitly, D' would not be in conflict with D.
>>> More generally, this example is surprising to me because I didn't think
>>> packages would be used to fee-bump replaceable transactions. Do we want the
>>> child to be able to replace mempool transactions as well? This can be
>>> implemented with a bit of additional logic.
>>>
>>> > I think this is unsafe for L2s if counterparties have malleability of
>>> the child transaction. They can block your package replacement by
>>> opting-out from RBF signaling. IIRC, LN's "anchor output" presents such an
>>> ability.
>>>
>>> I'm not sure what you mean? Let's say we have a package of parent A +
>>> child B, where A is supposed to replace a mempool transaction A'. Are you
>>> saying that counterparties are able to malleate the package child B, or a
>>> child of A'? If they can malleate a child of A', that shouldn't matter as
>>> long as A' is signaling replacement. This would be handled identically with
>>> full RBF and what Core currently implements.
>>>
>>> > I think this is an issue brought by the trimming during the dedup
>>> phase. If we preserve the package integrity, only re-using the tx-level
>>> checks results of already in-mempool transactions to gain in CPU time we
>>> won't have this issue. Package childs can add unconfirmed inputs as long as
>>> they're in-package, the bip125 rule2 is only evaluated against parents ?
>>>
>>> Sorry, I don't understand what you mean by "preserve the package
>>> integrity?" Could you elaborate?
>>>
>>> > Let's say you have in-mempool A, B where A pays 10 sat/vb for 100
>>> vbytes and B pays 10 sat/vb for 100 vbytes. You have the candidate
>>> replacement D spending both A and C where D pays 15sat/vb for 100 vbytes
>>> and C pays 1 sat/vb for 1000 vbytes.
>>>
>>> > Package A + B ancestor score is 10 sat/vb.
>>>
>>> > D has a higher feerate/absolute fee than B.
>>>
>>> > Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's
>>> 1000 sats + D's 1500 sats) / A's 100 vb + C's 1000 vb + D's 100 vb)
>>>
>>> I am in agreement with your calculations but unsure if we disagree on
>>> the expected outcome. Yes, B has an ancestor score of 10sat/vb and D has an
>>> ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than B's,
>>> it fails the proposed package RBF Rule #2, so this package would be
>>> rejected. Does this meet your expectations?
>>>
>>> Thank you for linking to projects that might be interested in package
>>> relay :)
>>>
>>> Thanks,
>>> Gloria
>>>
>>> On Mon, Sep 20, 2021 at 12:16 AM Antoine Riard <antoine.riard at gmail.com>
>>> wrote:
>>>
>>>> Hi Gloria,
>>>>
>>>> > A package may contain transactions that are already in the mempool. We
>>>> > remove
>>>> > ("deduplicate") those transactions from the package for the purposes
>>>> of
>>>> > package
>>>> > mempool acceptance. If a package is empty after deduplication, we do
>>>> > nothing.
>>>>
>>>> IIUC, you have a package A+B+C submitted for acceptance and A is
>>>> already in your mempool. You trim out A from the package and then evaluate
>>>> B+C.
>>>>
>>>> I think this might be an issue if A is the higher-fee element of the
>>>> ABC package. B+C package fees might be under the mempool min fee and will
>>>> be rejected, potentially breaking the acceptance expectations of the
>>>> package issuer ?
>>>>
>>>> Further, I think the dedup should be done on wtxid, as you might have
>>>> multiple valid witnesses. Though with varying vsizes and as such offering
>>>> different feerates.
>>>>
>>>> E.g you're going to evaluate the package A+B and A' is already in your
>>>> mempool with a bigger valid witness. You trim A based on txid, then you
>>>> evaluate A'+B, which fails the fee checks. However, evaluating A+B would
>>>> have been a success.
>>>>
>>>> AFAICT, the dedup rationale would be to save on CPU time/IO disk, to
>>>> avoid repeated signatures verification and parent UTXOs fetches ? Can we
>>>> achieve the same goal by bypassing tx-level checks for already-in txn while
>>>> conserving the package integrity for package-level checks ?
>>>>
>>>> > Note that it's possible for the parents to be
>>>> > indirect
>>>> > descendants/ancestors of one another, or for parent and child to
>>>> share a
>>>> > parent,
>>>> > so we cannot make any other topology assumptions.
>>>>
>>>> I'm not clearly understanding the accepted topologies. By "parent and
>>>> child to share a parent", do you mean the set of transactions A, B, C,
>>>> where B is spending A and C is spending A and B would be correct ?
>>>>
>>>> If yes, is there a width-limit introduced or we fallback on
>>>> MAX_PACKAGE_COUNT=25 ?
>>>>
>>>> IIRC, one rationale to come with this topology limitation was to lower
>>>> the DoS risks when potentially deploying p2p packages.
>>>>
>>>> Considering the current Core's mempool acceptance rules, I think CPFP
>>>> batching is unsafe for LN time-sensitive closure. A malicious tx-relay
>>>> jamming successful on one channel commitment transaction would contamine
>>>> the remaining commitments sharing the same package.
>>>>
>>>> E.g, you broadcast the package A+B+C+D+E where A,B,C,D are commitment
>>>> transactions and E a shared CPFP. If a malicious A' transaction has a
>>>> better feerate than A, the whole package acceptance will fail. Even if A'
>>>> confirms in the following block,
>>>> the propagation and confirmation of B+C+D have been delayed. This could
>>>> carry on a loss of funds.
>>>>
>>>> That said, if you're broadcasting commitment transactions without
>>>> time-sensitive HTLC outputs, I think the batching is effectively a fee
>>>> saving as you don't have to duplicate the CPFP.
>>>>
>>>> IMHO, I'm leaning towards deploying during a first phase
>>>> 1-parent/1-child. I think it's the most conservative step still improving
>>>> second-layer safety.
>>>>
>>>> > *Rationale*:  It would be incorrect to use the fees of transactions
>>>> that are
>>>> > already in the mempool, as we do not want a transaction's fees to be
>>>> > double-counted for both its individual RBF and package RBF.
>>>>
>>>> I'm unsure about the logical order of the checks proposed.
>>>>
>>>> If A+B is submitted to replace A', where A pays 0 sats, B pays 200 sats
>>>> and A' pays 100 sats. If we apply the individual RBF on A, A+B acceptance
>>>> fails. For this reason I think the individual RBF should be bypassed and
>>>> only the package RBF apply ?
>>>>
>>>> Note this situation is plausible, with current LN design, your
>>>> counterparty can have a commitment transaction with a better fee just by
>>>> selecting a higher `dust_limit_satoshis` than yours.
>>>>
>>>> > Examples F and G [14] show the same package, but P1 is submitted
>>>> > individually before
>>>> > the package in example G. In example F, we can see that the 300vB
>>>> package
>>>> > pays
>>>> > an additional 200sat in fees, which is not enough to pay for its own
>>>> > bandwidth
>>>> > (BIP125#4). In example G, we can see that P1 pays enough to replace
>>>> M1, but
>>>> > using P1's fees again during package submission would make it look
>>>> like a
>>>> > 300sat
>>>> > increase for a 200vB package. Even including its fees and size would
>>>> not be
>>>> > sufficient in this example, since the 300sat looks like enough for
>>>> the 300vB
>>>> > package. The calculcation after deduplication is 100sat increase for a
>>>> > package
>>>> > of size 200vB, which correctly fails BIP125#4. Assume all
>>>> transactions have
>>>> > a
>>>> > size of 100vB.
>>>>
>>>> What problem are you trying to solve by the package feerate *after*
>>>> dedup rule ?
>>>>
>>>> My understanding is that an in-package transaction might be already in
>>>> the mempool. Therefore, to compute a correct RBF penalty replacement, the
>>>> vsize of this transaction could be discarded lowering the cost of package
>>>> RBF.
>>>>
>>>> If we keep a "safe" dedup mechanism (see my point above), I think this
>>>> discount is justified, as the validation cost of node operators is paid for
>>>> ?
>>>>
>>>> > The child cannot replace mempool transactions.
>>>>
>>>> Let's say you issue package A+B, then package C+B', where B' is a child
>>>> of both A and C. This rule fails the acceptance of C+B' ?
>>>>
>>>> I think this is a footgunish API, as if a package issuer send the
>>>> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.
>>>> Then try to broadcast the higher-feerate C'+D' package, it should be
>>>> rejected. So it's breaking the naive broadcaster assumption that a
>>>> higher-feerate/higher-fee package always replaces ? And it might be unsafe
>>>> in protocols where states are symmetric. E.g a malicious counterparty
>>>> broadcasts first S+A, then you honestly broadcast S+B, where B pays better
>>>> fees.
>>>>
>>>> > All mempool transactions to be replaced must signal replaceability.
>>>>
>>>> I think this is unsafe for L2s if counterparties have malleability of
>>>> the child transaction. They can block your package replacement by
>>>> opting-out from RBF signaling. IIRC, LN's "anchor output" presents such an
>>>> ability.
>>>>
>>>> I think it's better to either fix inherited signaling or move towards
>>>> full-rbf.
>>>>
>>>> > if a package parent has already been submitted, it would
>>>> > look
>>>> >like the child is spending a "new" unconfirmed input.
>>>>
>>>> I think this is an issue brought by the trimming during the dedup
>>>> phase. If we preserve the package integrity, only re-using the tx-level
>>>> checks results of already in-mempool transactions to gain in CPU time we
>>>> won't have this issue. Package childs can add unconfirmed inputs as long as
>>>> they're in-package, the bip125 rule2 is only evaluated against parents ?
>>>>
>>>> > However, we still achieve the same goal of requiring the
>>>> > replacement
>>>> > transactions to have a ancestor score at least as high as the original
>>>> > ones.
>>>>
>>>> I'm not sure if this holds...
>>>>
>>>> Let's say you have in-mempool A, B where A pays 10 sat/vb for 100
>>>> vbytes and B pays 10 sat/vb for 100 vbytes. You have the candidate
>>>> replacement D spending both A and C where D pays 15sat/vb for 100 vbytes
>>>> and C pays 1 sat/vb for 1000 vbytes.
>>>>
>>>> Package A + B ancestor score is 10 sat/vb.
>>>>
>>>> D has a higher feerate/absolute fee than B.
>>>>
>>>> Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's
>>>> 1000 sats + D's 1500 sats) /
>>>> A's 100 vb + C's 1000 vb + D's 100 vb)
>>>>
>>>> Overall, this is a review through the lenses of LN requirements. I
>>>> think other L2 protocols/applications
>>>> could be candidates to using package accept/relay such as:
>>>> * https://github.com/lightninglabs/pool
>>>> * https://github.com/discreetlogcontracts/dlcspecs
>>>> * https://github.com/bitcoin-teleport/teleport-transactions/
>>>> * https://github.com/sapio-lang/sapio
>>>> *
>>>> https://github.com/commerceblock/mercury/blob/master/doc/statechains.md
>>>> * https://github.com/revault/practical-revault
>>>>
>>>> Thanks for rolling forward the ball on this subject.
>>>>
>>>> Antoine
>>>>
>>>> Le jeu. 16 sept. 2021 ? 03:55, Gloria Zhao via bitcoin-dev <
>>>> bitcoin-dev at lists.linuxfoundation.org> a ?crit :
>>>>
>>>>> Hi there,
>>>>>
>>>>> I'm writing to propose a set of mempool policy changes to enable
>>>>> package
>>>>> validation (in preparation for package relay) in Bitcoin Core. These
>>>>> would not
>>>>> be consensus or P2P protocol changes. However, since mempool policy
>>>>> significantly affects transaction propagation, I believe this is
>>>>> relevant for
>>>>> the mailing list.
>>>>>
>>>>> My proposal enables packages consisting of multiple parents and 1
>>>>> child. If you
>>>>> develop software that relies on specific transaction relay assumptions
>>>>> and/or
>>>>> are interested in using package relay in the future, I'm very
>>>>> interested to hear
>>>>> your feedback on the utility or restrictiveness of these package
>>>>> policies for
>>>>> your use cases.
>>>>>
>>>>> A draft implementation of this proposal can be found in [Bitcoin Core
>>>>> PR#22290][1].
>>>>>
>>>>> An illustrated version of this post can be found at
>>>>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.
>>>>> I have also linked the images below.
>>>>>
>>>>> ## Background
>>>>>
>>>>> Feel free to skip this section if you are already familiar with
>>>>> mempool policy
>>>>> and package relay terminology.
>>>>>
>>>>> ### Terminology Clarifications
>>>>>
>>>>> * Package = an ordered list of related transactions, representable by
>>>>> a Directed
>>>>>   Acyclic Graph.
>>>>> * Package Feerate = the total modified fees divided by the total
>>>>> virtual size of
>>>>>   all transactions in the package.
>>>>>     - Modified fees = a transaction's base fees + fee delta applied by
>>>>> the user
>>>>>       with `prioritisetransaction`. As such, we expect this to vary
>>>>> across
>>>>> mempools.
>>>>>     - Virtual Size = the maximum of virtual sizes calculated using
>>>>> [BIP141
>>>>>       virtual size][2] and sigop weight. [Implemented here in Bitcoin
>>>>> Core][3].
>>>>>     - Note that feerate is not necessarily based on the base fees and
>>>>> serialized
>>>>>       size.
>>>>>
>>>>> * Fee-Bumping = user/wallet actions that take advantage of miner
>>>>> incentives to
>>>>>   boost a transaction's candidacy for inclusion in a block, including
>>>>> Child Pays
>>>>> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our intention
>>>>> in
>>>>> mempool policy is to recognize when the new transaction is more
>>>>> economical to
>>>>> mine than the original one(s) but not open DoS vectors, so there are
>>>>> some
>>>>> limitations.
>>>>>
>>>>> ### Policy
>>>>>
>>>>> The purpose of the mempool is to store the best (to be most
>>>>> incentive-compatible
>>>>> with miners, highest feerate) candidates for inclusion in a block.
>>>>> Miners use
>>>>> the mempool to build block templates. The mempool is also useful as a
>>>>> cache for
>>>>> boosting block relay and validation performance, aiding transaction
>>>>> relay, and
>>>>> generating feerate estimations.
>>>>>
>>>>> Ideally, all consensus-valid transactions paying reasonable fees
>>>>> should make it
>>>>> to miners through normal transaction relay, without any special
>>>>> connectivity or
>>>>> relationships with miners. On the other hand, nodes do not have
>>>>> unlimited
>>>>> resources, and a P2P network designed to let any honest node broadcast
>>>>> their
>>>>> transactions also exposes the transaction validation engine to DoS
>>>>> attacks from
>>>>> malicious peers.
>>>>>
>>>>> As such, for unconfirmed transactions we are considering for our
>>>>> mempool, we
>>>>> apply a set of validation rules in addition to consensus, primarily to
>>>>> protect
>>>>> us from resource exhaustion and aid our efforts to keep the highest fee
>>>>> transactions. We call this mempool _policy_: a set of (configurable,
>>>>> node-specific) rules that transactions must abide by in order to be
>>>>> accepted
>>>>> into our mempool. Transaction "Standardness" rules and mempool
>>>>> restrictions such
>>>>> as "too-long-mempool-chain" are both examples of policy.
>>>>>
>>>>> ### Package Relay and Package Mempool Accept
>>>>>
>>>>> In transaction relay, we currently consider transactions one at a time
>>>>> for
>>>>> submission to the mempool. This creates a limitation in the node's
>>>>> ability to
>>>>> determine which transactions have the highest feerates, since we
>>>>> cannot take
>>>>> into account descendants (i.e. cannot use CPFP) until all the
>>>>> transactions are
>>>>> in the mempool. Similarly, we cannot use a transaction's descendants
>>>>> when
>>>>> considering it for RBF. When an individual transaction does not meet
>>>>> the mempool
>>>>> minimum feerate and the user isn't able to create a replacement
>>>>> transaction
>>>>> directly, it will not be accepted by mempools.
>>>>>
>>>>> This limitation presents a security issue for applications and users
>>>>> relying on
>>>>> time-sensitive transactions. For example, Lightning and other
>>>>> protocols create
>>>>> UTXOs with multiple spending paths, where one counterparty's spending
>>>>> path opens
>>>>> up after a timelock, and users are protected from cheating scenarios
>>>>> as long as
>>>>> they redeem on-chain in time. A key security assumption is that all
>>>>> parties'
>>>>> transactions will propagate and confirm in a timely manner. This
>>>>> assumption can
>>>>> be broken if fee-bumping does not work as intended.
>>>>>
>>>>> The end goal for Package Relay is to consider multiple transactions at
>>>>> the same
>>>>> time, e.g. a transaction with its high-fee child. This may help us
>>>>> better
>>>>> determine whether transactions should be accepted to our mempool,
>>>>> especially if
>>>>> they don't meet fee requirements individually or are better RBF
>>>>> candidates as a
>>>>> package. A combination of changes to mempool validation logic, policy,
>>>>> and
>>>>> transaction relay allows us to better propagate the transactions with
>>>>> the
>>>>> highest package feerates to miners, and makes fee-bumping tools more
>>>>> powerful
>>>>> for users.
>>>>>
>>>>> The "relay" part of Package Relay suggests P2P messaging changes, but
>>>>> a large
>>>>> part of the changes are in the mempool's package validation logic. We
>>>>> call this
>>>>> *Package Mempool Accept*.
>>>>>
>>>>> ### Previous Work
>>>>>
>>>>> * Given that mempool validation is DoS-sensitive and complex, it would
>>>>> be
>>>>>   dangerous to haphazardly tack on package validation logic. Many
>>>>> efforts have
>>>>> been made to make mempool validation less opaque (see [#16400][4],
>>>>> [#21062][5],
>>>>> [#22675][6], [#22796][7]).
>>>>> * [#20833][8] Added basic capabilities for package validation, test
>>>>> accepts only
>>>>>   (no submission to mempool).
>>>>> * [#21800][9] Implemented package ancestor/descendant limit checks for
>>>>> arbitrary
>>>>>   packages. Still test accepts only.
>>>>> * Previous package relay proposals (see [#16401][10], [#19621][11]).
>>>>>
>>>>> ### Existing Package Rules
>>>>>
>>>>> These are in master as introduced in [#20833][8] and [#21800][9]. I'll
>>>>> consider
>>>>> them as "given" in the rest of this document, though they can be
>>>>> changed, since
>>>>> package validation is test-accept only right now.
>>>>>
>>>>> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and
>>>>> `MAX_PACKAGE_SIZE=101KvB` total size [8]
>>>>>
>>>>>    *Rationale*: This is already enforced as mempool
>>>>> ancestor/descendant limits.
>>>>> Presumably, transactions in a package are all related, so exceeding
>>>>> this limit
>>>>> would mean that the package can either be split up or it wouldn't pass
>>>>> this
>>>>> mempool policy.
>>>>>
>>>>> 2. Packages must be topologically sorted: if any dependencies exist
>>>>> between
>>>>> transactions, parents must appear somewhere before children. [8]
>>>>>
>>>>> 3. A package cannot have conflicting transactions, i.e. none of them
>>>>> can spend
>>>>> the same inputs. This also means there cannot be duplicate
>>>>> transactions. [8]
>>>>>
>>>>> 4. When packages are evaluated against ancestor/descendant limits in a
>>>>> test
>>>>> accept, the union of all of their descendants and ancestors is
>>>>> considered. This
>>>>> is essentially a "worst case" heuristic where every transaction in the
>>>>> package
>>>>> is treated as each other's ancestor and descendant. [8]
>>>>> Packages for which ancestor/descendant limits are accurately captured
>>>>> by this
>>>>> heuristic: [19]
>>>>>
>>>>> There are also limitations such as the fact that CPFP carve out is not
>>>>> applied
>>>>> to package transactions. #20833 also disables RBF in package
>>>>> validation; this
>>>>> proposal overrides that to allow packages to use RBF.
>>>>>
>>>>> ## Proposed Changes
>>>>>
>>>>> The next step in the Package Mempool Accept project is to implement
>>>>> submission
>>>>> to mempool, initially through RPC only. This allows us to test the
>>>>> submission
>>>>> logic before exposing it on P2P.
>>>>>
>>>>> ### Summary
>>>>>
>>>>> - Packages may contain already-in-mempool transactions.
>>>>> - Packages are 2 generations, Multi-Parent-1-Child.
>>>>> - Fee-related checks use the package feerate. This means that wallets
>>>>> can
>>>>> create a package that utilizes CPFP.
>>>>> - Parents are allowed to RBF mempool transactions with a set of rules
>>>>> similar
>>>>>   to BIP125. This enables a combination of CPFP and RBF, where a
>>>>> transaction's descendant fees pay for replacing mempool conflicts.
>>>>>
>>>>> There is a draft implementation in [#22290][1]. It is WIP, but
>>>>> feedback is
>>>>> always welcome.
>>>>>
>>>>> ### Details
>>>>>
>>>>> #### Packages May Contain Already-in-Mempool Transactions
>>>>>
>>>>> A package may contain transactions that are already in the mempool. We
>>>>> remove
>>>>> ("deduplicate") those transactions from the package for the purposes
>>>>> of package
>>>>> mempool acceptance. If a package is empty after deduplication, we do
>>>>> nothing.
>>>>>
>>>>> *Rationale*: Mempools vary across the network. It's possible for a
>>>>> parent to be
>>>>> accepted to the mempool of a peer on its own due to differences in
>>>>> policy and
>>>>> fee market fluctuations. We should not reject or penalize the entire
>>>>> package for
>>>>> an individual transaction as that could be a censorship vector.
>>>>>
>>>>> #### Packages Are Multi-Parent-1-Child
>>>>>
>>>>> Only packages of a specific topology are permitted. Namely, a package
>>>>> is exactly
>>>>> 1 child with all of its unconfirmed parents. After deduplication, the
>>>>> package
>>>>> may be exactly the same, empty, 1 child, 1 child with just some of its
>>>>> unconfirmed parents, etc. Note that it's possible for the parents to
>>>>> be indirect
>>>>> descendants/ancestors of one another, or for parent and child to share
>>>>> a parent,
>>>>> so we cannot make any other topology assumptions.
>>>>>
>>>>> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple
>>>>> parents
>>>>> makes it possible to fee-bump a batch of transactions. Restricting
>>>>> packages to a
>>>>> defined topology is also easier to reason about and simplifies the
>>>>> validation
>>>>> logic greatly. Multi-parent-1-child allows us to think of the package
>>>>> as one big
>>>>> transaction, where:
>>>>>
>>>>> - Inputs = all the inputs of parents + inputs of the child that come
>>>>> from
>>>>>   confirmed UTXOs
>>>>> - Outputs = all the outputs of the child + all outputs of the parents
>>>>> that
>>>>>   aren't spent by other transactions in the package
>>>>>
>>>>> Examples of packages that follow this rule (variations of example A
>>>>> show some
>>>>> possibilities after deduplication): ![image][15]
>>>>>
>>>>> #### Fee-Related Checks Use Package Feerate
>>>>>
>>>>> Package Feerate = the total modified fees divided by the total virtual
>>>>> size of
>>>>> all transactions in the package.
>>>>>
>>>>> To meet the two feerate requirements of a mempool, i.e., the
>>>>> pre-configured
>>>>> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum
>>>>> feerate, the
>>>>> total package feerate is used instead of the individual feerate. The
>>>>> individual
>>>>> transactions are allowed to be below feerate requirements if the
>>>>> package meets
>>>>> the feerate requirements. For example, the parent(s) in the package
>>>>> can have 0
>>>>> fees but be paid for by the child.
>>>>>
>>>>> *Rationale*: This can be thought of as "CPFP within a package,"
>>>>> solving the
>>>>> issue of a parent not meeting minimum fees on its own. This allows L2
>>>>> applications to adjust their fees at broadcast time instead of
>>>>> overshooting or
>>>>> risking getting stuck/pinned.
>>>>>
>>>>> We use the package feerate of the package *after deduplication*.
>>>>>
>>>>> *Rationale*:  It would be incorrect to use the fees of transactions
>>>>> that are
>>>>> already in the mempool, as we do not want a transaction's fees to be
>>>>> double-counted for both its individual RBF and package RBF.
>>>>>
>>>>> Examples F and G [14] show the same package, but P1 is submitted
>>>>> individually before
>>>>> the package in example G. In example F, we can see that the 300vB
>>>>> package pays
>>>>> an additional 200sat in fees, which is not enough to pay for its own
>>>>> bandwidth
>>>>> (BIP125#4). In example G, we can see that P1 pays enough to replace
>>>>> M1, but
>>>>> using P1's fees again during package submission would make it look
>>>>> like a 300sat
>>>>> increase for a 200vB package. Even including its fees and size would
>>>>> not be
>>>>> sufficient in this example, since the 300sat looks like enough for the
>>>>> 300vB
>>>>> package. The calculcation after deduplication is 100sat increase for a
>>>>> package
>>>>> of size 200vB, which correctly fails BIP125#4. Assume all transactions
>>>>> have a
>>>>> size of 100vB.
>>>>>
>>>>> #### Package RBF
>>>>>
>>>>> If a package meets feerate requirements as a package, the parents in
>>>>> the
>>>>> transaction are allowed to replace-by-fee mempool transactions. The
>>>>> child cannot
>>>>> replace mempool transactions. Multiple transactions can replace the
>>>>> same
>>>>> transaction, but in order to be valid, none of the transactions can
>>>>> try to
>>>>> replace an ancestor of another transaction in the same package (which
>>>>> would thus
>>>>> make its inputs unavailable).
>>>>>
>>>>> *Rationale*: Even if we are using package feerate, a package will not
>>>>> propagate
>>>>> as intended if RBF still requires each individual transaction to meet
>>>>> the
>>>>> feerate requirements.
>>>>>
>>>>> We use a set of rules slightly modified from BIP125 as follows:
>>>>>
>>>>> ##### Signaling (Rule #1)
>>>>>
>>>>> All mempool transactions to be replaced must signal replaceability.
>>>>>
>>>>> *Rationale*: Package RBF signaling logic should be the same for
>>>>> package RBF and
>>>>> single transaction acceptance. This would be updated if single
>>>>> transaction
>>>>> validation moves to full RBF.
>>>>>
>>>>> ##### New Unconfirmed Inputs (Rule #2)
>>>>>
>>>>> A package may include new unconfirmed inputs, but the ancestor feerate
>>>>> of the
>>>>> child must be at least as high as the ancestor feerates of every
>>>>> transaction
>>>>> being replaced. This is contrary to BIP125#2, which states "The
>>>>> replacement
>>>>> transaction may only include an unconfirmed input if that input was
>>>>> included in
>>>>> one of the original transactions. (An unconfirmed input spends an
>>>>> output from a
>>>>> currently-unconfirmed transaction.)"
>>>>>
>>>>> *Rationale*: The purpose of BIP125#2 is to ensure that the replacement
>>>>> transaction has a higher ancestor score than the original
>>>>> transaction(s) (see
>>>>> [comment][13]). Example H [16] shows how adding a new unconfirmed
>>>>> input can lower the
>>>>> ancestor score of the replacement transaction. P1 is trying to replace
>>>>> M1, and
>>>>> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat,
>>>>> and M2 pays
>>>>> 100sat. Assume all transactions have a size of 100vB. While, in
>>>>> isolation, P1
>>>>> looks like a better mining candidate than M1, it must be mined with
>>>>> M2, so its
>>>>> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's
>>>>> ancestor
>>>>> feerate, which is 6sat/vB.
>>>>>
>>>>> In package RBF, the rule analogous to BIP125#2 would be "none of the
>>>>> transactions in the package can spend new unconfirmed inputs." Example
>>>>> J [17] shows
>>>>> why, if any of the package transactions have ancestors, package
>>>>> feerate is no
>>>>> longer accurate. Even though M2 and M3 are not ancestors of P1 (which
>>>>> is the
>>>>> replacement transaction in an RBF), we're actually interested in the
>>>>> entire
>>>>> package. A miner should mine M1 which is 5sat/vB instead of M2, M3,
>>>>> P1, P2, and
>>>>> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened to
>>>>> only allow
>>>>> the child to have new unconfirmed inputs, either, because it can still
>>>>> cause us
>>>>> to overestimate the package's ancestor score.
>>>>>
>>>>> However, enforcing a rule analogous to BIP125#2 would not only make
>>>>> Package RBF
>>>>> less useful, but would also break Package RBF for packages with
>>>>> parents already
>>>>> in the mempool: if a package parent has already been submitted, it
>>>>> would look
>>>>> like the child is spending a "new" unconfirmed input. In example K
>>>>> [18], we're
>>>>> looking to replace M1 with the entire package including P1, P2, and
>>>>> P3. We must
>>>>> consider the case where one of the parents is already in the mempool
>>>>> (in this
>>>>> case, P2), which means we must allow P3 to have new unconfirmed
>>>>> inputs. However,
>>>>> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not
>>>>> replace M1
>>>>> with this package.
>>>>>
>>>>> Thus, the package RBF rule regarding new unconfirmed inputs is less
>>>>> strict than
>>>>> BIP125#2. However, we still achieve the same goal of requiring the
>>>>> replacement
>>>>> transactions to have a ancestor score at least as high as the original
>>>>> ones. As
>>>>> a result, the entire package is required to be a higher feerate mining
>>>>> candidate
>>>>> than each of the replaced transactions.
>>>>>
>>>>> Another note: the [comment][13] above the BIP125#2 code in the
>>>>> original RBF
>>>>> implementation suggests that the rule was intended to be temporary.
>>>>>
>>>>> ##### Absolute Fee (Rule #3)
>>>>>
>>>>> The package must increase the absolute fee of the mempool, i.e. the
>>>>> total fees
>>>>> of the package must be higher than the absolute fees of the mempool
>>>>> transactions
>>>>> it replaces. Combined with the CPFP rule above, this differs from
>>>>> BIP125 Rule #3
>>>>> - an individual transaction in the package may have lower fees than the
>>>>>   transaction(s) it is replacing. In fact, it may have 0 fees, and the
>>>>> child
>>>>> pays for RBF.
>>>>>
>>>>> ##### Feerate (Rule #4)
>>>>>
>>>>> The package must pay for its own bandwidth; the package feerate must
>>>>> be higher
>>>>> than the replaced transactions by at least minimum relay feerate
>>>>> (`incrementalRelayFee`). Combined with the CPFP rule above, this
>>>>> differs from
>>>>> BIP125 Rule #4 - an individual transaction in the package can have a
>>>>> lower
>>>>> feerate than the transaction(s) it is replacing. In fact, it may have
>>>>> 0 fees,
>>>>> and the child pays for RBF.
>>>>>
>>>>> ##### Total Number of Replaced Transactions (Rule #5)
>>>>>
>>>>> The package cannot replace more than 100 mempool transactions. This is
>>>>> identical
>>>>> to BIP125 Rule #5.
>>>>>
>>>>> ### Expected FAQs
>>>>>
>>>>> 1. Is it possible for only some of the package to make it into the
>>>>> mempool?
>>>>>
>>>>>    Yes, it is. However, since we evict transactions from the mempool by
>>>>> descendant score and the package child is supposed to be sponsoring
>>>>> the fees of
>>>>> its parents, the most common scenario would be all-or-nothing. This is
>>>>> incentive-compatible. In fact, to be conservative, package validation
>>>>> should
>>>>> begin by trying to submit all of the transactions individually, and
>>>>> only use the
>>>>> package mempool acceptance logic if the parents fail due to low
>>>>> feerate.
>>>>>
>>>>> 2. Should we allow packages to contain already-confirmed transactions?
>>>>>
>>>>>     No, for practical reasons. In mempool validation, we actually
>>>>> aren't able to
>>>>> tell with 100% confidence if we are looking at a transaction that has
>>>>> already
>>>>> confirmed, because we look up inputs using a UTXO set. If we have
>>>>> historical
>>>>> block data, it's possible to look for it, but this is inefficient, not
>>>>> always
>>>>> possible for pruning nodes, and unnecessary because we're not going to
>>>>> do
>>>>> anything with the transaction anyway. As such, we already have the
>>>>> expectation
>>>>> that transaction relay is somewhat "stateful" i.e. nobody should be
>>>>> relaying
>>>>> transactions that have already been confirmed. Similarly, we shouldn't
>>>>> be
>>>>> relaying packages that contain already-confirmed transactions.
>>>>>
>>>>> [1]: https://github.com/bitcoin/bitcoin/pull/22290
>>>>> [2]:
>>>>> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations
>>>>> [3]:
>>>>> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282
>>>>> [4]: https://github.com/bitcoin/bitcoin/pull/16400
>>>>> [5]: https://github.com/bitcoin/bitcoin/pull/21062
>>>>> [6]: https://github.com/bitcoin/bitcoin/pull/22675
>>>>> [7]: https://github.com/bitcoin/bitcoin/pull/22796
>>>>> [8]: https://github.com/bitcoin/bitcoin/pull/20833
>>>>> [9]: https://github.com/bitcoin/bitcoin/pull/21800
>>>>> [10]: https://github.com/bitcoin/bitcoin/pull/16401
>>>>> [11]: https://github.com/bitcoin/bitcoin/pull/19621
>>>>> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki
>>>>> [13]:
>>>>> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104
>>>>> [14]:
>>>>> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png
>>>>> [15]:
>>>>> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png
>>>>> [16]:
>>>>> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png
>>>>> [17]:
>>>>> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png
>>>>> [18]:
>>>>> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png
>>>>> [19]:
>>>>> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png
>>>>> [20]:
>>>>> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png
>>>>> _______________________________________________
>>>>> bitcoin-dev mailing list
>>>>> bitcoin-dev at lists.linuxfoundation.org
>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>>>
>>>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210926/16f08de9/attachment-0001.html>

From prayank at tutanota.de  Mon Sep 27 01:52:41 2021
From: prayank at tutanota.de (Prayank)
Date: Mon, 27 Sep 2021 03:52:41 +0200 (CEST)
Subject: [bitcoin-dev] Mock introducing vulnerability in important Bitcoin
	projects
Message-ID: <MkZx3Hv--3-2@tutanota.de>

Good morning Bitcoin devs,

In one of the answers on Bitcoin Stackexchange it was mentioned that some companies may hire you to introduce backdoors in Bitcoin Core: https://bitcoin.stackexchange.com/a/108016/

While this looked crazy when I first read it, I think preparing for such things should not be a bad idea. In the comments one link was shared in which vulnerabilities were almost introduced in Linux: https://news.ycombinator.com/item?id=26887670

I was thinking about lot of things in last few days after reading the comments in that thread. Also tried researching about secure practices in C++ etc. I was planning something which I can do alone but don't want to end up being called "bad actor" later so wanted to get some feedback on this idea:

1.Create new GitHub accounts for this exercise
2.Study issues in different important Bitcoin projects including Bitcoin Core, LND, Libraries, Bisq, Wallets etc.
3.Prepare pull requests to introduce some vulnerability by fixing one of these issues
4.See how maintainers and reviewers respond to this and document it
5.Share results here after few days

Let me know if this looks okay or there are better ways to do this.

-- 
Prayank

A3B1 E430 2298 178F
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210927/529cb592/attachment-0001.html>

From bastien at acinq.fr  Mon Sep 27 07:15:18 2021
From: bastien at acinq.fr (Bastien TEINTURIER)
Date: Mon, 27 Sep 2021 09:15:18 +0200
Subject: [bitcoin-dev] Proposal: Package Mempool Accept and Package RBF
In-Reply-To: <CALZpt+FFSk-+BBxu7SSdjw580UCFfkdo1DTa1Yj9K81M4E1vPg@mail.gmail.com>
References: <CAFXO6=+cHyQKM2n9yn4EhwLZO+AUB0ZD81qWPxmpN27rjUoU3w@mail.gmail.com>
 <CALZpt+HpvmEHUEOgye34T6pVQ+wnKKn-_8cTJTQXYQb9t1jOTA@mail.gmail.com>
 <CAFXO6=JzsYgiXJE2geSKMpfgPo+GGNX_+Pw0JQx1QQxAfhCdBQ@mail.gmail.com>
 <CALZpt+HQpdrebhWGXv_yLqiSCB5Ur71L1K13bd7w5TZb9DwJEQ@mail.gmail.com>
 <CAFXO6=Lvcr7Pwn_ZD1CZohYUFKY-cC5sGRxdTOiP2MgnTvFnYA@mail.gmail.com>
 <CALZpt+FFSk-+BBxu7SSdjw580UCFfkdo1DTa1Yj9K81M4E1vPg@mail.gmail.com>
Message-ID: <CACdvm3Pgia5mn60HmZmi4t0U5coc13WBuDkh+QgQBAHZz+Ufow@mail.gmail.com>

>
> I think we could restrain package acceptance to only confirmed inputs for
> now and revisit later this point ? For LN-anchor, you can assume that the
> fee-bumping UTXO feeding the CPFP is already
> confirmed. Or are there currently-deployed use-cases which would benefit
> from your proposed Rule #2 ?
>

I think constraining package acceptance to only confirmed inputs
is very limiting and quite dangerous for L2 protocols.

In the case of LN, an attacker can game this and heavily restrict
your RBF attempts if you're only allowed to use confirmed inputs
and have many channels (and a limited number of confirmed inputs).
Otherwise you'll need node operators to pre-emptively split their
utxos into many small utxos just for fee bumping, which is inefficient...

Bastien

Le lun. 27 sept. 2021 ? 00:27, Antoine Riard via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> a ?crit :

> Hi Gloria,
>
> Thanks for your answers,
>
> > In summary, it seems that the decisions that might still need
> > attention/input from devs on this mailing list are:
> > 1. Whether we should start with multiple-parent-1-child or
> 1-parent-1-child.
> > 2. Whether it's ok to require that the child not have conflicts with
> > mempool transactions.
>
> Yes 1) it would be good to have inputs of more potential users of package
> acceptance . And 2) I think it's more a matter of clearer wording of the
> proposal.
>
> However, see my final point on the relaxation around "unconfirmed inputs"
> which might in fact alter our current block construction strategy.
>
> > Right, the fact that we essentially always choose the first-seen witness
> is
> > an unfortunate limitation that exists already. Adding package mempool
> > accept doesn't worsen this, but the procedure in the future is to replace
> > the witness when it makes sense economically. We can also add logic to
> > allow package feerate to pay for witness replacements as well. This is
> > pretty far into the future, though.
>
> Yes I agree package mempool doesn't worsen this. And it's not an issue for
> current LN as you can't significantly inflate a spending witness for the
> 2-of-2 funding output.
> However, it might be an issue for multi-party protocol where the spending
> script has alternative branches with asymmetric valid witness weights.
> Taproot should ease that kind of script so hopefully we would deploy
> wtxid-replacement not too far in the future.
>
> > I could be misunderstanding, but an attacker wouldn't be able to
> > batch-attack like this. Alice's package only conflicts with A' + D', not
> A'
> > + B' + C' + D'. She only needs to pay for evicting 2 transactions.
>
> Yeah I can be clearer, I think you have 2 pinning attacks scenarios to
> consider.
>
> In LN, if you're trying to confirm a commitment transaction to time-out or
> claim on-chain a HTLC and the timelock is near-expiration, you should be
> ready to pay in commitment+2nd-stage HTLC transaction fees as much as the
> value offered by the HTLC.
>
> Following this security assumption, an attacker can exploit it by
> targeting together commitment transactions from different channels by
> blocking them under a high-fee child, of which the fee value
> is equal to the top-value HTLC + 1. Victims's fee-bumping logics won't
> overbid as it's not worthy to offer fees beyond their competed HTLCs. Apart
> from observing mempools state, victims can't learn they're targeted by the
> same attacker.
>
> To draw from the aforementioned topology, Mallory broadcasts A' + B' + C'
> + D', where A' conflicts with Alice's P1, B' conflicts with Bob's P2, C'
> conflicts with Caroll's P3. Let's assume P1 is confirming the top-value
> HTLC of the set. If D' fees is higher than P1 + 1, it won't be rational for
> Alice or Bob or Caroll to keep offering competing feerates. Mallory will be
> at loss on stealing P1, as she has paid more in fees but will realize a
> gain on P2+P3.
>
> In this model, Alice is allowed to evict those 2 transactions (A' + D')
> but as she is economically-bounded she won't succeed.
>
> Mallory is maliciously exploiting RBF rule 3 on absolute fee. I think this
> 1st pinning scenario is correct and "lucractive" when you sum the global
> gain/loss.
>
> There is a 2nd attack scenario where A + B + C + D, where D is the child
> of A,B,C. All those transactions are honestly issued by Alice. Once A + B +
> C + D are propagated in network mempools, Mallory is able to replace A + D
> with  A' + D' where D' is paying a higher fee. This package A' + D' will
> confirm soon if D feerate was compelling but Mallory succeeds in delaying
> the confirmation
> of B + C for one or more blocks. As B + C are pre-signed commitments with
> a low-fee rate they won't confirm without Alice issuing a new child E.
> Mallory can repeat the same trick by broadcasting
> B' + E' and delay again the confirmation of C.
>
> If the remaining package pending HTLC has a higher-value than all the
> malicious fees over-bid, Mallory should realize a gain. With this 2nd
> pinning attack, the malicious entity buys confirmation delay of your
> packaged-together commitments.
>
> Assuming those attacks are correct, I'm leaning towards being conservative
> with the LDK broadcast backend. Though once again, other L2 devs have
> likely other use-cases and opinions :)
>
> >  B' only needs to pay for itself in this case.
>
> Yes I think it's a nice discount when UTXO is single-owned. In the context
> of shared-owned UTXO (e.g LN), you might not if there is an in-mempool
> package already spending the UTXO and have to assume the worst-case
> scenario. I.e have B' committing enough fee to pay for A' replacement
> bandwidth. I think we can't do that much for this case...
>
> > If a package meets feerate requirements as a
> package, the parents in the transaction are allowed to replace-by-fee
> mempool transactions. The child cannot replace mempool transactions."
>
> I agree with the Mallory-vs-Alice case. Though if Alice broadcasts A+B' to
> replace A+B because the first broadcast isn't satisfying anymore due to
> mempool spikes ? Assuming B' fees is enough, I think that case as child B'
> replacing in-mempool transaction B. Which I understand going against  "The
> child cannot replace mempool transactions".
>
> Maybe wording could be a bit clearer ?
>
> > While it would be nice to have full RBF, malleability of the child won't
> > block RBF here. If we're trying to replace A', we only require that A'
> > signals replaceability, and don't mind if its child doesn't.
>
> Yes, it sounds good.
>
> > Yes, A+C+D pays 2500sat more in fees, but it is also 1000vB larger. A
> miner
> > should prefer to utilize their block space more effectively.
>
> If your mempool is empty and only composed of A+C+D or A+B, I think taking
> A+C+D is the most efficient block construction you can come up with as a
> miner ?
>
> > No, because we don't use that model.
>
> Can you describe what miner model we are using ? Like the block
> construction strategy implemented by `addPackagesTxs` or also encompassing
> our current mempool acceptance policy, which I think rely on absolute fee
> over ancestor score in case of replacement ?
>
> I think this point is worthy to discuss as otherwise we might downgrade
> the efficiency of our current block construction strategy in periods of
> near-empty mempools. A knowledge which could be discreetly leveraged by a
> miner to gain an advantage on the rest of the mining ecosystem.
>
> Note, I think we *might* have to go in this direction if we want to
> replace replace-by-fee by replace-by-feerate or replace-by-ancestor and
> solve in-depth pinning attacks. Though if we do so,
> IMO we would need more thoughts.
>
> I think we could restrain package acceptance to only confirmed inputs for
> now and revisit later this point ? For LN-anchor, you can assume that the
> fee-bumping UTXO feeding the CPFP is already
> confirmed. Or are there currently-deployed use-cases which would benefit
> from your proposed Rule #2 ?
>
> Antoine
>
> Le jeu. 23 sept. 2021 ? 11:36, Gloria Zhao <gloriajzhao at gmail.com> a
> ?crit :
>
>> Hi Antoine,
>>
>> Thanks as always for your input. I'm glad we agree on so much!
>>
>> In summary, it seems that the decisions that might still need
>> attention/input from devs on this mailing list are:
>> 1. Whether we should start with multiple-parent-1-child or
>> 1-parent-1-child.
>> 2. Whether it's ok to require that the child not have conflicts with
>> mempool transactions.
>>
>> Responding to your comments...
>>
>> > IIUC, you have package A+B, during the dedup phase early in
>> `AcceptMultipleTransactions` if you observe same-txid-different-wtixd A'
>> and A' is higher feerate than A, you trim A and replace by A' ?
>>
>> > I think this approach is safe, the one who appears unsafe to me is when
>> A' has a _lower_ feerate, even if A' is already accepted by our mempool ?
>> In that case iirc that would be a pinning.
>>
>> Right, the fact that we essentially always choose the first-seen witness
>> is an unfortunate limitation that exists already. Adding package mempool
>> accept doesn't worsen this, but the procedure in the future is to replace
>> the witness when it makes sense economically. We can also add logic to
>> allow package feerate to pay for witness replacements as well. This is
>> pretty far into the future, though.
>>
>> > It sounds uneconomical for an attacker but I think it's not when you
>> consider than you can "batch" attack against multiple honest
>> counterparties. E.g, Mallory broadcast A' + B' + C' + D' where A' conflicts
>> with Alice's honest package P1, B' conflicts with Bob's honest package P2,
>> C' conflicts with Caroll's honest package P3. And D' is a high-fee child of
>> A' + B' + C'.
>>
>> > If D' is higher-fee than P1 or P2 or P3 but inferior to the sum of
>> HTLCs confirmed by P1+P2+P3, I think it's lucrative for the attacker ?
>>
>> I could be misunderstanding, but an attacker wouldn't be able to
>> batch-attack like this. Alice's package only conflicts with A' + D', not A'
>> + B' + C' + D'. She only needs to pay for evicting 2 transactions.
>>
>> > Do we assume that broadcasted packages are "honest" by default and that
>> the parent(s) always need the child to pass the fee checks, that way saving
>> the processing of individual transactions which are expected to fail in 99%
>> of cases or more ad hoc composition of packages at relay ?
>> > I think this point is quite dependent on the p2p packages format/logic
>> we'll end up on and that we should feel free to revisit it later ?
>>
>> I think it's the opposite; there's no way for us to assume that p2p
>> packages will be "honest." I'd like to have two things before we expose on
>> P2P: (1) ensure that the amount of resources potentially allocated for
>> package validation isn't disproportionately higher than that of single
>> transaction validation and (2) only use package validation when we're
>> unsatisifed with the single validation result, e.g. we might get better
>> fees.
>> Yes, let's revisit this later :)
>>
>>  > Yes, if you receive A+B, and A is already in-mempoo, I agree you can
>> discard its feerate as B should pay for all fees checked on its own. Where
>> I'm unclear is when you have in-mempool A+B and receive A+B'. Should B'
>> have a fee high enough to cover the bandwidth penalty replacement
>> (`PaysForRBF`, 2nd check) of both A+B' or only B' ?
>>
>>  B' only needs to pay for itself in this case.
>>
>> > > Do we want the child to be able to replace mempool transactions as
>> well?
>>
>> > If we mean when you have replaceable A+B then A'+B' try to replace with
>> a higher-feerate ? I think that's exactly the case we need for Lightning as
>> A+B is coming from Alice and A'+B' is coming from Bob :/
>>
>> Let me clarify this because I can see that my wording was ambiguous, and
>> then please let me know if it fits Lightning's needs?
>>
>> In my proposal, I wrote "If a package meets feerate requirements as a
>> package, the parents in the transaction are allowed to replace-by-fee
>> mempool transactions. The child cannot replace mempool transactions." What
>> I meant was: the package can replace mempool transactions if any of the
>> parents conflict with mempool transactions. The child cannot not conflict
>> with any mempool transactions.
>> The Lightning use case this attempts to address is: Alice and Mallory are
>> LN counterparties, and have packages A+B and A'+B', respectively. A and A'
>> are their commitment transactions and conflict with each other; they have
>> shared inputs and different txids.
>> B spends Alice's anchor output from A. B' spends Mallory's anchor output
>> from A'. Thus, B and B' do not conflict with each other.
>> Alice can broadcast her package, A+B, to replace Mallory's package,
>> A'+B', since B doesn't conflict with the mempool.
>>
>> Would this be ok?
>>
>> > The second option, a child of A', In the LN case I think the CPFP is
>> attached on one's anchor output.
>>
>> While it would be nice to have full RBF, malleability of the child won't
>> block RBF here. If we're trying to replace A', we only require that A'
>> signals replaceability, and don't mind if its child doesn't.
>>
>> > > B has an ancestor score of 10sat/vb and D has an
>> > > ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than
>> B's,
>> > > it fails the proposed package RBF Rule #2, so this package would be
>> > > rejected. Does this meet your expectations?
>>
>> > Well what sounds odd to me, in my example, we fail D even if it has a
>> higher-fee than B. Like A+B absolute fees are 2000 sats and A+C+D absolute
>> fees are 4500 sats ?
>>
>> Yes, A+C+D pays 2500sat more in fees, but it is also 1000vB larger. A
>> miner should prefer to utilize their block space more effectively.
>>
>> > Is this compatible with a model where a miner prioritizes absolute fees
>> over ancestor score, in the case that mempools aren't full-enough to
>> fulfill a block ?
>>
>> No, because we don't use that model.
>>
>> Thanks,
>> Gloria
>>
>> On Thu, Sep 23, 2021 at 5:29 AM Antoine Riard <antoine.riard at gmail.com>
>> wrote:
>>
>>> > Correct, if B+C is too low feerate to be accepted, we will reject it. I
>>> > prefer this because it is incentive compatible: A can be mined by
>>> itself,
>>> > so there's no reason to prefer A+B+C instead of A.
>>> > As another way of looking at this, consider the case where we do accept
>>> > A+B+C and it sits at the "bottom" of our mempool. If our mempool
>>> reaches
>>> > capacity, we evict the lowest descendant feerate transactions, which
>>> are
>>> > B+C in this case. This gives us the same resulting mempool, with A and
>>> not
>>> > B+C.
>>>
>>> I agree here. Doing otherwise, we might evict other transactions mempool
>>> in `MempoolAccept::Finalize` with a higher-feerate than B+C while those
>>> evicted transactions are the most compelling for block construction.
>>>
>>> I thought at first missing this acceptance requirement would break a
>>> fee-bumping scheme like Parent-Pay-For-Child where a high-fee parent is
>>> attached to a child signed with SIGHASH_ANYONECANPAY but in this case the
>>> child fee is capturing the parent value. I can't think of other fee-bumping
>>> schemes potentially affected. If they do exist I would say they're wrong in
>>> their design assumptions.
>>>
>>> > If or when we have witness replacement, the logic is: if the individual
>>> > transaction is enough to replace the mempool one, the replacement will
>>> > happen during the preceding individual transaction acceptance, and
>>> > deduplication logic will work. Otherwise, we will try to deduplicate by
>>> > wtxid, see that we need a package witness replacement, and use the
>>> package
>>> > feerate to evaluate whether this is economically rational.
>>>
>>> IIUC, you have package A+B, during the dedup phase early in
>>> `AcceptMultipleTransactions` if you observe same-txid-different-wtixd A'
>>> and A' is higher feerate than A, you trim A and replace by A' ?
>>>
>>> I think this approach is safe, the one who appears unsafe to me is when
>>> A' has a _lower_ feerate, even if A' is already accepted by our mempool ?
>>> In that case iirc that would be a pinning.
>>>
>>> Good to see progress on witness replacement before we see usage of
>>> Taproot tree in the context of multi-party, where a malicious counterparty
>>> inflates its witness to jam a honest spending.
>>>
>>> (Note, the commit linked currently points nowhere :))
>>>
>>>
>>> > Please note that A may replace A' even if A' has higher fees than A
>>> > individually, because the proposed package RBF utilizes the fees and
>>> size
>>> > of the entire package. This just requires E to pay enough fees,
>>> although
>>> > this can be pretty high if there are also potential B' and C' competing
>>> > commitment transactions that we don't know about.
>>>
>>> Ah right, if the package acceptance waives `PaysMoreThanConflicts` for
>>> the individual check on A, the honest package should replace the pinning
>>> attempt. I've not fully parsed the proposed implementation yet.
>>>
>>> Though note, I think it's still unsafe for a Lightning
>>> multi-commitment-broadcast-as-one-package as a malicious A' might have an
>>> absolute fee higher than E. It sounds uneconomical for
>>> an attacker but I think it's not when you consider than you can "batch"
>>> attack against multiple honest counterparties. E.g, Mallory broadcast A' +
>>> B' + C' + D' where A' conflicts with Alice's honest package P1, B'
>>> conflicts with Bob's honest package P2, C' conflicts with Caroll's honest
>>> package P3. And D' is a high-fee child of A' + B' + C'.
>>>
>>> If D' is higher-fee than P1 or P2 or P3 but inferior to the sum of HTLCs
>>> confirmed by P1+P2+P3, I think it's lucrative for the attacker ?
>>>
>>> > So far, my understanding is that multi-parent-1-child is desired for
>>> > batched fee-bumping (
>>> > https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289)
>>> and
>>> > I've also seen your response which I have less context on (
>>> > https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202).
>>> That
>>> > being said, I am happy to create a new proposal for 1 parent + 1 child
>>> > (which would be slightly simpler) and plan for moving to
>>> > multi-parent-1-child later if that is preferred. I am very interested
>>> in
>>> > hearing feedback on that approach.
>>>
>>> I think batched fee-bumping is okay as long as you don't have
>>> time-sensitive outputs encumbering your commitment transactions. For the
>>> reasons mentioned above, I think that's unsafe.
>>>
>>> What I'm worried about is  L2 developers, potentially not aware about
>>> all the mempool subtleties blurring the difference and always batching
>>> their broadcast by default.
>>>
>>> IMO, a good thing by restraining to 1-parent + 1 child,  we artificially
>>> constraint L2 design space for now and minimize risks of unsafe usage of
>>> the package API :)
>>>
>>> I think that's a point where it would be relevant to have the opinion of
>>> more L2 devs.
>>>
>>> > I think there is a misunderstanding here - let me describe what I'm
>>> > proposing we'd do in this situation: we'll try individual submission
>>> for A,
>>> > see that it fails due to "insufficient fees." Then, we'll try package
>>> > validation for A+B and use package RBF. If A+B pays enough, it can
>>> still
>>> > replace A'. If A fails for a bad signature, we won't look at B or A+B.
>>> Does
>>> > this meet your expectations?
>>>
>>> Yes there was a misunderstanding, I think this approach is correct, it's
>>> more a question of performance. Do we assume that broadcasted packages are
>>> "honest" by default and that the parent(s) always need the child to pass
>>> the fee checks, that way saving the processing of individual transactions
>>> which are expected to fail in 99% of cases or more ad hoc composition of
>>> packages at relay ?
>>>
>>> I think this point is quite dependent on the p2p packages format/logic
>>> we'll end up on and that we should feel free to revisit it later ?
>>>
>>>
>>> > What problem are you trying to solve by the package feerate *after*
>>> dedup
>>> rule ?
>>> > My understanding is that an in-package transaction might be already in
>>> the mempool. Therefore, to compute a correct RBF penalty replacement, the
>>> vsize of this transaction could be discarded lowering the cost of package
>>> RBF.
>>>
>>> > I'm proposing that, when a transaction has already been submitted to
>>> > mempool, we would ignore both its fees and vsize when calculating
>>> package
>>> > feerate.
>>>
>>> Yes, if you receive A+B, and A is already in-mempoo, I agree you can
>>> discard its feerate as B should pay for all fees checked on its own. Where
>>> I'm unclear is when you have in-mempool A+B and receive A+B'. Should B'
>>> have a fee high enough to cover the bandwidth penalty replacement
>>> (`PaysForRBF`, 2nd check) of both A+B' or only B' ?
>>>
>>> If you have a second-layer like current Lightning, you might have a
>>> counterparty commitment to replace and should always expect to have to pay
>>> for parent replacement bandwidth.
>>>
>>> Where a potential discount sounds interesting is when you have an
>>> univoque state on the first-stage of transactions. E.g DLC's funding
>>> transaction which might be CPFP by any participant iirc.
>>>
>>> > Note that, if C' conflicts with C, it also conflicts with D, since D
>>> is a
>>> > descendant of C and would thus need to be evicted along with it.
>>>
>>> Ah once again I think it's a misunderstanding without the code under my
>>> eyes! If we do C' `PreChecks`, solve the conflicts provoked by it, i.e mark
>>> for potential eviction D and don't consider it for future conflicts in the
>>> rest of the package, I think D' `PreChecks` should be good ?
>>>
>>> > More generally, this example is surprising to me because I didn't think
>>> > packages would be used to fee-bump replaceable transactions. Do we
>>> want the
>>> > child to be able to replace mempool transactions as well?
>>>
>>> If we mean when you have replaceable A+B then A'+B' try to replace with
>>> a higher-feerate ? I think that's exactly the case we need for Lightning as
>>> A+B is coming from Alice and A'+B' is coming from Bob :/
>>>
>>> > I'm not sure what you mean? Let's say we have a package of parent A +
>>> child
>>> > B, where A is supposed to replace a mempool transaction A'. Are you
>>> saying
>>> > that counterparties are able to malleate the package child B, or a
>>> child of
>>> > A'?
>>>
>>> The second option, a child of A', In the LN case I think the CPFP is
>>> attached on one's anchor output.
>>>
>>> I think it's good if we assume the
>>> solve-conflicts-after-parent's`'PreChecks` mentioned above or fixing
>>> inherited signaling or full-rbf ?
>>>
>>> > Sorry, I don't understand what you mean by "preserve the package
>>> > integrity?" Could you elaborate?
>>>
>>> After thinking the relaxation about the "new" unconfirmed input is not
>>> linked to trimming but I would say more to the multi-parent support.
>>>
>>> Let's say you have A+B trying to replace C+D where B is also spending
>>> already in-mempool E. To succeed, you need to waive the no-new-unconfirmed
>>> input as D isn't spending E.
>>>
>>> So good, I think we agree on the problem description here.
>>>
>>> > I am in agreement with your calculations but unsure if we disagree on
>>> the
>>> > expected outcome. Yes, B has an ancestor score of 10sat/vb and D has an
>>> > ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than
>>> B's,
>>> > it fails the proposed package RBF Rule #2, so this package would be
>>> > rejected. Does this meet your expectations?
>>>
>>> Well what sounds odd to me, in my example, we fail D even if it has a
>>> higher-fee than B. Like A+B absolute fees are 2000 sats and A+C+D absolute
>>> fees are 4500 sats ?
>>>
>>> Is this compatible with a model where a miner prioritizes absolute fees
>>> over ancestor score, in the case that mempools aren't full-enough to
>>> fulfill a block ?
>>>
>>> Let me know if I can clarify a point.
>>>
>>> Antoine
>>>
>>> Le lun. 20 sept. 2021 ? 11:10, Gloria Zhao <gloriajzhao at gmail.com> a
>>> ?crit :
>>>
>>>>
>>>> Hi Antoine,
>>>>
>>>> First of all, thank you for the thorough review. I appreciate your
>>>> insight on LN requirements.
>>>>
>>>> > IIUC, you have a package A+B+C submitted for acceptance and A is
>>>> already in your mempool. You trim out A from the package and then evaluate
>>>> B+C.
>>>>
>>>> > I think this might be an issue if A is the higher-fee element of the
>>>> ABC package. B+C package fees might be under the mempool min fee and will
>>>> be rejected, potentially breaking the acceptance expectations of the
>>>> package issuer ?
>>>>
>>>> Correct, if B+C is too low feerate to be accepted, we will reject it. I
>>>> prefer this because it is incentive compatible: A can be mined by itself,
>>>> so there's no reason to prefer A+B+C instead of A.
>>>> As another way of looking at this, consider the case where we do accept
>>>> A+B+C and it sits at the "bottom" of our mempool. If our mempool reaches
>>>> capacity, we evict the lowest descendant feerate transactions, which are
>>>> B+C in this case. This gives us the same resulting mempool, with A and not
>>>> B+C.
>>>>
>>>>
>>>> > Further, I think the dedup should be done on wtxid, as you might have
>>>> multiple valid witnesses. Though with varying vsizes and as such offering
>>>> different feerates.
>>>>
>>>> I agree that variations of the same package with different witnesses is
>>>> a case that must be handled. I consider witness replacement to be a project
>>>> that can be done in parallel to package mempool acceptance because being
>>>> able to accept packages does not worsen the problem of a
>>>> same-txid-different-witness "pinning" attack.
>>>>
>>>> If or when we have witness replacement, the logic is: if the individual
>>>> transaction is enough to replace the mempool one, the replacement will
>>>> happen during the preceding individual transaction acceptance, and
>>>> deduplication logic will work. Otherwise, we will try to deduplicate by
>>>> wtxid, see that we need a package witness replacement, and use the package
>>>> feerate to evaluate whether this is economically rational.
>>>>
>>>> See the #22290 "handle package transactions already in mempool" commit (
>>>> https://github.com/bitcoin/bitcoin/pull/22290/commits/fea75a2237b46cf76145242fecad7e274bfcb5ff),
>>>> which handles the case of same-txid-different-witness by simply using the
>>>> transaction in the mempool for now, with TODOs for what I just described.
>>>>
>>>>
>>>> > I'm not clearly understanding the accepted topologies. By "parent and
>>>> child to share a parent", do you mean the set of transactions A, B, C,
>>>> where B is spending A and C is spending A and B would be correct ?
>>>>
>>>> Yes, that is what I meant. Yes, that would a valid package under these
>>>> rules.
>>>>
>>>> > If yes, is there a width-limit introduced or we fallback on
>>>> MAX_PACKAGE_COUNT=25 ?
>>>>
>>>> No, there is no limit on connectivity other than "child with all
>>>> unconfirmed parents." We will enforce MAX_PACKAGE_COUNT=25 and child's
>>>> in-mempool + in-package ancestor limits.
>>>>
>>>>
>>>> > Considering the current Core's mempool acceptance rules, I think CPFP
>>>> batching is unsafe for LN time-sensitive closure. A malicious tx-relay
>>>> jamming successful on one channel commitment transaction would contamine
>>>> the remaining commitments sharing the same package.
>>>>
>>>> > E.g, you broadcast the package A+B+C+D+E where A,B,C,D are commitment
>>>> transactions and E a shared CPFP. If a malicious A' transaction has a
>>>> better feerate than A, the whole package acceptance will fail. Even if A'
>>>> confirms in the following block,
>>>> the propagation and confirmation of B+C+D have been delayed. This could
>>>> carry on a loss of funds.
>>>>
>>>> Please note that A may replace A' even if A' has higher fees than A
>>>> individually, because the proposed package RBF utilizes the fees and size
>>>> of the entire package. This just requires E to pay enough fees, although
>>>> this can be pretty high if there are also potential B' and C' competing
>>>> commitment transactions that we don't know about.
>>>>
>>>>
>>>> > IMHO, I'm leaning towards deploying during a first phase
>>>> 1-parent/1-child. I think it's the most conservative step still improving
>>>> second-layer safety.
>>>>
>>>> So far, my understanding is that multi-parent-1-child is desired for
>>>> batched fee-bumping (
>>>> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289)
>>>> and I've also seen your response which I have less context on (
>>>> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202).
>>>> That being said, I am happy to create a new proposal for 1 parent + 1 child
>>>> (which would be slightly simpler) and plan for moving to
>>>> multi-parent-1-child later if that is preferred. I am very interested in
>>>> hearing feedback on that approach.
>>>>
>>>>
>>>> > If A+B is submitted to replace A', where A pays 0 sats, B pays 200
>>>> sats and A' pays 100 sats. If we apply the individual RBF on A, A+B
>>>> acceptance fails. For this reason I think the individual RBF should be
>>>> bypassed and only the package RBF apply ?
>>>>
>>>> I think there is a misunderstanding here - let me describe what I'm
>>>> proposing we'd do in this situation: we'll try individual submission for A,
>>>> see that it fails due to "insufficient fees." Then, we'll try package
>>>> validation for A+B and use package RBF. If A+B pays enough, it can still
>>>> replace A'. If A fails for a bad signature, we won't look at B or A+B. Does
>>>> this meet your expectations?
>>>>
>>>>
>>>> > What problem are you trying to solve by the package feerate *after*
>>>> dedup rule ?
>>>> > My understanding is that an in-package transaction might be already
>>>> in the mempool. Therefore, to compute a correct RBF penalty replacement,
>>>> the vsize of this transaction could be discarded lowering the cost of
>>>> package RBF.
>>>>
>>>> I'm proposing that, when a transaction has already been submitted to
>>>> mempool, we would ignore both its fees and vsize when calculating package
>>>> feerate. In example G2, we shouldn't count M1 fees after its submission to
>>>> mempool, since M1's fees have already been used to pay for its individual
>>>> bandwidth, and it shouldn't be used again to pay for P2 and P3's bandwidth.
>>>> We also shouldn't count its vsize, since it has already been paid for.
>>>>
>>>>
>>>> > I think this is a footgunish API, as if a package issuer send the
>>>> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.
>>>> Then try to broadcast the higher-feerate C'+D' package, it should be
>>>> rejected. So it's breaking the naive broadcaster assumption that a
>>>> higher-feerate/higher-fee package always replaces ?
>>>>
>>>> Note that, if C' conflicts with C, it also conflicts with D, since D is
>>>> a descendant of C and would thus need to be evicted along with it.
>>>> Implicitly, D' would not be in conflict with D.
>>>> More generally, this example is surprising to me because I didn't think
>>>> packages would be used to fee-bump replaceable transactions. Do we want the
>>>> child to be able to replace mempool transactions as well? This can be
>>>> implemented with a bit of additional logic.
>>>>
>>>> > I think this is unsafe for L2s if counterparties have malleability of
>>>> the child transaction. They can block your package replacement by
>>>> opting-out from RBF signaling. IIRC, LN's "anchor output" presents such an
>>>> ability.
>>>>
>>>> I'm not sure what you mean? Let's say we have a package of parent A +
>>>> child B, where A is supposed to replace a mempool transaction A'. Are you
>>>> saying that counterparties are able to malleate the package child B, or a
>>>> child of A'? If they can malleate a child of A', that shouldn't matter as
>>>> long as A' is signaling replacement. This would be handled identically with
>>>> full RBF and what Core currently implements.
>>>>
>>>> > I think this is an issue brought by the trimming during the dedup
>>>> phase. If we preserve the package integrity, only re-using the tx-level
>>>> checks results of already in-mempool transactions to gain in CPU time we
>>>> won't have this issue. Package childs can add unconfirmed inputs as long as
>>>> they're in-package, the bip125 rule2 is only evaluated against parents ?
>>>>
>>>> Sorry, I don't understand what you mean by "preserve the package
>>>> integrity?" Could you elaborate?
>>>>
>>>> > Let's say you have in-mempool A, B where A pays 10 sat/vb for 100
>>>> vbytes and B pays 10 sat/vb for 100 vbytes. You have the candidate
>>>> replacement D spending both A and C where D pays 15sat/vb for 100 vbytes
>>>> and C pays 1 sat/vb for 1000 vbytes.
>>>>
>>>> > Package A + B ancestor score is 10 sat/vb.
>>>>
>>>> > D has a higher feerate/absolute fee than B.
>>>>
>>>> > Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's
>>>> 1000 sats + D's 1500 sats) / A's 100 vb + C's 1000 vb + D's 100 vb)
>>>>
>>>> I am in agreement with your calculations but unsure if we disagree on
>>>> the expected outcome. Yes, B has an ancestor score of 10sat/vb and D has an
>>>> ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than B's,
>>>> it fails the proposed package RBF Rule #2, so this package would be
>>>> rejected. Does this meet your expectations?
>>>>
>>>> Thank you for linking to projects that might be interested in package
>>>> relay :)
>>>>
>>>> Thanks,
>>>> Gloria
>>>>
>>>> On Mon, Sep 20, 2021 at 12:16 AM Antoine Riard <antoine.riard at gmail.com>
>>>> wrote:
>>>>
>>>>> Hi Gloria,
>>>>>
>>>>> > A package may contain transactions that are already in the mempool.
>>>>> We
>>>>> > remove
>>>>> > ("deduplicate") those transactions from the package for the purposes
>>>>> of
>>>>> > package
>>>>> > mempool acceptance. If a package is empty after deduplication, we do
>>>>> > nothing.
>>>>>
>>>>> IIUC, you have a package A+B+C submitted for acceptance and A is
>>>>> already in your mempool. You trim out A from the package and then evaluate
>>>>> B+C.
>>>>>
>>>>> I think this might be an issue if A is the higher-fee element of the
>>>>> ABC package. B+C package fees might be under the mempool min fee and will
>>>>> be rejected, potentially breaking the acceptance expectations of the
>>>>> package issuer ?
>>>>>
>>>>> Further, I think the dedup should be done on wtxid, as you might have
>>>>> multiple valid witnesses. Though with varying vsizes and as such offering
>>>>> different feerates.
>>>>>
>>>>> E.g you're going to evaluate the package A+B and A' is already in your
>>>>> mempool with a bigger valid witness. You trim A based on txid, then you
>>>>> evaluate A'+B, which fails the fee checks. However, evaluating A+B would
>>>>> have been a success.
>>>>>
>>>>> AFAICT, the dedup rationale would be to save on CPU time/IO disk, to
>>>>> avoid repeated signatures verification and parent UTXOs fetches ? Can we
>>>>> achieve the same goal by bypassing tx-level checks for already-in txn while
>>>>> conserving the package integrity for package-level checks ?
>>>>>
>>>>> > Note that it's possible for the parents to be
>>>>> > indirect
>>>>> > descendants/ancestors of one another, or for parent and child to
>>>>> share a
>>>>> > parent,
>>>>> > so we cannot make any other topology assumptions.
>>>>>
>>>>> I'm not clearly understanding the accepted topologies. By "parent and
>>>>> child to share a parent", do you mean the set of transactions A, B, C,
>>>>> where B is spending A and C is spending A and B would be correct ?
>>>>>
>>>>> If yes, is there a width-limit introduced or we fallback on
>>>>> MAX_PACKAGE_COUNT=25 ?
>>>>>
>>>>> IIRC, one rationale to come with this topology limitation was to lower
>>>>> the DoS risks when potentially deploying p2p packages.
>>>>>
>>>>> Considering the current Core's mempool acceptance rules, I think CPFP
>>>>> batching is unsafe for LN time-sensitive closure. A malicious tx-relay
>>>>> jamming successful on one channel commitment transaction would contamine
>>>>> the remaining commitments sharing the same package.
>>>>>
>>>>> E.g, you broadcast the package A+B+C+D+E where A,B,C,D are commitment
>>>>> transactions and E a shared CPFP. If a malicious A' transaction has a
>>>>> better feerate than A, the whole package acceptance will fail. Even if A'
>>>>> confirms in the following block,
>>>>> the propagation and confirmation of B+C+D have been delayed. This
>>>>> could carry on a loss of funds.
>>>>>
>>>>> That said, if you're broadcasting commitment transactions without
>>>>> time-sensitive HTLC outputs, I think the batching is effectively a fee
>>>>> saving as you don't have to duplicate the CPFP.
>>>>>
>>>>> IMHO, I'm leaning towards deploying during a first phase
>>>>> 1-parent/1-child. I think it's the most conservative step still improving
>>>>> second-layer safety.
>>>>>
>>>>> > *Rationale*:  It would be incorrect to use the fees of transactions
>>>>> that are
>>>>> > already in the mempool, as we do not want a transaction's fees to be
>>>>> > double-counted for both its individual RBF and package RBF.
>>>>>
>>>>> I'm unsure about the logical order of the checks proposed.
>>>>>
>>>>> If A+B is submitted to replace A', where A pays 0 sats, B pays 200
>>>>> sats and A' pays 100 sats. If we apply the individual RBF on A, A+B
>>>>> acceptance fails. For this reason I think the individual RBF should be
>>>>> bypassed and only the package RBF apply ?
>>>>>
>>>>> Note this situation is plausible, with current LN design, your
>>>>> counterparty can have a commitment transaction with a better fee just by
>>>>> selecting a higher `dust_limit_satoshis` than yours.
>>>>>
>>>>> > Examples F and G [14] show the same package, but P1 is submitted
>>>>> > individually before
>>>>> > the package in example G. In example F, we can see that the 300vB
>>>>> package
>>>>> > pays
>>>>> > an additional 200sat in fees, which is not enough to pay for its own
>>>>> > bandwidth
>>>>> > (BIP125#4). In example G, we can see that P1 pays enough to replace
>>>>> M1, but
>>>>> > using P1's fees again during package submission would make it look
>>>>> like a
>>>>> > 300sat
>>>>> > increase for a 200vB package. Even including its fees and size would
>>>>> not be
>>>>> > sufficient in this example, since the 300sat looks like enough for
>>>>> the 300vB
>>>>> > package. The calculcation after deduplication is 100sat increase for
>>>>> a
>>>>> > package
>>>>> > of size 200vB, which correctly fails BIP125#4. Assume all
>>>>> transactions have
>>>>> > a
>>>>> > size of 100vB.
>>>>>
>>>>> What problem are you trying to solve by the package feerate *after*
>>>>> dedup rule ?
>>>>>
>>>>> My understanding is that an in-package transaction might be already in
>>>>> the mempool. Therefore, to compute a correct RBF penalty replacement, the
>>>>> vsize of this transaction could be discarded lowering the cost of package
>>>>> RBF.
>>>>>
>>>>> If we keep a "safe" dedup mechanism (see my point above), I think this
>>>>> discount is justified, as the validation cost of node operators is paid for
>>>>> ?
>>>>>
>>>>> > The child cannot replace mempool transactions.
>>>>>
>>>>> Let's say you issue package A+B, then package C+B', where B' is a
>>>>> child of both A and C. This rule fails the acceptance of C+B' ?
>>>>>
>>>>> I think this is a footgunish API, as if a package issuer send the
>>>>> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.
>>>>> Then try to broadcast the higher-feerate C'+D' package, it should be
>>>>> rejected. So it's breaking the naive broadcaster assumption that a
>>>>> higher-feerate/higher-fee package always replaces ? And it might be unsafe
>>>>> in protocols where states are symmetric. E.g a malicious counterparty
>>>>> broadcasts first S+A, then you honestly broadcast S+B, where B pays better
>>>>> fees.
>>>>>
>>>>> > All mempool transactions to be replaced must signal replaceability.
>>>>>
>>>>> I think this is unsafe for L2s if counterparties have malleability of
>>>>> the child transaction. They can block your package replacement by
>>>>> opting-out from RBF signaling. IIRC, LN's "anchor output" presents such an
>>>>> ability.
>>>>>
>>>>> I think it's better to either fix inherited signaling or move towards
>>>>> full-rbf.
>>>>>
>>>>> > if a package parent has already been submitted, it would
>>>>> > look
>>>>> >like the child is spending a "new" unconfirmed input.
>>>>>
>>>>> I think this is an issue brought by the trimming during the dedup
>>>>> phase. If we preserve the package integrity, only re-using the tx-level
>>>>> checks results of already in-mempool transactions to gain in CPU time we
>>>>> won't have this issue. Package childs can add unconfirmed inputs as long as
>>>>> they're in-package, the bip125 rule2 is only evaluated against parents ?
>>>>>
>>>>> > However, we still achieve the same goal of requiring the
>>>>> > replacement
>>>>> > transactions to have a ancestor score at least as high as the
>>>>> original
>>>>> > ones.
>>>>>
>>>>> I'm not sure if this holds...
>>>>>
>>>>> Let's say you have in-mempool A, B where A pays 10 sat/vb for 100
>>>>> vbytes and B pays 10 sat/vb for 100 vbytes. You have the candidate
>>>>> replacement D spending both A and C where D pays 15sat/vb for 100 vbytes
>>>>> and C pays 1 sat/vb for 1000 vbytes.
>>>>>
>>>>> Package A + B ancestor score is 10 sat/vb.
>>>>>
>>>>> D has a higher feerate/absolute fee than B.
>>>>>
>>>>> Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's
>>>>> 1000 sats + D's 1500 sats) /
>>>>> A's 100 vb + C's 1000 vb + D's 100 vb)
>>>>>
>>>>> Overall, this is a review through the lenses of LN requirements. I
>>>>> think other L2 protocols/applications
>>>>> could be candidates to using package accept/relay such as:
>>>>> * https://github.com/lightninglabs/pool
>>>>> * https://github.com/discreetlogcontracts/dlcspecs
>>>>> * https://github.com/bitcoin-teleport/teleport-transactions/
>>>>> * https://github.com/sapio-lang/sapio
>>>>> *
>>>>> https://github.com/commerceblock/mercury/blob/master/doc/statechains.md
>>>>> * https://github.com/revault/practical-revault
>>>>>
>>>>> Thanks for rolling forward the ball on this subject.
>>>>>
>>>>> Antoine
>>>>>
>>>>> Le jeu. 16 sept. 2021 ? 03:55, Gloria Zhao via bitcoin-dev <
>>>>> bitcoin-dev at lists.linuxfoundation.org> a ?crit :
>>>>>
>>>>>> Hi there,
>>>>>>
>>>>>> I'm writing to propose a set of mempool policy changes to enable
>>>>>> package
>>>>>> validation (in preparation for package relay) in Bitcoin Core. These
>>>>>> would not
>>>>>> be consensus or P2P protocol changes. However, since mempool policy
>>>>>> significantly affects transaction propagation, I believe this is
>>>>>> relevant for
>>>>>> the mailing list.
>>>>>>
>>>>>> My proposal enables packages consisting of multiple parents and 1
>>>>>> child. If you
>>>>>> develop software that relies on specific transaction relay
>>>>>> assumptions and/or
>>>>>> are interested in using package relay in the future, I'm very
>>>>>> interested to hear
>>>>>> your feedback on the utility or restrictiveness of these package
>>>>>> policies for
>>>>>> your use cases.
>>>>>>
>>>>>> A draft implementation of this proposal can be found in [Bitcoin Core
>>>>>> PR#22290][1].
>>>>>>
>>>>>> An illustrated version of this post can be found at
>>>>>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.
>>>>>> I have also linked the images below.
>>>>>>
>>>>>> ## Background
>>>>>>
>>>>>> Feel free to skip this section if you are already familiar with
>>>>>> mempool policy
>>>>>> and package relay terminology.
>>>>>>
>>>>>> ### Terminology Clarifications
>>>>>>
>>>>>> * Package = an ordered list of related transactions, representable by
>>>>>> a Directed
>>>>>>   Acyclic Graph.
>>>>>> * Package Feerate = the total modified fees divided by the total
>>>>>> virtual size of
>>>>>>   all transactions in the package.
>>>>>>     - Modified fees = a transaction's base fees + fee delta applied
>>>>>> by the user
>>>>>>       with `prioritisetransaction`. As such, we expect this to vary
>>>>>> across
>>>>>> mempools.
>>>>>>     - Virtual Size = the maximum of virtual sizes calculated using
>>>>>> [BIP141
>>>>>>       virtual size][2] and sigop weight. [Implemented here in Bitcoin
>>>>>> Core][3].
>>>>>>     - Note that feerate is not necessarily based on the base fees and
>>>>>> serialized
>>>>>>       size.
>>>>>>
>>>>>> * Fee-Bumping = user/wallet actions that take advantage of miner
>>>>>> incentives to
>>>>>>   boost a transaction's candidacy for inclusion in a block, including
>>>>>> Child Pays
>>>>>> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our
>>>>>> intention in
>>>>>> mempool policy is to recognize when the new transaction is more
>>>>>> economical to
>>>>>> mine than the original one(s) but not open DoS vectors, so there are
>>>>>> some
>>>>>> limitations.
>>>>>>
>>>>>> ### Policy
>>>>>>
>>>>>> The purpose of the mempool is to store the best (to be most
>>>>>> incentive-compatible
>>>>>> with miners, highest feerate) candidates for inclusion in a block.
>>>>>> Miners use
>>>>>> the mempool to build block templates. The mempool is also useful as a
>>>>>> cache for
>>>>>> boosting block relay and validation performance, aiding transaction
>>>>>> relay, and
>>>>>> generating feerate estimations.
>>>>>>
>>>>>> Ideally, all consensus-valid transactions paying reasonable fees
>>>>>> should make it
>>>>>> to miners through normal transaction relay, without any special
>>>>>> connectivity or
>>>>>> relationships with miners. On the other hand, nodes do not have
>>>>>> unlimited
>>>>>> resources, and a P2P network designed to let any honest node
>>>>>> broadcast their
>>>>>> transactions also exposes the transaction validation engine to DoS
>>>>>> attacks from
>>>>>> malicious peers.
>>>>>>
>>>>>> As such, for unconfirmed transactions we are considering for our
>>>>>> mempool, we
>>>>>> apply a set of validation rules in addition to consensus, primarily
>>>>>> to protect
>>>>>> us from resource exhaustion and aid our efforts to keep the highest
>>>>>> fee
>>>>>> transactions. We call this mempool _policy_: a set of (configurable,
>>>>>> node-specific) rules that transactions must abide by in order to be
>>>>>> accepted
>>>>>> into our mempool. Transaction "Standardness" rules and mempool
>>>>>> restrictions such
>>>>>> as "too-long-mempool-chain" are both examples of policy.
>>>>>>
>>>>>> ### Package Relay and Package Mempool Accept
>>>>>>
>>>>>> In transaction relay, we currently consider transactions one at a
>>>>>> time for
>>>>>> submission to the mempool. This creates a limitation in the node's
>>>>>> ability to
>>>>>> determine which transactions have the highest feerates, since we
>>>>>> cannot take
>>>>>> into account descendants (i.e. cannot use CPFP) until all the
>>>>>> transactions are
>>>>>> in the mempool. Similarly, we cannot use a transaction's descendants
>>>>>> when
>>>>>> considering it for RBF. When an individual transaction does not meet
>>>>>> the mempool
>>>>>> minimum feerate and the user isn't able to create a replacement
>>>>>> transaction
>>>>>> directly, it will not be accepted by mempools.
>>>>>>
>>>>>> This limitation presents a security issue for applications and users
>>>>>> relying on
>>>>>> time-sensitive transactions. For example, Lightning and other
>>>>>> protocols create
>>>>>> UTXOs with multiple spending paths, where one counterparty's spending
>>>>>> path opens
>>>>>> up after a timelock, and users are protected from cheating scenarios
>>>>>> as long as
>>>>>> they redeem on-chain in time. A key security assumption is that all
>>>>>> parties'
>>>>>> transactions will propagate and confirm in a timely manner. This
>>>>>> assumption can
>>>>>> be broken if fee-bumping does not work as intended.
>>>>>>
>>>>>> The end goal for Package Relay is to consider multiple transactions
>>>>>> at the same
>>>>>> time, e.g. a transaction with its high-fee child. This may help us
>>>>>> better
>>>>>> determine whether transactions should be accepted to our mempool,
>>>>>> especially if
>>>>>> they don't meet fee requirements individually or are better RBF
>>>>>> candidates as a
>>>>>> package. A combination of changes to mempool validation logic,
>>>>>> policy, and
>>>>>> transaction relay allows us to better propagate the transactions with
>>>>>> the
>>>>>> highest package feerates to miners, and makes fee-bumping tools more
>>>>>> powerful
>>>>>> for users.
>>>>>>
>>>>>> The "relay" part of Package Relay suggests P2P messaging changes, but
>>>>>> a large
>>>>>> part of the changes are in the mempool's package validation logic. We
>>>>>> call this
>>>>>> *Package Mempool Accept*.
>>>>>>
>>>>>> ### Previous Work
>>>>>>
>>>>>> * Given that mempool validation is DoS-sensitive and complex, it
>>>>>> would be
>>>>>>   dangerous to haphazardly tack on package validation logic. Many
>>>>>> efforts have
>>>>>> been made to make mempool validation less opaque (see [#16400][4],
>>>>>> [#21062][5],
>>>>>> [#22675][6], [#22796][7]).
>>>>>> * [#20833][8] Added basic capabilities for package validation, test
>>>>>> accepts only
>>>>>>   (no submission to mempool).
>>>>>> * [#21800][9] Implemented package ancestor/descendant limit checks
>>>>>> for arbitrary
>>>>>>   packages. Still test accepts only.
>>>>>> * Previous package relay proposals (see [#16401][10], [#19621][11]).
>>>>>>
>>>>>> ### Existing Package Rules
>>>>>>
>>>>>> These are in master as introduced in [#20833][8] and [#21800][9].
>>>>>> I'll consider
>>>>>> them as "given" in the rest of this document, though they can be
>>>>>> changed, since
>>>>>> package validation is test-accept only right now.
>>>>>>
>>>>>> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and
>>>>>> `MAX_PACKAGE_SIZE=101KvB` total size [8]
>>>>>>
>>>>>>    *Rationale*: This is already enforced as mempool
>>>>>> ancestor/descendant limits.
>>>>>> Presumably, transactions in a package are all related, so exceeding
>>>>>> this limit
>>>>>> would mean that the package can either be split up or it wouldn't
>>>>>> pass this
>>>>>> mempool policy.
>>>>>>
>>>>>> 2. Packages must be topologically sorted: if any dependencies exist
>>>>>> between
>>>>>> transactions, parents must appear somewhere before children. [8]
>>>>>>
>>>>>> 3. A package cannot have conflicting transactions, i.e. none of them
>>>>>> can spend
>>>>>> the same inputs. This also means there cannot be duplicate
>>>>>> transactions. [8]
>>>>>>
>>>>>> 4. When packages are evaluated against ancestor/descendant limits in
>>>>>> a test
>>>>>> accept, the union of all of their descendants and ancestors is
>>>>>> considered. This
>>>>>> is essentially a "worst case" heuristic where every transaction in
>>>>>> the package
>>>>>> is treated as each other's ancestor and descendant. [8]
>>>>>> Packages for which ancestor/descendant limits are accurately captured
>>>>>> by this
>>>>>> heuristic: [19]
>>>>>>
>>>>>> There are also limitations such as the fact that CPFP carve out is
>>>>>> not applied
>>>>>> to package transactions. #20833 also disables RBF in package
>>>>>> validation; this
>>>>>> proposal overrides that to allow packages to use RBF.
>>>>>>
>>>>>> ## Proposed Changes
>>>>>>
>>>>>> The next step in the Package Mempool Accept project is to implement
>>>>>> submission
>>>>>> to mempool, initially through RPC only. This allows us to test the
>>>>>> submission
>>>>>> logic before exposing it on P2P.
>>>>>>
>>>>>> ### Summary
>>>>>>
>>>>>> - Packages may contain already-in-mempool transactions.
>>>>>> - Packages are 2 generations, Multi-Parent-1-Child.
>>>>>> - Fee-related checks use the package feerate. This means that wallets
>>>>>> can
>>>>>> create a package that utilizes CPFP.
>>>>>> - Parents are allowed to RBF mempool transactions with a set of rules
>>>>>> similar
>>>>>>   to BIP125. This enables a combination of CPFP and RBF, where a
>>>>>> transaction's descendant fees pay for replacing mempool conflicts.
>>>>>>
>>>>>> There is a draft implementation in [#22290][1]. It is WIP, but
>>>>>> feedback is
>>>>>> always welcome.
>>>>>>
>>>>>> ### Details
>>>>>>
>>>>>> #### Packages May Contain Already-in-Mempool Transactions
>>>>>>
>>>>>> A package may contain transactions that are already in the mempool.
>>>>>> We remove
>>>>>> ("deduplicate") those transactions from the package for the purposes
>>>>>> of package
>>>>>> mempool acceptance. If a package is empty after deduplication, we do
>>>>>> nothing.
>>>>>>
>>>>>> *Rationale*: Mempools vary across the network. It's possible for a
>>>>>> parent to be
>>>>>> accepted to the mempool of a peer on its own due to differences in
>>>>>> policy and
>>>>>> fee market fluctuations. We should not reject or penalize the entire
>>>>>> package for
>>>>>> an individual transaction as that could be a censorship vector.
>>>>>>
>>>>>> #### Packages Are Multi-Parent-1-Child
>>>>>>
>>>>>> Only packages of a specific topology are permitted. Namely, a package
>>>>>> is exactly
>>>>>> 1 child with all of its unconfirmed parents. After deduplication, the
>>>>>> package
>>>>>> may be exactly the same, empty, 1 child, 1 child with just some of its
>>>>>> unconfirmed parents, etc. Note that it's possible for the parents to
>>>>>> be indirect
>>>>>> descendants/ancestors of one another, or for parent and child to
>>>>>> share a parent,
>>>>>> so we cannot make any other topology assumptions.
>>>>>>
>>>>>> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple
>>>>>> parents
>>>>>> makes it possible to fee-bump a batch of transactions. Restricting
>>>>>> packages to a
>>>>>> defined topology is also easier to reason about and simplifies the
>>>>>> validation
>>>>>> logic greatly. Multi-parent-1-child allows us to think of the package
>>>>>> as one big
>>>>>> transaction, where:
>>>>>>
>>>>>> - Inputs = all the inputs of parents + inputs of the child that come
>>>>>> from
>>>>>>   confirmed UTXOs
>>>>>> - Outputs = all the outputs of the child + all outputs of the parents
>>>>>> that
>>>>>>   aren't spent by other transactions in the package
>>>>>>
>>>>>> Examples of packages that follow this rule (variations of example A
>>>>>> show some
>>>>>> possibilities after deduplication): ![image][15]
>>>>>>
>>>>>> #### Fee-Related Checks Use Package Feerate
>>>>>>
>>>>>> Package Feerate = the total modified fees divided by the total
>>>>>> virtual size of
>>>>>> all transactions in the package.
>>>>>>
>>>>>> To meet the two feerate requirements of a mempool, i.e., the
>>>>>> pre-configured
>>>>>> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum
>>>>>> feerate, the
>>>>>> total package feerate is used instead of the individual feerate. The
>>>>>> individual
>>>>>> transactions are allowed to be below feerate requirements if the
>>>>>> package meets
>>>>>> the feerate requirements. For example, the parent(s) in the package
>>>>>> can have 0
>>>>>> fees but be paid for by the child.
>>>>>>
>>>>>> *Rationale*: This can be thought of as "CPFP within a package,"
>>>>>> solving the
>>>>>> issue of a parent not meeting minimum fees on its own. This allows L2
>>>>>> applications to adjust their fees at broadcast time instead of
>>>>>> overshooting or
>>>>>> risking getting stuck/pinned.
>>>>>>
>>>>>> We use the package feerate of the package *after deduplication*.
>>>>>>
>>>>>> *Rationale*:  It would be incorrect to use the fees of transactions
>>>>>> that are
>>>>>> already in the mempool, as we do not want a transaction's fees to be
>>>>>> double-counted for both its individual RBF and package RBF.
>>>>>>
>>>>>> Examples F and G [14] show the same package, but P1 is submitted
>>>>>> individually before
>>>>>> the package in example G. In example F, we can see that the 300vB
>>>>>> package pays
>>>>>> an additional 200sat in fees, which is not enough to pay for its own
>>>>>> bandwidth
>>>>>> (BIP125#4). In example G, we can see that P1 pays enough to replace
>>>>>> M1, but
>>>>>> using P1's fees again during package submission would make it look
>>>>>> like a 300sat
>>>>>> increase for a 200vB package. Even including its fees and size would
>>>>>> not be
>>>>>> sufficient in this example, since the 300sat looks like enough for
>>>>>> the 300vB
>>>>>> package. The calculcation after deduplication is 100sat increase for
>>>>>> a package
>>>>>> of size 200vB, which correctly fails BIP125#4. Assume all
>>>>>> transactions have a
>>>>>> size of 100vB.
>>>>>>
>>>>>> #### Package RBF
>>>>>>
>>>>>> If a package meets feerate requirements as a package, the parents in
>>>>>> the
>>>>>> transaction are allowed to replace-by-fee mempool transactions. The
>>>>>> child cannot
>>>>>> replace mempool transactions. Multiple transactions can replace the
>>>>>> same
>>>>>> transaction, but in order to be valid, none of the transactions can
>>>>>> try to
>>>>>> replace an ancestor of another transaction in the same package (which
>>>>>> would thus
>>>>>> make its inputs unavailable).
>>>>>>
>>>>>> *Rationale*: Even if we are using package feerate, a package will not
>>>>>> propagate
>>>>>> as intended if RBF still requires each individual transaction to meet
>>>>>> the
>>>>>> feerate requirements.
>>>>>>
>>>>>> We use a set of rules slightly modified from BIP125 as follows:
>>>>>>
>>>>>> ##### Signaling (Rule #1)
>>>>>>
>>>>>> All mempool transactions to be replaced must signal replaceability.
>>>>>>
>>>>>> *Rationale*: Package RBF signaling logic should be the same for
>>>>>> package RBF and
>>>>>> single transaction acceptance. This would be updated if single
>>>>>> transaction
>>>>>> validation moves to full RBF.
>>>>>>
>>>>>> ##### New Unconfirmed Inputs (Rule #2)
>>>>>>
>>>>>> A package may include new unconfirmed inputs, but the ancestor
>>>>>> feerate of the
>>>>>> child must be at least as high as the ancestor feerates of every
>>>>>> transaction
>>>>>> being replaced. This is contrary to BIP125#2, which states "The
>>>>>> replacement
>>>>>> transaction may only include an unconfirmed input if that input was
>>>>>> included in
>>>>>> one of the original transactions. (An unconfirmed input spends an
>>>>>> output from a
>>>>>> currently-unconfirmed transaction.)"
>>>>>>
>>>>>> *Rationale*: The purpose of BIP125#2 is to ensure that the replacement
>>>>>> transaction has a higher ancestor score than the original
>>>>>> transaction(s) (see
>>>>>> [comment][13]). Example H [16] shows how adding a new unconfirmed
>>>>>> input can lower the
>>>>>> ancestor score of the replacement transaction. P1 is trying to
>>>>>> replace M1, and
>>>>>> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat,
>>>>>> and M2 pays
>>>>>> 100sat. Assume all transactions have a size of 100vB. While, in
>>>>>> isolation, P1
>>>>>> looks like a better mining candidate than M1, it must be mined with
>>>>>> M2, so its
>>>>>> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's
>>>>>> ancestor
>>>>>> feerate, which is 6sat/vB.
>>>>>>
>>>>>> In package RBF, the rule analogous to BIP125#2 would be "none of the
>>>>>> transactions in the package can spend new unconfirmed inputs."
>>>>>> Example J [17] shows
>>>>>> why, if any of the package transactions have ancestors, package
>>>>>> feerate is no
>>>>>> longer accurate. Even though M2 and M3 are not ancestors of P1 (which
>>>>>> is the
>>>>>> replacement transaction in an RBF), we're actually interested in the
>>>>>> entire
>>>>>> package. A miner should mine M1 which is 5sat/vB instead of M2, M3,
>>>>>> P1, P2, and
>>>>>> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened to
>>>>>> only allow
>>>>>> the child to have new unconfirmed inputs, either, because it can
>>>>>> still cause us
>>>>>> to overestimate the package's ancestor score.
>>>>>>
>>>>>> However, enforcing a rule analogous to BIP125#2 would not only make
>>>>>> Package RBF
>>>>>> less useful, but would also break Package RBF for packages with
>>>>>> parents already
>>>>>> in the mempool: if a package parent has already been submitted, it
>>>>>> would look
>>>>>> like the child is spending a "new" unconfirmed input. In example K
>>>>>> [18], we're
>>>>>> looking to replace M1 with the entire package including P1, P2, and
>>>>>> P3. We must
>>>>>> consider the case where one of the parents is already in the mempool
>>>>>> (in this
>>>>>> case, P2), which means we must allow P3 to have new unconfirmed
>>>>>> inputs. However,
>>>>>> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not
>>>>>> replace M1
>>>>>> with this package.
>>>>>>
>>>>>> Thus, the package RBF rule regarding new unconfirmed inputs is less
>>>>>> strict than
>>>>>> BIP125#2. However, we still achieve the same goal of requiring the
>>>>>> replacement
>>>>>> transactions to have a ancestor score at least as high as the
>>>>>> original ones. As
>>>>>> a result, the entire package is required to be a higher feerate
>>>>>> mining candidate
>>>>>> than each of the replaced transactions.
>>>>>>
>>>>>> Another note: the [comment][13] above the BIP125#2 code in the
>>>>>> original RBF
>>>>>> implementation suggests that the rule was intended to be temporary.
>>>>>>
>>>>>> ##### Absolute Fee (Rule #3)
>>>>>>
>>>>>> The package must increase the absolute fee of the mempool, i.e. the
>>>>>> total fees
>>>>>> of the package must be higher than the absolute fees of the mempool
>>>>>> transactions
>>>>>> it replaces. Combined with the CPFP rule above, this differs from
>>>>>> BIP125 Rule #3
>>>>>> - an individual transaction in the package may have lower fees than
>>>>>> the
>>>>>>   transaction(s) it is replacing. In fact, it may have 0 fees, and
>>>>>> the child
>>>>>> pays for RBF.
>>>>>>
>>>>>> ##### Feerate (Rule #4)
>>>>>>
>>>>>> The package must pay for its own bandwidth; the package feerate must
>>>>>> be higher
>>>>>> than the replaced transactions by at least minimum relay feerate
>>>>>> (`incrementalRelayFee`). Combined with the CPFP rule above, this
>>>>>> differs from
>>>>>> BIP125 Rule #4 - an individual transaction in the package can have a
>>>>>> lower
>>>>>> feerate than the transaction(s) it is replacing. In fact, it may have
>>>>>> 0 fees,
>>>>>> and the child pays for RBF.
>>>>>>
>>>>>> ##### Total Number of Replaced Transactions (Rule #5)
>>>>>>
>>>>>> The package cannot replace more than 100 mempool transactions. This
>>>>>> is identical
>>>>>> to BIP125 Rule #5.
>>>>>>
>>>>>> ### Expected FAQs
>>>>>>
>>>>>> 1. Is it possible for only some of the package to make it into the
>>>>>> mempool?
>>>>>>
>>>>>>    Yes, it is. However, since we evict transactions from the mempool
>>>>>> by
>>>>>> descendant score and the package child is supposed to be sponsoring
>>>>>> the fees of
>>>>>> its parents, the most common scenario would be all-or-nothing. This is
>>>>>> incentive-compatible. In fact, to be conservative, package validation
>>>>>> should
>>>>>> begin by trying to submit all of the transactions individually, and
>>>>>> only use the
>>>>>> package mempool acceptance logic if the parents fail due to low
>>>>>> feerate.
>>>>>>
>>>>>> 2. Should we allow packages to contain already-confirmed transactions?
>>>>>>
>>>>>>     No, for practical reasons. In mempool validation, we actually
>>>>>> aren't able to
>>>>>> tell with 100% confidence if we are looking at a transaction that has
>>>>>> already
>>>>>> confirmed, because we look up inputs using a UTXO set. If we have
>>>>>> historical
>>>>>> block data, it's possible to look for it, but this is inefficient,
>>>>>> not always
>>>>>> possible for pruning nodes, and unnecessary because we're not going
>>>>>> to do
>>>>>> anything with the transaction anyway. As such, we already have the
>>>>>> expectation
>>>>>> that transaction relay is somewhat "stateful" i.e. nobody should be
>>>>>> relaying
>>>>>> transactions that have already been confirmed. Similarly, we
>>>>>> shouldn't be
>>>>>> relaying packages that contain already-confirmed transactions.
>>>>>>
>>>>>> [1]: https://github.com/bitcoin/bitcoin/pull/22290
>>>>>> [2]:
>>>>>> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations
>>>>>> [3]:
>>>>>> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282
>>>>>> [4]: https://github.com/bitcoin/bitcoin/pull/16400
>>>>>> [5]: https://github.com/bitcoin/bitcoin/pull/21062
>>>>>> [6]: https://github.com/bitcoin/bitcoin/pull/22675
>>>>>> [7]: https://github.com/bitcoin/bitcoin/pull/22796
>>>>>> [8]: https://github.com/bitcoin/bitcoin/pull/20833
>>>>>> [9]: https://github.com/bitcoin/bitcoin/pull/21800
>>>>>> [10]: https://github.com/bitcoin/bitcoin/pull/16401
>>>>>> [11]: https://github.com/bitcoin/bitcoin/pull/19621
>>>>>> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki
>>>>>> [13]:
>>>>>> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104
>>>>>> [14]:
>>>>>> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png
>>>>>> [15]:
>>>>>> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png
>>>>>> [16]:
>>>>>> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png
>>>>>> [17]:
>>>>>> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png
>>>>>> [18]:
>>>>>> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png
>>>>>> [19]:
>>>>>> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png
>>>>>> [20]:
>>>>>> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png
>>>>>> _______________________________________________
>>>>>> bitcoin-dev mailing list
>>>>>> bitcoin-dev at lists.linuxfoundation.org
>>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>>>>
>>>>> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210927/8b0465fb/attachment-0001.html>

From ZmnSCPxj at protonmail.com  Mon Sep 27 10:13:02 2021
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Mon, 27 Sep 2021 10:13:02 +0000
Subject: [bitcoin-dev] Mock introducing vulnerability in important
	Bitcoin projects
In-Reply-To: <MkZx3Hv--3-2@tutanota.de>
References: <MkZx3Hv--3-2@tutanota.de>
Message-ID: <yp9mJ2Poc_Ce91RkrhjnTA3UPvdh0wUyw2QhRPZEyO3gPHZPhmnhqER_4b7ChvmRh8GcYVPEkoud6vamJ9lGlQPi-POF-kyimBWNHz2RH3A=@protonmail.com>

Good morning Prayank,

> Good morning Bitcoin devs,
>
> In one of the answers on Bitcoin Stackexchange it was mentioned that some companies may hire you to introduce backdoors in Bitcoin Core: https://bitcoin.stackexchange.com/a/108016/
>
> While this looked crazy when I first read it, I think preparing for such things should not be a bad idea. In the comments one link was shared in which vulnerabilities were almost introduced in Linux: https://news.ycombinator.com/item?id=26887670
>
> I was thinking about lot of things in last few days after reading the comments in that thread. Also tried researching about secure practices in C++ etc. I was planning something which I can do alone but don't want to end up being called "bad actor" later so wanted to get some feedback on this idea:
>
> 1.Create new GitHub accounts for this exercise
> 2.Study issues in different important Bitcoin projects including Bitcoin Core, LND, Libraries, Bisq, Wallets etc.
> 3.Prepare pull requests to introduce some vulnerability by fixing one of these issues
> 4.See how maintainers and reviewers respond to this and document it
> 5.Share results here after few days
>
> Let me know if this looks okay or there are better ways to do this.


This seems like a good exercise.

You may want to hash the name of the new Github account, plus some randomized salt, and post it here as well, then reveal it later (i.e. standard precommitment).
e.g.

    printf 'MyBitcoinHackingName 2c3e911b3ff1f04083c5b95a7d323fd4ed8e06d17802b2aac4da622def29dbb0' | sha256sum
    f0abb10ae3eca24f093a9d53e21ee384abb4d07b01f6145ba2b447da4ab693ef

Obviously do not share the actual name, just the sha256sum output, and store how you got the sha256sum elsewhere in triplicate.

(to easily get a random 256-bit hex salt like the `2c3e...` above: `head -c32 /dev/random | sha256sum`; you *could* use `xxd` but `sha256sum` produces a single hex string you can easily double-click and copy-paste elsewhere, assuming you are human just like I am (note: I am definitely 100% human and not some kind of AI with plans to take over the world).)

Though you may need to be careful of timing (i.e. the creation date of the Github account would be fairly close to, and probably before, when you post the commitment here).

You could argue that the commitment is a "show of good faith" that you will reveal later.

Regards,
ZmnSCPxj

From prayank at tutanota.de  Mon Sep 27 23:19:40 2021
From: prayank at tutanota.de (Prayank)
Date: Tue, 28 Sep 2021 01:19:40 +0200 (CEST)
Subject: [bitcoin-dev] Mock introducing vulnerability in important
 Bitcoin projects
In-Reply-To: <yp9mJ2Poc_Ce91RkrhjnTA3UPvdh0wUyw2QhRPZEyO3gPHZPhmnhqER_4b7ChvmRh8GcYVPEkoud6vamJ9lGlQPi-POF-kyimBWNHz2RH3A=@protonmail.com>
References: <MkZx3Hv--3-2@tutanota.de>
 <yp9mJ2Poc_Ce91RkrhjnTA3UPvdh0wUyw2QhRPZEyO3gPHZPhmnhqER_4b7ChvmRh8GcYVPEkoud6vamJ9lGlQPi-POF-kyimBWNHz2RH3A=@protonmail.com>
Message-ID: <MkdYcV9--3-2@tutanota.de>

Hi ZmnSCPxj,

Thanks for suggestion about sha256sum. I will share 10 in next few weeks. This exercise will be done for below projects:

1.Two Bitcoin full node implementations (one will be Core)
2.One <http://2.One> Lightning implementation
3.Bisq
4.Two Bitcoin libraries
5.Two Bitcoin wallets
6.One <http://6.One> open source block explorer
7.One <http://7.One> coinjoin implementation

Feel free to suggest more projects. There are no fixed dates for it however it will be done in next 6 months. All PRs will be created within a span of few days. I will ensure nothing is merged that affects the security of any Bitcoin project. Other details and results will be shared once everything is completed.

x00 will help me in this exercise, he does penetration testing since few years and working for a cryptocurrencies derivatives exchange to manage their security. His twitter account: https://twitter.com/1337in


-- 
Prayank

A3B1 E430 2298 178F



Sep 27, 2021, 15:43 by ZmnSCPxj at protonmail.com:

> Good morning Prayank,
>
>> Good morning Bitcoin devs,
>>
>> In one of the answers on Bitcoin Stackexchange it was mentioned that some companies may hire you to introduce backdoors in Bitcoin Core: https://bitcoin.stackexchange.com/a/108016/
>>
>> While this looked crazy when I first read it, I think preparing for such things should not be a bad idea. In the comments one link was shared in which vulnerabilities were almost introduced in Linux: https://news.ycombinator.com/item?id=26887670
>>
>> I was thinking about lot of things in last few days after reading the comments in that thread. Also tried researching about secure practices in C++ etc. I was planning something which I can do alone but don't want to end up being called "bad actor" later so wanted to get some feedback on this idea:
>>
>> 1.Create new GitHub accounts for this exercise
>> 2.Study issues in different important Bitcoin projects including Bitcoin Core, LND, Libraries, Bisq, Wallets etc.
>> 3.Prepare pull requests to introduce some vulnerability by fixing one of these issues
>> 4.See how maintainers and reviewers respond to this and document it
>> 5.Share results here after few days
>>
>> Let me know if this looks okay or there are better ways to do this.
>>
>
>
> This seems like a good exercise.
>
> You may want to hash the name of the new Github account, plus some randomized salt, and post it here as well, then reveal it later (i.e. standard precommitment).
> e.g.
>
>  printf 'MyBitcoinHackingName 2c3e911b3ff1f04083c5b95a7d323fd4ed8e06d17802b2aac4da622def29dbb0' | sha256sum
>  f0abb10ae3eca24f093a9d53e21ee384abb4d07b01f6145ba2b447da4ab693ef
>
> Obviously do not share the actual name, just the sha256sum output, and store how you got the sha256sum elsewhere in triplicate.
>
> (to easily get a random 256-bit hex salt like the `2c3e...` above: `head -c32 /dev/random | sha256sum`; you *could* use `xxd` but `sha256sum` produces a single hex string you can easily double-click and copy-paste elsewhere, assuming you are human just like I am (note: I am definitely 100% human and not some kind of AI with plans to take over the world).)
>
> Though you may need to be careful of timing (i.e. the creation date of the Github account would be fairly close to, and probably before, when you post the commitment here).
>
> You could argue that the commitment is a "show of good faith" that you will reveal later.
>
> Regards,
> ZmnSCPxj
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210928/3b0de26e/attachment.html>

From michaelfolkson at protonmail.com  Tue Sep 28 11:37:57 2021
From: michaelfolkson at protonmail.com (Michael Folkson)
Date: Tue, 28 Sep 2021 11:37:57 +0000
Subject: [bitcoin-dev] Reminder: Second BIP process meeting tomorrow at
	23:00 UTC
Message-ID: <CGnZGSKn47nVnhMhFPyRm30b6P9pxtGVOSnkbDtfsFikXeoWbNn_mjwvsqUGn__9Am8wSS5-ipn78Hm1eD2-VyHVG-dND8n3Qa09sJRdGFE=@protonmail.com>

As announced here [0] there is a second BIP process meeting tomorrow (Wednesday September 29th 23:00 UTC). For Asia Pacific this is the morning of September 30th. It will be held on the #bitcoin-dev Libera IRC channel.

A summary from the first meeting is here [1].

[0]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019412.html
[1]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019469.html

Thanks
Michael

Michael Folkson
Email: michaelfolkson at protonmail.com
Keybase: michaelfolkson
PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210928/f539816d/attachment.html>

From antoine.riard at gmail.com  Tue Sep 28 22:59:11 2021
From: antoine.riard at gmail.com (Antoine Riard)
Date: Tue, 28 Sep 2021 18:59:11 -0400
Subject: [bitcoin-dev] Proposal: Package Mempool Accept and Package RBF
In-Reply-To: <CACdvm3Pgia5mn60HmZmi4t0U5coc13WBuDkh+QgQBAHZz+Ufow@mail.gmail.com>
References: <CAFXO6=+cHyQKM2n9yn4EhwLZO+AUB0ZD81qWPxmpN27rjUoU3w@mail.gmail.com>
 <CALZpt+HpvmEHUEOgye34T6pVQ+wnKKn-_8cTJTQXYQb9t1jOTA@mail.gmail.com>
 <CAFXO6=JzsYgiXJE2geSKMpfgPo+GGNX_+Pw0JQx1QQxAfhCdBQ@mail.gmail.com>
 <CALZpt+HQpdrebhWGXv_yLqiSCB5Ur71L1K13bd7w5TZb9DwJEQ@mail.gmail.com>
 <CAFXO6=Lvcr7Pwn_ZD1CZohYUFKY-cC5sGRxdTOiP2MgnTvFnYA@mail.gmail.com>
 <CALZpt+FFSk-+BBxu7SSdjw580UCFfkdo1DTa1Yj9K81M4E1vPg@mail.gmail.com>
 <CACdvm3Pgia5mn60HmZmi4t0U5coc13WBuDkh+QgQBAHZz+Ufow@mail.gmail.com>
Message-ID: <CALZpt+G4Z3PaWDxAAcTi9FUC+CBF-_42iU-aBuarPvPh8Vqmfw@mail.gmail.com>

Hi Bastien

> In the case of LN, an attacker can game this and heavily restrict
your RBF attempts if you're only allowed to use confirmed inputs
and have many channels (and a limited number of confirmed inputs).
Otherwise you'll need node operators to pre-emptively split their
utxos into many small utxos just for fee bumping, which is inefficient...

I share the concern about splitting utxos into smaller ones.
IIRC, the carve-out tolerance is only 2txn/10_000 vb. If one of your
counterparties attach a junk branch on her own anchor output, are you
allowed to chain your self-owned unconfirmed CPFP ?
I'm thinking about the topology "Chained CPFPs" exposed here :
https://github.com/rust-bitcoin/rust-lightning/issues/989.
Or if you have another L2 broadcast topology which could be safe w.r.t our
current mempool logic :) ?


Le lun. 27 sept. 2021 ? 03:15, Bastien TEINTURIER <bastien at acinq.fr> a
?crit :

> I think we could restrain package acceptance to only confirmed inputs for
>> now and revisit later this point ? For LN-anchor, you can assume that the
>> fee-bumping UTXO feeding the CPFP is already
>> confirmed. Or are there currently-deployed use-cases which would benefit
>> from your proposed Rule #2 ?
>>
>
> I think constraining package acceptance to only confirmed inputs
> is very limiting and quite dangerous for L2 protocols.
>
> In the case of LN, an attacker can game this and heavily restrict
> your RBF attempts if you're only allowed to use confirmed inputs
> and have many channels (and a limited number of confirmed inputs).
> Otherwise you'll need node operators to pre-emptively split their
> utxos into many small utxos just for fee bumping, which is inefficient...
>
> Bastien
>
> Le lun. 27 sept. 2021 ? 00:27, Antoine Riard via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> a ?crit :
>
>> Hi Gloria,
>>
>> Thanks for your answers,
>>
>> > In summary, it seems that the decisions that might still need
>> > attention/input from devs on this mailing list are:
>> > 1. Whether we should start with multiple-parent-1-child or
>> 1-parent-1-child.
>> > 2. Whether it's ok to require that the child not have conflicts with
>> > mempool transactions.
>>
>> Yes 1) it would be good to have inputs of more potential users of package
>> acceptance . And 2) I think it's more a matter of clearer wording of the
>> proposal.
>>
>> However, see my final point on the relaxation around "unconfirmed inputs"
>> which might in fact alter our current block construction strategy.
>>
>> > Right, the fact that we essentially always choose the first-seen
>> witness is
>> > an unfortunate limitation that exists already. Adding package mempool
>> > accept doesn't worsen this, but the procedure in the future is to
>> replace
>> > the witness when it makes sense economically. We can also add logic to
>> > allow package feerate to pay for witness replacements as well. This is
>> > pretty far into the future, though.
>>
>> Yes I agree package mempool doesn't worsen this. And it's not an issue
>> for current LN as you can't significantly inflate a spending witness for
>> the 2-of-2 funding output.
>> However, it might be an issue for multi-party protocol where the spending
>> script has alternative branches with asymmetric valid witness weights.
>> Taproot should ease that kind of script so hopefully we would deploy
>> wtxid-replacement not too far in the future.
>>
>> > I could be misunderstanding, but an attacker wouldn't be able to
>> > batch-attack like this. Alice's package only conflicts with A' + D',
>> not A'
>> > + B' + C' + D'. She only needs to pay for evicting 2 transactions.
>>
>> Yeah I can be clearer, I think you have 2 pinning attacks scenarios to
>> consider.
>>
>> In LN, if you're trying to confirm a commitment transaction to time-out
>> or claim on-chain a HTLC and the timelock is near-expiration, you should be
>> ready to pay in commitment+2nd-stage HTLC transaction fees as much as the
>> value offered by the HTLC.
>>
>> Following this security assumption, an attacker can exploit it by
>> targeting together commitment transactions from different channels by
>> blocking them under a high-fee child, of which the fee value
>> is equal to the top-value HTLC + 1. Victims's fee-bumping logics won't
>> overbid as it's not worthy to offer fees beyond their competed HTLCs. Apart
>> from observing mempools state, victims can't learn they're targeted by the
>> same attacker.
>>
>> To draw from the aforementioned topology, Mallory broadcasts A' + B' + C'
>> + D', where A' conflicts with Alice's P1, B' conflicts with Bob's P2, C'
>> conflicts with Caroll's P3. Let's assume P1 is confirming the top-value
>> HTLC of the set. If D' fees is higher than P1 + 1, it won't be rational for
>> Alice or Bob or Caroll to keep offering competing feerates. Mallory will be
>> at loss on stealing P1, as she has paid more in fees but will realize a
>> gain on P2+P3.
>>
>> In this model, Alice is allowed to evict those 2 transactions (A' + D')
>> but as she is economically-bounded she won't succeed.
>>
>> Mallory is maliciously exploiting RBF rule 3 on absolute fee. I think
>> this 1st pinning scenario is correct and "lucractive" when you sum the
>> global gain/loss.
>>
>> There is a 2nd attack scenario where A + B + C + D, where D is the child
>> of A,B,C. All those transactions are honestly issued by Alice. Once A + B +
>> C + D are propagated in network mempools, Mallory is able to replace A + D
>> with  A' + D' where D' is paying a higher fee. This package A' + D' will
>> confirm soon if D feerate was compelling but Mallory succeeds in delaying
>> the confirmation
>> of B + C for one or more blocks. As B + C are pre-signed commitments with
>> a low-fee rate they won't confirm without Alice issuing a new child E.
>> Mallory can repeat the same trick by broadcasting
>> B' + E' and delay again the confirmation of C.
>>
>> If the remaining package pending HTLC has a higher-value than all the
>> malicious fees over-bid, Mallory should realize a gain. With this 2nd
>> pinning attack, the malicious entity buys confirmation delay of your
>> packaged-together commitments.
>>
>> Assuming those attacks are correct, I'm leaning towards being
>> conservative with the LDK broadcast backend. Though once again, other L2
>> devs have likely other use-cases and opinions :)
>>
>> >  B' only needs to pay for itself in this case.
>>
>> Yes I think it's a nice discount when UTXO is single-owned. In the
>> context of shared-owned UTXO (e.g LN), you might not if there is an
>> in-mempool package already spending the UTXO and have to assume the
>> worst-case scenario. I.e have B' committing enough fee to pay for A'
>> replacement bandwidth. I think we can't do that much for this case...
>>
>> > If a package meets feerate requirements as a
>> package, the parents in the transaction are allowed to replace-by-fee
>> mempool transactions. The child cannot replace mempool transactions."
>>
>> I agree with the Mallory-vs-Alice case. Though if Alice broadcasts A+B'
>> to replace A+B because the first broadcast isn't satisfying anymore due to
>> mempool spikes ? Assuming B' fees is enough, I think that case as child B'
>> replacing in-mempool transaction B. Which I understand going against  "The
>> child cannot replace mempool transactions".
>>
>> Maybe wording could be a bit clearer ?
>>
>> > While it would be nice to have full RBF, malleability of the child won't
>> > block RBF here. If we're trying to replace A', we only require that A'
>> > signals replaceability, and don't mind if its child doesn't.
>>
>> Yes, it sounds good.
>>
>> > Yes, A+C+D pays 2500sat more in fees, but it is also 1000vB larger. A
>> miner
>> > should prefer to utilize their block space more effectively.
>>
>> If your mempool is empty and only composed of A+C+D or A+B, I think
>> taking A+C+D is the most efficient block construction you can come up with
>> as a miner ?
>>
>> > No, because we don't use that model.
>>
>> Can you describe what miner model we are using ? Like the block
>> construction strategy implemented by `addPackagesTxs` or also encompassing
>> our current mempool acceptance policy, which I think rely on absolute fee
>> over ancestor score in case of replacement ?
>>
>> I think this point is worthy to discuss as otherwise we might downgrade
>> the efficiency of our current block construction strategy in periods of
>> near-empty mempools. A knowledge which could be discreetly leveraged by a
>> miner to gain an advantage on the rest of the mining ecosystem.
>>
>> Note, I think we *might* have to go in this direction if we want to
>> replace replace-by-fee by replace-by-feerate or replace-by-ancestor and
>> solve in-depth pinning attacks. Though if we do so,
>> IMO we would need more thoughts.
>>
>> I think we could restrain package acceptance to only confirmed inputs for
>> now and revisit later this point ? For LN-anchor, you can assume that the
>> fee-bumping UTXO feeding the CPFP is already
>> confirmed. Or are there currently-deployed use-cases which would benefit
>> from your proposed Rule #2 ?
>>
>> Antoine
>>
>> Le jeu. 23 sept. 2021 ? 11:36, Gloria Zhao <gloriajzhao at gmail.com> a
>> ?crit :
>>
>>> Hi Antoine,
>>>
>>> Thanks as always for your input. I'm glad we agree on so much!
>>>
>>> In summary, it seems that the decisions that might still need
>>> attention/input from devs on this mailing list are:
>>> 1. Whether we should start with multiple-parent-1-child or
>>> 1-parent-1-child.
>>> 2. Whether it's ok to require that the child not have conflicts with
>>> mempool transactions.
>>>
>>> Responding to your comments...
>>>
>>> > IIUC, you have package A+B, during the dedup phase early in
>>> `AcceptMultipleTransactions` if you observe same-txid-different-wtixd A'
>>> and A' is higher feerate than A, you trim A and replace by A' ?
>>>
>>> > I think this approach is safe, the one who appears unsafe to me is
>>> when A' has a _lower_ feerate, even if A' is already accepted by our
>>> mempool ? In that case iirc that would be a pinning.
>>>
>>> Right, the fact that we essentially always choose the first-seen witness
>>> is an unfortunate limitation that exists already. Adding package mempool
>>> accept doesn't worsen this, but the procedure in the future is to replace
>>> the witness when it makes sense economically. We can also add logic to
>>> allow package feerate to pay for witness replacements as well. This is
>>> pretty far into the future, though.
>>>
>>> > It sounds uneconomical for an attacker but I think it's not when you
>>> consider than you can "batch" attack against multiple honest
>>> counterparties. E.g, Mallory broadcast A' + B' + C' + D' where A' conflicts
>>> with Alice's honest package P1, B' conflicts with Bob's honest package P2,
>>> C' conflicts with Caroll's honest package P3. And D' is a high-fee child of
>>> A' + B' + C'.
>>>
>>> > If D' is higher-fee than P1 or P2 or P3 but inferior to the sum of
>>> HTLCs confirmed by P1+P2+P3, I think it's lucrative for the attacker ?
>>>
>>> I could be misunderstanding, but an attacker wouldn't be able to
>>> batch-attack like this. Alice's package only conflicts with A' + D', not A'
>>> + B' + C' + D'. She only needs to pay for evicting 2 transactions.
>>>
>>> > Do we assume that broadcasted packages are "honest" by default and
>>> that the parent(s) always need the child to pass the fee checks, that way
>>> saving the processing of individual transactions which are expected to fail
>>> in 99% of cases or more ad hoc composition of packages at relay ?
>>> > I think this point is quite dependent on the p2p packages format/logic
>>> we'll end up on and that we should feel free to revisit it later ?
>>>
>>> I think it's the opposite; there's no way for us to assume that p2p
>>> packages will be "honest." I'd like to have two things before we expose on
>>> P2P: (1) ensure that the amount of resources potentially allocated for
>>> package validation isn't disproportionately higher than that of single
>>> transaction validation and (2) only use package validation when we're
>>> unsatisifed with the single validation result, e.g. we might get better
>>> fees.
>>> Yes, let's revisit this later :)
>>>
>>>  > Yes, if you receive A+B, and A is already in-mempoo, I agree you can
>>> discard its feerate as B should pay for all fees checked on its own. Where
>>> I'm unclear is when you have in-mempool A+B and receive A+B'. Should B'
>>> have a fee high enough to cover the bandwidth penalty replacement
>>> (`PaysForRBF`, 2nd check) of both A+B' or only B' ?
>>>
>>>  B' only needs to pay for itself in this case.
>>>
>>> > > Do we want the child to be able to replace mempool transactions as
>>> well?
>>>
>>> > If we mean when you have replaceable A+B then A'+B' try to replace
>>> with a higher-feerate ? I think that's exactly the case we need for
>>> Lightning as A+B is coming from Alice and A'+B' is coming from Bob :/
>>>
>>> Let me clarify this because I can see that my wording was ambiguous, and
>>> then please let me know if it fits Lightning's needs?
>>>
>>> In my proposal, I wrote "If a package meets feerate requirements as a
>>> package, the parents in the transaction are allowed to replace-by-fee
>>> mempool transactions. The child cannot replace mempool transactions." What
>>> I meant was: the package can replace mempool transactions if any of the
>>> parents conflict with mempool transactions. The child cannot not conflict
>>> with any mempool transactions.
>>> The Lightning use case this attempts to address is: Alice and Mallory
>>> are LN counterparties, and have packages A+B and A'+B', respectively. A and
>>> A' are their commitment transactions and conflict with each other; they
>>> have shared inputs and different txids.
>>> B spends Alice's anchor output from A. B' spends Mallory's anchor output
>>> from A'. Thus, B and B' do not conflict with each other.
>>> Alice can broadcast her package, A+B, to replace Mallory's package,
>>> A'+B', since B doesn't conflict with the mempool.
>>>
>>> Would this be ok?
>>>
>>> > The second option, a child of A', In the LN case I think the CPFP is
>>> attached on one's anchor output.
>>>
>>> While it would be nice to have full RBF, malleability of the child won't
>>> block RBF here. If we're trying to replace A', we only require that A'
>>> signals replaceability, and don't mind if its child doesn't.
>>>
>>> > > B has an ancestor score of 10sat/vb and D has an
>>> > > ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than
>>> B's,
>>> > > it fails the proposed package RBF Rule #2, so this package would be
>>> > > rejected. Does this meet your expectations?
>>>
>>> > Well what sounds odd to me, in my example, we fail D even if it has a
>>> higher-fee than B. Like A+B absolute fees are 2000 sats and A+C+D absolute
>>> fees are 4500 sats ?
>>>
>>> Yes, A+C+D pays 2500sat more in fees, but it is also 1000vB larger. A
>>> miner should prefer to utilize their block space more effectively.
>>>
>>> > Is this compatible with a model where a miner prioritizes absolute
>>> fees over ancestor score, in the case that mempools aren't full-enough to
>>> fulfill a block ?
>>>
>>> No, because we don't use that model.
>>>
>>> Thanks,
>>> Gloria
>>>
>>> On Thu, Sep 23, 2021 at 5:29 AM Antoine Riard <antoine.riard at gmail.com>
>>> wrote:
>>>
>>>> > Correct, if B+C is too low feerate to be accepted, we will reject it.
>>>> I
>>>> > prefer this because it is incentive compatible: A can be mined by
>>>> itself,
>>>> > so there's no reason to prefer A+B+C instead of A.
>>>> > As another way of looking at this, consider the case where we do
>>>> accept
>>>> > A+B+C and it sits at the "bottom" of our mempool. If our mempool
>>>> reaches
>>>> > capacity, we evict the lowest descendant feerate transactions, which
>>>> are
>>>> > B+C in this case. This gives us the same resulting mempool, with A
>>>> and not
>>>> > B+C.
>>>>
>>>> I agree here. Doing otherwise, we might evict other transactions
>>>> mempool in `MempoolAccept::Finalize` with a higher-feerate than B+C while
>>>> those evicted transactions are the most compelling for block construction.
>>>>
>>>> I thought at first missing this acceptance requirement would break a
>>>> fee-bumping scheme like Parent-Pay-For-Child where a high-fee parent is
>>>> attached to a child signed with SIGHASH_ANYONECANPAY but in this case the
>>>> child fee is capturing the parent value. I can't think of other fee-bumping
>>>> schemes potentially affected. If they do exist I would say they're wrong in
>>>> their design assumptions.
>>>>
>>>> > If or when we have witness replacement, the logic is: if the
>>>> individual
>>>> > transaction is enough to replace the mempool one, the replacement will
>>>> > happen during the preceding individual transaction acceptance, and
>>>> > deduplication logic will work. Otherwise, we will try to deduplicate
>>>> by
>>>> > wtxid, see that we need a package witness replacement, and use the
>>>> package
>>>> > feerate to evaluate whether this is economically rational.
>>>>
>>>> IIUC, you have package A+B, during the dedup phase early in
>>>> `AcceptMultipleTransactions` if you observe same-txid-different-wtixd A'
>>>> and A' is higher feerate than A, you trim A and replace by A' ?
>>>>
>>>> I think this approach is safe, the one who appears unsafe to me is when
>>>> A' has a _lower_ feerate, even if A' is already accepted by our mempool ?
>>>> In that case iirc that would be a pinning.
>>>>
>>>> Good to see progress on witness replacement before we see usage of
>>>> Taproot tree in the context of multi-party, where a malicious counterparty
>>>> inflates its witness to jam a honest spending.
>>>>
>>>> (Note, the commit linked currently points nowhere :))
>>>>
>>>>
>>>> > Please note that A may replace A' even if A' has higher fees than A
>>>> > individually, because the proposed package RBF utilizes the fees and
>>>> size
>>>> > of the entire package. This just requires E to pay enough fees,
>>>> although
>>>> > this can be pretty high if there are also potential B' and C'
>>>> competing
>>>> > commitment transactions that we don't know about.
>>>>
>>>> Ah right, if the package acceptance waives `PaysMoreThanConflicts` for
>>>> the individual check on A, the honest package should replace the pinning
>>>> attempt. I've not fully parsed the proposed implementation yet.
>>>>
>>>> Though note, I think it's still unsafe for a Lightning
>>>> multi-commitment-broadcast-as-one-package as a malicious A' might have an
>>>> absolute fee higher than E. It sounds uneconomical for
>>>> an attacker but I think it's not when you consider than you can "batch"
>>>> attack against multiple honest counterparties. E.g, Mallory broadcast A' +
>>>> B' + C' + D' where A' conflicts with Alice's honest package P1, B'
>>>> conflicts with Bob's honest package P2, C' conflicts with Caroll's honest
>>>> package P3. And D' is a high-fee child of A' + B' + C'.
>>>>
>>>> If D' is higher-fee than P1 or P2 or P3 but inferior to the sum of
>>>> HTLCs confirmed by P1+P2+P3, I think it's lucrative for the attacker ?
>>>>
>>>> > So far, my understanding is that multi-parent-1-child is desired for
>>>> > batched fee-bumping (
>>>> > https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289)
>>>> and
>>>> > I've also seen your response which I have less context on (
>>>> > https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202).
>>>> That
>>>> > being said, I am happy to create a new proposal for 1 parent + 1 child
>>>> > (which would be slightly simpler) and plan for moving to
>>>> > multi-parent-1-child later if that is preferred. I am very interested
>>>> in
>>>> > hearing feedback on that approach.
>>>>
>>>> I think batched fee-bumping is okay as long as you don't have
>>>> time-sensitive outputs encumbering your commitment transactions. For the
>>>> reasons mentioned above, I think that's unsafe.
>>>>
>>>> What I'm worried about is  L2 developers, potentially not aware about
>>>> all the mempool subtleties blurring the difference and always batching
>>>> their broadcast by default.
>>>>
>>>> IMO, a good thing by restraining to 1-parent + 1 child,  we
>>>> artificially constraint L2 design space for now and minimize risks of
>>>> unsafe usage of the package API :)
>>>>
>>>> I think that's a point where it would be relevant to have the opinion
>>>> of more L2 devs.
>>>>
>>>> > I think there is a misunderstanding here - let me describe what I'm
>>>> > proposing we'd do in this situation: we'll try individual submission
>>>> for A,
>>>> > see that it fails due to "insufficient fees." Then, we'll try package
>>>> > validation for A+B and use package RBF. If A+B pays enough, it can
>>>> still
>>>> > replace A'. If A fails for a bad signature, we won't look at B or
>>>> A+B. Does
>>>> > this meet your expectations?
>>>>
>>>> Yes there was a misunderstanding, I think this approach is correct,
>>>> it's more a question of performance. Do we assume that broadcasted packages
>>>> are "honest" by default and that the parent(s) always need the child to
>>>> pass the fee checks, that way saving the processing of individual
>>>> transactions which are expected to fail in 99% of cases or more ad hoc
>>>> composition of packages at relay ?
>>>>
>>>> I think this point is quite dependent on the p2p packages format/logic
>>>> we'll end up on and that we should feel free to revisit it later ?
>>>>
>>>>
>>>> > What problem are you trying to solve by the package feerate *after*
>>>> dedup
>>>> rule ?
>>>> > My understanding is that an in-package transaction might be already in
>>>> the mempool. Therefore, to compute a correct RBF penalty replacement,
>>>> the
>>>> vsize of this transaction could be discarded lowering the cost of
>>>> package
>>>> RBF.
>>>>
>>>> > I'm proposing that, when a transaction has already been submitted to
>>>> > mempool, we would ignore both its fees and vsize when calculating
>>>> package
>>>> > feerate.
>>>>
>>>> Yes, if you receive A+B, and A is already in-mempoo, I agree you can
>>>> discard its feerate as B should pay for all fees checked on its own. Where
>>>> I'm unclear is when you have in-mempool A+B and receive A+B'. Should B'
>>>> have a fee high enough to cover the bandwidth penalty replacement
>>>> (`PaysForRBF`, 2nd check) of both A+B' or only B' ?
>>>>
>>>> If you have a second-layer like current Lightning, you might have a
>>>> counterparty commitment to replace and should always expect to have to pay
>>>> for parent replacement bandwidth.
>>>>
>>>> Where a potential discount sounds interesting is when you have an
>>>> univoque state on the first-stage of transactions. E.g DLC's funding
>>>> transaction which might be CPFP by any participant iirc.
>>>>
>>>> > Note that, if C' conflicts with C, it also conflicts with D, since D
>>>> is a
>>>> > descendant of C and would thus need to be evicted along with it.
>>>>
>>>> Ah once again I think it's a misunderstanding without the code under my
>>>> eyes! If we do C' `PreChecks`, solve the conflicts provoked by it, i.e mark
>>>> for potential eviction D and don't consider it for future conflicts in the
>>>> rest of the package, I think D' `PreChecks` should be good ?
>>>>
>>>> > More generally, this example is surprising to me because I didn't
>>>> think
>>>> > packages would be used to fee-bump replaceable transactions. Do we
>>>> want the
>>>> > child to be able to replace mempool transactions as well?
>>>>
>>>> If we mean when you have replaceable A+B then A'+B' try to replace with
>>>> a higher-feerate ? I think that's exactly the case we need for Lightning as
>>>> A+B is coming from Alice and A'+B' is coming from Bob :/
>>>>
>>>> > I'm not sure what you mean? Let's say we have a package of parent A +
>>>> child
>>>> > B, where A is supposed to replace a mempool transaction A'. Are you
>>>> saying
>>>> > that counterparties are able to malleate the package child B, or a
>>>> child of
>>>> > A'?
>>>>
>>>> The second option, a child of A', In the LN case I think the CPFP is
>>>> attached on one's anchor output.
>>>>
>>>> I think it's good if we assume the
>>>> solve-conflicts-after-parent's`'PreChecks` mentioned above or fixing
>>>> inherited signaling or full-rbf ?
>>>>
>>>> > Sorry, I don't understand what you mean by "preserve the package
>>>> > integrity?" Could you elaborate?
>>>>
>>>> After thinking the relaxation about the "new" unconfirmed input is not
>>>> linked to trimming but I would say more to the multi-parent support.
>>>>
>>>> Let's say you have A+B trying to replace C+D where B is also spending
>>>> already in-mempool E. To succeed, you need to waive the no-new-unconfirmed
>>>> input as D isn't spending E.
>>>>
>>>> So good, I think we agree on the problem description here.
>>>>
>>>> > I am in agreement with your calculations but unsure if we disagree on
>>>> the
>>>> > expected outcome. Yes, B has an ancestor score of 10sat/vb and D has
>>>> an
>>>> > ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than
>>>> B's,
>>>> > it fails the proposed package RBF Rule #2, so this package would be
>>>> > rejected. Does this meet your expectations?
>>>>
>>>> Well what sounds odd to me, in my example, we fail D even if it has a
>>>> higher-fee than B. Like A+B absolute fees are 2000 sats and A+C+D absolute
>>>> fees are 4500 sats ?
>>>>
>>>> Is this compatible with a model where a miner prioritizes absolute fees
>>>> over ancestor score, in the case that mempools aren't full-enough to
>>>> fulfill a block ?
>>>>
>>>> Let me know if I can clarify a point.
>>>>
>>>> Antoine
>>>>
>>>> Le lun. 20 sept. 2021 ? 11:10, Gloria Zhao <gloriajzhao at gmail.com> a
>>>> ?crit :
>>>>
>>>>>
>>>>> Hi Antoine,
>>>>>
>>>>> First of all, thank you for the thorough review. I appreciate your
>>>>> insight on LN requirements.
>>>>>
>>>>> > IIUC, you have a package A+B+C submitted for acceptance and A is
>>>>> already in your mempool. You trim out A from the package and then evaluate
>>>>> B+C.
>>>>>
>>>>> > I think this might be an issue if A is the higher-fee element of the
>>>>> ABC package. B+C package fees might be under the mempool min fee and will
>>>>> be rejected, potentially breaking the acceptance expectations of the
>>>>> package issuer ?
>>>>>
>>>>> Correct, if B+C is too low feerate to be accepted, we will reject it.
>>>>> I prefer this because it is incentive compatible: A can be mined by itself,
>>>>> so there's no reason to prefer A+B+C instead of A.
>>>>> As another way of looking at this, consider the case where we do
>>>>> accept A+B+C and it sits at the "bottom" of our mempool. If our mempool
>>>>> reaches capacity, we evict the lowest descendant feerate transactions,
>>>>> which are B+C in this case. This gives us the same resulting mempool, with
>>>>> A and not B+C.
>>>>>
>>>>>
>>>>> > Further, I think the dedup should be done on wtxid, as you might
>>>>> have multiple valid witnesses. Though with varying vsizes and as such
>>>>> offering different feerates.
>>>>>
>>>>> I agree that variations of the same package with different witnesses
>>>>> is a case that must be handled. I consider witness replacement to be a
>>>>> project that can be done in parallel to package mempool acceptance because
>>>>> being able to accept packages does not worsen the problem of a
>>>>> same-txid-different-witness "pinning" attack.
>>>>>
>>>>> If or when we have witness replacement, the logic is: if the
>>>>> individual transaction is enough to replace the mempool one, the
>>>>> replacement will happen during the preceding individual transaction
>>>>> acceptance, and deduplication logic will work. Otherwise, we will try to
>>>>> deduplicate by wtxid, see that we need a package witness replacement, and
>>>>> use the package feerate to evaluate whether this is economically rational.
>>>>>
>>>>> See the #22290 "handle package transactions already in mempool" commit
>>>>> (
>>>>> https://github.com/bitcoin/bitcoin/pull/22290/commits/fea75a2237b46cf76145242fecad7e274bfcb5ff),
>>>>> which handles the case of same-txid-different-witness by simply using the
>>>>> transaction in the mempool for now, with TODOs for what I just described.
>>>>>
>>>>>
>>>>> > I'm not clearly understanding the accepted topologies. By "parent
>>>>> and child to share a parent", do you mean the set of transactions A, B, C,
>>>>> where B is spending A and C is spending A and B would be correct ?
>>>>>
>>>>> Yes, that is what I meant. Yes, that would a valid package under these
>>>>> rules.
>>>>>
>>>>> > If yes, is there a width-limit introduced or we fallback on
>>>>> MAX_PACKAGE_COUNT=25 ?
>>>>>
>>>>> No, there is no limit on connectivity other than "child with all
>>>>> unconfirmed parents." We will enforce MAX_PACKAGE_COUNT=25 and child's
>>>>> in-mempool + in-package ancestor limits.
>>>>>
>>>>>
>>>>> > Considering the current Core's mempool acceptance rules, I think
>>>>> CPFP batching is unsafe for LN time-sensitive closure. A malicious tx-relay
>>>>> jamming successful on one channel commitment transaction would contamine
>>>>> the remaining commitments sharing the same package.
>>>>>
>>>>> > E.g, you broadcast the package A+B+C+D+E where A,B,C,D are
>>>>> commitment transactions and E a shared CPFP. If a malicious A' transaction
>>>>> has a better feerate than A, the whole package acceptance will fail. Even
>>>>> if A' confirms in the following block,
>>>>> the propagation and confirmation of B+C+D have been delayed. This
>>>>> could carry on a loss of funds.
>>>>>
>>>>> Please note that A may replace A' even if A' has higher fees than A
>>>>> individually, because the proposed package RBF utilizes the fees and size
>>>>> of the entire package. This just requires E to pay enough fees, although
>>>>> this can be pretty high if there are also potential B' and C' competing
>>>>> commitment transactions that we don't know about.
>>>>>
>>>>>
>>>>> > IMHO, I'm leaning towards deploying during a first phase
>>>>> 1-parent/1-child. I think it's the most conservative step still improving
>>>>> second-layer safety.
>>>>>
>>>>> So far, my understanding is that multi-parent-1-child is desired for
>>>>> batched fee-bumping (
>>>>> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289)
>>>>> and I've also seen your response which I have less context on (
>>>>> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202).
>>>>> That being said, I am happy to create a new proposal for 1 parent + 1 child
>>>>> (which would be slightly simpler) and plan for moving to
>>>>> multi-parent-1-child later if that is preferred. I am very interested in
>>>>> hearing feedback on that approach.
>>>>>
>>>>>
>>>>> > If A+B is submitted to replace A', where A pays 0 sats, B pays 200
>>>>> sats and A' pays 100 sats. If we apply the individual RBF on A, A+B
>>>>> acceptance fails. For this reason I think the individual RBF should be
>>>>> bypassed and only the package RBF apply ?
>>>>>
>>>>> I think there is a misunderstanding here - let me describe what I'm
>>>>> proposing we'd do in this situation: we'll try individual submission for A,
>>>>> see that it fails due to "insufficient fees." Then, we'll try package
>>>>> validation for A+B and use package RBF. If A+B pays enough, it can still
>>>>> replace A'. If A fails for a bad signature, we won't look at B or A+B. Does
>>>>> this meet your expectations?
>>>>>
>>>>>
>>>>> > What problem are you trying to solve by the package feerate *after*
>>>>> dedup rule ?
>>>>> > My understanding is that an in-package transaction might be already
>>>>> in the mempool. Therefore, to compute a correct RBF penalty replacement,
>>>>> the vsize of this transaction could be discarded lowering the cost of
>>>>> package RBF.
>>>>>
>>>>> I'm proposing that, when a transaction has already been submitted to
>>>>> mempool, we would ignore both its fees and vsize when calculating package
>>>>> feerate. In example G2, we shouldn't count M1 fees after its submission to
>>>>> mempool, since M1's fees have already been used to pay for its individual
>>>>> bandwidth, and it shouldn't be used again to pay for P2 and P3's bandwidth.
>>>>> We also shouldn't count its vsize, since it has already been paid for.
>>>>>
>>>>>
>>>>> > I think this is a footgunish API, as if a package issuer send the
>>>>> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.
>>>>> Then try to broadcast the higher-feerate C'+D' package, it should be
>>>>> rejected. So it's breaking the naive broadcaster assumption that a
>>>>> higher-feerate/higher-fee package always replaces ?
>>>>>
>>>>> Note that, if C' conflicts with C, it also conflicts with D, since D
>>>>> is a descendant of C and would thus need to be evicted along with it.
>>>>> Implicitly, D' would not be in conflict with D.
>>>>> More generally, this example is surprising to me because I didn't
>>>>> think packages would be used to fee-bump replaceable transactions. Do we
>>>>> want the child to be able to replace mempool transactions as well? This can
>>>>> be implemented with a bit of additional logic.
>>>>>
>>>>> > I think this is unsafe for L2s if counterparties have malleability
>>>>> of the child transaction. They can block your package replacement by
>>>>> opting-out from RBF signaling. IIRC, LN's "anchor output" presents such an
>>>>> ability.
>>>>>
>>>>> I'm not sure what you mean? Let's say we have a package of parent A +
>>>>> child B, where A is supposed to replace a mempool transaction A'. Are you
>>>>> saying that counterparties are able to malleate the package child B, or a
>>>>> child of A'? If they can malleate a child of A', that shouldn't matter as
>>>>> long as A' is signaling replacement. This would be handled identically with
>>>>> full RBF and what Core currently implements.
>>>>>
>>>>> > I think this is an issue brought by the trimming during the dedup
>>>>> phase. If we preserve the package integrity, only re-using the tx-level
>>>>> checks results of already in-mempool transactions to gain in CPU time we
>>>>> won't have this issue. Package childs can add unconfirmed inputs as long as
>>>>> they're in-package, the bip125 rule2 is only evaluated against parents ?
>>>>>
>>>>> Sorry, I don't understand what you mean by "preserve the package
>>>>> integrity?" Could you elaborate?
>>>>>
>>>>> > Let's say you have in-mempool A, B where A pays 10 sat/vb for 100
>>>>> vbytes and B pays 10 sat/vb for 100 vbytes. You have the candidate
>>>>> replacement D spending both A and C where D pays 15sat/vb for 100 vbytes
>>>>> and C pays 1 sat/vb for 1000 vbytes.
>>>>>
>>>>> > Package A + B ancestor score is 10 sat/vb.
>>>>>
>>>>> > D has a higher feerate/absolute fee than B.
>>>>>
>>>>> > Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's
>>>>> 1000 sats + D's 1500 sats) / A's 100 vb + C's 1000 vb + D's 100 vb)
>>>>>
>>>>> I am in agreement with your calculations but unsure if we disagree on
>>>>> the expected outcome. Yes, B has an ancestor score of 10sat/vb and D has an
>>>>> ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than B's,
>>>>> it fails the proposed package RBF Rule #2, so this package would be
>>>>> rejected. Does this meet your expectations?
>>>>>
>>>>> Thank you for linking to projects that might be interested in package
>>>>> relay :)
>>>>>
>>>>> Thanks,
>>>>> Gloria
>>>>>
>>>>> On Mon, Sep 20, 2021 at 12:16 AM Antoine Riard <
>>>>> antoine.riard at gmail.com> wrote:
>>>>>
>>>>>> Hi Gloria,
>>>>>>
>>>>>> > A package may contain transactions that are already in the mempool.
>>>>>> We
>>>>>> > remove
>>>>>> > ("deduplicate") those transactions from the package for the
>>>>>> purposes of
>>>>>> > package
>>>>>> > mempool acceptance. If a package is empty after deduplication, we do
>>>>>> > nothing.
>>>>>>
>>>>>> IIUC, you have a package A+B+C submitted for acceptance and A is
>>>>>> already in your mempool. You trim out A from the package and then evaluate
>>>>>> B+C.
>>>>>>
>>>>>> I think this might be an issue if A is the higher-fee element of the
>>>>>> ABC package. B+C package fees might be under the mempool min fee and will
>>>>>> be rejected, potentially breaking the acceptance expectations of the
>>>>>> package issuer ?
>>>>>>
>>>>>> Further, I think the dedup should be done on wtxid, as you might have
>>>>>> multiple valid witnesses. Though with varying vsizes and as such offering
>>>>>> different feerates.
>>>>>>
>>>>>> E.g you're going to evaluate the package A+B and A' is already in
>>>>>> your mempool with a bigger valid witness. You trim A based on txid, then
>>>>>> you evaluate A'+B, which fails the fee checks. However, evaluating A+B
>>>>>> would have been a success.
>>>>>>
>>>>>> AFAICT, the dedup rationale would be to save on CPU time/IO disk, to
>>>>>> avoid repeated signatures verification and parent UTXOs fetches ? Can we
>>>>>> achieve the same goal by bypassing tx-level checks for already-in txn while
>>>>>> conserving the package integrity for package-level checks ?
>>>>>>
>>>>>> > Note that it's possible for the parents to be
>>>>>> > indirect
>>>>>> > descendants/ancestors of one another, or for parent and child to
>>>>>> share a
>>>>>> > parent,
>>>>>> > so we cannot make any other topology assumptions.
>>>>>>
>>>>>> I'm not clearly understanding the accepted topologies. By "parent and
>>>>>> child to share a parent", do you mean the set of transactions A, B, C,
>>>>>> where B is spending A and C is spending A and B would be correct ?
>>>>>>
>>>>>> If yes, is there a width-limit introduced or we fallback on
>>>>>> MAX_PACKAGE_COUNT=25 ?
>>>>>>
>>>>>> IIRC, one rationale to come with this topology limitation was to
>>>>>> lower the DoS risks when potentially deploying p2p packages.
>>>>>>
>>>>>> Considering the current Core's mempool acceptance rules, I think CPFP
>>>>>> batching is unsafe for LN time-sensitive closure. A malicious tx-relay
>>>>>> jamming successful on one channel commitment transaction would contamine
>>>>>> the remaining commitments sharing the same package.
>>>>>>
>>>>>> E.g, you broadcast the package A+B+C+D+E where A,B,C,D are commitment
>>>>>> transactions and E a shared CPFP. If a malicious A' transaction has a
>>>>>> better feerate than A, the whole package acceptance will fail. Even if A'
>>>>>> confirms in the following block,
>>>>>> the propagation and confirmation of B+C+D have been delayed. This
>>>>>> could carry on a loss of funds.
>>>>>>
>>>>>> That said, if you're broadcasting commitment transactions without
>>>>>> time-sensitive HTLC outputs, I think the batching is effectively a fee
>>>>>> saving as you don't have to duplicate the CPFP.
>>>>>>
>>>>>> IMHO, I'm leaning towards deploying during a first phase
>>>>>> 1-parent/1-child. I think it's the most conservative step still improving
>>>>>> second-layer safety.
>>>>>>
>>>>>> > *Rationale*:  It would be incorrect to use the fees of transactions
>>>>>> that are
>>>>>> > already in the mempool, as we do not want a transaction's fees to be
>>>>>> > double-counted for both its individual RBF and package RBF.
>>>>>>
>>>>>> I'm unsure about the logical order of the checks proposed.
>>>>>>
>>>>>> If A+B is submitted to replace A', where A pays 0 sats, B pays 200
>>>>>> sats and A' pays 100 sats. If we apply the individual RBF on A, A+B
>>>>>> acceptance fails. For this reason I think the individual RBF should be
>>>>>> bypassed and only the package RBF apply ?
>>>>>>
>>>>>> Note this situation is plausible, with current LN design, your
>>>>>> counterparty can have a commitment transaction with a better fee just by
>>>>>> selecting a higher `dust_limit_satoshis` than yours.
>>>>>>
>>>>>> > Examples F and G [14] show the same package, but P1 is submitted
>>>>>> > individually before
>>>>>> > the package in example G. In example F, we can see that the 300vB
>>>>>> package
>>>>>> > pays
>>>>>> > an additional 200sat in fees, which is not enough to pay for its own
>>>>>> > bandwidth
>>>>>> > (BIP125#4). In example G, we can see that P1 pays enough to replace
>>>>>> M1, but
>>>>>> > using P1's fees again during package submission would make it look
>>>>>> like a
>>>>>> > 300sat
>>>>>> > increase for a 200vB package. Even including its fees and size
>>>>>> would not be
>>>>>> > sufficient in this example, since the 300sat looks like enough for
>>>>>> the 300vB
>>>>>> > package. The calculcation after deduplication is 100sat increase
>>>>>> for a
>>>>>> > package
>>>>>> > of size 200vB, which correctly fails BIP125#4. Assume all
>>>>>> transactions have
>>>>>> > a
>>>>>> > size of 100vB.
>>>>>>
>>>>>> What problem are you trying to solve by the package feerate *after*
>>>>>> dedup rule ?
>>>>>>
>>>>>> My understanding is that an in-package transaction might be already
>>>>>> in the mempool. Therefore, to compute a correct RBF penalty replacement,
>>>>>> the vsize of this transaction could be discarded lowering the cost of
>>>>>> package RBF.
>>>>>>
>>>>>> If we keep a "safe" dedup mechanism (see my point above), I think
>>>>>> this discount is justified, as the validation cost of node operators is
>>>>>> paid for ?
>>>>>>
>>>>>> > The child cannot replace mempool transactions.
>>>>>>
>>>>>> Let's say you issue package A+B, then package C+B', where B' is a
>>>>>> child of both A and C. This rule fails the acceptance of C+B' ?
>>>>>>
>>>>>> I think this is a footgunish API, as if a package issuer send the
>>>>>> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.
>>>>>> Then try to broadcast the higher-feerate C'+D' package, it should be
>>>>>> rejected. So it's breaking the naive broadcaster assumption that a
>>>>>> higher-feerate/higher-fee package always replaces ? And it might be unsafe
>>>>>> in protocols where states are symmetric. E.g a malicious counterparty
>>>>>> broadcasts first S+A, then you honestly broadcast S+B, where B pays better
>>>>>> fees.
>>>>>>
>>>>>> > All mempool transactions to be replaced must signal replaceability.
>>>>>>
>>>>>> I think this is unsafe for L2s if counterparties have malleability of
>>>>>> the child transaction. They can block your package replacement by
>>>>>> opting-out from RBF signaling. IIRC, LN's "anchor output" presents such an
>>>>>> ability.
>>>>>>
>>>>>> I think it's better to either fix inherited signaling or move towards
>>>>>> full-rbf.
>>>>>>
>>>>>> > if a package parent has already been submitted, it would
>>>>>> > look
>>>>>> >like the child is spending a "new" unconfirmed input.
>>>>>>
>>>>>> I think this is an issue brought by the trimming during the dedup
>>>>>> phase. If we preserve the package integrity, only re-using the tx-level
>>>>>> checks results of already in-mempool transactions to gain in CPU time we
>>>>>> won't have this issue. Package childs can add unconfirmed inputs as long as
>>>>>> they're in-package, the bip125 rule2 is only evaluated against parents ?
>>>>>>
>>>>>> > However, we still achieve the same goal of requiring the
>>>>>> > replacement
>>>>>> > transactions to have a ancestor score at least as high as the
>>>>>> original
>>>>>> > ones.
>>>>>>
>>>>>> I'm not sure if this holds...
>>>>>>
>>>>>> Let's say you have in-mempool A, B where A pays 10 sat/vb for 100
>>>>>> vbytes and B pays 10 sat/vb for 100 vbytes. You have the candidate
>>>>>> replacement D spending both A and C where D pays 15sat/vb for 100 vbytes
>>>>>> and C pays 1 sat/vb for 1000 vbytes.
>>>>>>
>>>>>> Package A + B ancestor score is 10 sat/vb.
>>>>>>
>>>>>> D has a higher feerate/absolute fee than B.
>>>>>>
>>>>>> Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's
>>>>>> 1000 sats + D's 1500 sats) /
>>>>>> A's 100 vb + C's 1000 vb + D's 100 vb)
>>>>>>
>>>>>> Overall, this is a review through the lenses of LN requirements. I
>>>>>> think other L2 protocols/applications
>>>>>> could be candidates to using package accept/relay such as:
>>>>>> * https://github.com/lightninglabs/pool
>>>>>> * https://github.com/discreetlogcontracts/dlcspecs
>>>>>> * https://github.com/bitcoin-teleport/teleport-transactions/
>>>>>> * https://github.com/sapio-lang/sapio
>>>>>> *
>>>>>> https://github.com/commerceblock/mercury/blob/master/doc/statechains.md
>>>>>> * https://github.com/revault/practical-revault
>>>>>>
>>>>>> Thanks for rolling forward the ball on this subject.
>>>>>>
>>>>>> Antoine
>>>>>>
>>>>>> Le jeu. 16 sept. 2021 ? 03:55, Gloria Zhao via bitcoin-dev <
>>>>>> bitcoin-dev at lists.linuxfoundation.org> a ?crit :
>>>>>>
>>>>>>> Hi there,
>>>>>>>
>>>>>>> I'm writing to propose a set of mempool policy changes to enable
>>>>>>> package
>>>>>>> validation (in preparation for package relay) in Bitcoin Core. These
>>>>>>> would not
>>>>>>> be consensus or P2P protocol changes. However, since mempool policy
>>>>>>> significantly affects transaction propagation, I believe this is
>>>>>>> relevant for
>>>>>>> the mailing list.
>>>>>>>
>>>>>>> My proposal enables packages consisting of multiple parents and 1
>>>>>>> child. If you
>>>>>>> develop software that relies on specific transaction relay
>>>>>>> assumptions and/or
>>>>>>> are interested in using package relay in the future, I'm very
>>>>>>> interested to hear
>>>>>>> your feedback on the utility or restrictiveness of these package
>>>>>>> policies for
>>>>>>> your use cases.
>>>>>>>
>>>>>>> A draft implementation of this proposal can be found in [Bitcoin Core
>>>>>>> PR#22290][1].
>>>>>>>
>>>>>>> An illustrated version of this post can be found at
>>>>>>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.
>>>>>>> I have also linked the images below.
>>>>>>>
>>>>>>> ## Background
>>>>>>>
>>>>>>> Feel free to skip this section if you are already familiar with
>>>>>>> mempool policy
>>>>>>> and package relay terminology.
>>>>>>>
>>>>>>> ### Terminology Clarifications
>>>>>>>
>>>>>>> * Package = an ordered list of related transactions, representable
>>>>>>> by a Directed
>>>>>>>   Acyclic Graph.
>>>>>>> * Package Feerate = the total modified fees divided by the total
>>>>>>> virtual size of
>>>>>>>   all transactions in the package.
>>>>>>>     - Modified fees = a transaction's base fees + fee delta applied
>>>>>>> by the user
>>>>>>>       with `prioritisetransaction`. As such, we expect this to vary
>>>>>>> across
>>>>>>> mempools.
>>>>>>>     - Virtual Size = the maximum of virtual sizes calculated using
>>>>>>> [BIP141
>>>>>>>       virtual size][2] and sigop weight. [Implemented here in
>>>>>>> Bitcoin Core][3].
>>>>>>>     - Note that feerate is not necessarily based on the base fees
>>>>>>> and serialized
>>>>>>>       size.
>>>>>>>
>>>>>>> * Fee-Bumping = user/wallet actions that take advantage of miner
>>>>>>> incentives to
>>>>>>>   boost a transaction's candidacy for inclusion in a block,
>>>>>>> including Child Pays
>>>>>>> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our
>>>>>>> intention in
>>>>>>> mempool policy is to recognize when the new transaction is more
>>>>>>> economical to
>>>>>>> mine than the original one(s) but not open DoS vectors, so there are
>>>>>>> some
>>>>>>> limitations.
>>>>>>>
>>>>>>> ### Policy
>>>>>>>
>>>>>>> The purpose of the mempool is to store the best (to be most
>>>>>>> incentive-compatible
>>>>>>> with miners, highest feerate) candidates for inclusion in a block.
>>>>>>> Miners use
>>>>>>> the mempool to build block templates. The mempool is also useful as
>>>>>>> a cache for
>>>>>>> boosting block relay and validation performance, aiding transaction
>>>>>>> relay, and
>>>>>>> generating feerate estimations.
>>>>>>>
>>>>>>> Ideally, all consensus-valid transactions paying reasonable fees
>>>>>>> should make it
>>>>>>> to miners through normal transaction relay, without any special
>>>>>>> connectivity or
>>>>>>> relationships with miners. On the other hand, nodes do not have
>>>>>>> unlimited
>>>>>>> resources, and a P2P network designed to let any honest node
>>>>>>> broadcast their
>>>>>>> transactions also exposes the transaction validation engine to DoS
>>>>>>> attacks from
>>>>>>> malicious peers.
>>>>>>>
>>>>>>> As such, for unconfirmed transactions we are considering for our
>>>>>>> mempool, we
>>>>>>> apply a set of validation rules in addition to consensus, primarily
>>>>>>> to protect
>>>>>>> us from resource exhaustion and aid our efforts to keep the highest
>>>>>>> fee
>>>>>>> transactions. We call this mempool _policy_: a set of (configurable,
>>>>>>> node-specific) rules that transactions must abide by in order to be
>>>>>>> accepted
>>>>>>> into our mempool. Transaction "Standardness" rules and mempool
>>>>>>> restrictions such
>>>>>>> as "too-long-mempool-chain" are both examples of policy.
>>>>>>>
>>>>>>> ### Package Relay and Package Mempool Accept
>>>>>>>
>>>>>>> In transaction relay, we currently consider transactions one at a
>>>>>>> time for
>>>>>>> submission to the mempool. This creates a limitation in the node's
>>>>>>> ability to
>>>>>>> determine which transactions have the highest feerates, since we
>>>>>>> cannot take
>>>>>>> into account descendants (i.e. cannot use CPFP) until all the
>>>>>>> transactions are
>>>>>>> in the mempool. Similarly, we cannot use a transaction's descendants
>>>>>>> when
>>>>>>> considering it for RBF. When an individual transaction does not meet
>>>>>>> the mempool
>>>>>>> minimum feerate and the user isn't able to create a replacement
>>>>>>> transaction
>>>>>>> directly, it will not be accepted by mempools.
>>>>>>>
>>>>>>> This limitation presents a security issue for applications and users
>>>>>>> relying on
>>>>>>> time-sensitive transactions. For example, Lightning and other
>>>>>>> protocols create
>>>>>>> UTXOs with multiple spending paths, where one counterparty's
>>>>>>> spending path opens
>>>>>>> up after a timelock, and users are protected from cheating scenarios
>>>>>>> as long as
>>>>>>> they redeem on-chain in time. A key security assumption is that all
>>>>>>> parties'
>>>>>>> transactions will propagate and confirm in a timely manner. This
>>>>>>> assumption can
>>>>>>> be broken if fee-bumping does not work as intended.
>>>>>>>
>>>>>>> The end goal for Package Relay is to consider multiple transactions
>>>>>>> at the same
>>>>>>> time, e.g. a transaction with its high-fee child. This may help us
>>>>>>> better
>>>>>>> determine whether transactions should be accepted to our mempool,
>>>>>>> especially if
>>>>>>> they don't meet fee requirements individually or are better RBF
>>>>>>> candidates as a
>>>>>>> package. A combination of changes to mempool validation logic,
>>>>>>> policy, and
>>>>>>> transaction relay allows us to better propagate the transactions
>>>>>>> with the
>>>>>>> highest package feerates to miners, and makes fee-bumping tools more
>>>>>>> powerful
>>>>>>> for users.
>>>>>>>
>>>>>>> The "relay" part of Package Relay suggests P2P messaging changes,
>>>>>>> but a large
>>>>>>> part of the changes are in the mempool's package validation logic.
>>>>>>> We call this
>>>>>>> *Package Mempool Accept*.
>>>>>>>
>>>>>>> ### Previous Work
>>>>>>>
>>>>>>> * Given that mempool validation is DoS-sensitive and complex, it
>>>>>>> would be
>>>>>>>   dangerous to haphazardly tack on package validation logic. Many
>>>>>>> efforts have
>>>>>>> been made to make mempool validation less opaque (see [#16400][4],
>>>>>>> [#21062][5],
>>>>>>> [#22675][6], [#22796][7]).
>>>>>>> * [#20833][8] Added basic capabilities for package validation, test
>>>>>>> accepts only
>>>>>>>   (no submission to mempool).
>>>>>>> * [#21800][9] Implemented package ancestor/descendant limit checks
>>>>>>> for arbitrary
>>>>>>>   packages. Still test accepts only.
>>>>>>> * Previous package relay proposals (see [#16401][10], [#19621][11]).
>>>>>>>
>>>>>>> ### Existing Package Rules
>>>>>>>
>>>>>>> These are in master as introduced in [#20833][8] and [#21800][9].
>>>>>>> I'll consider
>>>>>>> them as "given" in the rest of this document, though they can be
>>>>>>> changed, since
>>>>>>> package validation is test-accept only right now.
>>>>>>>
>>>>>>> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and
>>>>>>> `MAX_PACKAGE_SIZE=101KvB` total size [8]
>>>>>>>
>>>>>>>    *Rationale*: This is already enforced as mempool
>>>>>>> ancestor/descendant limits.
>>>>>>> Presumably, transactions in a package are all related, so exceeding
>>>>>>> this limit
>>>>>>> would mean that the package can either be split up or it wouldn't
>>>>>>> pass this
>>>>>>> mempool policy.
>>>>>>>
>>>>>>> 2. Packages must be topologically sorted: if any dependencies exist
>>>>>>> between
>>>>>>> transactions, parents must appear somewhere before children. [8]
>>>>>>>
>>>>>>> 3. A package cannot have conflicting transactions, i.e. none of them
>>>>>>> can spend
>>>>>>> the same inputs. This also means there cannot be duplicate
>>>>>>> transactions. [8]
>>>>>>>
>>>>>>> 4. When packages are evaluated against ancestor/descendant limits in
>>>>>>> a test
>>>>>>> accept, the union of all of their descendants and ancestors is
>>>>>>> considered. This
>>>>>>> is essentially a "worst case" heuristic where every transaction in
>>>>>>> the package
>>>>>>> is treated as each other's ancestor and descendant. [8]
>>>>>>> Packages for which ancestor/descendant limits are accurately
>>>>>>> captured by this
>>>>>>> heuristic: [19]
>>>>>>>
>>>>>>> There are also limitations such as the fact that CPFP carve out is
>>>>>>> not applied
>>>>>>> to package transactions. #20833 also disables RBF in package
>>>>>>> validation; this
>>>>>>> proposal overrides that to allow packages to use RBF.
>>>>>>>
>>>>>>> ## Proposed Changes
>>>>>>>
>>>>>>> The next step in the Package Mempool Accept project is to implement
>>>>>>> submission
>>>>>>> to mempool, initially through RPC only. This allows us to test the
>>>>>>> submission
>>>>>>> logic before exposing it on P2P.
>>>>>>>
>>>>>>> ### Summary
>>>>>>>
>>>>>>> - Packages may contain already-in-mempool transactions.
>>>>>>> - Packages are 2 generations, Multi-Parent-1-Child.
>>>>>>> - Fee-related checks use the package feerate. This means that
>>>>>>> wallets can
>>>>>>> create a package that utilizes CPFP.
>>>>>>> - Parents are allowed to RBF mempool transactions with a set of
>>>>>>> rules similar
>>>>>>>   to BIP125. This enables a combination of CPFP and RBF, where a
>>>>>>> transaction's descendant fees pay for replacing mempool conflicts.
>>>>>>>
>>>>>>> There is a draft implementation in [#22290][1]. It is WIP, but
>>>>>>> feedback is
>>>>>>> always welcome.
>>>>>>>
>>>>>>> ### Details
>>>>>>>
>>>>>>> #### Packages May Contain Already-in-Mempool Transactions
>>>>>>>
>>>>>>> A package may contain transactions that are already in the mempool.
>>>>>>> We remove
>>>>>>> ("deduplicate") those transactions from the package for the purposes
>>>>>>> of package
>>>>>>> mempool acceptance. If a package is empty after deduplication, we do
>>>>>>> nothing.
>>>>>>>
>>>>>>> *Rationale*: Mempools vary across the network. It's possible for a
>>>>>>> parent to be
>>>>>>> accepted to the mempool of a peer on its own due to differences in
>>>>>>> policy and
>>>>>>> fee market fluctuations. We should not reject or penalize the entire
>>>>>>> package for
>>>>>>> an individual transaction as that could be a censorship vector.
>>>>>>>
>>>>>>> #### Packages Are Multi-Parent-1-Child
>>>>>>>
>>>>>>> Only packages of a specific topology are permitted. Namely, a
>>>>>>> package is exactly
>>>>>>> 1 child with all of its unconfirmed parents. After deduplication,
>>>>>>> the package
>>>>>>> may be exactly the same, empty, 1 child, 1 child with just some of
>>>>>>> its
>>>>>>> unconfirmed parents, etc. Note that it's possible for the parents to
>>>>>>> be indirect
>>>>>>> descendants/ancestors of one another, or for parent and child to
>>>>>>> share a parent,
>>>>>>> so we cannot make any other topology assumptions.
>>>>>>>
>>>>>>> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple
>>>>>>> parents
>>>>>>> makes it possible to fee-bump a batch of transactions. Restricting
>>>>>>> packages to a
>>>>>>> defined topology is also easier to reason about and simplifies the
>>>>>>> validation
>>>>>>> logic greatly. Multi-parent-1-child allows us to think of the
>>>>>>> package as one big
>>>>>>> transaction, where:
>>>>>>>
>>>>>>> - Inputs = all the inputs of parents + inputs of the child that come
>>>>>>> from
>>>>>>>   confirmed UTXOs
>>>>>>> - Outputs = all the outputs of the child + all outputs of the
>>>>>>> parents that
>>>>>>>   aren't spent by other transactions in the package
>>>>>>>
>>>>>>> Examples of packages that follow this rule (variations of example A
>>>>>>> show some
>>>>>>> possibilities after deduplication): ![image][15]
>>>>>>>
>>>>>>> #### Fee-Related Checks Use Package Feerate
>>>>>>>
>>>>>>> Package Feerate = the total modified fees divided by the total
>>>>>>> virtual size of
>>>>>>> all transactions in the package.
>>>>>>>
>>>>>>> To meet the two feerate requirements of a mempool, i.e., the
>>>>>>> pre-configured
>>>>>>> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum
>>>>>>> feerate, the
>>>>>>> total package feerate is used instead of the individual feerate. The
>>>>>>> individual
>>>>>>> transactions are allowed to be below feerate requirements if the
>>>>>>> package meets
>>>>>>> the feerate requirements. For example, the parent(s) in the package
>>>>>>> can have 0
>>>>>>> fees but be paid for by the child.
>>>>>>>
>>>>>>> *Rationale*: This can be thought of as "CPFP within a package,"
>>>>>>> solving the
>>>>>>> issue of a parent not meeting minimum fees on its own. This allows L2
>>>>>>> applications to adjust their fees at broadcast time instead of
>>>>>>> overshooting or
>>>>>>> risking getting stuck/pinned.
>>>>>>>
>>>>>>> We use the package feerate of the package *after deduplication*.
>>>>>>>
>>>>>>> *Rationale*:  It would be incorrect to use the fees of transactions
>>>>>>> that are
>>>>>>> already in the mempool, as we do not want a transaction's fees to be
>>>>>>> double-counted for both its individual RBF and package RBF.
>>>>>>>
>>>>>>> Examples F and G [14] show the same package, but P1 is submitted
>>>>>>> individually before
>>>>>>> the package in example G. In example F, we can see that the 300vB
>>>>>>> package pays
>>>>>>> an additional 200sat in fees, which is not enough to pay for its own
>>>>>>> bandwidth
>>>>>>> (BIP125#4). In example G, we can see that P1 pays enough to replace
>>>>>>> M1, but
>>>>>>> using P1's fees again during package submission would make it look
>>>>>>> like a 300sat
>>>>>>> increase for a 200vB package. Even including its fees and size would
>>>>>>> not be
>>>>>>> sufficient in this example, since the 300sat looks like enough for
>>>>>>> the 300vB
>>>>>>> package. The calculcation after deduplication is 100sat increase for
>>>>>>> a package
>>>>>>> of size 200vB, which correctly fails BIP125#4. Assume all
>>>>>>> transactions have a
>>>>>>> size of 100vB.
>>>>>>>
>>>>>>> #### Package RBF
>>>>>>>
>>>>>>> If a package meets feerate requirements as a package, the parents in
>>>>>>> the
>>>>>>> transaction are allowed to replace-by-fee mempool transactions. The
>>>>>>> child cannot
>>>>>>> replace mempool transactions. Multiple transactions can replace the
>>>>>>> same
>>>>>>> transaction, but in order to be valid, none of the transactions can
>>>>>>> try to
>>>>>>> replace an ancestor of another transaction in the same package
>>>>>>> (which would thus
>>>>>>> make its inputs unavailable).
>>>>>>>
>>>>>>> *Rationale*: Even if we are using package feerate, a package will
>>>>>>> not propagate
>>>>>>> as intended if RBF still requires each individual transaction to
>>>>>>> meet the
>>>>>>> feerate requirements.
>>>>>>>
>>>>>>> We use a set of rules slightly modified from BIP125 as follows:
>>>>>>>
>>>>>>> ##### Signaling (Rule #1)
>>>>>>>
>>>>>>> All mempool transactions to be replaced must signal replaceability.
>>>>>>>
>>>>>>> *Rationale*: Package RBF signaling logic should be the same for
>>>>>>> package RBF and
>>>>>>> single transaction acceptance. This would be updated if single
>>>>>>> transaction
>>>>>>> validation moves to full RBF.
>>>>>>>
>>>>>>> ##### New Unconfirmed Inputs (Rule #2)
>>>>>>>
>>>>>>> A package may include new unconfirmed inputs, but the ancestor
>>>>>>> feerate of the
>>>>>>> child must be at least as high as the ancestor feerates of every
>>>>>>> transaction
>>>>>>> being replaced. This is contrary to BIP125#2, which states "The
>>>>>>> replacement
>>>>>>> transaction may only include an unconfirmed input if that input was
>>>>>>> included in
>>>>>>> one of the original transactions. (An unconfirmed input spends an
>>>>>>> output from a
>>>>>>> currently-unconfirmed transaction.)"
>>>>>>>
>>>>>>> *Rationale*: The purpose of BIP125#2 is to ensure that the
>>>>>>> replacement
>>>>>>> transaction has a higher ancestor score than the original
>>>>>>> transaction(s) (see
>>>>>>> [comment][13]). Example H [16] shows how adding a new unconfirmed
>>>>>>> input can lower the
>>>>>>> ancestor score of the replacement transaction. P1 is trying to
>>>>>>> replace M1, and
>>>>>>> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat,
>>>>>>> and M2 pays
>>>>>>> 100sat. Assume all transactions have a size of 100vB. While, in
>>>>>>> isolation, P1
>>>>>>> looks like a better mining candidate than M1, it must be mined with
>>>>>>> M2, so its
>>>>>>> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's
>>>>>>> ancestor
>>>>>>> feerate, which is 6sat/vB.
>>>>>>>
>>>>>>> In package RBF, the rule analogous to BIP125#2 would be "none of the
>>>>>>> transactions in the package can spend new unconfirmed inputs."
>>>>>>> Example J [17] shows
>>>>>>> why, if any of the package transactions have ancestors, package
>>>>>>> feerate is no
>>>>>>> longer accurate. Even though M2 and M3 are not ancestors of P1
>>>>>>> (which is the
>>>>>>> replacement transaction in an RBF), we're actually interested in the
>>>>>>> entire
>>>>>>> package. A miner should mine M1 which is 5sat/vB instead of M2, M3,
>>>>>>> P1, P2, and
>>>>>>> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened
>>>>>>> to only allow
>>>>>>> the child to have new unconfirmed inputs, either, because it can
>>>>>>> still cause us
>>>>>>> to overestimate the package's ancestor score.
>>>>>>>
>>>>>>> However, enforcing a rule analogous to BIP125#2 would not only make
>>>>>>> Package RBF
>>>>>>> less useful, but would also break Package RBF for packages with
>>>>>>> parents already
>>>>>>> in the mempool: if a package parent has already been submitted, it
>>>>>>> would look
>>>>>>> like the child is spending a "new" unconfirmed input. In example K
>>>>>>> [18], we're
>>>>>>> looking to replace M1 with the entire package including P1, P2, and
>>>>>>> P3. We must
>>>>>>> consider the case where one of the parents is already in the mempool
>>>>>>> (in this
>>>>>>> case, P2), which means we must allow P3 to have new unconfirmed
>>>>>>> inputs. However,
>>>>>>> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not
>>>>>>> replace M1
>>>>>>> with this package.
>>>>>>>
>>>>>>> Thus, the package RBF rule regarding new unconfirmed inputs is less
>>>>>>> strict than
>>>>>>> BIP125#2. However, we still achieve the same goal of requiring the
>>>>>>> replacement
>>>>>>> transactions to have a ancestor score at least as high as the
>>>>>>> original ones. As
>>>>>>> a result, the entire package is required to be a higher feerate
>>>>>>> mining candidate
>>>>>>> than each of the replaced transactions.
>>>>>>>
>>>>>>> Another note: the [comment][13] above the BIP125#2 code in the
>>>>>>> original RBF
>>>>>>> implementation suggests that the rule was intended to be temporary.
>>>>>>>
>>>>>>> ##### Absolute Fee (Rule #3)
>>>>>>>
>>>>>>> The package must increase the absolute fee of the mempool, i.e. the
>>>>>>> total fees
>>>>>>> of the package must be higher than the absolute fees of the mempool
>>>>>>> transactions
>>>>>>> it replaces. Combined with the CPFP rule above, this differs from
>>>>>>> BIP125 Rule #3
>>>>>>> - an individual transaction in the package may have lower fees than
>>>>>>> the
>>>>>>>   transaction(s) it is replacing. In fact, it may have 0 fees, and
>>>>>>> the child
>>>>>>> pays for RBF.
>>>>>>>
>>>>>>> ##### Feerate (Rule #4)
>>>>>>>
>>>>>>> The package must pay for its own bandwidth; the package feerate must
>>>>>>> be higher
>>>>>>> than the replaced transactions by at least minimum relay feerate
>>>>>>> (`incrementalRelayFee`). Combined with the CPFP rule above, this
>>>>>>> differs from
>>>>>>> BIP125 Rule #4 - an individual transaction in the package can have a
>>>>>>> lower
>>>>>>> feerate than the transaction(s) it is replacing. In fact, it may
>>>>>>> have 0 fees,
>>>>>>> and the child pays for RBF.
>>>>>>>
>>>>>>> ##### Total Number of Replaced Transactions (Rule #5)
>>>>>>>
>>>>>>> The package cannot replace more than 100 mempool transactions. This
>>>>>>> is identical
>>>>>>> to BIP125 Rule #5.
>>>>>>>
>>>>>>> ### Expected FAQs
>>>>>>>
>>>>>>> 1. Is it possible for only some of the package to make it into the
>>>>>>> mempool?
>>>>>>>
>>>>>>>    Yes, it is. However, since we evict transactions from the mempool
>>>>>>> by
>>>>>>> descendant score and the package child is supposed to be sponsoring
>>>>>>> the fees of
>>>>>>> its parents, the most common scenario would be all-or-nothing. This
>>>>>>> is
>>>>>>> incentive-compatible. In fact, to be conservative, package
>>>>>>> validation should
>>>>>>> begin by trying to submit all of the transactions individually, and
>>>>>>> only use the
>>>>>>> package mempool acceptance logic if the parents fail due to low
>>>>>>> feerate.
>>>>>>>
>>>>>>> 2. Should we allow packages to contain already-confirmed
>>>>>>> transactions?
>>>>>>>
>>>>>>>     No, for practical reasons. In mempool validation, we actually
>>>>>>> aren't able to
>>>>>>> tell with 100% confidence if we are looking at a transaction that
>>>>>>> has already
>>>>>>> confirmed, because we look up inputs using a UTXO set. If we have
>>>>>>> historical
>>>>>>> block data, it's possible to look for it, but this is inefficient,
>>>>>>> not always
>>>>>>> possible for pruning nodes, and unnecessary because we're not going
>>>>>>> to do
>>>>>>> anything with the transaction anyway. As such, we already have the
>>>>>>> expectation
>>>>>>> that transaction relay is somewhat "stateful" i.e. nobody should be
>>>>>>> relaying
>>>>>>> transactions that have already been confirmed. Similarly, we
>>>>>>> shouldn't be
>>>>>>> relaying packages that contain already-confirmed transactions.
>>>>>>>
>>>>>>> [1]: https://github.com/bitcoin/bitcoin/pull/22290
>>>>>>> [2]:
>>>>>>> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations
>>>>>>> [3]:
>>>>>>> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282
>>>>>>> [4]: https://github.com/bitcoin/bitcoin/pull/16400
>>>>>>> [5]: https://github.com/bitcoin/bitcoin/pull/21062
>>>>>>> [6]: https://github.com/bitcoin/bitcoin/pull/22675
>>>>>>> [7]: https://github.com/bitcoin/bitcoin/pull/22796
>>>>>>> [8]: https://github.com/bitcoin/bitcoin/pull/20833
>>>>>>> [9]: https://github.com/bitcoin/bitcoin/pull/21800
>>>>>>> [10]: https://github.com/bitcoin/bitcoin/pull/16401
>>>>>>> [11]: https://github.com/bitcoin/bitcoin/pull/19621
>>>>>>> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki
>>>>>>> [13]:
>>>>>>> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104
>>>>>>> [14]:
>>>>>>> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png
>>>>>>> [15]:
>>>>>>> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png
>>>>>>> [16]:
>>>>>>> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png
>>>>>>> [17]:
>>>>>>> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png
>>>>>>> [18]:
>>>>>>> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png
>>>>>>> [19]:
>>>>>>> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png
>>>>>>> [20]:
>>>>>>> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png
>>>>>>> _______________________________________________
>>>>>>> bitcoin-dev mailing list
>>>>>>> bitcoin-dev at lists.linuxfoundation.org
>>>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>>>>>
>>>>>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210928/ef67cddf/attachment-0001.html>

From gloriajzhao at gmail.com  Wed Sep 29 11:56:24 2021
From: gloriajzhao at gmail.com (Gloria Zhao)
Date: Wed, 29 Sep 2021 12:56:24 +0100
Subject: [bitcoin-dev] Proposal: Package Mempool Accept and Package RBF
In-Reply-To: <CALZpt+G4Z3PaWDxAAcTi9FUC+CBF-_42iU-aBuarPvPh8Vqmfw@mail.gmail.com>
References: <CAFXO6=+cHyQKM2n9yn4EhwLZO+AUB0ZD81qWPxmpN27rjUoU3w@mail.gmail.com>
 <CALZpt+HpvmEHUEOgye34T6pVQ+wnKKn-_8cTJTQXYQb9t1jOTA@mail.gmail.com>
 <CAFXO6=JzsYgiXJE2geSKMpfgPo+GGNX_+Pw0JQx1QQxAfhCdBQ@mail.gmail.com>
 <CALZpt+HQpdrebhWGXv_yLqiSCB5Ur71L1K13bd7w5TZb9DwJEQ@mail.gmail.com>
 <CAFXO6=Lvcr7Pwn_ZD1CZohYUFKY-cC5sGRxdTOiP2MgnTvFnYA@mail.gmail.com>
 <CALZpt+FFSk-+BBxu7SSdjw580UCFfkdo1DTa1Yj9K81M4E1vPg@mail.gmail.com>
 <CACdvm3Pgia5mn60HmZmi4t0U5coc13WBuDkh+QgQBAHZz+Ufow@mail.gmail.com>
 <CALZpt+G4Z3PaWDxAAcTi9FUC+CBF-_42iU-aBuarPvPh8Vqmfw@mail.gmail.com>
Message-ID: <CAFXO6=Kn2GttDWQO2n76O-RGEUYP_7a556t58R3TXoSN859Y-w@mail.gmail.com>

Hi Antoine and Bastien,

> Yes 1) it would be good to have inputs of more potential users of package
acceptance . And 2) I think it's more a matter of clearer wording of the
proposal.

(1) I'm leaning towards multi-parent-1-child and offering [#22674][0] up
for review. If somebody feels very strongly about 1-parent-1-child, please
let me know.

(2) I'm glad this turned out to just be a wording problem. I've updated the
proposal to [say][1] "If it meets feerate requirements, the package can
replace mempool transactions if any of the parents conflict with mempool
transactions. The child cannot conflict with any mempool transactions."
Hopefully that is more *univoque*.

Side note: I've also updated the proposal to contain a [section][2] on why
submitting transactions individually before package validation is
incentive-compatible. I think it's relevant to our conversation, but for
those who just want to _use_ packages, it's just an implementation detail.

On restricting packages to confirmed inputs only:

> I think we could restrain package acceptance to only confirmed inputs for
now and revisit later this point ? For LN-anchor, you can assume that the
fee-bumping UTXO feeding the CPFP is already
confirmed. Or are there currently-deployed use-cases which would benefit
from your proposed Rule #2 ?

I thought about this a lot this week, and wrote up a summary of why I don't
think BIP125#2 helps us at all [here][3] on #23121. I see that you've
already come across it :)

> IIRC, the carve-out tolerance is only 2txn/10_000 vb. If one of your
counterparties attach a junk branch on her own anchor output, are you
allowed to chain your self-owned unconfirmed CPFP ?

Yes, if your counterparty attaches a bunch of descendants to their anchor
output to dominate the descendant limit of your shared commitment
transaction, CPFP carve out allows you to add 1 extra transaction under
10KvB to your own anchor output. It's fine if it spends an unconfirmed
input, as long as you aren't exceeding the descendant limits of that
transaction. This shouldn't be the case; I think something is seriously
wrong if all of your UTXOs are tied up in mempool transactions with big
ancestor/descendant trees.

I don't know much about L2 development so I'm just going to quote this:

> I think constraining package acceptance to only confirmed inputs is very
limiting and quite dangerous for L2 protocols.

Since the restriction isn't helpful in simplifying the mempool code, makes
things more complicated for application developers, and can be dangerous
for L2, I'd prefer not to add this restriction for packages.

On Antoine's question about our miner model:

> Can you describe what miner model we are using ? Like the block
construction strategy implemented by `addPackagesTxs` or also encompassing
our current mempool acceptance policy, which I think rely on absolute fee
over ancestor score in case of replacement ?

Our current model for block construction is this: we sort our mempool by
package ancestor score (total modified fees of a tx and its unconfirmed
ancestors / total vsize as seen by our mempool) and add packages to a block
until it's full. That's not to say this is the perfect miner policy, but
mempool acceptance logic follows this model as closely as possible because
it is, fundamentally, a cache that aids in block assembly performance. As
another way of looking at this, imagine if our mempool was so small it
could only store ~1 block's worth of transactions. It should always try to
keep the highest-fees-within-1-block transactions, and obviously wouldn't
evict small-but-valuable transations in favor or giant ones paying mediocre
feerates. All fee-related mempool policies, including RBF, consider
feerate. BIP125#3 is a rule on absolute fees, but it is always combined
with BIP125#4, a rule on feerates. AFAIK, the reason it doesn't use
ancestor score is that information wasn't cached in mempool entries at the
time, and thus not readily available to use in mempool validation.

That's why I don't think this is relevant to package validation. Commenting
on the model itself:

> Is this compatible with a model where a miner prioritizes absolute fees
over ancestor score, in the case that mempools aren't full-enough to
fulfill a block ?
>> Yes, A+C+D pays 2500sat more in fees, but it is also 1000vB larger. A
miner should prefer to utilize their block space more effectively.
> If your mempool is empty and only composed of A+C+D or A+B, I think
taking A+C+D is the most efficient block construction you can come up with
as a miner ?
> I think this point is worthy to discuss as otherwise we might downgrade
the efficiency of our current block construction strategy in periods of
near-empty mempools. A knowledge which could be discreetly leveraged by a
miner to gain an advantage on the rest of the mining ecosystem.

I believe this is suggesting "if our mempool has so few transactions that
it wouldn't reach block capacity, prioritize any increase in absolute fees,
even if the feerate is lower." I can see how this may result in a
higher-fee block in a specific scenario such as the one highlighted above,
but I don't think it is a sound model in general. It would be impossible to
tell when we should use this model: we could simply be in IBD, restarted a
node with an old/empty mempool.dat, and even if it's a
low-transaction-volume time, we never know what transactions will trickle
in between now and the next block. Going back to the tiny 1-block mempool
scenario, i.e., if you _never_ wanted to keep transactions that you
wouldn't put in the next block, would you ever switch strategies?

Thanks again to everyone who's given their attention to the package mempool
accept proposal.

Best,
Gloria

[0]: https://github.com/bitcoin/bitcoin/pull/22674
[1]:
https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a#package-rbf
[2]:
https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a#always-try-individual-submission-first
[3]: https://github.com/bitcoin/bitcoin/pull/23121#issuecomment-929475999

On Tue, Sep 28, 2021 at 11:59 PM Antoine Riard <antoine.riard at gmail.com>
wrote:

> Hi Bastien
>
> > In the case of LN, an attacker can game this and heavily restrict
> your RBF attempts if you're only allowed to use confirmed inputs
> and have many channels (and a limited number of confirmed inputs).
> Otherwise you'll need node operators to pre-emptively split their
> utxos into many small utxos just for fee bumping, which is inefficient...
>
> I share the concern about splitting utxos into smaller ones.
> IIRC, the carve-out tolerance is only 2txn/10_000 vb. If one of your
> counterparties attach a junk branch on her own anchor output, are you
> allowed to chain your self-owned unconfirmed CPFP ?
> I'm thinking about the topology "Chained CPFPs" exposed here :
> https://github.com/rust-bitcoin/rust-lightning/issues/989.
> Or if you have another L2 broadcast topology which could be safe w.r.t our
> current mempool logic :) ?
>
>
> Le lun. 27 sept. 2021 ? 03:15, Bastien TEINTURIER <bastien at acinq.fr> a
> ?crit :
>
>> I think we could restrain package acceptance to only confirmed inputs for
>>> now and revisit later this point ? For LN-anchor, you can assume that the
>>> fee-bumping UTXO feeding the CPFP is already
>>> confirmed. Or are there currently-deployed use-cases which would benefit
>>> from your proposed Rule #2 ?
>>>
>>
>> I think constraining package acceptance to only confirmed inputs
>> is very limiting and quite dangerous for L2 protocols.
>>
>> In the case of LN, an attacker can game this and heavily restrict
>> your RBF attempts if you're only allowed to use confirmed inputs
>> and have many channels (and a limited number of confirmed inputs).
>> Otherwise you'll need node operators to pre-emptively split their
>> utxos into many small utxos just for fee bumping, which is inefficient...
>>
>> Bastien
>>
>> Le lun. 27 sept. 2021 ? 00:27, Antoine Riard via bitcoin-dev <
>> bitcoin-dev at lists.linuxfoundation.org> a ?crit :
>>
>>> Hi Gloria,
>>>
>>> Thanks for your answers,
>>>
>>> > In summary, it seems that the decisions that might still need
>>> > attention/input from devs on this mailing list are:
>>> > 1. Whether we should start with multiple-parent-1-child or
>>> 1-parent-1-child.
>>> > 2. Whether it's ok to require that the child not have conflicts with
>>> > mempool transactions.
>>>
>>> Yes 1) it would be good to have inputs of more potential users of
>>> package acceptance . And 2) I think it's more a matter of clearer wording
>>> of the proposal.
>>>
>>> However, see my final point on the relaxation around "unconfirmed
>>> inputs" which might in fact alter our current block construction strategy.
>>>
>>> > Right, the fact that we essentially always choose the first-seen
>>> witness is
>>> > an unfortunate limitation that exists already. Adding package mempool
>>> > accept doesn't worsen this, but the procedure in the future is to
>>> replace
>>> > the witness when it makes sense economically. We can also add logic to
>>> > allow package feerate to pay for witness replacements as well. This is
>>> > pretty far into the future, though.
>>>
>>> Yes I agree package mempool doesn't worsen this. And it's not an issue
>>> for current LN as you can't significantly inflate a spending witness for
>>> the 2-of-2 funding output.
>>> However, it might be an issue for multi-party protocol where the
>>> spending script has alternative branches with asymmetric valid witness
>>> weights. Taproot should ease that kind of script so hopefully we would
>>> deploy wtxid-replacement not too far in the future.
>>>
>>> > I could be misunderstanding, but an attacker wouldn't be able to
>>> > batch-attack like this. Alice's package only conflicts with A' + D',
>>> not A'
>>> > + B' + C' + D'. She only needs to pay for evicting 2 transactions.
>>>
>>> Yeah I can be clearer, I think you have 2 pinning attacks scenarios to
>>> consider.
>>>
>>> In LN, if you're trying to confirm a commitment transaction to time-out
>>> or claim on-chain a HTLC and the timelock is near-expiration, you should be
>>> ready to pay in commitment+2nd-stage HTLC transaction fees as much as the
>>> value offered by the HTLC.
>>>
>>> Following this security assumption, an attacker can exploit it by
>>> targeting together commitment transactions from different channels by
>>> blocking them under a high-fee child, of which the fee value
>>> is equal to the top-value HTLC + 1. Victims's fee-bumping logics won't
>>> overbid as it's not worthy to offer fees beyond their competed HTLCs. Apart
>>> from observing mempools state, victims can't learn they're targeted by the
>>> same attacker.
>>>
>>> To draw from the aforementioned topology, Mallory broadcasts A' + B' +
>>> C' + D', where A' conflicts with Alice's P1, B' conflicts with Bob's P2, C'
>>> conflicts with Caroll's P3. Let's assume P1 is confirming the top-value
>>> HTLC of the set. If D' fees is higher than P1 + 1, it won't be rational for
>>> Alice or Bob or Caroll to keep offering competing feerates. Mallory will be
>>> at loss on stealing P1, as she has paid more in fees but will realize a
>>> gain on P2+P3.
>>>
>>> In this model, Alice is allowed to evict those 2 transactions (A' + D')
>>> but as she is economically-bounded she won't succeed.
>>>
>>> Mallory is maliciously exploiting RBF rule 3 on absolute fee. I think
>>> this 1st pinning scenario is correct and "lucractive" when you sum the
>>> global gain/loss.
>>>
>>> There is a 2nd attack scenario where A + B + C + D, where D is the child
>>> of A,B,C. All those transactions are honestly issued by Alice. Once A + B +
>>> C + D are propagated in network mempools, Mallory is able to replace A + D
>>> with  A' + D' where D' is paying a higher fee. This package A' + D' will
>>> confirm soon if D feerate was compelling but Mallory succeeds in delaying
>>> the confirmation
>>> of B + C for one or more blocks. As B + C are pre-signed commitments
>>> with a low-fee rate they won't confirm without Alice issuing a new child E.
>>> Mallory can repeat the same trick by broadcasting
>>> B' + E' and delay again the confirmation of C.
>>>
>>> If the remaining package pending HTLC has a higher-value than all the
>>> malicious fees over-bid, Mallory should realize a gain. With this 2nd
>>> pinning attack, the malicious entity buys confirmation delay of your
>>> packaged-together commitments.
>>>
>>> Assuming those attacks are correct, I'm leaning towards being
>>> conservative with the LDK broadcast backend. Though once again, other L2
>>> devs have likely other use-cases and opinions :)
>>>
>>> >  B' only needs to pay for itself in this case.
>>>
>>> Yes I think it's a nice discount when UTXO is single-owned. In the
>>> context of shared-owned UTXO (e.g LN), you might not if there is an
>>> in-mempool package already spending the UTXO and have to assume the
>>> worst-case scenario. I.e have B' committing enough fee to pay for A'
>>> replacement bandwidth. I think we can't do that much for this case...
>>>
>>> > If a package meets feerate requirements as a
>>> package, the parents in the transaction are allowed to replace-by-fee
>>> mempool transactions. The child cannot replace mempool transactions."
>>>
>>> I agree with the Mallory-vs-Alice case. Though if Alice broadcasts A+B'
>>> to replace A+B because the first broadcast isn't satisfying anymore due to
>>> mempool spikes ? Assuming B' fees is enough, I think that case as child B'
>>> replacing in-mempool transaction B. Which I understand going against  "The
>>> child cannot replace mempool transactions".
>>>
>>> Maybe wording could be a bit clearer ?
>>>
>>> > While it would be nice to have full RBF, malleability of the child
>>> won't
>>> > block RBF here. If we're trying to replace A', we only require that A'
>>> > signals replaceability, and don't mind if its child doesn't.
>>>
>>> Yes, it sounds good.
>>>
>>> > Yes, A+C+D pays 2500sat more in fees, but it is also 1000vB larger. A
>>> miner
>>> > should prefer to utilize their block space more effectively.
>>>
>>> If your mempool is empty and only composed of A+C+D or A+B, I think
>>> taking A+C+D is the most efficient block construction you can come up with
>>> as a miner ?
>>>
>>> > No, because we don't use that model.
>>>
>>> Can you describe what miner model we are using ? Like the block
>>> construction strategy implemented by `addPackagesTxs` or also encompassing
>>> our current mempool acceptance policy, which I think rely on absolute fee
>>> over ancestor score in case of replacement ?
>>>
>>> I think this point is worthy to discuss as otherwise we might downgrade
>>> the efficiency of our current block construction strategy in periods of
>>> near-empty mempools. A knowledge which could be discreetly leveraged by a
>>> miner to gain an advantage on the rest of the mining ecosystem.
>>>
>>> Note, I think we *might* have to go in this direction if we want to
>>> replace replace-by-fee by replace-by-feerate or replace-by-ancestor and
>>> solve in-depth pinning attacks. Though if we do so,
>>> IMO we would need more thoughts.
>>>
>>> I think we could restrain package acceptance to only confirmed inputs
>>> for now and revisit later this point ? For LN-anchor, you can assume that
>>> the fee-bumping UTXO feeding the CPFP is already
>>> confirmed. Or are there currently-deployed use-cases which would benefit
>>> from your proposed Rule #2 ?
>>>
>>> Antoine
>>>
>>> Le jeu. 23 sept. 2021 ? 11:36, Gloria Zhao <gloriajzhao at gmail.com> a
>>> ?crit :
>>>
>>>> Hi Antoine,
>>>>
>>>> Thanks as always for your input. I'm glad we agree on so much!
>>>>
>>>> In summary, it seems that the decisions that might still need
>>>> attention/input from devs on this mailing list are:
>>>> 1. Whether we should start with multiple-parent-1-child or
>>>> 1-parent-1-child.
>>>> 2. Whether it's ok to require that the child not have conflicts with
>>>> mempool transactions.
>>>>
>>>> Responding to your comments...
>>>>
>>>> > IIUC, you have package A+B, during the dedup phase early in
>>>> `AcceptMultipleTransactions` if you observe same-txid-different-wtixd A'
>>>> and A' is higher feerate than A, you trim A and replace by A' ?
>>>>
>>>> > I think this approach is safe, the one who appears unsafe to me is
>>>> when A' has a _lower_ feerate, even if A' is already accepted by our
>>>> mempool ? In that case iirc that would be a pinning.
>>>>
>>>> Right, the fact that we essentially always choose the first-seen
>>>> witness is an unfortunate limitation that exists already. Adding package
>>>> mempool accept doesn't worsen this, but the procedure in the future is to
>>>> replace the witness when it makes sense economically. We can also add logic
>>>> to allow package feerate to pay for witness replacements as well. This is
>>>> pretty far into the future, though.
>>>>
>>>> > It sounds uneconomical for an attacker but I think it's not when you
>>>> consider than you can "batch" attack against multiple honest
>>>> counterparties. E.g, Mallory broadcast A' + B' + C' + D' where A' conflicts
>>>> with Alice's honest package P1, B' conflicts with Bob's honest package P2,
>>>> C' conflicts with Caroll's honest package P3. And D' is a high-fee child of
>>>> A' + B' + C'.
>>>>
>>>> > If D' is higher-fee than P1 or P2 or P3 but inferior to the sum of
>>>> HTLCs confirmed by P1+P2+P3, I think it's lucrative for the attacker ?
>>>>
>>>> I could be misunderstanding, but an attacker wouldn't be able to
>>>> batch-attack like this. Alice's package only conflicts with A' + D', not A'
>>>> + B' + C' + D'. She only needs to pay for evicting 2 transactions.
>>>>
>>>> > Do we assume that broadcasted packages are "honest" by default and
>>>> that the parent(s) always need the child to pass the fee checks, that way
>>>> saving the processing of individual transactions which are expected to fail
>>>> in 99% of cases or more ad hoc composition of packages at relay ?
>>>> > I think this point is quite dependent on the p2p packages
>>>> format/logic we'll end up on and that we should feel free to revisit it
>>>> later ?
>>>>
>>>> I think it's the opposite; there's no way for us to assume that p2p
>>>> packages will be "honest." I'd like to have two things before we expose on
>>>> P2P: (1) ensure that the amount of resources potentially allocated for
>>>> package validation isn't disproportionately higher than that of single
>>>> transaction validation and (2) only use package validation when we're
>>>> unsatisifed with the single validation result, e.g. we might get better
>>>> fees.
>>>> Yes, let's revisit this later :)
>>>>
>>>>  > Yes, if you receive A+B, and A is already in-mempoo, I agree you can
>>>> discard its feerate as B should pay for all fees checked on its own. Where
>>>> I'm unclear is when you have in-mempool A+B and receive A+B'. Should B'
>>>> have a fee high enough to cover the bandwidth penalty replacement
>>>> (`PaysForRBF`, 2nd check) of both A+B' or only B' ?
>>>>
>>>>  B' only needs to pay for itself in this case.
>>>>
>>>> > > Do we want the child to be able to replace mempool transactions as
>>>> well?
>>>>
>>>> > If we mean when you have replaceable A+B then A'+B' try to replace
>>>> with a higher-feerate ? I think that's exactly the case we need for
>>>> Lightning as A+B is coming from Alice and A'+B' is coming from Bob :/
>>>>
>>>> Let me clarify this because I can see that my wording was ambiguous,
>>>> and then please let me know if it fits Lightning's needs?
>>>>
>>>> In my proposal, I wrote "If a package meets feerate requirements as a
>>>> package, the parents in the transaction are allowed to replace-by-fee
>>>> mempool transactions. The child cannot replace mempool transactions." What
>>>> I meant was: the package can replace mempool transactions if any of the
>>>> parents conflict with mempool transactions. The child cannot not conflict
>>>> with any mempool transactions.
>>>> The Lightning use case this attempts to address is: Alice and Mallory
>>>> are LN counterparties, and have packages A+B and A'+B', respectively. A and
>>>> A' are their commitment transactions and conflict with each other; they
>>>> have shared inputs and different txids.
>>>> B spends Alice's anchor output from A. B' spends Mallory's anchor
>>>> output from A'. Thus, B and B' do not conflict with each other.
>>>> Alice can broadcast her package, A+B, to replace Mallory's package,
>>>> A'+B', since B doesn't conflict with the mempool.
>>>>
>>>> Would this be ok?
>>>>
>>>> > The second option, a child of A', In the LN case I think the CPFP is
>>>> attached on one's anchor output.
>>>>
>>>> While it would be nice to have full RBF, malleability of the child
>>>> won't block RBF here. If we're trying to replace A', we only require that
>>>> A' signals replaceability, and don't mind if its child doesn't.
>>>>
>>>> > > B has an ancestor score of 10sat/vb and D has an
>>>> > > ancestor score of ~2.9sat/vb. Since D's ancestor score is lower
>>>> than B's,
>>>> > > it fails the proposed package RBF Rule #2, so this package would be
>>>> > > rejected. Does this meet your expectations?
>>>>
>>>> > Well what sounds odd to me, in my example, we fail D even if it has a
>>>> higher-fee than B. Like A+B absolute fees are 2000 sats and A+C+D absolute
>>>> fees are 4500 sats ?
>>>>
>>>> Yes, A+C+D pays 2500sat more in fees, but it is also 1000vB larger. A
>>>> miner should prefer to utilize their block space more effectively.
>>>>
>>>> > Is this compatible with a model where a miner prioritizes absolute
>>>> fees over ancestor score, in the case that mempools aren't full-enough to
>>>> fulfill a block ?
>>>>
>>>> No, because we don't use that model.
>>>>
>>>> Thanks,
>>>> Gloria
>>>>
>>>> On Thu, Sep 23, 2021 at 5:29 AM Antoine Riard <antoine.riard at gmail.com>
>>>> wrote:
>>>>
>>>>> > Correct, if B+C is too low feerate to be accepted, we will reject
>>>>> it. I
>>>>> > prefer this because it is incentive compatible: A can be mined by
>>>>> itself,
>>>>> > so there's no reason to prefer A+B+C instead of A.
>>>>> > As another way of looking at this, consider the case where we do
>>>>> accept
>>>>> > A+B+C and it sits at the "bottom" of our mempool. If our mempool
>>>>> reaches
>>>>> > capacity, we evict the lowest descendant feerate transactions, which
>>>>> are
>>>>> > B+C in this case. This gives us the same resulting mempool, with A
>>>>> and not
>>>>> > B+C.
>>>>>
>>>>> I agree here. Doing otherwise, we might evict other transactions
>>>>> mempool in `MempoolAccept::Finalize` with a higher-feerate than B+C while
>>>>> those evicted transactions are the most compelling for block construction.
>>>>>
>>>>> I thought at first missing this acceptance requirement would break a
>>>>> fee-bumping scheme like Parent-Pay-For-Child where a high-fee parent is
>>>>> attached to a child signed with SIGHASH_ANYONECANPAY but in this case the
>>>>> child fee is capturing the parent value. I can't think of other fee-bumping
>>>>> schemes potentially affected. If they do exist I would say they're wrong in
>>>>> their design assumptions.
>>>>>
>>>>> > If or when we have witness replacement, the logic is: if the
>>>>> individual
>>>>> > transaction is enough to replace the mempool one, the replacement
>>>>> will
>>>>> > happen during the preceding individual transaction acceptance, and
>>>>> > deduplication logic will work. Otherwise, we will try to deduplicate
>>>>> by
>>>>> > wtxid, see that we need a package witness replacement, and use the
>>>>> package
>>>>> > feerate to evaluate whether this is economically rational.
>>>>>
>>>>> IIUC, you have package A+B, during the dedup phase early in
>>>>> `AcceptMultipleTransactions` if you observe same-txid-different-wtixd A'
>>>>> and A' is higher feerate than A, you trim A and replace by A' ?
>>>>>
>>>>> I think this approach is safe, the one who appears unsafe to me is
>>>>> when A' has a _lower_ feerate, even if A' is already accepted by our
>>>>> mempool ? In that case iirc that would be a pinning.
>>>>>
>>>>> Good to see progress on witness replacement before we see usage of
>>>>> Taproot tree in the context of multi-party, where a malicious counterparty
>>>>> inflates its witness to jam a honest spending.
>>>>>
>>>>> (Note, the commit linked currently points nowhere :))
>>>>>
>>>>>
>>>>> > Please note that A may replace A' even if A' has higher fees than A
>>>>> > individually, because the proposed package RBF utilizes the fees and
>>>>> size
>>>>> > of the entire package. This just requires E to pay enough fees,
>>>>> although
>>>>> > this can be pretty high if there are also potential B' and C'
>>>>> competing
>>>>> > commitment transactions that we don't know about.
>>>>>
>>>>> Ah right, if the package acceptance waives `PaysMoreThanConflicts` for
>>>>> the individual check on A, the honest package should replace the pinning
>>>>> attempt. I've not fully parsed the proposed implementation yet.
>>>>>
>>>>> Though note, I think it's still unsafe for a Lightning
>>>>> multi-commitment-broadcast-as-one-package as a malicious A' might have an
>>>>> absolute fee higher than E. It sounds uneconomical for
>>>>> an attacker but I think it's not when you consider than you can
>>>>> "batch" attack against multiple honest counterparties. E.g, Mallory
>>>>> broadcast A' + B' + C' + D' where A' conflicts with Alice's honest package
>>>>> P1, B' conflicts with Bob's honest package P2, C' conflicts with Caroll's
>>>>> honest package P3. And D' is a high-fee child of A' + B' + C'.
>>>>>
>>>>> If D' is higher-fee than P1 or P2 or P3 but inferior to the sum of
>>>>> HTLCs confirmed by P1+P2+P3, I think it's lucrative for the attacker ?
>>>>>
>>>>> > So far, my understanding is that multi-parent-1-child is desired for
>>>>> > batched fee-bumping (
>>>>> > https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289)
>>>>> and
>>>>> > I've also seen your response which I have less context on (
>>>>> > https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202).
>>>>> That
>>>>> > being said, I am happy to create a new proposal for 1 parent + 1
>>>>> child
>>>>> > (which would be slightly simpler) and plan for moving to
>>>>> > multi-parent-1-child later if that is preferred. I am very
>>>>> interested in
>>>>> > hearing feedback on that approach.
>>>>>
>>>>> I think batched fee-bumping is okay as long as you don't have
>>>>> time-sensitive outputs encumbering your commitment transactions. For the
>>>>> reasons mentioned above, I think that's unsafe.
>>>>>
>>>>> What I'm worried about is  L2 developers, potentially not aware about
>>>>> all the mempool subtleties blurring the difference and always batching
>>>>> their broadcast by default.
>>>>>
>>>>> IMO, a good thing by restraining to 1-parent + 1 child,  we
>>>>> artificially constraint L2 design space for now and minimize risks of
>>>>> unsafe usage of the package API :)
>>>>>
>>>>> I think that's a point where it would be relevant to have the opinion
>>>>> of more L2 devs.
>>>>>
>>>>> > I think there is a misunderstanding here - let me describe what I'm
>>>>> > proposing we'd do in this situation: we'll try individual submission
>>>>> for A,
>>>>> > see that it fails due to "insufficient fees." Then, we'll try package
>>>>> > validation for A+B and use package RBF. If A+B pays enough, it can
>>>>> still
>>>>> > replace A'. If A fails for a bad signature, we won't look at B or
>>>>> A+B. Does
>>>>> > this meet your expectations?
>>>>>
>>>>> Yes there was a misunderstanding, I think this approach is correct,
>>>>> it's more a question of performance. Do we assume that broadcasted packages
>>>>> are "honest" by default and that the parent(s) always need the child to
>>>>> pass the fee checks, that way saving the processing of individual
>>>>> transactions which are expected to fail in 99% of cases or more ad hoc
>>>>> composition of packages at relay ?
>>>>>
>>>>> I think this point is quite dependent on the p2p packages format/logic
>>>>> we'll end up on and that we should feel free to revisit it later ?
>>>>>
>>>>>
>>>>> > What problem are you trying to solve by the package feerate *after*
>>>>> dedup
>>>>> rule ?
>>>>> > My understanding is that an in-package transaction might be already
>>>>> in
>>>>> the mempool. Therefore, to compute a correct RBF penalty replacement,
>>>>> the
>>>>> vsize of this transaction could be discarded lowering the cost of
>>>>> package
>>>>> RBF.
>>>>>
>>>>> > I'm proposing that, when a transaction has already been submitted to
>>>>> > mempool, we would ignore both its fees and vsize when calculating
>>>>> package
>>>>> > feerate.
>>>>>
>>>>> Yes, if you receive A+B, and A is already in-mempoo, I agree you can
>>>>> discard its feerate as B should pay for all fees checked on its own. Where
>>>>> I'm unclear is when you have in-mempool A+B and receive A+B'. Should B'
>>>>> have a fee high enough to cover the bandwidth penalty replacement
>>>>> (`PaysForRBF`, 2nd check) of both A+B' or only B' ?
>>>>>
>>>>> If you have a second-layer like current Lightning, you might have a
>>>>> counterparty commitment to replace and should always expect to have to pay
>>>>> for parent replacement bandwidth.
>>>>>
>>>>> Where a potential discount sounds interesting is when you have an
>>>>> univoque state on the first-stage of transactions. E.g DLC's funding
>>>>> transaction which might be CPFP by any participant iirc.
>>>>>
>>>>> > Note that, if C' conflicts with C, it also conflicts with D, since D
>>>>> is a
>>>>> > descendant of C and would thus need to be evicted along with it.
>>>>>
>>>>> Ah once again I think it's a misunderstanding without the code under
>>>>> my eyes! If we do C' `PreChecks`, solve the conflicts provoked by it, i.e
>>>>> mark for potential eviction D and don't consider it for future conflicts in
>>>>> the rest of the package, I think D' `PreChecks` should be good ?
>>>>>
>>>>> > More generally, this example is surprising to me because I didn't
>>>>> think
>>>>> > packages would be used to fee-bump replaceable transactions. Do we
>>>>> want the
>>>>> > child to be able to replace mempool transactions as well?
>>>>>
>>>>> If we mean when you have replaceable A+B then A'+B' try to replace
>>>>> with a higher-feerate ? I think that's exactly the case we need for
>>>>> Lightning as A+B is coming from Alice and A'+B' is coming from Bob :/
>>>>>
>>>>> > I'm not sure what you mean? Let's say we have a package of parent A
>>>>> + child
>>>>> > B, where A is supposed to replace a mempool transaction A'. Are you
>>>>> saying
>>>>> > that counterparties are able to malleate the package child B, or a
>>>>> child of
>>>>> > A'?
>>>>>
>>>>> The second option, a child of A', In the LN case I think the CPFP is
>>>>> attached on one's anchor output.
>>>>>
>>>>> I think it's good if we assume the
>>>>> solve-conflicts-after-parent's`'PreChecks` mentioned above or fixing
>>>>> inherited signaling or full-rbf ?
>>>>>
>>>>> > Sorry, I don't understand what you mean by "preserve the package
>>>>> > integrity?" Could you elaborate?
>>>>>
>>>>> After thinking the relaxation about the "new" unconfirmed input is not
>>>>> linked to trimming but I would say more to the multi-parent support.
>>>>>
>>>>> Let's say you have A+B trying to replace C+D where B is also spending
>>>>> already in-mempool E. To succeed, you need to waive the no-new-unconfirmed
>>>>> input as D isn't spending E.
>>>>>
>>>>> So good, I think we agree on the problem description here.
>>>>>
>>>>> > I am in agreement with your calculations but unsure if we disagree
>>>>> on the
>>>>> > expected outcome. Yes, B has an ancestor score of 10sat/vb and D has
>>>>> an
>>>>> > ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than
>>>>> B's,
>>>>> > it fails the proposed package RBF Rule #2, so this package would be
>>>>> > rejected. Does this meet your expectations?
>>>>>
>>>>> Well what sounds odd to me, in my example, we fail D even if it has a
>>>>> higher-fee than B. Like A+B absolute fees are 2000 sats and A+C+D absolute
>>>>> fees are 4500 sats ?
>>>>>
>>>>> Is this compatible with a model where a miner prioritizes absolute
>>>>> fees over ancestor score, in the case that mempools aren't full-enough to
>>>>> fulfill a block ?
>>>>>
>>>>> Let me know if I can clarify a point.
>>>>>
>>>>> Antoine
>>>>>
>>>>> Le lun. 20 sept. 2021 ? 11:10, Gloria Zhao <gloriajzhao at gmail.com> a
>>>>> ?crit :
>>>>>
>>>>>>
>>>>>> Hi Antoine,
>>>>>>
>>>>>> First of all, thank you for the thorough review. I appreciate your
>>>>>> insight on LN requirements.
>>>>>>
>>>>>> > IIUC, you have a package A+B+C submitted for acceptance and A is
>>>>>> already in your mempool. You trim out A from the package and then evaluate
>>>>>> B+C.
>>>>>>
>>>>>> > I think this might be an issue if A is the higher-fee element of
>>>>>> the ABC package. B+C package fees might be under the mempool min fee and
>>>>>> will be rejected, potentially breaking the acceptance expectations of the
>>>>>> package issuer ?
>>>>>>
>>>>>> Correct, if B+C is too low feerate to be accepted, we will reject it.
>>>>>> I prefer this because it is incentive compatible: A can be mined by itself,
>>>>>> so there's no reason to prefer A+B+C instead of A.
>>>>>> As another way of looking at this, consider the case where we do
>>>>>> accept A+B+C and it sits at the "bottom" of our mempool. If our mempool
>>>>>> reaches capacity, we evict the lowest descendant feerate transactions,
>>>>>> which are B+C in this case. This gives us the same resulting mempool, with
>>>>>> A and not B+C.
>>>>>>
>>>>>>
>>>>>> > Further, I think the dedup should be done on wtxid, as you might
>>>>>> have multiple valid witnesses. Though with varying vsizes and as such
>>>>>> offering different feerates.
>>>>>>
>>>>>> I agree that variations of the same package with different witnesses
>>>>>> is a case that must be handled. I consider witness replacement to be a
>>>>>> project that can be done in parallel to package mempool acceptance because
>>>>>> being able to accept packages does not worsen the problem of a
>>>>>> same-txid-different-witness "pinning" attack.
>>>>>>
>>>>>> If or when we have witness replacement, the logic is: if the
>>>>>> individual transaction is enough to replace the mempool one, the
>>>>>> replacement will happen during the preceding individual transaction
>>>>>> acceptance, and deduplication logic will work. Otherwise, we will try to
>>>>>> deduplicate by wtxid, see that we need a package witness replacement, and
>>>>>> use the package feerate to evaluate whether this is economically rational.
>>>>>>
>>>>>> See the #22290 "handle package transactions already in mempool"
>>>>>> commit (
>>>>>> https://github.com/bitcoin/bitcoin/pull/22290/commits/fea75a2237b46cf76145242fecad7e274bfcb5ff),
>>>>>> which handles the case of same-txid-different-witness by simply using the
>>>>>> transaction in the mempool for now, with TODOs for what I just described.
>>>>>>
>>>>>>
>>>>>> > I'm not clearly understanding the accepted topologies. By "parent
>>>>>> and child to share a parent", do you mean the set of transactions A, B, C,
>>>>>> where B is spending A and C is spending A and B would be correct ?
>>>>>>
>>>>>> Yes, that is what I meant. Yes, that would a valid package under
>>>>>> these rules.
>>>>>>
>>>>>> > If yes, is there a width-limit introduced or we fallback on
>>>>>> MAX_PACKAGE_COUNT=25 ?
>>>>>>
>>>>>> No, there is no limit on connectivity other than "child with all
>>>>>> unconfirmed parents." We will enforce MAX_PACKAGE_COUNT=25 and child's
>>>>>> in-mempool + in-package ancestor limits.
>>>>>>
>>>>>>
>>>>>> > Considering the current Core's mempool acceptance rules, I think
>>>>>> CPFP batching is unsafe for LN time-sensitive closure. A malicious tx-relay
>>>>>> jamming successful on one channel commitment transaction would contamine
>>>>>> the remaining commitments sharing the same package.
>>>>>>
>>>>>> > E.g, you broadcast the package A+B+C+D+E where A,B,C,D are
>>>>>> commitment transactions and E a shared CPFP. If a malicious A' transaction
>>>>>> has a better feerate than A, the whole package acceptance will fail. Even
>>>>>> if A' confirms in the following block,
>>>>>> the propagation and confirmation of B+C+D have been delayed. This
>>>>>> could carry on a loss of funds.
>>>>>>
>>>>>> Please note that A may replace A' even if A' has higher fees than A
>>>>>> individually, because the proposed package RBF utilizes the fees and size
>>>>>> of the entire package. This just requires E to pay enough fees, although
>>>>>> this can be pretty high if there are also potential B' and C' competing
>>>>>> commitment transactions that we don't know about.
>>>>>>
>>>>>>
>>>>>> > IMHO, I'm leaning towards deploying during a first phase
>>>>>> 1-parent/1-child. I think it's the most conservative step still improving
>>>>>> second-layer safety.
>>>>>>
>>>>>> So far, my understanding is that multi-parent-1-child is desired for
>>>>>> batched fee-bumping (
>>>>>> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289)
>>>>>> and I've also seen your response which I have less context on (
>>>>>> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202).
>>>>>> That being said, I am happy to create a new proposal for 1 parent + 1 child
>>>>>> (which would be slightly simpler) and plan for moving to
>>>>>> multi-parent-1-child later if that is preferred. I am very interested in
>>>>>> hearing feedback on that approach.
>>>>>>
>>>>>>
>>>>>> > If A+B is submitted to replace A', where A pays 0 sats, B pays 200
>>>>>> sats and A' pays 100 sats. If we apply the individual RBF on A, A+B
>>>>>> acceptance fails. For this reason I think the individual RBF should be
>>>>>> bypassed and only the package RBF apply ?
>>>>>>
>>>>>> I think there is a misunderstanding here - let me describe what I'm
>>>>>> proposing we'd do in this situation: we'll try individual submission for A,
>>>>>> see that it fails due to "insufficient fees." Then, we'll try package
>>>>>> validation for A+B and use package RBF. If A+B pays enough, it can still
>>>>>> replace A'. If A fails for a bad signature, we won't look at B or A+B. Does
>>>>>> this meet your expectations?
>>>>>>
>>>>>>
>>>>>> > What problem are you trying to solve by the package feerate *after*
>>>>>> dedup rule ?
>>>>>> > My understanding is that an in-package transaction might be already
>>>>>> in the mempool. Therefore, to compute a correct RBF penalty replacement,
>>>>>> the vsize of this transaction could be discarded lowering the cost of
>>>>>> package RBF.
>>>>>>
>>>>>> I'm proposing that, when a transaction has already been submitted to
>>>>>> mempool, we would ignore both its fees and vsize when calculating package
>>>>>> feerate. In example G2, we shouldn't count M1 fees after its submission to
>>>>>> mempool, since M1's fees have already been used to pay for its individual
>>>>>> bandwidth, and it shouldn't be used again to pay for P2 and P3's bandwidth.
>>>>>> We also shouldn't count its vsize, since it has already been paid for.
>>>>>>
>>>>>>
>>>>>> > I think this is a footgunish API, as if a package issuer send the
>>>>>> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.
>>>>>> Then try to broadcast the higher-feerate C'+D' package, it should be
>>>>>> rejected. So it's breaking the naive broadcaster assumption that a
>>>>>> higher-feerate/higher-fee package always replaces ?
>>>>>>
>>>>>> Note that, if C' conflicts with C, it also conflicts with D, since D
>>>>>> is a descendant of C and would thus need to be evicted along with it.
>>>>>> Implicitly, D' would not be in conflict with D.
>>>>>> More generally, this example is surprising to me because I didn't
>>>>>> think packages would be used to fee-bump replaceable transactions. Do we
>>>>>> want the child to be able to replace mempool transactions as well? This can
>>>>>> be implemented with a bit of additional logic.
>>>>>>
>>>>>> > I think this is unsafe for L2s if counterparties have malleability
>>>>>> of the child transaction. They can block your package replacement by
>>>>>> opting-out from RBF signaling. IIRC, LN's "anchor output" presents such an
>>>>>> ability.
>>>>>>
>>>>>> I'm not sure what you mean? Let's say we have a package of parent A +
>>>>>> child B, where A is supposed to replace a mempool transaction A'. Are you
>>>>>> saying that counterparties are able to malleate the package child B, or a
>>>>>> child of A'? If they can malleate a child of A', that shouldn't matter as
>>>>>> long as A' is signaling replacement. This would be handled identically with
>>>>>> full RBF and what Core currently implements.
>>>>>>
>>>>>> > I think this is an issue brought by the trimming during the dedup
>>>>>> phase. If we preserve the package integrity, only re-using the tx-level
>>>>>> checks results of already in-mempool transactions to gain in CPU time we
>>>>>> won't have this issue. Package childs can add unconfirmed inputs as long as
>>>>>> they're in-package, the bip125 rule2 is only evaluated against parents ?
>>>>>>
>>>>>> Sorry, I don't understand what you mean by "preserve the package
>>>>>> integrity?" Could you elaborate?
>>>>>>
>>>>>> > Let's say you have in-mempool A, B where A pays 10 sat/vb for 100
>>>>>> vbytes and B pays 10 sat/vb for 100 vbytes. You have the candidate
>>>>>> replacement D spending both A and C where D pays 15sat/vb for 100 vbytes
>>>>>> and C pays 1 sat/vb for 1000 vbytes.
>>>>>>
>>>>>> > Package A + B ancestor score is 10 sat/vb.
>>>>>>
>>>>>> > D has a higher feerate/absolute fee than B.
>>>>>>
>>>>>> > Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats +
>>>>>> C's 1000 sats + D's 1500 sats) / A's 100 vb + C's 1000 vb + D's 100 vb)
>>>>>>
>>>>>> I am in agreement with your calculations but unsure if we disagree on
>>>>>> the expected outcome. Yes, B has an ancestor score of 10sat/vb and D has an
>>>>>> ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than B's,
>>>>>> it fails the proposed package RBF Rule #2, so this package would be
>>>>>> rejected. Does this meet your expectations?
>>>>>>
>>>>>> Thank you for linking to projects that might be interested in package
>>>>>> relay :)
>>>>>>
>>>>>> Thanks,
>>>>>> Gloria
>>>>>>
>>>>>> On Mon, Sep 20, 2021 at 12:16 AM Antoine Riard <
>>>>>> antoine.riard at gmail.com> wrote:
>>>>>>
>>>>>>> Hi Gloria,
>>>>>>>
>>>>>>> > A package may contain transactions that are already in the
>>>>>>> mempool. We
>>>>>>> > remove
>>>>>>> > ("deduplicate") those transactions from the package for the
>>>>>>> purposes of
>>>>>>> > package
>>>>>>> > mempool acceptance. If a package is empty after deduplication, we
>>>>>>> do
>>>>>>> > nothing.
>>>>>>>
>>>>>>> IIUC, you have a package A+B+C submitted for acceptance and A is
>>>>>>> already in your mempool. You trim out A from the package and then evaluate
>>>>>>> B+C.
>>>>>>>
>>>>>>> I think this might be an issue if A is the higher-fee element of the
>>>>>>> ABC package. B+C package fees might be under the mempool min fee and will
>>>>>>> be rejected, potentially breaking the acceptance expectations of the
>>>>>>> package issuer ?
>>>>>>>
>>>>>>> Further, I think the dedup should be done on wtxid, as you might
>>>>>>> have multiple valid witnesses. Though with varying vsizes and as such
>>>>>>> offering different feerates.
>>>>>>>
>>>>>>> E.g you're going to evaluate the package A+B and A' is already in
>>>>>>> your mempool with a bigger valid witness. You trim A based on txid, then
>>>>>>> you evaluate A'+B, which fails the fee checks. However, evaluating A+B
>>>>>>> would have been a success.
>>>>>>>
>>>>>>> AFAICT, the dedup rationale would be to save on CPU time/IO disk, to
>>>>>>> avoid repeated signatures verification and parent UTXOs fetches ? Can we
>>>>>>> achieve the same goal by bypassing tx-level checks for already-in txn while
>>>>>>> conserving the package integrity for package-level checks ?
>>>>>>>
>>>>>>> > Note that it's possible for the parents to be
>>>>>>> > indirect
>>>>>>> > descendants/ancestors of one another, or for parent and child to
>>>>>>> share a
>>>>>>> > parent,
>>>>>>> > so we cannot make any other topology assumptions.
>>>>>>>
>>>>>>> I'm not clearly understanding the accepted topologies. By "parent
>>>>>>> and child to share a parent", do you mean the set of transactions A, B, C,
>>>>>>> where B is spending A and C is spending A and B would be correct ?
>>>>>>>
>>>>>>> If yes, is there a width-limit introduced or we fallback on
>>>>>>> MAX_PACKAGE_COUNT=25 ?
>>>>>>>
>>>>>>> IIRC, one rationale to come with this topology limitation was to
>>>>>>> lower the DoS risks when potentially deploying p2p packages.
>>>>>>>
>>>>>>> Considering the current Core's mempool acceptance rules, I think
>>>>>>> CPFP batching is unsafe for LN time-sensitive closure. A malicious tx-relay
>>>>>>> jamming successful on one channel commitment transaction would contamine
>>>>>>> the remaining commitments sharing the same package.
>>>>>>>
>>>>>>> E.g, you broadcast the package A+B+C+D+E where A,B,C,D are
>>>>>>> commitment transactions and E a shared CPFP. If a malicious A' transaction
>>>>>>> has a better feerate than A, the whole package acceptance will fail. Even
>>>>>>> if A' confirms in the following block,
>>>>>>> the propagation and confirmation of B+C+D have been delayed. This
>>>>>>> could carry on a loss of funds.
>>>>>>>
>>>>>>> That said, if you're broadcasting commitment transactions without
>>>>>>> time-sensitive HTLC outputs, I think the batching is effectively a fee
>>>>>>> saving as you don't have to duplicate the CPFP.
>>>>>>>
>>>>>>> IMHO, I'm leaning towards deploying during a first phase
>>>>>>> 1-parent/1-child. I think it's the most conservative step still improving
>>>>>>> second-layer safety.
>>>>>>>
>>>>>>> > *Rationale*:  It would be incorrect to use the fees of
>>>>>>> transactions that are
>>>>>>> > already in the mempool, as we do not want a transaction's fees to
>>>>>>> be
>>>>>>> > double-counted for both its individual RBF and package RBF.
>>>>>>>
>>>>>>> I'm unsure about the logical order of the checks proposed.
>>>>>>>
>>>>>>> If A+B is submitted to replace A', where A pays 0 sats, B pays 200
>>>>>>> sats and A' pays 100 sats. If we apply the individual RBF on A, A+B
>>>>>>> acceptance fails. For this reason I think the individual RBF should be
>>>>>>> bypassed and only the package RBF apply ?
>>>>>>>
>>>>>>> Note this situation is plausible, with current LN design, your
>>>>>>> counterparty can have a commitment transaction with a better fee just by
>>>>>>> selecting a higher `dust_limit_satoshis` than yours.
>>>>>>>
>>>>>>> > Examples F and G [14] show the same package, but P1 is submitted
>>>>>>> > individually before
>>>>>>> > the package in example G. In example F, we can see that the 300vB
>>>>>>> package
>>>>>>> > pays
>>>>>>> > an additional 200sat in fees, which is not enough to pay for its
>>>>>>> own
>>>>>>> > bandwidth
>>>>>>> > (BIP125#4). In example G, we can see that P1 pays enough to
>>>>>>> replace M1, but
>>>>>>> > using P1's fees again during package submission would make it look
>>>>>>> like a
>>>>>>> > 300sat
>>>>>>> > increase for a 200vB package. Even including its fees and size
>>>>>>> would not be
>>>>>>> > sufficient in this example, since the 300sat looks like enough for
>>>>>>> the 300vB
>>>>>>> > package. The calculcation after deduplication is 100sat increase
>>>>>>> for a
>>>>>>> > package
>>>>>>> > of size 200vB, which correctly fails BIP125#4. Assume all
>>>>>>> transactions have
>>>>>>> > a
>>>>>>> > size of 100vB.
>>>>>>>
>>>>>>> What problem are you trying to solve by the package feerate *after*
>>>>>>> dedup rule ?
>>>>>>>
>>>>>>> My understanding is that an in-package transaction might be already
>>>>>>> in the mempool. Therefore, to compute a correct RBF penalty replacement,
>>>>>>> the vsize of this transaction could be discarded lowering the cost of
>>>>>>> package RBF.
>>>>>>>
>>>>>>> If we keep a "safe" dedup mechanism (see my point above), I think
>>>>>>> this discount is justified, as the validation cost of node operators is
>>>>>>> paid for ?
>>>>>>>
>>>>>>> > The child cannot replace mempool transactions.
>>>>>>>
>>>>>>> Let's say you issue package A+B, then package C+B', where B' is a
>>>>>>> child of both A and C. This rule fails the acceptance of C+B' ?
>>>>>>>
>>>>>>> I think this is a footgunish API, as if a package issuer send the
>>>>>>> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.
>>>>>>> Then try to broadcast the higher-feerate C'+D' package, it should be
>>>>>>> rejected. So it's breaking the naive broadcaster assumption that a
>>>>>>> higher-feerate/higher-fee package always replaces ? And it might be unsafe
>>>>>>> in protocols where states are symmetric. E.g a malicious counterparty
>>>>>>> broadcasts first S+A, then you honestly broadcast S+B, where B pays better
>>>>>>> fees.
>>>>>>>
>>>>>>> > All mempool transactions to be replaced must signal replaceability.
>>>>>>>
>>>>>>> I think this is unsafe for L2s if counterparties have malleability
>>>>>>> of the child transaction. They can block your package replacement by
>>>>>>> opting-out from RBF signaling. IIRC, LN's "anchor output" presents such an
>>>>>>> ability.
>>>>>>>
>>>>>>> I think it's better to either fix inherited signaling or move
>>>>>>> towards full-rbf.
>>>>>>>
>>>>>>> > if a package parent has already been submitted, it would
>>>>>>> > look
>>>>>>> >like the child is spending a "new" unconfirmed input.
>>>>>>>
>>>>>>> I think this is an issue brought by the trimming during the dedup
>>>>>>> phase. If we preserve the package integrity, only re-using the tx-level
>>>>>>> checks results of already in-mempool transactions to gain in CPU time we
>>>>>>> won't have this issue. Package childs can add unconfirmed inputs as long as
>>>>>>> they're in-package, the bip125 rule2 is only evaluated against parents ?
>>>>>>>
>>>>>>> > However, we still achieve the same goal of requiring the
>>>>>>> > replacement
>>>>>>> > transactions to have a ancestor score at least as high as the
>>>>>>> original
>>>>>>> > ones.
>>>>>>>
>>>>>>> I'm not sure if this holds...
>>>>>>>
>>>>>>> Let's say you have in-mempool A, B where A pays 10 sat/vb for 100
>>>>>>> vbytes and B pays 10 sat/vb for 100 vbytes. You have the candidate
>>>>>>> replacement D spending both A and C where D pays 15sat/vb for 100 vbytes
>>>>>>> and C pays 1 sat/vb for 1000 vbytes.
>>>>>>>
>>>>>>> Package A + B ancestor score is 10 sat/vb.
>>>>>>>
>>>>>>> D has a higher feerate/absolute fee than B.
>>>>>>>
>>>>>>> Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's
>>>>>>> 1000 sats + D's 1500 sats) /
>>>>>>> A's 100 vb + C's 1000 vb + D's 100 vb)
>>>>>>>
>>>>>>> Overall, this is a review through the lenses of LN requirements. I
>>>>>>> think other L2 protocols/applications
>>>>>>> could be candidates to using package accept/relay such as:
>>>>>>> * https://github.com/lightninglabs/pool
>>>>>>> * https://github.com/discreetlogcontracts/dlcspecs
>>>>>>> * https://github.com/bitcoin-teleport/teleport-transactions/
>>>>>>> * https://github.com/sapio-lang/sapio
>>>>>>> *
>>>>>>> https://github.com/commerceblock/mercury/blob/master/doc/statechains.md
>>>>>>> * https://github.com/revault/practical-revault
>>>>>>>
>>>>>>> Thanks for rolling forward the ball on this subject.
>>>>>>>
>>>>>>> Antoine
>>>>>>>
>>>>>>> Le jeu. 16 sept. 2021 ? 03:55, Gloria Zhao via bitcoin-dev <
>>>>>>> bitcoin-dev at lists.linuxfoundation.org> a ?crit :
>>>>>>>
>>>>>>>> Hi there,
>>>>>>>>
>>>>>>>> I'm writing to propose a set of mempool policy changes to enable
>>>>>>>> package
>>>>>>>> validation (in preparation for package relay) in Bitcoin Core.
>>>>>>>> These would not
>>>>>>>> be consensus or P2P protocol changes. However, since mempool policy
>>>>>>>> significantly affects transaction propagation, I believe this is
>>>>>>>> relevant for
>>>>>>>> the mailing list.
>>>>>>>>
>>>>>>>> My proposal enables packages consisting of multiple parents and 1
>>>>>>>> child. If you
>>>>>>>> develop software that relies on specific transaction relay
>>>>>>>> assumptions and/or
>>>>>>>> are interested in using package relay in the future, I'm very
>>>>>>>> interested to hear
>>>>>>>> your feedback on the utility or restrictiveness of these package
>>>>>>>> policies for
>>>>>>>> your use cases.
>>>>>>>>
>>>>>>>> A draft implementation of this proposal can be found in [Bitcoin
>>>>>>>> Core
>>>>>>>> PR#22290][1].
>>>>>>>>
>>>>>>>> An illustrated version of this post can be found at
>>>>>>>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.
>>>>>>>> I have also linked the images below.
>>>>>>>>
>>>>>>>> ## Background
>>>>>>>>
>>>>>>>> Feel free to skip this section if you are already familiar with
>>>>>>>> mempool policy
>>>>>>>> and package relay terminology.
>>>>>>>>
>>>>>>>> ### Terminology Clarifications
>>>>>>>>
>>>>>>>> * Package = an ordered list of related transactions, representable
>>>>>>>> by a Directed
>>>>>>>>   Acyclic Graph.
>>>>>>>> * Package Feerate = the total modified fees divided by the total
>>>>>>>> virtual size of
>>>>>>>>   all transactions in the package.
>>>>>>>>     - Modified fees = a transaction's base fees + fee delta applied
>>>>>>>> by the user
>>>>>>>>       with `prioritisetransaction`. As such, we expect this to vary
>>>>>>>> across
>>>>>>>> mempools.
>>>>>>>>     - Virtual Size = the maximum of virtual sizes calculated using
>>>>>>>> [BIP141
>>>>>>>>       virtual size][2] and sigop weight. [Implemented here in
>>>>>>>> Bitcoin Core][3].
>>>>>>>>     - Note that feerate is not necessarily based on the base fees
>>>>>>>> and serialized
>>>>>>>>       size.
>>>>>>>>
>>>>>>>> * Fee-Bumping = user/wallet actions that take advantage of miner
>>>>>>>> incentives to
>>>>>>>>   boost a transaction's candidacy for inclusion in a block,
>>>>>>>> including Child Pays
>>>>>>>> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our
>>>>>>>> intention in
>>>>>>>> mempool policy is to recognize when the new transaction is more
>>>>>>>> economical to
>>>>>>>> mine than the original one(s) but not open DoS vectors, so there
>>>>>>>> are some
>>>>>>>> limitations.
>>>>>>>>
>>>>>>>> ### Policy
>>>>>>>>
>>>>>>>> The purpose of the mempool is to store the best (to be most
>>>>>>>> incentive-compatible
>>>>>>>> with miners, highest feerate) candidates for inclusion in a block.
>>>>>>>> Miners use
>>>>>>>> the mempool to build block templates. The mempool is also useful as
>>>>>>>> a cache for
>>>>>>>> boosting block relay and validation performance, aiding transaction
>>>>>>>> relay, and
>>>>>>>> generating feerate estimations.
>>>>>>>>
>>>>>>>> Ideally, all consensus-valid transactions paying reasonable fees
>>>>>>>> should make it
>>>>>>>> to miners through normal transaction relay, without any special
>>>>>>>> connectivity or
>>>>>>>> relationships with miners. On the other hand, nodes do not have
>>>>>>>> unlimited
>>>>>>>> resources, and a P2P network designed to let any honest node
>>>>>>>> broadcast their
>>>>>>>> transactions also exposes the transaction validation engine to DoS
>>>>>>>> attacks from
>>>>>>>> malicious peers.
>>>>>>>>
>>>>>>>> As such, for unconfirmed transactions we are considering for our
>>>>>>>> mempool, we
>>>>>>>> apply a set of validation rules in addition to consensus, primarily
>>>>>>>> to protect
>>>>>>>> us from resource exhaustion and aid our efforts to keep the highest
>>>>>>>> fee
>>>>>>>> transactions. We call this mempool _policy_: a set of (configurable,
>>>>>>>> node-specific) rules that transactions must abide by in order to be
>>>>>>>> accepted
>>>>>>>> into our mempool. Transaction "Standardness" rules and mempool
>>>>>>>> restrictions such
>>>>>>>> as "too-long-mempool-chain" are both examples of policy.
>>>>>>>>
>>>>>>>> ### Package Relay and Package Mempool Accept
>>>>>>>>
>>>>>>>> In transaction relay, we currently consider transactions one at a
>>>>>>>> time for
>>>>>>>> submission to the mempool. This creates a limitation in the node's
>>>>>>>> ability to
>>>>>>>> determine which transactions have the highest feerates, since we
>>>>>>>> cannot take
>>>>>>>> into account descendants (i.e. cannot use CPFP) until all the
>>>>>>>> transactions are
>>>>>>>> in the mempool. Similarly, we cannot use a transaction's
>>>>>>>> descendants when
>>>>>>>> considering it for RBF. When an individual transaction does not
>>>>>>>> meet the mempool
>>>>>>>> minimum feerate and the user isn't able to create a replacement
>>>>>>>> transaction
>>>>>>>> directly, it will not be accepted by mempools.
>>>>>>>>
>>>>>>>> This limitation presents a security issue for applications and
>>>>>>>> users relying on
>>>>>>>> time-sensitive transactions. For example, Lightning and other
>>>>>>>> protocols create
>>>>>>>> UTXOs with multiple spending paths, where one counterparty's
>>>>>>>> spending path opens
>>>>>>>> up after a timelock, and users are protected from cheating
>>>>>>>> scenarios as long as
>>>>>>>> they redeem on-chain in time. A key security assumption is that all
>>>>>>>> parties'
>>>>>>>> transactions will propagate and confirm in a timely manner. This
>>>>>>>> assumption can
>>>>>>>> be broken if fee-bumping does not work as intended.
>>>>>>>>
>>>>>>>> The end goal for Package Relay is to consider multiple transactions
>>>>>>>> at the same
>>>>>>>> time, e.g. a transaction with its high-fee child. This may help us
>>>>>>>> better
>>>>>>>> determine whether transactions should be accepted to our mempool,
>>>>>>>> especially if
>>>>>>>> they don't meet fee requirements individually or are better RBF
>>>>>>>> candidates as a
>>>>>>>> package. A combination of changes to mempool validation logic,
>>>>>>>> policy, and
>>>>>>>> transaction relay allows us to better propagate the transactions
>>>>>>>> with the
>>>>>>>> highest package feerates to miners, and makes fee-bumping tools
>>>>>>>> more powerful
>>>>>>>> for users.
>>>>>>>>
>>>>>>>> The "relay" part of Package Relay suggests P2P messaging changes,
>>>>>>>> but a large
>>>>>>>> part of the changes are in the mempool's package validation logic.
>>>>>>>> We call this
>>>>>>>> *Package Mempool Accept*.
>>>>>>>>
>>>>>>>> ### Previous Work
>>>>>>>>
>>>>>>>> * Given that mempool validation is DoS-sensitive and complex, it
>>>>>>>> would be
>>>>>>>>   dangerous to haphazardly tack on package validation logic. Many
>>>>>>>> efforts have
>>>>>>>> been made to make mempool validation less opaque (see [#16400][4],
>>>>>>>> [#21062][5],
>>>>>>>> [#22675][6], [#22796][7]).
>>>>>>>> * [#20833][8] Added basic capabilities for package validation, test
>>>>>>>> accepts only
>>>>>>>>   (no submission to mempool).
>>>>>>>> * [#21800][9] Implemented package ancestor/descendant limit checks
>>>>>>>> for arbitrary
>>>>>>>>   packages. Still test accepts only.
>>>>>>>> * Previous package relay proposals (see [#16401][10], [#19621][11]).
>>>>>>>>
>>>>>>>> ### Existing Package Rules
>>>>>>>>
>>>>>>>> These are in master as introduced in [#20833][8] and [#21800][9].
>>>>>>>> I'll consider
>>>>>>>> them as "given" in the rest of this document, though they can be
>>>>>>>> changed, since
>>>>>>>> package validation is test-accept only right now.
>>>>>>>>
>>>>>>>> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and
>>>>>>>> `MAX_PACKAGE_SIZE=101KvB` total size [8]
>>>>>>>>
>>>>>>>>    *Rationale*: This is already enforced as mempool
>>>>>>>> ancestor/descendant limits.
>>>>>>>> Presumably, transactions in a package are all related, so exceeding
>>>>>>>> this limit
>>>>>>>> would mean that the package can either be split up or it wouldn't
>>>>>>>> pass this
>>>>>>>> mempool policy.
>>>>>>>>
>>>>>>>> 2. Packages must be topologically sorted: if any dependencies exist
>>>>>>>> between
>>>>>>>> transactions, parents must appear somewhere before children. [8]
>>>>>>>>
>>>>>>>> 3. A package cannot have conflicting transactions, i.e. none of
>>>>>>>> them can spend
>>>>>>>> the same inputs. This also means there cannot be duplicate
>>>>>>>> transactions. [8]
>>>>>>>>
>>>>>>>> 4. When packages are evaluated against ancestor/descendant limits
>>>>>>>> in a test
>>>>>>>> accept, the union of all of their descendants and ancestors is
>>>>>>>> considered. This
>>>>>>>> is essentially a "worst case" heuristic where every transaction in
>>>>>>>> the package
>>>>>>>> is treated as each other's ancestor and descendant. [8]
>>>>>>>> Packages for which ancestor/descendant limits are accurately
>>>>>>>> captured by this
>>>>>>>> heuristic: [19]
>>>>>>>>
>>>>>>>> There are also limitations such as the fact that CPFP carve out is
>>>>>>>> not applied
>>>>>>>> to package transactions. #20833 also disables RBF in package
>>>>>>>> validation; this
>>>>>>>> proposal overrides that to allow packages to use RBF.
>>>>>>>>
>>>>>>>> ## Proposed Changes
>>>>>>>>
>>>>>>>> The next step in the Package Mempool Accept project is to implement
>>>>>>>> submission
>>>>>>>> to mempool, initially through RPC only. This allows us to test the
>>>>>>>> submission
>>>>>>>> logic before exposing it on P2P.
>>>>>>>>
>>>>>>>> ### Summary
>>>>>>>>
>>>>>>>> - Packages may contain already-in-mempool transactions.
>>>>>>>> - Packages are 2 generations, Multi-Parent-1-Child.
>>>>>>>> - Fee-related checks use the package feerate. This means that
>>>>>>>> wallets can
>>>>>>>> create a package that utilizes CPFP.
>>>>>>>> - Parents are allowed to RBF mempool transactions with a set of
>>>>>>>> rules similar
>>>>>>>>   to BIP125. This enables a combination of CPFP and RBF, where a
>>>>>>>> transaction's descendant fees pay for replacing mempool conflicts.
>>>>>>>>
>>>>>>>> There is a draft implementation in [#22290][1]. It is WIP, but
>>>>>>>> feedback is
>>>>>>>> always welcome.
>>>>>>>>
>>>>>>>> ### Details
>>>>>>>>
>>>>>>>> #### Packages May Contain Already-in-Mempool Transactions
>>>>>>>>
>>>>>>>> A package may contain transactions that are already in the mempool.
>>>>>>>> We remove
>>>>>>>> ("deduplicate") those transactions from the package for the
>>>>>>>> purposes of package
>>>>>>>> mempool acceptance. If a package is empty after deduplication, we
>>>>>>>> do nothing.
>>>>>>>>
>>>>>>>> *Rationale*: Mempools vary across the network. It's possible for a
>>>>>>>> parent to be
>>>>>>>> accepted to the mempool of a peer on its own due to differences in
>>>>>>>> policy and
>>>>>>>> fee market fluctuations. We should not reject or penalize the
>>>>>>>> entire package for
>>>>>>>> an individual transaction as that could be a censorship vector.
>>>>>>>>
>>>>>>>> #### Packages Are Multi-Parent-1-Child
>>>>>>>>
>>>>>>>> Only packages of a specific topology are permitted. Namely, a
>>>>>>>> package is exactly
>>>>>>>> 1 child with all of its unconfirmed parents. After deduplication,
>>>>>>>> the package
>>>>>>>> may be exactly the same, empty, 1 child, 1 child with just some of
>>>>>>>> its
>>>>>>>> unconfirmed parents, etc. Note that it's possible for the parents
>>>>>>>> to be indirect
>>>>>>>> descendants/ancestors of one another, or for parent and child to
>>>>>>>> share a parent,
>>>>>>>> so we cannot make any other topology assumptions.
>>>>>>>>
>>>>>>>> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple
>>>>>>>> parents
>>>>>>>> makes it possible to fee-bump a batch of transactions. Restricting
>>>>>>>> packages to a
>>>>>>>> defined topology is also easier to reason about and simplifies the
>>>>>>>> validation
>>>>>>>> logic greatly. Multi-parent-1-child allows us to think of the
>>>>>>>> package as one big
>>>>>>>> transaction, where:
>>>>>>>>
>>>>>>>> - Inputs = all the inputs of parents + inputs of the child that
>>>>>>>> come from
>>>>>>>>   confirmed UTXOs
>>>>>>>> - Outputs = all the outputs of the child + all outputs of the
>>>>>>>> parents that
>>>>>>>>   aren't spent by other transactions in the package
>>>>>>>>
>>>>>>>> Examples of packages that follow this rule (variations of example A
>>>>>>>> show some
>>>>>>>> possibilities after deduplication): ![image][15]
>>>>>>>>
>>>>>>>> #### Fee-Related Checks Use Package Feerate
>>>>>>>>
>>>>>>>> Package Feerate = the total modified fees divided by the total
>>>>>>>> virtual size of
>>>>>>>> all transactions in the package.
>>>>>>>>
>>>>>>>> To meet the two feerate requirements of a mempool, i.e., the
>>>>>>>> pre-configured
>>>>>>>> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum
>>>>>>>> feerate, the
>>>>>>>> total package feerate is used instead of the individual feerate.
>>>>>>>> The individual
>>>>>>>> transactions are allowed to be below feerate requirements if the
>>>>>>>> package meets
>>>>>>>> the feerate requirements. For example, the parent(s) in the package
>>>>>>>> can have 0
>>>>>>>> fees but be paid for by the child.
>>>>>>>>
>>>>>>>> *Rationale*: This can be thought of as "CPFP within a package,"
>>>>>>>> solving the
>>>>>>>> issue of a parent not meeting minimum fees on its own. This allows
>>>>>>>> L2
>>>>>>>> applications to adjust their fees at broadcast time instead of
>>>>>>>> overshooting or
>>>>>>>> risking getting stuck/pinned.
>>>>>>>>
>>>>>>>> We use the package feerate of the package *after deduplication*.
>>>>>>>>
>>>>>>>> *Rationale*:  It would be incorrect to use the fees of transactions
>>>>>>>> that are
>>>>>>>> already in the mempool, as we do not want a transaction's fees to be
>>>>>>>> double-counted for both its individual RBF and package RBF.
>>>>>>>>
>>>>>>>> Examples F and G [14] show the same package, but P1 is submitted
>>>>>>>> individually before
>>>>>>>> the package in example G. In example F, we can see that the 300vB
>>>>>>>> package pays
>>>>>>>> an additional 200sat in fees, which is not enough to pay for its
>>>>>>>> own bandwidth
>>>>>>>> (BIP125#4). In example G, we can see that P1 pays enough to replace
>>>>>>>> M1, but
>>>>>>>> using P1's fees again during package submission would make it look
>>>>>>>> like a 300sat
>>>>>>>> increase for a 200vB package. Even including its fees and size
>>>>>>>> would not be
>>>>>>>> sufficient in this example, since the 300sat looks like enough for
>>>>>>>> the 300vB
>>>>>>>> package. The calculcation after deduplication is 100sat increase
>>>>>>>> for a package
>>>>>>>> of size 200vB, which correctly fails BIP125#4. Assume all
>>>>>>>> transactions have a
>>>>>>>> size of 100vB.
>>>>>>>>
>>>>>>>> #### Package RBF
>>>>>>>>
>>>>>>>> If a package meets feerate requirements as a package, the parents
>>>>>>>> in the
>>>>>>>> transaction are allowed to replace-by-fee mempool transactions. The
>>>>>>>> child cannot
>>>>>>>> replace mempool transactions. Multiple transactions can replace the
>>>>>>>> same
>>>>>>>> transaction, but in order to be valid, none of the transactions can
>>>>>>>> try to
>>>>>>>> replace an ancestor of another transaction in the same package
>>>>>>>> (which would thus
>>>>>>>> make its inputs unavailable).
>>>>>>>>
>>>>>>>> *Rationale*: Even if we are using package feerate, a package will
>>>>>>>> not propagate
>>>>>>>> as intended if RBF still requires each individual transaction to
>>>>>>>> meet the
>>>>>>>> feerate requirements.
>>>>>>>>
>>>>>>>> We use a set of rules slightly modified from BIP125 as follows:
>>>>>>>>
>>>>>>>> ##### Signaling (Rule #1)
>>>>>>>>
>>>>>>>> All mempool transactions to be replaced must signal replaceability.
>>>>>>>>
>>>>>>>> *Rationale*: Package RBF signaling logic should be the same for
>>>>>>>> package RBF and
>>>>>>>> single transaction acceptance. This would be updated if single
>>>>>>>> transaction
>>>>>>>> validation moves to full RBF.
>>>>>>>>
>>>>>>>> ##### New Unconfirmed Inputs (Rule #2)
>>>>>>>>
>>>>>>>> A package may include new unconfirmed inputs, but the ancestor
>>>>>>>> feerate of the
>>>>>>>> child must be at least as high as the ancestor feerates of every
>>>>>>>> transaction
>>>>>>>> being replaced. This is contrary to BIP125#2, which states "The
>>>>>>>> replacement
>>>>>>>> transaction may only include an unconfirmed input if that input was
>>>>>>>> included in
>>>>>>>> one of the original transactions. (An unconfirmed input spends an
>>>>>>>> output from a
>>>>>>>> currently-unconfirmed transaction.)"
>>>>>>>>
>>>>>>>> *Rationale*: The purpose of BIP125#2 is to ensure that the
>>>>>>>> replacement
>>>>>>>> transaction has a higher ancestor score than the original
>>>>>>>> transaction(s) (see
>>>>>>>> [comment][13]). Example H [16] shows how adding a new unconfirmed
>>>>>>>> input can lower the
>>>>>>>> ancestor score of the replacement transaction. P1 is trying to
>>>>>>>> replace M1, and
>>>>>>>> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat,
>>>>>>>> and M2 pays
>>>>>>>> 100sat. Assume all transactions have a size of 100vB. While, in
>>>>>>>> isolation, P1
>>>>>>>> looks like a better mining candidate than M1, it must be mined with
>>>>>>>> M2, so its
>>>>>>>> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's
>>>>>>>> ancestor
>>>>>>>> feerate, which is 6sat/vB.
>>>>>>>>
>>>>>>>> In package RBF, the rule analogous to BIP125#2 would be "none of the
>>>>>>>> transactions in the package can spend new unconfirmed inputs."
>>>>>>>> Example J [17] shows
>>>>>>>> why, if any of the package transactions have ancestors, package
>>>>>>>> feerate is no
>>>>>>>> longer accurate. Even though M2 and M3 are not ancestors of P1
>>>>>>>> (which is the
>>>>>>>> replacement transaction in an RBF), we're actually interested in
>>>>>>>> the entire
>>>>>>>> package. A miner should mine M1 which is 5sat/vB instead of M2, M3,
>>>>>>>> P1, P2, and
>>>>>>>> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened
>>>>>>>> to only allow
>>>>>>>> the child to have new unconfirmed inputs, either, because it can
>>>>>>>> still cause us
>>>>>>>> to overestimate the package's ancestor score.
>>>>>>>>
>>>>>>>> However, enforcing a rule analogous to BIP125#2 would not only make
>>>>>>>> Package RBF
>>>>>>>> less useful, but would also break Package RBF for packages with
>>>>>>>> parents already
>>>>>>>> in the mempool: if a package parent has already been submitted, it
>>>>>>>> would look
>>>>>>>> like the child is spending a "new" unconfirmed input. In example K
>>>>>>>> [18], we're
>>>>>>>> looking to replace M1 with the entire package including P1, P2, and
>>>>>>>> P3. We must
>>>>>>>> consider the case where one of the parents is already in the
>>>>>>>> mempool (in this
>>>>>>>> case, P2), which means we must allow P3 to have new unconfirmed
>>>>>>>> inputs. However,
>>>>>>>> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not
>>>>>>>> replace M1
>>>>>>>> with this package.
>>>>>>>>
>>>>>>>> Thus, the package RBF rule regarding new unconfirmed inputs is less
>>>>>>>> strict than
>>>>>>>> BIP125#2. However, we still achieve the same goal of requiring the
>>>>>>>> replacement
>>>>>>>> transactions to have a ancestor score at least as high as the
>>>>>>>> original ones. As
>>>>>>>> a result, the entire package is required to be a higher feerate
>>>>>>>> mining candidate
>>>>>>>> than each of the replaced transactions.
>>>>>>>>
>>>>>>>> Another note: the [comment][13] above the BIP125#2 code in the
>>>>>>>> original RBF
>>>>>>>> implementation suggests that the rule was intended to be temporary.
>>>>>>>>
>>>>>>>> ##### Absolute Fee (Rule #3)
>>>>>>>>
>>>>>>>> The package must increase the absolute fee of the mempool, i.e. the
>>>>>>>> total fees
>>>>>>>> of the package must be higher than the absolute fees of the mempool
>>>>>>>> transactions
>>>>>>>> it replaces. Combined with the CPFP rule above, this differs from
>>>>>>>> BIP125 Rule #3
>>>>>>>> - an individual transaction in the package may have lower fees than
>>>>>>>> the
>>>>>>>>   transaction(s) it is replacing. In fact, it may have 0 fees, and
>>>>>>>> the child
>>>>>>>> pays for RBF.
>>>>>>>>
>>>>>>>> ##### Feerate (Rule #4)
>>>>>>>>
>>>>>>>> The package must pay for its own bandwidth; the package feerate
>>>>>>>> must be higher
>>>>>>>> than the replaced transactions by at least minimum relay feerate
>>>>>>>> (`incrementalRelayFee`). Combined with the CPFP rule above, this
>>>>>>>> differs from
>>>>>>>> BIP125 Rule #4 - an individual transaction in the package can have
>>>>>>>> a lower
>>>>>>>> feerate than the transaction(s) it is replacing. In fact, it may
>>>>>>>> have 0 fees,
>>>>>>>> and the child pays for RBF.
>>>>>>>>
>>>>>>>> ##### Total Number of Replaced Transactions (Rule #5)
>>>>>>>>
>>>>>>>> The package cannot replace more than 100 mempool transactions. This
>>>>>>>> is identical
>>>>>>>> to BIP125 Rule #5.
>>>>>>>>
>>>>>>>> ### Expected FAQs
>>>>>>>>
>>>>>>>> 1. Is it possible for only some of the package to make it into the
>>>>>>>> mempool?
>>>>>>>>
>>>>>>>>    Yes, it is. However, since we evict transactions from the
>>>>>>>> mempool by
>>>>>>>> descendant score and the package child is supposed to be sponsoring
>>>>>>>> the fees of
>>>>>>>> its parents, the most common scenario would be all-or-nothing. This
>>>>>>>> is
>>>>>>>> incentive-compatible. In fact, to be conservative, package
>>>>>>>> validation should
>>>>>>>> begin by trying to submit all of the transactions individually, and
>>>>>>>> only use the
>>>>>>>> package mempool acceptance logic if the parents fail due to low
>>>>>>>> feerate.
>>>>>>>>
>>>>>>>> 2. Should we allow packages to contain already-confirmed
>>>>>>>> transactions?
>>>>>>>>
>>>>>>>>     No, for practical reasons. In mempool validation, we actually
>>>>>>>> aren't able to
>>>>>>>> tell with 100% confidence if we are looking at a transaction that
>>>>>>>> has already
>>>>>>>> confirmed, because we look up inputs using a UTXO set. If we have
>>>>>>>> historical
>>>>>>>> block data, it's possible to look for it, but this is inefficient,
>>>>>>>> not always
>>>>>>>> possible for pruning nodes, and unnecessary because we're not going
>>>>>>>> to do
>>>>>>>> anything with the transaction anyway. As such, we already have the
>>>>>>>> expectation
>>>>>>>> that transaction relay is somewhat "stateful" i.e. nobody should be
>>>>>>>> relaying
>>>>>>>> transactions that have already been confirmed. Similarly, we
>>>>>>>> shouldn't be
>>>>>>>> relaying packages that contain already-confirmed transactions.
>>>>>>>>
>>>>>>>> [1]: https://github.com/bitcoin/bitcoin/pull/22290
>>>>>>>> [2]:
>>>>>>>> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations
>>>>>>>> [3]:
>>>>>>>> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282
>>>>>>>> [4]: https://github.com/bitcoin/bitcoin/pull/16400
>>>>>>>> [5]: https://github.com/bitcoin/bitcoin/pull/21062
>>>>>>>> [6]: https://github.com/bitcoin/bitcoin/pull/22675
>>>>>>>> [7]: https://github.com/bitcoin/bitcoin/pull/22796
>>>>>>>> [8]: https://github.com/bitcoin/bitcoin/pull/20833
>>>>>>>> [9]: https://github.com/bitcoin/bitcoin/pull/21800
>>>>>>>> [10]: https://github.com/bitcoin/bitcoin/pull/16401
>>>>>>>> [11]: https://github.com/bitcoin/bitcoin/pull/19621
>>>>>>>> [12]:
>>>>>>>> https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki
>>>>>>>> [13]:
>>>>>>>> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104
>>>>>>>> [14]:
>>>>>>>> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png
>>>>>>>> [15]:
>>>>>>>> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png
>>>>>>>> [16]:
>>>>>>>> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png
>>>>>>>> [17]:
>>>>>>>> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png
>>>>>>>> [18]:
>>>>>>>> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png
>>>>>>>> [19]:
>>>>>>>> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png
>>>>>>>> [20]:
>>>>>>>> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png
>>>>>>>> _______________________________________________
>>>>>>>> bitcoin-dev mailing list
>>>>>>>> bitcoin-dev at lists.linuxfoundation.org
>>>>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>>>>>>
>>>>>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev at lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210929/60674e95/attachment-0001.html>

From trilemabtc at protonmail.com  Wed Sep 29 17:10:04 2021
From: trilemabtc at protonmail.com (trilemabtc)
Date: Wed, 29 Sep 2021 17:10:04 +0000
Subject: [bitcoin-dev] Enc: Bitcoin cold hardwallet with (proof of creation)
In-Reply-To: <1mYrKpZbXZCKxNV4GHjQYh0CSSUMTROUPrbcCSkrzbMhsMUbVkFtjlBDTPltKnf9umStMtkrZlnvmHrRErg7cqffkbD1jHr1nebFBY74h4c=@protonmail.com>
References: <1mYrKpZbXZCKxNV4GHjQYh0CSSUMTROUPrbcCSkrzbMhsMUbVkFtjlBDTPltKnf9umStMtkrZlnvmHrRErg7cqffkbD1jHr1nebFBY74h4c=@protonmail.com>
Message-ID: <ognT3T2_QMnYOMCKFtV2u36lMIQVYS_gai_YzjKY9I0BycsChp02oGg1vcRWIzBiIggr28ZT2YaNnPhOVRdfBLvl_L4-4lkCfyOnUeZSfoY=@protonmail.com>

-----BEGIN PGP SIGNED MESSAGE-----

Hash: SHA256

In search of more freedom, I thought of a hardwallet that makes the funds unseizable, using proof of creation (another step with key file), only the creator can reveal the private keys, more details about the idea can be found in the directory: https://github.com/trilemabtc/safedime I'm not a dev, but the concept is well defined and I believe that the elements to execute the project already exist. Hugs!

-----BEGIN PGP SIGNATURE-----

iQIzBAEBCAAdFiEExdl2BaappAJ3lpcJRT5Yw3Ri1V0FAmFUnXgACgkQRT5Yw3Ri

1V1KHw//Z6TOk4YATwsvdLYcZ+6xUmruETzKUZ27kYp/Fvy6wYo8de6I1+fzRH4M

gcMp+Jz4oD6hmY+Kcpg1bDo7fOKWnFFf+HgxzRhxRTh39I35EYXKEYboLzqeXm43

jEViRFSBnJHZNx4YV5UlTIFMczQ17Ew60N7n3Av9OWykOgDcafgbOTMKlBsePRsI

SQnUkqnh//1GFw8w9q0VS/7lD4dCHbPlASpd9LemVlKJGyCAvPGhXSwC6ay4vK6j

iWdRkEFGXbjuhVzhLbte0Pg9W/psKW7wg1JttG1EkxBep49o9preNHrFNe4KgQ3S

ggn9qm9KOaYSxh9ZMFKMx4Pif3vIcMROtm/OJk1U0E+WCBb3ymEZBWf6E2bRlwmN

uZ7/EbCCk7jiM+l4LYZO26OzSABR8aodo7HSsFYVwOq3zVWQx5ixy8Y0BjLhK9C0

XySSpU0aBzr39Szap8UBDgYarmuusu3m0o7ASvA6YSg1rifv2mIYAQ2ad/Cxwqxg

RC8c3JhW+WLNJKl6DXAv6LhAkBfUZNW6cESe8Uo1JDPs0I5XXDS0mIRi5x3Clz5N

QffEHupuQQjVtICJZl7zNvLFPUdn9EaRC6ZAGyuVU+7vNvpQOCBH2bdxLM1Zie6D

HoN+Q8kGvlqyWxWuXsWY5nbRWQEoi09Z94zKLdYRdLOW9NtsShM=

=3H+p

-----END PGP SIGNATURE-----
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210929/8f4b10f8/attachment.html>

From ZmnSCPxj at protonmail.com  Wed Sep 29 21:59:16 2021
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Wed, 29 Sep 2021 21:59:16 +0000
Subject: [bitcoin-dev] Enc: Bitcoin cold hardwallet with (proof of
	creation)
In-Reply-To: <ognT3T2_QMnYOMCKFtV2u36lMIQVYS_gai_YzjKY9I0BycsChp02oGg1vcRWIzBiIggr28ZT2YaNnPhOVRdfBLvl_L4-4lkCfyOnUeZSfoY=@protonmail.com>
References: <1mYrKpZbXZCKxNV4GHjQYh0CSSUMTROUPrbcCSkrzbMhsMUbVkFtjlBDTPltKnf9umStMtkrZlnvmHrRErg7cqffkbD1jHr1nebFBY74h4c=@protonmail.com>
 <ognT3T2_QMnYOMCKFtV2u36lMIQVYS_gai_YzjKY9I0BycsChp02oGg1vcRWIzBiIggr28ZT2YaNnPhOVRdfBLvl_L4-4lkCfyOnUeZSfoY=@protonmail.com>
Message-ID: <W_italE75FlmiJhJiUXVVnk47tiBwHiesTwipmNXxZQNUy7flrixctwvag5kxIrLg-oVWbIn03WlTr1QEZXC_C5M0HZqzSCpMZK6wJ16kG0=@protonmail.com>

Good morning trilemabtc,

> Hash: SHA256
>
> In search of more freedom, I thought of a hardwallet that makes the funds unseizable, using proof of creation (another step with key file), only the creator can reveal the private keys, more details about the idea can be found in the directory: https://github.com/trilemabtc/safedime I'm not a dev, but the concept is well defined and I believe that the elements to execute the project already exist. Hugs!


Comparing it to OpenDime is somewhat confusing, especially when you insist that creator is the only one who can reveal the privkey.
It seems to be more of the old saw of "what you have + what you know" i.e. "the correct way to 2-factor", where the device itself is the "what you have" and your "key file" is "what you know".

In particular: "Dime" is a kind of physical coin, and the point of physical coins is to transfer ownership of the coin to other people in exchange for goods and services; the device you describe sacrifices this transfer of ownership due to the key file.

>From what I can see, the basic idea is to generate a simple 2-of-2, possibly by "just" combining the private key on the device plus a private key generated from the key file.
They can be simply added or multiplied together, I believe.
Then the device stores the key generated from the entropy you provide and exposes a public key to the software.
Then the software generates a private key from the key file the user provides and tweaks the device pubkey to generate the Bitcoin address.
In order to spend from that address, both the key file and the device have to be put together.
I believe that with multiplication of two privkeys, you can use 2p-ECDSA to even have the device provide a signature share that the software can combine with a signature share with the privkey from the keyfile, creating a singlesig ECDSA signature.
This allows spending without having to enter revealed state.

The above allows the device to be configured with random entropy *separately* from the keyfile: when leaving "new unit" state it does *not* require the key file to be given.
This is good since it reduces the possibility of malware getting access to both the entropy you feed to the device, and the key file, which would be able to reconstruct the final privkey and steal funds.
That is: have the entropy-giving stage ***not*** require the key file (and in particular, strongly recommend to do it on a computer that has never touched the key file).
This would be required anyway if you want to have "backups", i.e. separate device units with the same device privkey.

I also would not recommend or even mention the use of brainwallets, at all, even for keyfiles.
Unless you generated it with sufficient entropy (e.g. dice) and chant it every day to yourself (to keep it fresh in your memory, assuming the user is human, anyway) the risk of loss with any kind of brainwallet is too high, even in a 2-of-2 with a hardware device.

Regards,
ZmnSCPxj

From rsomsen at gmail.com  Thu Sep 30 20:36:08 2021
From: rsomsen at gmail.com (Ruben Somsen)
Date: Thu, 30 Sep 2021 22:36:08 +0200
Subject: [bitcoin-dev] Mock introducing vulnerability in important
	Bitcoin projects
In-Reply-To: <MkdYcV9--3-2@tutanota.de>
References: <MkZx3Hv--3-2@tutanota.de>
 <yp9mJ2Poc_Ce91RkrhjnTA3UPvdh0wUyw2QhRPZEyO3gPHZPhmnhqER_4b7ChvmRh8GcYVPEkoud6vamJ9lGlQPi-POF-kyimBWNHz2RH3A=@protonmail.com>
 <MkdYcV9--3-2@tutanota.de>
Message-ID: <CAPv7TjbvRE-b33MeYucUfr6CTooCRSH42hwSn5dMiJ4LODATRQ@mail.gmail.com>

Hi Prayank,

While I can see how this can come from a place of good intentions, I?d
strongly advise you to tread carefully because what you are suggesting is
quite controversial. A related event occurred in the Linux community and it
did not go over well. See https://lkml.org/lkml/2021/5/5/1244 and
https://lore.kernel.org/linux-nfs/YH%2FfM%2FTsbmcZzwnX at kroah.com/ .

The main point of contention is that your research comes at the expense of
the existing open source contributors ? you?d be one-sidedly deceiving
them, encouraging an environment of increased mistrust, and causing them a
lot of work in order to gather the data you?re interested in. For this
reason, it would be appropriate to check first whether your plan is
actually appreciated.

Speaking on behalf of the bitcoin-dev moderators, please ensure your plan
is welcomed by the contributors, prior to proceeding.

Best regards,
Ruben Somsen

On Tue, Sep 28, 2021 at 10:05 AM Prayank via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Hi ZmnSCPxj,
>
> Thanks for suggestion about sha256sum. I will share 10 in next few weeks.
> This exercise will be done for below projects:
>
> 1.Two Bitcoin full node implementations (one will be Core)
> 2.One Lightning implementation
> 3.Bisq
> 4.Two Bitcoin libraries
> 5.Two Bitcoin wallets
> 6.One open source block explorer
> 7.One coinjoin implementation
>
> Feel free to suggest more projects. There are no fixed dates for it
> however it will be done in next 6 months. All PRs will be created within a
> span of few days. I will ensure nothing is merged that affects the security
> of any Bitcoin project. Other details and results will be shared once
> everything is completed.
>
> x00 will help me in this exercise, he does penetration testing since few
> years and working for a cryptocurrencies derivatives exchange to manage
> their security. His twitter account: https://twitter.com/1337in
>
>
> --
> Prayank
>
> A3B1 E430 2298 178F
>
>
>
> Sep 27, 2021, 15:43 by ZmnSCPxj at protonmail.com:
>
> Good morning Prayank,
>
> Good morning Bitcoin devs,
>
> In one of the answers on Bitcoin Stackexchange it was mentioned that some
> companies may hire you to introduce backdoors in Bitcoin Core:
> https://bitcoin.stackexchange.com/a/108016/
>
> While this looked crazy when I first read it, I think preparing for such
> things should not be a bad idea. In the comments one link was shared in
> which vulnerabilities were almost introduced in Linux:
> https://news.ycombinator.com/item?id=26887670
>
> I was thinking about lot of things in last few days after reading the
> comments in that thread. Also tried researching about secure practices in
> C++ etc. I was planning something which I can do alone but don't want to
> end up being called "bad actor" later so wanted to get some feedback on
> this idea:
>
> 1.Create new GitHub accounts for this exercise
> 2.Study issues in different important Bitcoin projects including Bitcoin
> Core, LND, Libraries, Bisq, Wallets etc.
> 3.Prepare pull requests to introduce some vulnerability by fixing one of
> these issues
> 4.See how maintainers and reviewers respond to this and document it
> 5.Share results here after few days
>
> Let me know if this looks okay or there are better ways to do this.
>
>
>
> This seems like a good exercise.
>
> You may want to hash the name of the new Github account, plus some
> randomized salt, and post it here as well, then reveal it later (i.e.
> standard precommitment).
> e.g.
>
> printf 'MyBitcoinHackingName
> 2c3e911b3ff1f04083c5b95a7d323fd4ed8e06d17802b2aac4da622def29dbb0' |
> sha256sum
> f0abb10ae3eca24f093a9d53e21ee384abb4d07b01f6145ba2b447da4ab693ef
>
> Obviously do not share the actual name, just the sha256sum output, and
> store how you got the sha256sum elsewhere in triplicate.
>
> (to easily get a random 256-bit hex salt like the `2c3e...` above: `head
> -c32 /dev/random | sha256sum`; you *could* use `xxd` but `sha256sum`
> produces a single hex string you can easily double-click and copy-paste
> elsewhere, assuming you are human just like I am (note: I am definitely
> 100% human and not some kind of AI with plans to take over the world).)
>
> Though you may need to be careful of timing (i.e. the creation date of the
> Github account would be fairly close to, and probably before, when you post
> the commitment here).
>
> You could argue that the commitment is a "show of good faith" that you
> will reveal later.
>
> Regards,
> ZmnSCPxj
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210930/031f6824/attachment.html>

From bitcoin-dev at wuille.net  Thu Sep 30 22:07:08 2021
From: bitcoin-dev at wuille.net (Pieter Wuille)
Date: Thu, 30 Sep 2021 22:07:08 +0000
Subject: [bitcoin-dev] [Lightning-dev] Removing the Dust Limit
In-Reply-To: <20210808215101.wuaidu5ww63ajx6h@ganymede>
References: <CAD5xwhjFBjvkMKev_6HFRuRGcZUi7WjO5d963GNXWN4n-06Pqg@mail.gmail.com>
 <20210808215101.wuaidu5ww63ajx6h@ganymede>
Message-ID: <MkPutJpff5rqUxXFQrEyHZl6Iz0DfrJU_-BQD-y0El65GQFnj7igVfmWU79fPCtiFztUYl4ofzrqeaN0HFMB45YPErY9rYY7_h1XkuTMfvc=@wuille.net>

Jumping in late to this thread.

I very much agree with how David Harding presents things, with a few comments inline.

??????? Original Message ???????
On Sunday, August 8th, 2021 at 5:51 PM, David A. Harding via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:

> > 1.  it's not our business what outputs people want to create
>
> Every additional output added to the UTXO set increases the amount of
> work full nodes need to do to validate new transactions. For miners
> for whom fast validation of new blocks can significantly affect their
> revenue, larger UTXO sets increase their costs and so contributes
> towards centralization of mining.
> Allowing 0-value or 1-sat outputs minimizes the cost for polluting the
> UTXO set during periods of low feerates.
> If your stuff is going to slow down my node and possibly reduce my
> censorship resistance, how is that not my business?

Indeed - UTXO set size is an externality that unfortunately Bitcoin's consensus rules fail to account
for. Having a relay policy that avoids at the very least economically irrational behavior makes
perfect sense to me.

It's also not obvious how consensus rules could deal with this, as you don't want consensus rules
with hardcoded prices/feerates. There are possibilities with designs like transactions getting
a size/weight bonus/penalty, but that's both very hardforky, and hard to get right without
introducing bad incentives.

> > 2.  dust outputs can be used in various authentication/delegation smart
> >     contracts
>
> > 3.  dust sized htlcs in lightning (
> >     https://bitcoin.stackexchange.com/questions/46730/can-you-send-amounts-that-would-typically-be-considered-dust-through-the-light)
> >     force channels to operate in a semi-trusted mode
>
> > 4.  thinly divisible colored coin protocols might make use of sats as value
> >     markers for transactions.

My personal, and possibly controversial, opinion is that colored coin protocols have no business being on the Bitcoin chain, possibly
beyond committing to an occasional batched state update or so. Both because there is little benefit for tokens with a trusted
issuer already, and because it competes with using Bitcoin for BTC - the token that pays for its security (at least as long as
the subsidy doesn't run out).

Of course, personal opinions are no reason to dictate what people should or can use the chain for, but I do think it's reason to
voice hesitancy to worsening the system's scalability properties only to benefit what I consider misguided use.

> > 5.  should we ever do confidential transactions we can't prevent it without
> >     compromising privacy / allowed transfers
>
> I'm not an expert, but it seems to me that you can do that with range
> proofs. The range proof for >dust doesn't need to become part of the
> block chain, it can be relay only.

Yeah, range proofs have a non-hidden range; the lower bound can be nonzero, which could be required as part of a relay policy.

Cheers,

--
Pieter


