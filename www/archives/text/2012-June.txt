From etotheipi at gmail.com  Sat Jun  2 15:40:27 2012
From: etotheipi at gmail.com (Alan Reiner)
Date: Sat, 02 Jun 2012 11:40:27 -0400
Subject: [Bitcoin-development] Full Clients in the future - Blockchain
	management
Message-ID: <4FCA33EB.5030706@gmail.com>

Devs,

I have decided to upgrade Armory's blockchain utilities, partly out of 
necessity due to a poor code decision I made before I even decided I was 
making a client.  In an effort to avoid such mistakes again, I want to 
do it "right" this time around, and realize that this is a good 
discussion for all the devs that will have to deal with this eventually...

The part I'm having difficulty with, is the idea that in a few years 
from now, it just may not be feasible to hold transactions 
file-/pointers/ in RAM, because even that would overwhelm standard RAM 
sizes.  Without any degree of blockchain compression, I see that the 
most general, scalable solution is probably a complicated one.

On the other hand, where this fails may be where we have already 
predicted that the network will have to split into "super-nodes" and 
"lite nodes."  In which case, this discussion is still a good one, but 
just directed more towards the super-nodes.  But, there may still be a 
point at which super-nodes don't have enough RAM to hold this data...

(1)  As for how small you can get the data:  my original idea was that 
the entire blockchain is stored on disk as blkXXXX.dat files.  I store 
all transactions as 10-byte "file-references."  10 bytes would be

     -- X in blkX.dat (2 bytes)
     -- Tx start byte (4 bytes)
     -- Tx size bytes (4 bytes)

The file-refs would be stored in a multimap indexed by the first 6 bytes 
of the tx-hash.  In this way, when I search the multimap, I potentially 
get a list of file-refs, and I might have to retrieve a couple of tx 
from disk before finding the right one, but it would be a good trade-off 
compared to storing all 32 bytes (that's assuming that multimap nodes 
don't have too much overhead).

But even with this, if there are 1,000,000,000 transactions in the 
blockchain, each node is probably 48 bytes  (16 bytes + map/container 
overhead), then you're talking about 48 GB to track all the data in 
RAM.  mmap() may help here, but I'm not sure it's the right solution

(2) What other ways are there, besides some kind of blockchain 
compression, to maintain a multi-terabyte blockchain, assuming that 
storing references to each tx would overwhelm available RAM?   Maybe 
that assumption isn't necessary, but I think it prepares for the worst.

Or maybe I'm too narrow in my focus.  How do other people envision this 
will be handled in the future.  I've heard so many vague notions of 
"well we could do /this/ or /that/, or it wouldn't be hard to do /that/" 
but I haven't heard any serious proposals for it.  And while I believe 
that blockchain compression will become ubiquitous in the future, not 
everyone believes that, and there will undoubtedly be users/devs that 
/want/ to maintain everything under all circumstances.

-Alan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120602/e99d820c/attachment.html>

From etotheipi at gmail.com  Sat Jun  2 17:15:26 2012
From: etotheipi at gmail.com (Alan Reiner)
Date: Sat, 02 Jun 2012 13:15:26 -0400
Subject: [Bitcoin-development] Fwd: Re: Full Clients in the future -
 Blockchain management
In-Reply-To: <89639546-2608-44B2-AF84-E600A33CB514@jrbobdobbs.org>
References: <89639546-2608-44B2-AF84-E600A33CB514@jrbobdobbs.org>
Message-ID: <4FCA4A2E.3000900@gmail.com>

(response from Doug forwarded below)

It's a very good point.  I have no experience with database engines.  I 
had assumed that in most cases, data could always be indexed in RAM, and 
wanted to know where to go when that's not the case.  I will look into 
that solution, further.

I am very interested to solve the blockchain compression problem, and 
think I've got a great way that will not just compress the blockchain, 
but improve the network for lightweight clients.  But the idea is not 
fully formed yet, so I was holding off...



-------- Original Message --------
Subject: 	Re: [Bitcoin-development] Full Clients in the future - 
Blockchain management
Date: 	Sat, 2 Jun 2012 12:07:44 -0500
From: 	Douglas Huff <mith at jrbobdobbs.org>
To: 	Alan Reiner <etotheipi at gmail.com>



I think you're trying to solve something a little out of scope, really. 
Most of the issues aren't really issues for other clients using 
established storage mechanisms (bdb,SQLite,etc) and they're using them 
precisely because this is a problem that people have been working on for 
decades and a poor candidate for reinvention.

If you really look at what you're proposing it's fundamentally how bdb 
operates except your indexing format is usage domain specific and you're 
in charge of all the resource management semantics. While at the same 
time you'll be missing many of the newer features that make working 
with/recovering/diagnosing issues in the storage layer easier.

If you're really wanting to talk about pruning methods to prevent the 
massive amount of duplicated; but no longer pertinent, data that's a 
different story and please continue. :)

-- 
Douglas Huff

On Jun 2, 2012, at 10:40, Alan Reiner <etotheipi at gmail.com 
<mailto:etotheipi at gmail.com>> wrote:

> Devs,
>
> I have decided to upgrade Armory's blockchain utilities, partly out of 
> necessity due to a poor code decision I made before I even decided I 
> was making a client.  In an effort to avoid such mistakes again, I 
> want to do it "right" this time around, and realize that this is a 
> good discussion for all the devs that will have to deal with this 
> eventually...
>
> The part I'm having difficulty with, is the idea that in a few years 
> from now, it just may not be feasible to hold transactions 
> file-/pointers/ in RAM, because even that would overwhelm standard RAM 
> sizes.  Without any degree of blockchain compression, I see that the 
> most general, scalable solution is probably a complicated one.
>
> On the other hand, where this fails may be where we have already 
> predicted that the network will have to split into "super-nodes" and 
> "lite nodes."  In which case, this discussion is still a good one, but 
> just directed more towards the super-nodes.  But, there may still be a 
> point at which super-nodes don't have enough RAM to hold this data...
>
> (1)  As for how small you can get the data:  my original idea was that 
> the entire blockchain is stored on disk as blkXXXX.dat files.  I store 
> all transactions as 10-byte "file-references."  10 bytes would be
>
>     -- X in blkX.dat (2 bytes)
>     -- Tx start byte (4 bytes)
>     -- Tx size bytes (4 bytes)
>
> The file-refs would be stored in a multimap indexed by the first 6 
> bytes of the tx-hash.  In this way, when I search the multimap, I 
> potentially get a list of file-refs, and I might have to retrieve a 
> couple of tx from disk before finding the right one, but it would be a 
> good trade-off compared to storing all 32 bytes (that's assuming that 
> multimap nodes don't have too much overhead).
>
> But even with this, if there are 1,000,000,000 transactions in the 
> blockchain, each node is probably 48 bytes  (16 bytes + map/container 
> overhead), then you're talking about 48 GB to track all the data in 
> RAM.  mmap() may help here, but I'm not sure it's the right solution
>
> (2) What other ways are there, besides some kind of blockchain 
> compression, to maintain a multi-terabyte blockchain, assuming that 
> storing references to each tx would overwhelm available RAM?   Maybe 
> that assumption isn't necessary, but I think it prepares for the worst.
>
> Or maybe I'm too narrow in my focus.  How do other people envision 
> this will be handled in the future.  I've heard so many vague notions 
> of "well we could do /this/ or /that/, or it wouldn't be hard to do 
> /that/" but I haven't heard any serious proposals for it.  And while I 
> believe that blockchain compression will become ubiquitous in the 
> future, not everyone believes that, and there will undoubtedly be 
> users/devs that /want/ to maintain everything under all circumstances.
>
> -Alan
> ------------------------------------------------------------------------------
> Live Security Virtual Conference
> Exclusive live event will cover all the ways today's security and
> threat landscape has changed and how IT managers can respond. Discussions
> will include endpoint security, mobile security and the latest in malware
> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
> _______________________________________________
> Bitcoin-development mailing list
> Bitcoin-development at lists.sourceforge.net 
> <mailto:Bitcoin-development at lists.sourceforge.net>
> https://lists.sourceforge.net/lists/listinfo/bitcoin-development
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120602/e8fd5daf/attachment.html>

From luke at dashjr.org  Sun Jun  3 00:52:14 2012
From: luke at dashjr.org (Luke-Jr)
Date: Sun, 3 Jun 2012 00:52:14 +0000
Subject: [Bitcoin-development] Defeating the block withholding attack
Message-ID: <201206030052.17128.luke@dashjr.org>

Analysis, comments, constructive criticism, etc welcome for the following:

==Background==
At present, an attacker can harm a pool by intentionally NOT submitting shares 
that are also valid blocks. All pools are vulnerable to this attack, whether 
centralized or decentralized and regardless of reward system used. The 
attack's effectiveness is proportional to ratio of the attacker's hashrate to 
the rest of the pool.

There are obvious solutions that can be used to defeat this attack on 
centralized pools. For example, including a secret in the coinbase transaction 
that is accepted by the network as a partial preimage proof-of-work. All these 
solutions require changes to Bitcoin's proof-of-work acceptance terms, and 
since centralized pools can be harmful to the network's security, these rule 
changes are not likely to gain enough acceptance among the greater Bitcoin 
community.

==Proposed Solution==
Please comment on the viability of this new proof-of-work algorithm, which I 
think should be viable for even decentralized pools:

Blocks are accepted at a lower difficulty N (choosable by the pool; eg, the 
share difficulty) iff they are submitted with a candidate for the next block 
and SHA256(SHA256(NewBlockHash + NextBlockCandidateHash)) meets difficulty M.
The relationship between M and N must be comparable to the normal network 
difficulty; details on the specifics of this can be figured out later, ideally 
by someone more qualified than me. M and N must be chosen prior to searching 
for the block: it should be safe to steal some always-zero bytes from the 
prevblock header for this.

This algorithm should guarantee that every share has an equal chance of being 
a valid block at the time it is found, and that which ones are actually blocks 
cannot be known until the subsequent block is found. Thus, attackers have no 
way to identify which shares to withhold even while they have full knowledge 
of the shares/blocks themselves.

==Backward Compatibility==
Obviously, this change creates a hard-fork in the blockchain. I propose that 
if it solves the block withholding risk, the gain is sufficient that the 
community may approve a hard-fork to take place 1-2 years from consensus.

Since mining continues to use a double-SHA256 on a fixed 80 byte header, 
existing miners, FPGAs, etc should work unmodified. Poolservers will need to 
adapt significantly.



From wbl at uchicago.edu  Sun Jun  3 03:40:41 2012
From: wbl at uchicago.edu (Watson Ladd)
Date: Sat, 2 Jun 2012 22:40:41 -0500
Subject: [Bitcoin-development] Fwd: Defeating the block withholding attack
In-Reply-To: <CACsn0c=+xrVvGMAkPZffpVhRcAc09RuOW7LeOwi0TOD88VbuqQ@mail.gmail.com>
References: <201206030052.17128.luke@dashjr.org>
	<CACsn0c=+xrVvGMAkPZffpVhRcAc09RuOW7LeOwi0TOD88VbuqQ@mail.gmail.com>
Message-ID: <CACsn0cmvpY49R_nt=LhPaiOSFkL=Zywd-ZkUtOR9BK=hr2h1DQ@mail.gmail.com>

On Sat, Jun 2, 2012 at 7:52 PM, Luke-Jr <luke at dashjr.org> wrote:
> Analysis, comments, constructive criticism, etc welcome for the following:
>
> ==Background==
> At present, an attacker can harm a pool by intentionally NOT submitting shares
> that are also valid blocks. All pools are vulnerable to this attack, whether
> centralized or decentralized and regardless of reward system used. The
> attack's effectiveness is proportional to ratio of the attacker's hashrate to
> the rest of the pool.
This attack has an obvious signature: getting outworked on the same
block as the pool was trying to verify, and always by the same person.
>
> There are obvious solutions that can be used to defeat this attack on
> centralized pools. For example, including a secret in the coinbase transaction
> that is accepted by the network as a partial preimage proof-of-work. All these
> solutions require changes to Bitcoin's proof-of-work acceptance terms, and
> since centralized pools can be harmful to the network's security, these rule
> changes are not likely to gain enough acceptance among the greater Bitcoin
> community.
>
> ==Proposed Solution==
> Please comment on the viability of this new proof-of-work algorithm, which I
> think should be viable for even decentralized pools:
>
> Blocks are accepted at a lower difficulty N (choosable by the pool; eg, the
> share difficulty) iff they are submitted with a candidate for the next block
> and SHA256(SHA256(NewBlockHash + NextBlockCandidateHash)) meets difficulty M.
> The relationship between M and N must be comparable to the normal network
> difficulty; details on the specifics of this can be figured out later, ideally
> by someone more qualified than me. M and N must be chosen prior to searching
> for the block: it should be safe to steal some always-zero bytes from the
> prevblock header for this.
So the goal is to prevent the attacker double-dipping by submitting
cycles to the pool, which if he
found a correct answer he could submit himself. I don't see how this
does that: if he finds a valid
block he finds a valid block. Only if the operator has a secret is
this prevented.
>
> This algorithm should guarantee that every share has an equal chance of being
> a valid block at the time it is found, and that which ones are actually blocks
> cannot be known until the subsequent block is found. Thus, attackers have no
> way to identify which shares to withhold even while they have full knowledge
> of the shares/blocks themselves.
This further delays the finalization of a transaction. That's not a good thing.
>
> ==Backward Compatibility==
> Obviously, this change creates a hard-fork in the blockchain. I propose that
> if it solves the block withholding risk, the gain is sufficient that the
> community may approve a hard-fork to take place 1-2 years from consensus.
>
> Since mining continues to use a double-SHA256 on a fixed 80 byte header,
> existing miners, FPGAs, etc should work unmodified. Poolservers will need to
> adapt significantly.
>
> ------------------------------------------------------------------------------
> Live Security Virtual Conference
> Exclusive live event will cover all the ways today's security and
> threat landscape has changed and how IT managers can respond. Discussions
> will include endpoint security, mobile security and the latest in malware
> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
> _______________________________________________
> Bitcoin-development mailing list
> Bitcoin-development at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/bitcoin-development



--
"Those who would give up Essential Liberty to purchase a little
Temporary Safety deserve neither? Liberty nor Safety."
-- Benjamin Franklin


-- 
"Those who would give up Essential Liberty to purchase a little
Temporary Safety deserve neither? Liberty nor Safety."
-- Benjamin Franklin



From mike at plan99.net  Sun Jun  3 14:17:42 2012
From: mike at plan99.net (Mike Hearn)
Date: Sun, 3 Jun 2012 16:17:42 +0200
Subject: [Bitcoin-development] Full Clients in the future - Blockchain
	management
In-Reply-To: <4FCA33EB.5030706@gmail.com>
References: <4FCA33EB.5030706@gmail.com>
Message-ID: <CANEZrP2sDD0MhO5xHB==N-oCtChXJwDBa79X07MENFa_ZjqpFQ@mail.gmail.com>

Yeah, for actually storing transactions the approach Satoshi uses of
relying on a database engine makes sense and is what the code already does,
so I'm not sure why this is a problem.

The real problem with Satoshis code for scaling down to smaller devices
(and one day desktops too) is the need to store all the chain headers in
RAM. BitcoinJ avoids this but just creates more problems for itself in
other places, partly because we also try to avoid a database engine
(read/write traffic on phones can be insanely expensive, especially on
older ones, and so sqlite is known to be a serious cause of performance
pain on android apps).
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120603/639427f3/attachment.html>

From peter at coinlab.com  Mon Jun  4 01:43:42 2012
From: peter at coinlab.com (Peter Vessenes)
Date: Sun, 3 Jun 2012 21:43:42 -0400
Subject: [Bitcoin-development] Defeating the block withholding attack
In-Reply-To: <201206030052.17128.luke@dashjr.org>
References: <201206030052.17128.luke@dashjr.org>
Message-ID: <CAMGNxUu7SbnfpU8L+qp7KUmFLSU=VqcYGu2GhzRaYhkTT3Nz7A@mail.gmail.com>

On Sat, Jun 2, 2012 at 8:52 PM, Luke-Jr <luke at dashjr.org> wrote:

> Analysis, comments, constructive criticism, etc welcome for the following:
>
> ==Background==
> At present, an attacker can harm a pool by intentionally NOT submitting
> shares
> that are also valid blocks. All pools are vulnerable to this attack,
> whether
> centralized or decentralized and regardless of reward system used. The
> attack's effectiveness is proportional to ratio of the attacker's hashrate
> to
> the rest of the pool.
>
>
I'm unclear on the economics of this attack; we spent a bit of time talking
about it a few months ago at CoinLab and decided not to worry about it for
right now.

Does it have asymmetric payoff for an attacker, that is, over time does it
pay them more to spend their hashes attacking than just mining?

My gut is that it pays less well than mining, meaning I think this is
likely a small problem in the aggregate, and certainly not something we
should try and fork the blockchain for until there's real pain.

Consider, for instance, whether it pays better than just mining bitcoins
and spending those on 'bonuses' for getting users to switch from a pool you
hate.

Watson, I don't believe the attack signature you mention is a factor here,
since the pool controls the merkle, only that pool will benefit from block
submission. The nonce / coinbase combo is worthless otherwise, and so this
attack is just in brief "get lucky, but don't submit."

So, can anyone enlighten me as to some actual estimates of badness for this
attack?

Peter
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120603/40300cbd/attachment.html>

From luke at dashjr.org  Mon Jun  4 02:04:55 2012
From: luke at dashjr.org (Luke-Jr)
Date: Mon, 4 Jun 2012 02:04:55 +0000
Subject: [Bitcoin-development] Defeating the block withholding attack
In-Reply-To: <CAMGNxUu7SbnfpU8L+qp7KUmFLSU=VqcYGu2GhzRaYhkTT3Nz7A@mail.gmail.com>
References: <201206030052.17128.luke@dashjr.org>
	<CAMGNxUu7SbnfpU8L+qp7KUmFLSU=VqcYGu2GhzRaYhkTT3Nz7A@mail.gmail.com>
Message-ID: <201206040204.57503.luke@dashjr.org>

On Monday, June 04, 2012 1:43:42 AM Peter Vessenes wrote:
> Does it have asymmetric payoff for an attacker, that is, over time does it
> pay them more to spend their hashes attacking than just mining?

That depends on the pool's reward scheme. Some complicated forms are capable 
of getting "bonus" earnings out of the pool. Under all systems, the attacker 
at least gains the "hurt the pool" benefit. Given the frequency of DDoS 
attacks on pools, it is clear there are people who will even pay for attacks 
that provide no other benefit than harming pools. Under all systems, the 
attacker doesn't lose out in any significant way.

> My gut is that it pays less well than mining, meaning I think this is
> likely a small problem in the aggregate, and certainly not something we
> should try and fork the blockchain for until there's real pain.

If we wait until there's real pain, it will be a painful fork. If we plan it 
1-2 years out, people have time to upgrade on their own before it breaks.

> Consider, for instance, whether it pays better than just mining bitcoins
> and spending those on 'bonuses' for getting users to switch from a pool you
> hate.

With this attack, attackers can hurt the pool's "luck factor" *and* spend the 
bitcoins they earn to bribe users away.



From luke at dashjr.org  Mon Jun  4 21:05:25 2012
From: luke at dashjr.org (Luke-Jr)
Date: Mon, 4 Jun 2012 21:05:25 +0000
Subject: [Bitcoin-development] Defeating the block withholding attack
In-Reply-To: <CAErK2CjpSb=Rb+evWg+_fs0brBfTDe3_HM1z-an0picb_8tzpQ@mail.gmail.com>
References: <201206030052.17128.luke@dashjr.org>
	<201206040204.57503.luke@dashjr.org>
	<CAErK2CjpSb=Rb+evWg+_fs0brBfTDe3_HM1z-an0picb_8tzpQ@mail.gmail.com>
Message-ID: <201206042105.27064.luke@dashjr.org>

On Monday, June 04, 2012 8:49:48 PM Mike Koss wrote:
> As I understand the attack, the attacker gets compensated for the shares
> they earn, but the pool will be denied any valid blocks found.  The
> attacker DOES NOT have access to the Bitcoins earned in the unreported
> block (only the mining pool has access to the Coinbase address and
> transactions in the block).

With decentralized pools, the attacker does have access to the block, and can 
potentially submit it to the Bitcoin network directly bypassing the pool if it 
benefits them to do so.

> So it's a zero-net-cost attack for the attacker (but no chance of making a
> profit) to hurt the pool operator. 

Because of the above, there is a possibility an attacker can make a profit.

> The only way to detect such an attack now is to look for "unlucky" miners;
> at the current difficulty, you can't detect this cheat until many millions
> of shares have been earned w/o a qualifying block.  Since an attacker can
> also create many fake identities, they can avoid detection indefinitely by
> abandoning each account after a million earned shares.

There are other modes of detection, but nobody has bothered to implement them 
since attackers can easily do a simple workaround in an arms race.

> I don't understand your proposal for fixing this.  You would have to come
> up with a scheme where:
> 
> - The miner can detect a qualifying hash to earn a share.
> - Not be able to tell if the hash is for a valid block.

With my proposal, miners can find shares, but won't know if it's a valid block 
until the subsequent block is also found (that subsequent block might not end 
up being a real block in the big picture).

> The way I would do this is to have a secret part (not shared with the
> miners) of a block that is part of the merkle hash, which is also used in a
> secondary hash.  Difficulty is then divide into two parts: the first,
> solved by the miner (earning a "share" - e.g., 1 in 4 Billion hashes).  And
> a second, solved by the pool (1 in Difficulty shares).  A valid block would
> have to exhibit a valid Share Hash AND a valid Pool Hash in order to be
> accepted.

This only works for centralized pools, which are contrary to the health of the 
Bitcoin network. Decentralized pools cannot have a secret.



From mike at coinlab.com  Mon Jun  4 20:49:48 2012
From: mike at coinlab.com (Mike Koss)
Date: Mon, 4 Jun 2012 13:49:48 -0700
Subject: [Bitcoin-development] Defeating the block withholding attack
In-Reply-To: <201206040204.57503.luke@dashjr.org>
References: <201206030052.17128.luke@dashjr.org>
	<CAMGNxUu7SbnfpU8L+qp7KUmFLSU=VqcYGu2GhzRaYhkTT3Nz7A@mail.gmail.com>
	<201206040204.57503.luke@dashjr.org>
Message-ID: <CAErK2CjpSb=Rb+evWg+_fs0brBfTDe3_HM1z-an0picb_8tzpQ@mail.gmail.com>

As I understand the attack, the attacker gets compensated for the shares
they earn, but the pool will be denied any valid blocks found.  The
attacker DOES NOT have access to the Bitcoins earned in the unreported
block (only the mining pool has access to the Coinbase address and
transactions in the block).

So it's a zero-net-cost attack for the attacker (but no chance of making a
profit) to hurt the pool operator.  The only way to detect such an attack
now is to look for "unlucky" miners; at the current difficulty, you can't
detect this cheat until many millions of shares have been earned w/o a
qualifying block.  Since an attacker can also create many fake identities,
they can avoid detection indefinitely by abandoning each account after a
million earned shares.

I don't understand your proposal for fixing this.  You would have to come
up with a scheme where:

- The miner can detect a qualifying hash to earn a share.
- Not be able to tell if the hash is for a valid block.

The way I would do this is to have a secret part (not shared with the
miners) of a block that is part of the merkle hash, which is also used in a
secondary hash.  Difficulty is then divide into two parts: the first,
solved by the miner (earning a "share" - e.g., 1 in 4 Billion hashes).  And
a second, solved by the pool (1 in Difficulty shares).  A valid block would
have to exhibit a valid Share Hash AND a valid Pool Hash in order to be
accepted.

This would be a very major change to the Block structure.  Given that
attackers do not have direct monetary gain from this attack, I'm not sure
we can justify it at this point.

On Sun, Jun 3, 2012 at 7:04 PM, Luke-Jr <luke at dashjr.org> wrote:

> On Monday, June 04, 2012 1:43:42 AM Peter Vessenes wrote:
> > Does it have asymmetric payoff for an attacker, that is, over time does
> it
> > pay them more to spend their hashes attacking than just mining?
>
> That depends on the pool's reward scheme. Some complicated forms are
> capable
> of getting "bonus" earnings out of the pool. Under all systems, the
> attacker
> at least gains the "hurt the pool" benefit. Given the frequency of DDoS
> attacks on pools, it is clear there are people who will even pay for
> attacks
> that provide no other benefit than harming pools. Under all systems, the
> attacker doesn't lose out in any significant way.
>
> > My gut is that it pays less well than mining, meaning I think this is
> > likely a small problem in the aggregate, and certainly not something we
> > should try and fork the blockchain for until there's real pain.
>
> If we wait until there's real pain, it will be a painful fork. If we plan
> it
> 1-2 years out, people have time to upgrade on their own before it breaks.
>
> > Consider, for instance, whether it pays better than just mining bitcoins
> > and spending those on 'bonuses' for getting users to switch from a pool
> you
> > hate.
>
> With this attack, attackers can hurt the pool's "luck factor" *and* spend
> the
> bitcoins they earn to bribe users away.
>
>
> ------------------------------------------------------------------------------
> Live Security Virtual Conference
> Exclusive live event will cover all the ways today's security and
> threat landscape has changed and how IT managers can respond. Discussions
> will include endpoint security, mobile security and the latest in malware
> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
> _______________________________________________
> Bitcoin-development mailing list
> Bitcoin-development at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/bitcoin-development
>



-- 
Mike Koss
CTO, CoinLab
(425) 246-7701 (m)

A Bitcoin Primer <http://coinlab.com/a-bitcoin-primer.pdf> - What you need
to know about Bitcoins.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120604/ed21dae7/attachment.html>

From mike at coinlab.com  Tue Jun  5 00:00:25 2012
From: mike at coinlab.com (Mike Koss)
Date: Mon, 4 Jun 2012 17:00:25 -0700
Subject: [Bitcoin-development] Defeating the block withholding attack
In-Reply-To: <201206042105.27064.luke@dashjr.org>
References: <201206030052.17128.luke@dashjr.org>
	<201206040204.57503.luke@dashjr.org>
	<CAErK2CjpSb=Rb+evWg+_fs0brBfTDe3_HM1z-an0picb_8tzpQ@mail.gmail.com>
	<201206042105.27064.luke@dashjr.org>
Message-ID: <CAErK2Cj8hciP2FNtmvCf726gEWdFjO6=W5j4yTCrEYKFtXMKPA@mail.gmail.com>

I don't understand how your proposal will work for decentralized pools -
can you explain it more concretely?

What would the new block header look like?

What is required for a share to to be earned?

What is required for a block to be valid (added to Block Chain)?

I don't think I understand what you mean by NextBlockCandidate.  Perhaps a
concrete example using difficulty 1.7 million would be instructive.

On Mon, Jun 4, 2012 at 2:05 PM, Luke-Jr <luke at dashjr.org> wrote:

> On Monday, June 04, 2012 8:49:48 PM Mike Koss wrote:
> > As I understand the attack, the attacker gets compensated for the shares
> > they earn, but the pool will be denied any valid blocks found.  The
> > attacker DOES NOT have access to the Bitcoins earned in the unreported
> > block (only the mining pool has access to the Coinbase address and
> > transactions in the block).
>
> With decentralized pools, the attacker does have access to the block, and
> can
> potentially submit it to the Bitcoin network directly bypassing the pool
> if it
> benefits them to do so.
>
> > So it's a zero-net-cost attack for the attacker (but no chance of making
> a
> > profit) to hurt the pool operator.
>
> Because of the above, there is a possibility an attacker can make a profit.
>
> > The only way to detect such an attack now is to look for "unlucky"
> miners;
> > at the current difficulty, you can't detect this cheat until many
> millions
> > of shares have been earned w/o a qualifying block.  Since an attacker can
> > also create many fake identities, they can avoid detection indefinitely
> by
> > abandoning each account after a million earned shares.
>
> There are other modes of detection, but nobody has bothered to implement
> them
> since attackers can easily do a simple workaround in an arms race.
>
> > I don't understand your proposal for fixing this.  You would have to come
> > up with a scheme where:
> >
> > - The miner can detect a qualifying hash to earn a share.
> > - Not be able to tell if the hash is for a valid block.
>
> With my proposal, miners can find shares, but won't know if it's a valid
> block
> until the subsequent block is also found (that subsequent block might not
> end
> up being a real block in the big picture).
>
> > The way I would do this is to have a secret part (not shared with the
> > miners) of a block that is part of the merkle hash, which is also used
> in a
> > secondary hash.  Difficulty is then divide into two parts: the first,
> > solved by the miner (earning a "share" - e.g., 1 in 4 Billion hashes).
>  And
> > a second, solved by the pool (1 in Difficulty shares).  A valid block
> would
> > have to exhibit a valid Share Hash AND a valid Pool Hash in order to be
> > accepted.
>
> This only works for centralized pools, which are contrary to the health of
> the
> Bitcoin network. Decentralized pools cannot have a secret.
>



-- 
Mike Koss
CTO, CoinLab
(425) 246-7701 (m)

A Bitcoin Primer <http://coinlab.com/a-bitcoin-primer.pdf> - What you need
to know about Bitcoins.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120604/2e959c8a/attachment.html>

From luke at dashjr.org  Tue Jun  5 01:05:18 2012
From: luke at dashjr.org (Luke-Jr)
Date: Tue, 5 Jun 2012 01:05:18 +0000
Subject: [Bitcoin-development] Defeating the block withholding attack
In-Reply-To: <CAErK2Cj8hciP2FNtmvCf726gEWdFjO6=W5j4yTCrEYKFtXMKPA@mail.gmail.com>
References: <201206030052.17128.luke@dashjr.org>
	<201206042105.27064.luke@dashjr.org>
	<CAErK2Cj8hciP2FNtmvCf726gEWdFjO6=W5j4yTCrEYKFtXMKPA@mail.gmail.com>
Message-ID: <201206050105.21685.luke@dashjr.org>

On Tuesday, June 05, 2012 12:00:25 AM Mike Koss wrote:
> I don't understand how your proposal will work for decentralized pools -
> can you explain it more concretely?
> 
> What would the new block header look like?

For example (just a draft; in reality, merged mining would probably be
             integrated in a hardfork)
 4 bytes: Block version number = 2
31 bytes: Hash of the block 2 back, except for the minimum last 8 bits of zero
 1 byte : Share difficulty (measured in "zero" bits)
 4 bytes: Timestamp
 4 bytes: "Bits" (current target in compact format)
 4 bytes: Nonce

> What is required for a share to to be earned?

The final <share difficulty> bits (minimum 32) of the block header are zero.

> What is required for a block to be valid (added to Block Chain)?

The hash of this block header, concatenated with a valid share candidate for 
the next block header, must hash to a value less than the current target 
offset against the share difficulty (this algorithm may need adjustment).

> I don't think I understand what you mean by NextBlockCandidate.  Perhaps a
> concrete example using difficulty 1.7 million would be instructive.

The first share becomes a block only after a second share is found that 
combined hashes to meet the real difficulty. That second share becomes a block 
when a third is found. Etc.



From pieter.wuille at gmail.com  Sat Jun  9 23:10:55 2012
From: pieter.wuille at gmail.com (Pieter Wuille)
Date: Sun, 10 Jun 2012 01:10:55 +0200
Subject: [Bitcoin-development] getmemorypool
Message-ID: <20120609231054.GA16966@vps7135.xlshosting.net>

Hello everyone,

Luke's getmemorypool/BIP22 pull request has been open for a
long time, and didn't receive too much discussion.

I think that having a stable and flexible API for negotiating
block generation is important to be standardized. The fact that
it allows moving block generation to specialized programs is a
step in the right direction. However, it seems to me that too
few people (myself included) understand all the details of
BIP22 (or don't care enough) to judge their necessity. I gave
up trying to follow all design decisions some time ago, and as
it seems I'm not alone, nobody liked merging support for it in
the Satoshi client. This is a pity, and I hope the situation
can be improved soon.

I'm sorry for being this late with these comments, but I think
it's essential that the standard is not more complex than
necessary (making it as easy as possible to write either
servers or clients for it), and perhaps even more important,
that its purpose and intended use cases are clear.

>From what I understand, the three subrequests are template,
proposal and submit. The general idea is that 
  1) a miner requests a block template
  2) builds/modifies a block based on this, and optionally
     uses propose to check whether the server is willing to
     accept it before doing work
  3) submits when valid proof-of-work is found
I'd like to see this process described in the BIP at least,
it too me way too long to extract this.

Regarding the block template: is there a particular reason
for sending the full transactions (serialized in hex) both in
templates and submissions? The server will always need to have
access to the transaction database anyway, and clients will
(afaics) rarely care about the actual transactions. My
suggestion would be to just transfer txids - if the client is
interested in the actual transactions, we have the
gettransaction RPC call already. This seems to be captured by
the several "submit/*" and "share/*" variations, but making
it optional seems way more complex than just limiting the API
to that way of working.

That's another thing that bothers me in the standard: too many
optional features. In particular, I understand the usefulness of
having some flexibility in what miner clients are allowed to
modify, but I'm unconvinced we need 10 individually selectable
variations. In particular:
* coinbase outputs: can we just add a list of required coinbase
  outputs (amount + scriptPubKey) to the template? If no
  generation+fee amount remains, nothing can be added.
* coinbase input: put the required part in the template;
  miners can always add whatever they like. Is there any known
  use case where a server would not allow a client to add
  stuff to the coinbase?
* noncerange limiting: if coinbase input variation is not
  limited, there is certainly no reason to limit nonceranges.
  This adds unnecessary complexity to clients, in my option.
* time/*: put a minimum and maximum timestamp in the template
  (i believe those are already there anyway). Anything in
  between is valid.
* transactions/add: what is the use case?
* transactions/remove: i'd just standarize on having all
  transactions be removable (perhaps except those marked
  'required').
* prevblock: one getmemorypool per new block shouldn't be
  a problem imho, so do a longpoll instead of having the client
  able to modify prevblock themselves.

One more thing that I do not like is often several ways for
specifying the same behaviour. For example, txrequires specifies
that the first N transactions are mandatory, a 'required' field
in the transaction list itself specifies that that transaction is
mandatory, and the lack of transactions as variation means that
they must not be touched at all. Pick one way that is flexible
enough, and discard the others.

In summary, I'd like to see the standard simplified. I have
no problem merging code that makes getmemorypool compliant to
a standard that is agreed upon, but like to understand it first.

In my opinion - but I'm certainly open to discussion here - the
standard could be simplified to:
* getblocktemplate: create a new block template, and return it.
  The result contains:
  * bits, previousblockhash, version: as to be used in block
  * curtime, maxtimeoff, maxtimeoff: client chooses a timestamp
    between (curtime - local_time_at_receipt + local_time),
    decreased by mintimeoff and increased maxtimeoff
  * expires, sigoplimit, sizelimit: unchanged
  * subsidy: amount generated (5000000000 for now)
  * coinbaseaux: what generated coinbase's scriptSig must start
    with
  * coinbaseoutputs: list of objects, each specifying a required
    coinbase output. Each has fields:
    * amount: sent amount
    * scriptPubKey: hex serialized of the output script
  * transactions: list of object, each specifying a suggested
    transaction (except for the coinbase) in the generated block.
    Each has fields:
    * txid: transaction id
    * depends: list of dependencies (txids of earlier objects in
      this same transactions list).
    * fee: fee generated by this transaction, which increases the
      max output of the coinbase.
    * required: if present, transaction may not be dropped.
* submitblocktemplate: submit an object containing a hex serialized
  block header, hex serialized coinbase transaction, and a list of
  txids. Returns true or string describing the problem. Proof of
  work is checked last, so that error is only returned if there is
  no other problem with the suggested block (this allows it to
  replace both propose and submit).

Are there important use cases I'm missing?

-- 
Pieter



From pieter.wuille at gmail.com  Sun Jun 10 09:03:58 2012
From: pieter.wuille at gmail.com (Pieter Wuille)
Date: Sun, 10 Jun 2012 11:03:58 +0200
Subject: [Bitcoin-development] BIP22/getmemorypool
Message-ID: <20120610090357.GA25567@vps7135.xlshosting.net>

----- Forwarded message from Pieter Wuille <pieter.wuille at gmail.com> -----

Date: Sun, 10 Jun 2012 01:10:54 +0200
From: Pieter Wuille <pieter.wuille at gmail.com>
To: bitcoin-development at lists.sourceforge.net
Subject: getmemorypool
User-Agent: Mutt/1.5.20 (2009-06-14)

Hello everyone,

Luke's getmemorypool/BIP22 pull request has been open for a long time, and didn't receive too much discussion.

I think that having a stable and flexible API for negotiating block generation is important to be standardized. The fact that it allows moving block generation to specialized programs is a step in the right direction. However, it seems to me that too few people (myself included) understand all the details of BIP22 (or don't care enough) to judge their necessity. I gave up trying to follow all design decisions some time ago, and as it seems I'm not alone, nobody liked merging support for it in the Satoshi client. This is a pity, and I hope the situation can be improved soon.

I'm sorry for being this late with these comments, but I think it's essential that the standard is not more complex than necessary (making it as easy as possible to write either servers or clients for it), and perhaps even more important, that its purpose and intended use cases are clear.

>From what I understand, the three subrequests are template, proposal and submit. The general idea is that 
  1) a miner requests a block template
  2) builds/modifies a block based on this, and optionally uses propose to check whether the server is willing to accept it before doing work
  3) submits when valid proof-of-work is found
I'd like to see this process described in the BIP at least, it too me way too long to extract this.

Regarding the block template: is there a particular reason for sending the full transactions (serialized in hex) both in templates and submissions? The server will always need to have access to the transaction database anyway, and clients will (afaics) rarely care about the actual transactions. My suggestion would be to just transfer txids - if the client is interested in the actual transactions, we have the gettransaction RPC call already. This seems to be captured by the several "submit/*" and "share/*" variations, but making it optional seems way more complex than just limiting the API to that way of working.

That's another thing that bothers me in the standard: too many optional features. In particular, I understand the usefulness of having some flexibility in what miner clients are allowed to modify, but I'm unconvinced we need 10 individually selectable variations. In particular: 
* coinbase outputs: can we just add a list of required coinbase outputs (amount + scriptPubKey) to the template? If no generation+fee amount remains, nothing can be added.
* coinbase input: put the required part in the template; miners can always add whatever they like. Is there any known use case where a server would not allow a client to add stuff to the coinbase?
* noncerange limiting: if coinbase input variation is not limited, there is certainly no reason to limit nonceranges. This adds unnecessary complexity to clients, in my option.
* time/*: put a minimum and maximum timestamp in the template (i believe those are already there anyway). Anything in between is valid.
* transactions/add: what is the use case?
* transactions/remove: i'd just standarize on having all transactions be removable (perhaps except those marked 'required').
* prevblock: one getmemorypool per new block shouldn't be a problem imho, so do a longpoll instead of having the client able to modify prevblock themselves.

One more thing that I do not like is often several ways for specifying the same behaviour. For example, txrequires specifies that the first N transactions are mandatory, a 'required' field in the transaction list itself specifies that that transaction is mandatory, and the lack of transactions as variation means that they must not be touched at all. Pick one way that is flexible enough, and discard the others.

In summary, I'd like to see the standard simplified. I have no problem merging code that makes getmemorypool compliant to a standard that is agreed upon, but like to understand it first. 

In my opinion - but I'm certainly open to discussion here - the standard could be simplified to:
* getblocktemplate: create a new block template, and return it. The result contains:
  * bits, previousblockhash, version: as to be used in block
  * curtime, maxtimeoff, maxtimeoff: client chooses a timestamp between (curtime - local_time_at_receipt + local_time), decreased by mintimeoff and increased maxtimeoff
  * expires, sigoplimit, sizelimit: unchanged
  * subsidy: amount generated (5000000000 for now)
  * coinbaseaux: what generated coinbase's scriptSig must start with
  * coinbaseoutputs: list of objects, each specifying a required coinbase output. Each has fields:
    * amount: sent amount
    * scriptPubKey: hex serialized of the output script
  * transactions: list of object, each specifying a suggested transaction (except for the coinbase) in the generated block. Each has fields:
    * txid: transaction id
    * depends: list of dependencies (txids of earlier objects in this same transactions list).
    * fee: fee generated by this transaction, which increases the max output of the coinbase.
    * required: if present, transaction may not be dropped.
* submitblocktemplate: submit an object containing a hex serialized block header, hex serialized coinbase transaction, and a list of txids. Returns true or string describing the problem. Proof of work is checked last, so that error is only returned if there is no other problem with the suggested block (this allows it to replace both propose and submit).

Are there important use cases I'm missing?

-- 
Pieter



From pieter.wuille at gmail.com  Sun Jun 10 14:18:47 2012
From: pieter.wuille at gmail.com (Pieter Wuille)
Date: Sun, 10 Jun 2012 16:18:47 +0200
Subject: [Bitcoin-development] BIP22/getmemorypool
In-Reply-To: <20120610090357.GA25567@vps7135.xlshosting.net>
References: <20120610090357.GA25567@vps7135.xlshosting.net>
Message-ID: <CAPg+sBiqi_NDGOfhzrFy1V3o24kPnxvGtYzsSsk6iy9dyyhF8w@mail.gmail.com>

Hello everyone,

Luke's getmemorypool/BIP22 pull request has been open for a long time, and
didn't receive too much discussion.

I think that having a stable and flexible API for negotiating block
generation is important to be standardized. The fact that it allows moving
block generation to specialized programs is a step in the right direction.
However, it seems to me that too few people (myself included) understand
all the details of BIP22 (or don't care enough) to judge their necessity. I
gave up trying to follow all design decisions some time ago, and as it
seems I'm not alone, nobody liked merging support for it in the Satoshi
client. This is a pity, and I hope the situation can be improved soon.

I'm sorry for being this late with these comments, but I think it's
essential that the standard is not more complex than necessary (making it
as easy as possible to write either servers or clients for it), and perhaps
even more important, that its purpose and intended use cases are clear.

>From what I understand, the three subrequests are template, proposal and
submit. The general idea is that
 1) a miner requests a block template
 2) builds/modifies a block based on this, and optionally uses propose to
check whether the server is willing to accept it before doing work
 3) submits when valid proof-of-work is found
I'd like to see this process described in the BIP at least, it too me way
too long to extract this.

Regarding the block template: is there a particular reason for sending the
full transactions (serialized in hex) both in templates and submissions?
The server will always need to have access to the transaction database
anyway, and clients will (afaics) rarely care about the actual
transactions. My suggestion would be to just transfer txids - if the client
is interested in the actual transactions, we have the gettransaction RPC
call already. This seems to be captured by the several "submit/*" and
"share/*" variations, but making it optional seems way more complex than
just limiting the API to that way of working.

That's another thing that bothers me in the standard: too many optional
features. In particular, I understand the usefulness of having some
flexibility in what miner clients are allowed to modify, but I'm
unconvinced we need 10 individually selectable variations. In particular:
* coinbase outputs: can we just add a list of required coinbase outputs
(amount + scriptPubKey) to the template? If no generation+fee amount
remains, nothing can be added.
* coinbase input: put the required part in the template; miners can always
add whatever they like. Is there any known use case where a server would
not allow a client to add stuff to the coinbase?
* noncerange limiting: if coinbase input variation is not limited, there is
certainly no reason to limit nonceranges. This adds unnecessary complexity
to clients, in my option.
* time/*: put a minimum and maximum timestamp in the template (i believe
those are already there anyway). Anything in between is valid.
* transactions/add: what is the use case?
* transactions/remove: i'd just standarize on having all transactions be
removable (perhaps except those marked 'required').
* prevblock: one getmemorypool per new block shouldn't be a problem imho,
so do a longpoll instead of having the client able to modify prevblock
themselves.

One more thing that I do not like is often several ways for specifying the
same behaviour. For example, txrequires specifies that the first N
transactions are mandatory, a 'required' field in the transaction list
itself specifies that that transaction is mandatory, and the lack of
transactions as variation means that they must not be touched at all. Pick
one way that is flexible enough, and discard the others.

In summary, I'd like to see the standard simplified. I have no problem
merging code that makes getmemorypool compliant to a standard that is
agreed upon, but like to understand it first.

In my opinion - but I'm certainly open to discussion here - the standard
could be simplified to:
* getblocktemplate: create a new block template, and return it. The result
contains:
 * bits, previousblockhash, version: as to be used in block
 * curtime, maxtimeoff, maxtimeoff: client chooses a timestamp between
(curtime - local_time_at_receipt + local_time), decreased by mintimeoff and
increased maxtimeoff
 * expires, sigoplimit, sizelimit: unchanged
 * subsidy: amount generated (5000000000 for now)
 * coinbaseaux: what generated coinbase's scriptSig must start with
 * coinbaseoutputs: list of objects, each specifying a required coinbase
output. Each has fields:
   * amount: sent amount
   * scriptPubKey: hex serialized of the output script
 * transactions: list of object, each specifying a suggested transaction
(except for the coinbase) in the generated block. Each has fields:
   * txid: transaction id
   * depends: list of dependencies (txids of earlier objects in this same
transactions list).
   * fee: fee generated by this transaction, which increases the max output
of the coinbase.
   * required: if present, transaction may not be dropped.
* submitblocktemplate: submit an object containing a hex serialized block
header, hex serialized coinbase transaction, and a list of txids. Returns
true or string describing the problem. Proof of work is checked last, so
that error is only returned if there is no other problem with the suggested
block (this allows it to replace both propose and submit).

Are there important use cases I'm missing?

--
Pieter
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120610/84564648/attachment.html>

From mike at plan99.net  Sun Jun 10 23:06:50 2012
From: mike at plan99.net (Mike Hearn)
Date: Mon, 11 Jun 2012 01:06:50 +0200
Subject: [Bitcoin-development] Bootstrapping full nodes post-pruning
Message-ID: <CANEZrP3kOysjENpkHom5MHg0usq1jkQdEFAM3vuR1KgFAnJHhg@mail.gmail.com>

Apologies if this has been discussed elsewhere. I don't recall us ever
reaching a solid conclusion on it.

A node that has pruned its block chain cannot serve the chain to new
nodes. So there are three options for bootstrapping a newly installed
node:

1) Have some kind of special archival nodes that never prune
(advertised via the services field?). Encourage people to run them,
somehow.

2) Ship a post-pruning block chain and tx index with the client
downloads, so the client starts up already bootstrapped.

3) Some combination of both. It's safe to assume some people will keep
unpruned chains around no matter what. But for many users (2) is
easiest and archival nodes would be put under less load if they were
used only by users who wish to fully bootstrap from only the code.

I remember some people, Greg in particular, who were not a fan of
approach (2) at all, though it has the benefit of speeding startup for
new users as there's no indexing overhead.



From gmaxwell at gmail.com  Mon Jun 11 15:39:20 2012
From: gmaxwell at gmail.com (Gregory Maxwell)
Date: Mon, 11 Jun 2012 11:39:20 -0400
Subject: [Bitcoin-development] Bootstrapping full nodes post-pruning
In-Reply-To: <CANEZrP3kOysjENpkHom5MHg0usq1jkQdEFAM3vuR1KgFAnJHhg@mail.gmail.com>
References: <CANEZrP3kOysjENpkHom5MHg0usq1jkQdEFAM3vuR1KgFAnJHhg@mail.gmail.com>
Message-ID: <CAAS2fgSB6--PzpnTrx_DXrwZ7uzXrTCH3a1aMVFmWPBNO6FuqA@mail.gmail.com>

On Sun, Jun 10, 2012 at 7:06 PM, Mike Hearn <mike at plan99.net> wrote:
> I remember some people, Greg in particular, who were not a fan of
> approach (2) at all, though it has the benefit of speeding startup for
> new users as there's no indexing overhead.

I'm not a fan of anything which introduces unauditable single source
material.  "Trust us" is a bad place to be because it would greatly
increase the attractiveness of compromising developers.

If we wanted to go the route of shipping pruned chains I'd prefer to
have a deterministic process to produce archival chains and then start
introducing commitments to them in the blockchain or something like
that.   Then a client doing a reverse header sync[1] would bump into a
commitment for an archival chain that they have and would simply stop
syncing and use the archival chain for points before that.

This would leave it so that the distribution of the software could
still be audited.

More generally we should start doing something with the service
announcements so that full nodes that don't have enough bandwidth to
support a lot of syncing from new nodes can do so without turning off
listening.


[1] https://en.bitcoin.it/wiki/User:Gmaxwell/Reverse_header-fetching_sync



From gavinandresen at gmail.com  Mon Jun 11 17:47:59 2012
From: gavinandresen at gmail.com (Gavin Andresen)
Date: Mon, 11 Jun 2012 13:47:59 -0400
Subject: [Bitcoin-development] BIP22/getmemorypool
In-Reply-To: <20120610090357.GA25567@vps7135.xlshosting.net>
References: <20120610090357.GA25567@vps7135.xlshosting.net>
Message-ID: <CABsx9T3uaEBikhUo7j52mauBx0mnfk2_1mq0QOQBZVptoMB6HA@mail.gmail.com>

I think the sourceforge mailing list system had the hiccups this
weekend; sorry for Pieter's messages appearing in your inbox multiple
times, it is not his fault.

I deleted the extra copies from the mailing list archives.


As for the contents of his message, since this mailing list was not
working discussion wandered into the pull request:
  https://github.com/bitcoin/bitcoin/pull/936

Assuming this mailing list is now fixed, I'd like to pull that
discussion back here.  The executive summary:  Pieter and I feel like
BIP 22 is overly complicated, and would like it to be simpler. I'd
especially like to hear what people think will be the "will be used by
lots of pool customers" features and what are the "will be used by
less than 5% of pool customers" features.


-- 
--
Gavin Andresen



From thomasV1 at gmx.de  Mon Jun 11 18:10:22 2012
From: thomasV1 at gmx.de (thomasV1 at gmx.de)
Date: Mon, 11 Jun 2012 20:10:22 +0200
Subject: [Bitcoin-development] BIP22/getmemorypool
In-Reply-To: <CABsx9T3uaEBikhUo7j52mauBx0mnfk2_1mq0QOQBZVptoMB6HA@mail.gmail.com>
References: <20120610090357.GA25567@vps7135.xlshosting.net>
	<CABsx9T3uaEBikhUo7j52mauBx0mnfk2_1mq0QOQBZVptoMB6HA@mail.gmail.com>
Message-ID: <20120611181022.10710@gmx.net>


> discussion back here.  The executive summary:  Pieter and I feel like
> BIP 22 is overly complicated, and would like it to be simpler. I'd
> especially like to hear what people think will be the "will be used by
> lots of pool customers" features and what are the "will be used by
> less than 5% of pool customers" features.
> 

will be used by Electrum servers 

-- 
Empfehlen Sie GMX DSL Ihren Freunden und Bekannten und wir
belohnen Sie mit bis zu 50,- Euro! https://freundschaftswerbung.gmx.de



From mike at plan99.net  Mon Jun 11 20:36:13 2012
From: mike at plan99.net (Mike Hearn)
Date: Mon, 11 Jun 2012 22:36:13 +0200
Subject: [Bitcoin-development] Bootstrapping full nodes post-pruning
In-Reply-To: <CAAS2fgSB6--PzpnTrx_DXrwZ7uzXrTCH3a1aMVFmWPBNO6FuqA@mail.gmail.com>
References: <CANEZrP3kOysjENpkHom5MHg0usq1jkQdEFAM3vuR1KgFAnJHhg@mail.gmail.com>
	<CAAS2fgSB6--PzpnTrx_DXrwZ7uzXrTCH3a1aMVFmWPBNO6FuqA@mail.gmail.com>
Message-ID: <CANEZrP2TU3W08Pi7Wdw4rPYLHC=wesKtci8vopV8Hbi3eCMHcw@mail.gmail.com>

> If we wanted to go the route of shipping pruned chains I'd prefer to
> have a deterministic process to produce archival chains

Yeah, that sounds reasonable. I mean, I can't see why pruning would
not be deterministic. So if you download a binary that contains a
pre-indexed and pruned chain up to block 180,000 or whatever, you
should be able to blow away the data files and run with
"-syncto=180000 -prune", then check the hashes of the newly created
files vs what you downloaded.

Unless BDB has some weird behaviour in it, that shouldn't require any
additional effort, and anyone could set up a cron job to verify the
downloads match what is expected.

Even if a more complex scheme is used whereby commitments are in the
block chain, somebody still has to verify the binaries match the
source. If that isn't true, the software could do anything and you'd
never know.



From gmaxwell at gmail.com  Mon Jun 11 20:43:20 2012
From: gmaxwell at gmail.com (Gregory Maxwell)
Date: Mon, 11 Jun 2012 16:43:20 -0400
Subject: [Bitcoin-development] Bootstrapping full nodes post-pruning
In-Reply-To: <CANEZrP2TU3W08Pi7Wdw4rPYLHC=wesKtci8vopV8Hbi3eCMHcw@mail.gmail.com>
References: <CANEZrP3kOysjENpkHom5MHg0usq1jkQdEFAM3vuR1KgFAnJHhg@mail.gmail.com>
	<CAAS2fgSB6--PzpnTrx_DXrwZ7uzXrTCH3a1aMVFmWPBNO6FuqA@mail.gmail.com>
	<CANEZrP2TU3W08Pi7Wdw4rPYLHC=wesKtci8vopV8Hbi3eCMHcw@mail.gmail.com>
Message-ID: <CAAS2fgQ7sspfZ+aBDhCj7Y+Dzmv1ku7u4wO=VUFVcBSuuTzo6Q@mail.gmail.com>

On Mon, Jun 11, 2012 at 4:36 PM, Mike Hearn <mike at plan99.net> wrote:
> Unless BDB has some weird behaviour in it, that shouldn't require any

HAHAHA.   Have you consider doing comedy full time?

Actual BDB files are absolutely not deterministic. Nor is the raw
blockchain itself currently, because blocks aren't always added in the
same order (plus they get orphans in them)

But the serious inter-version compatibility problems as well as poor
space efficiency make BDB a poor candidate for read only pruned
indexes.

> Even if a more complex scheme is used whereby commitments are in the
> block chain, somebody still has to verify the binaries match the
> source. If that isn't true, the software could do anything and you'd
> never know.

The binaries distributed by bitcoin.org are all already compiled
deterministically and validated by multiple independent parties.  In
the future there will be a downloader tool (e.g. for updates) which
will automatically check for N approvals before accepting an update,
even for technically unsophisticated users.

This will produce a full chain of custody which tracks the actual
binaries people fetch to specific source code which can be audited, so
substitution attacks will at least in theory always be detectable. Of
course, you're left with Ken Thompson's compiler attack but even that
can be substantially closed.



From mike at plan99.net  Mon Jun 11 20:48:36 2012
From: mike at plan99.net (Mike Hearn)
Date: Mon, 11 Jun 2012 22:48:36 +0200
Subject: [Bitcoin-development] Bootstrapping full nodes post-pruning
In-Reply-To: <CAAS2fgQ7sspfZ+aBDhCj7Y+Dzmv1ku7u4wO=VUFVcBSuuTzo6Q@mail.gmail.com>
References: <CANEZrP3kOysjENpkHom5MHg0usq1jkQdEFAM3vuR1KgFAnJHhg@mail.gmail.com>
	<CAAS2fgSB6--PzpnTrx_DXrwZ7uzXrTCH3a1aMVFmWPBNO6FuqA@mail.gmail.com>
	<CANEZrP2TU3W08Pi7Wdw4rPYLHC=wesKtci8vopV8Hbi3eCMHcw@mail.gmail.com>
	<CAAS2fgQ7sspfZ+aBDhCj7Y+Dzmv1ku7u4wO=VUFVcBSuuTzo6Q@mail.gmail.com>
Message-ID: <CANEZrP3NKRiCHWFUeBDJLN9PzBmhFnEjAk+v+HZXOw3aeJWdeA@mail.gmail.com>

> Actual BDB files are absolutely not deterministic. Nor is the raw
> blockchain itself currently, because blocks aren't always added in the
> same order (plus they get orphans in them)

That's true. Though if you prune up to the last checkpoint, orphans
before that point can be safely thrown away.

I wonder if swapping out bdb for LevelDB might make sense at some
point. I'm not sure how deterministic that is either though :)



From pieter.wuille at gmail.com  Tue Jun 12 10:50:40 2012
From: pieter.wuille at gmail.com (Pieter Wuille)
Date: Tue, 12 Jun 2012 12:50:40 +0200
Subject: [Bitcoin-development] BIP22/getmemorypool
In-Reply-To: <20120611181022.10710@gmx.net>
References: <20120610090357.GA25567@vps7135.xlshosting.net>
	<CABsx9T3uaEBikhUo7j52mauBx0mnfk2_1mq0QOQBZVptoMB6HA@mail.gmail.com>
	<20120611181022.10710@gmx.net>
Message-ID: <20120612105038.GA29784@vps7135.xlshosting.net>

On Mon, Jun 11, 2012 at 08:10:22PM +0200, thomasV1 at gmx.de wrote:
> 
> > discussion back here.  The executive summary:  Pieter and I feel like
> > BIP 22 is overly complicated, and would like it to be simpler. I'd
> > especially like to hear what people think will be the "will be used by
> > lots of pool customers" features and what are the "will be used by
> > less than 5% of pool customers" features.
> 
> will be used by Electrum servers 

Please. This is not about whether getmemorypool is useful (at least I am
a big fan of BIP22's big picture). It's about whether it needs 20 optional
features.

-- 
Pieter



From jgarzik at exmulti.com  Wed Jun 13 20:46:37 2012
From: jgarzik at exmulti.com (Jeff Garzik)
Date: Wed, 13 Jun 2012 16:46:37 -0400
Subject: [Bitcoin-development] New P2P commands for diagnostics, SPV clients
Message-ID: <CA+8xBpecVQcTTbPxUm_3_GWC99dEd4=-VFWb+QT6jUy4rg8U4w@mail.gmail.com>

An IRC discussion today covered additional needs of lightweight
clients.  Here is a draft of proposed new P2P commands, and associated
behavior changes.  This is not meant to be a formal, detailed
specification but rather rough picture of the preferred direction.

     -----

filterinit(false positive rate, number of elements): initialize
per-connection bloom filter to the given parameters.  if the
parameters create a too-large table, the operation fails.  returns a
'filterparams' message, with bloom filter construction details.

filterload(data): input a serialized bloom filter table metadata and data.

filterclear(): remove any filtering associated with current connection.

filteradd(hash data): add a single hash to the bloom filter.  WARNING:
although easier to use, has privacy implications. filterload shrouds
the hash list; filteradd does not.  it is also less efficient to send
a stream of filteradd's to the remote node.

mempool():  list TX's in remote node's memory pool.

     -----

'filterload' and 'filteradd' enable special behavior changes for
'mempool' and existing P2P commands, whereby only transactions
matching the bloom filter will be announced to the connection, and
only matching transactions will be sent inside serialized blocks.

A lightweight ("SPV") client would issue 'filterload', sync up with
blocks, then use 'mempool' to sync up to current TX's.  The
'filterload' command also ensures that the client is only sent 'inv'
messages etc. for the TX's it is probably interested in.

The 'mempool' command is thought to be useful as a diagnostic, even if
a bloom filter is not applied to its output.

A bloom filter match would need to notice activity on existing coins
(via CTxIn->prevout) and activity on a bitcoin address (via CTxOut).

-- 
Jeff Garzik
exMULTI, Inc.
jgarzik at exmulti.com



From mike at plan99.net  Thu Jun 14 11:52:29 2012
From: mike at plan99.net (Mike Hearn)
Date: Thu, 14 Jun 2012 13:52:29 +0200
Subject: [Bitcoin-development] New P2P commands for diagnostics,
	SPV clients
In-Reply-To: <CA+8xBpecVQcTTbPxUm_3_GWC99dEd4=-VFWb+QT6jUy4rg8U4w@mail.gmail.com>
References: <CA+8xBpecVQcTTbPxUm_3_GWC99dEd4=-VFWb+QT6jUy4rg8U4w@mail.gmail.com>
Message-ID: <CANEZrP0kNZDByHpK2=UjP+ag0X1KmqHxnJdm=e_pWMitP4QvvA@mail.gmail.com>

> filterinit(false positive rate, number of elements): initialize
> filterload(data): input a serialized bloom filter table metadata and data.

Why not combine these two?

> 'filterload' and 'filteradd' enable special behavior changes for
> 'mempool' and existing P2P commands, whereby only transactions
> matching the bloom filter will be announced to the connection, and
> only matching transactions will be sent inside serialized blocks.

Need to specify the format of how these arrive. It means that when a
new block is found instead of inv<->getdata<->block we'd see something
like  inv<->getdata<->merkleblock where a "merkleblock" structure is a
header + list of transactions + list of merkle branches linking them
to the root. I think CMerkleTx already knows how to serialize this,
but it redundantly includes the block hash which would not be
necessary for a merkleblock message.



From gavinandresen at gmail.com  Thu Jun 14 13:22:08 2012
From: gavinandresen at gmail.com (Gavin Andresen)
Date: Thu, 14 Jun 2012 09:22:08 -0400
Subject: [Bitcoin-development] Raw Transaction RPC calls for bitcoind
Message-ID: <CABsx9T0oZvVB1CPfa7Rk3CTnPOx8-CEm4K45pjACSyoqLz6YPA@mail.gmail.com>

I submitted a pull request yesterday that implements low-level "raw"
transaction, and am looking for feedback on the API and help with
trying to test/break it.

Design doc:  https://gist.github.com/2839617
Pull request: https://github.com/bitcoin/bitcoin/pull/1456
Test plan: https://secure.bettermeans.com/projects/4180/wiki/Raw_Transaction_RPC_Test_Plan

Playing around with this API on the command line I'm pretty happy with
the level of abstraction and the way it interacts with existing RPC
commands; for example, "createrawtx" is just like "sendmany" in the
way outputs are specified.

The signrawtx method is the key new method; it takes a raw
transaction, signs as many inputs as it can, and returns the same raw
transaction with signatures. Typical usage would be:

Funds are sitting in a multisignature transaction output, and it is
time to gather signatures and spend them.

Assumption: you know the multisignature transaction's [txid,
outputNumber, amount].

Create a raw transaction to spend, using createrawtx.
Use signrawtx to add your signatures (after unlocking the wallet, if necessary).
Give the transaction to the other person(s) to sign.
You or they submit the transaction to the network using sendrawtx.
I don't imagine anybody but very-early-adopters or ultra-geeks will do
this by calling these RPC methods at a command-line. They are really
intended for people writing services on top of bitcoind. The service
should be careful to include an appropriate transaction fee, or the
sendrawtx method is likely to fail.

I've been asked a couple of times: why doesn't signrawtx handle the
BIP 0010 (https://en.bitcoin.it/wiki/BIP_0010) transaction format?

I considered parsing/writing BIP 10 format for raw transactions, but
decided that reading/writing BIP 10 format should happen at a higher
level and not in the low-level RPC calls. So 'raw transactions' are
simply hex-encoded into JSON strings, and encoding/decoding them is
just a couple of lines of already-written-and-debugged code.

------

Here is the help output and example use for all the new RPC calls:

listunspent [minconf=1] [maxconf=999999]
Returns array of unspent transaction outputs
with between minconf and maxconf (inclusive) confirmations.
Returns an array of 4-element arrays, each of which is:
[transaction id, output, amount, confirmations]

E.g:  listunspent 1 2
Returns:
[
    [
        "2881b33a8c0bbdb45b0a65b36aa6611a05201e316ea3ad718762d48ef9588fb3",
        0,
        40.00000000,
        2
    ],
    [
        "894a0fc535c7b49f434ceb633d8555ea24c8f9775144efb42da85b853280bcd7",
        0,
        50.00000000,
        1
    ]
]

getrawtx <txid>
Returns hexadecimal-encoded, serialized transaction data
for <txid>. Returns an error if <txid> is unknown.

E.g.: getrawtx fce46ea2448820f7bb8091b5f5e3fd75b7b267e60b9a22af88a9eeabfb084233
Returns:
01000000016d40da062b6a0edcaf643b6e25b943baf103941589d287e39d6f425d84ae8b1c000000004847304402203fb648ff8381d8961e66ef61ab88afe52826a5179b8a7312742c8d93785ca56302204240ea12de1211fffab49686f13ca0e78011d1985765be6e6aa8e747852f897d01ffffffff0100f2052a0100000017a914f96e358e80e8b3660256b211a23ce3377d2f9cb18700000000


createrawtx [["txid",n],...] {address:amount,...}
Create a transaction spending given inputs
(array of (hex transaction id, output number) pairs),
sending to given address(es).
Returns the same information as gettransaction, plus an
extra "rawtx" key with the hex-encoded transaction.
Note that the transaction's inputs are not signed, and
it is not stored in the wallet or transmitted to the network.

E.g.: createrawtx '[
["fce46ea2448820f7bb8091b5f5e3fd75b7b267e60b9a22af88a9eeabfb084233",0]
]' '{"mqYmZSQQuAWNQcdwBrDwmtTXg2TLNz748L":50}'
Returns:
{
    "version" : 1,
    "locktime" : 0,
    "size" : 85,
    "vin" : [
        {
            "prevout" : {
                "hash" :
"fce46ea2448820f7bb8091b5f5e3fd75b7b267e60b9a22af88a9eeabfb084233",
                "n" : 0
            },
            "scriptSig" : "",
            "sequence" : 4294967295
        }
    ],
    "vout" : [
        {
            "value" : 50.00000000,
            "scriptPubKey" : "OP_DUP OP_HASH160
6e0920fc26383dc7e6101bc417cf87169d0cedbd OP_EQUALVERIFY OP_CHECKSIG"
        }
    ],
    "rawtx" : "0100000001334208fbabeea988af229a0be667b2b775fde3f5b59180bbf7208844a26ee4fc0000000000ffffffff0100f2052a010000001976a9146e0920fc26383dc7e6101bc417cf87169d0cedbd88ac00000000"
}

signrawtx <hex string> [<prevtx1>,<prevtx2>...]
Sign inputs for raw transaction (serialized, hex-encoded).
Second argument is an array of raw previous transactions that
this transaction depends on but are not yet in the blockchain.
Returns json object with keys:
  rawtx : raw transaction with signature(s) (hex-encoded string)
  complete : 1 if transaction has a complete set of signature (0 if not)

E.g.: signrawtx
"0100000001334208fbabeea988af229a0be667b2b775fde3f5b59180bbf7208844a26ee4fc0000000000ffffffff0100f2052a010000001976a9146e0920fc26383dc7e6101bc417cf87169d0cedbd88ac00000000"
'["01000000016d40da062b6a0edcaf643b6e25b943baf103941589d287e39d6f425d84ae8b1c000000004847304402203fb648ff8381d8961e66ef61ab88afe52826a5179b8a7312742c8d93785ca56302204240ea12de1211fffab49686f13ca0e78011d1985765be6e6aa8e747852f897d01ffffffff0100f2052a0100000017a914f96e358e80e8b3660256b211a23ce3377d2f9cb18700000000"]'
Returns:
{
    "rawtx" : "0100000001334208fbabeea988af229a0be667b2b775fde3f5b59180bbf7208844a26ee4fc000000009100473044022007f3ba1b8bdc156f2340ef1222eb287c3f5481a8078a8dad43aa09fd289ba19002201cc72e97406d546dc918159978dc78aee8215a6418375956665ee44e6eacc1150147522102894ca6e7a6483d0f8fa6110c77c431035e8d462e3a932255d9dda65e8fada55c2103c556ef01e89a07ee9ba61581658fa007bf442232daed8b465c47c278550d3dab52aeffffffff0100f2052a010000001976a9146e0920fc26383dc7e6101bc417cf87169d0cedbd88ac00000000",
    "complete" : false
}

sendrawtx <hex string>
Submits raw transaction (serialized, hex-encoded) to local node and network.
E.g.: sendrawtx
0100000001334208fbabeea988af229a0be667b2b775fde3f5b59180bbf7208844a26ee4fc000000009100473044022007f3ba1b8bdc156f2340ef1222eb287c3f5481a8078a8dad43aa09fd289ba19002201cc72e97406d546dc918159978dc78aee8215a6418375956665ee44e6eacc1150147522102894ca6e7a6483d0f8fa6110c77c431035e8d462e3a932255d9dda65e8fada55c2103c556ef01e89a07ee9ba61581658fa007bf442232daed8b465c47c278550d3dab52aeffffffff0100f2052a010000001976a9146e0920fc26383dc7e6101bc417cf87169d0cedbd88ac00000000
Returns:
error: {"code":-22,"message":"TX rejected"}

(Rejected because it doesn't have all required signatures, if it was
accepted it would return the transaction id)

-- 
--
Gavin Andresen



From etotheipi at gmail.com  Thu Jun 14 14:25:07 2012
From: etotheipi at gmail.com (Alan Reiner)
Date: Thu, 14 Jun 2012 10:25:07 -0400
Subject: [Bitcoin-development] A tangent about BIP 10
Message-ID: <CALf2ePzWye8fFn8oV=q-izudbPFQ5wDyn+n=j+=9LiwhZxBozQ@mail.gmail.com>

On Thu, Jun 14, 2012 at 9:22 AM, Gavin Andresen <gavinandresen at gmail.com>wrote:

>
> I've been asked a couple of times: why doesn't signrawtx handle the
> BIP 0010 (https://en.bitcoin.it/wiki/BIP_0010) transaction format?
>
> I considered parsing/writing BIP 10 format for raw transactions, but
> decided that reading/writing BIP 10 format should happen at a higher
> level and not in the low-level RPC calls. So 'raw transactions' are
> simply hex-encoded into JSON strings, and encoding/decoding them is
> just a couple of lines of already-written-and-debugged code.
>
>
BIP 10 <https://en.bitcoin.it/wiki/BIP_0010> could use some improvement.  I
created it for offline and multi-sig tx but there was no reception to it
because no one was using offline or multi-sig tx at the time except for
Armory (which only currently implements offline tx).  So I made something
that fit my needs, and it has served its purpose well for me. But I also
think it could be expanded and improved before there is wider adoption of
it.  It's a little clunky and not very rigorous.

Elements of it that I'd really like to keep:

(1) Some aspects of human-readability -- even if regular users will never
look at it, it should be possible for advanced users to manually copy&paste
the data around and see what's going on in the transaction and what
signatures are present.  I'm thinking of super-high-security situations
where manual handling of such data may even be the norm.
(2) Should be compact -- I took the concept of ASCII-armoring from PGP/GPG,
because, for the reason above, it's much easier and cleaner to view/select
when copied inline.  If a random user accidentally runs across it, it will
partially self-identify itself
(3) Includes all previous transactions so the device can verify transaction
inputs without the blockchain.


Things that could be added:

-- It needs a BIP16 script entry (this was created for vanilla multi-sig
before BIP 16 was created)
-- Comment lines
-- Version number
-- Use base58/64 encoding
-- Rigorous formatting spec
-- Binary representation
-- A better name than "Tx Distribution Proposal"

I'll be releasing the Beta version of Armory soon, and after that, I'll
probably be thinking about a multi-signature support interface.  That would
be a good time for me to tie in a better version of BIP 10 -- one that is
compatible with other clients implementing the same thing.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120614/69ba203b/attachment.html>

From luke at dashjr.org  Thu Jun 14 15:00:45 2012
From: luke at dashjr.org (Luke-Jr)
Date: Thu, 14 Jun 2012 15:00:45 +0000
Subject: [Bitcoin-development] BIP16 backport bug (0.4.x and 0.5.x stuck on
	block 177617)
Message-ID: <201206141500.49573.luke@dashjr.org>

Block 177618 was rejected by BIP16-enabled backports (0.4.x and 0.5.x) due to 
containing a P2SH redemption with over 200 bytes in. Since the BIP16 code uses 
IsPushOnly to check the scriptSig for compliance, and IsPushOnly in these 
versions also enforced the 200-byte "is standard" rule, they were effectively 
treating it as a network rule. This was not a problem in 0.6 because the 
original OP_EVAL commit (e679ec9) moved the check outside of IsPushOnly.

This problem could have been avoided if either IsPushOnly was renamed when its 
semantics/behaviour changed significantly, or I inspected the OP_EVAL commit 
in detail instead of skipping it over as a new feature and not bugfixes. 
Additionally, it might have helped, if the commit message mentioned the 
change, but I'd probably have still missed it as it wasn't relevant until 
months later.

I will be releasing 0.4.7 and 0.5.6 hopefully in the next 24 hours to address 
this bug, along with instructions to get unstuck:
    1. Ensure you have the minimum required 1280 MB memory available
    2. Create a new file in your bitcoin directory (the same one with
       wallet.dat) called DB_CONFIG with the following two lines:
           set_lk_max_locks   1000000
           set_lk_max_objects 1000000
    3. Start bitcoind or Bitcoin-Qt
    4. WAIT AT LEAST SIX HOURS
       Your client will NOT show any signs of making progress during this time
    5. When complete, your client should be up-to-date on block count
    6. At this time, you may wish to delete the DB_CONFIG file and restart
       your client, to use less memory

Luke



From peter at coinlab.com  Thu Jun 14 14:37:26 2012
From: peter at coinlab.com (Peter Vessenes)
Date: Thu, 14 Jun 2012 10:37:26 -0400
Subject: [Bitcoin-development] Raw Transaction RPC calls for bitcoind
In-Reply-To: <CABsx9T0oZvVB1CPfa7Rk3CTnPOx8-CEm4K45pjACSyoqLz6YPA@mail.gmail.com>
References: <CABsx9T0oZvVB1CPfa7Rk3CTnPOx8-CEm4K45pjACSyoqLz6YPA@mail.gmail.com>
Message-ID: <CAMGNxUvYiOqr=4rTOPwC6pCa1MmxWcVAqMk62=85mLiAntgfwA@mail.gmail.com>

This is super cool!

I have a feature request: it would be awesome to be able to provide private
keys at the command line with the signature, turning the client into a
wallet-less signature machine.

Peter


On Thu, Jun 14, 2012 at 9:22 AM, Gavin Andresen <gavinandresen at gmail.com>wrote:

> I submitted a pull request yesterday that implements low-level "raw"
> transaction, and am looking for feedback on the API and help with
> trying to test/break it.
> ...
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120614/3dde553e/attachment.html>

From gavinandresen at gmail.com  Thu Jun 14 20:00:57 2012
From: gavinandresen at gmail.com (Gavin Andresen)
Date: Thu, 14 Jun 2012 16:00:57 -0400
Subject: [Bitcoin-development] Raw Transaction RPC calls for bitcoind
In-Reply-To: <CAMGNxUvYiOqr=4rTOPwC6pCa1MmxWcVAqMk62=85mLiAntgfwA@mail.gmail.com>
References: <CABsx9T0oZvVB1CPfa7Rk3CTnPOx8-CEm4K45pjACSyoqLz6YPA@mail.gmail.com>
	<CAMGNxUvYiOqr=4rTOPwC6pCa1MmxWcVAqMk62=85mLiAntgfwA@mail.gmail.com>
Message-ID: <CABsx9T1BUzn0ZBvWrEsrX_6BZ1LWCUdXqFsrO_vhfEfVUz=VHA@mail.gmail.com>

On Thu, Jun 14, 2012 at 10:37 AM, Peter Vessenes <peter at coinlab.com> wrote:
> This is super cool!
>
> I have a feature request: it would be awesome to be able to provide private
> keys at the command line with the signature, turning the client into a
> wallet-less signature machine.

I like that idea.

A third argument that is an array of private keys (in the same format
as the dumpprivkey RPC call) should be easy to support, assuming the
semantics are:

+ If third argument given, do not require that the wallet be unlocked,
and only sign using the private key(s) given (ignore the bitcoind
wallet entirely).
+ Private keys would stay in bitcoind memory only for the duration of
the RPC call.

-- 
--
Gavin Andresen



From mike at plan99.net  Fri Jun 15 11:29:51 2012
From: mike at plan99.net (Mike Hearn)
Date: Fri, 15 Jun 2012 13:29:51 +0200
Subject: [Bitcoin-development] Near-term scalability
Message-ID: <CANEZrP3w+AiTXmv9Wb3Zi5yyFmGPk82-ysVo4_DVvtg8HHBCdQ@mail.gmail.com>

I had to hit the sack last night as it was 2am CET, but I'd like to
sum up the discussion we had on IRC about scalability and SatoshiDice
in particular.

I think we all agreed on the following:

- Having senders/buyers pay no fees is psychologically desirable even
though we all understand that eventually, somebody, somewhere will be
paying fees to use Bitcoin

- In the ideal world Bitcoin would scale perfectly and there would be
no need for there to be some "winners" and some "losers" when it comes
to confirmation time.

There was discussion of some one-off changes to address the current
situation, namely de-ranking transactions that re-use addresses. Gavin
and myself were not keen on this idea, primarily because it just
avoids the real problem and Bitcoin already has a good way to
prioritize transactions via the fees mechanism itself. The real issue
is that SatoshiDice does indeed pay fees and generates a lot of
transactions, pushing more traditional traffic out due to artificial
throttles.

The following set of proposals were discussed:

(1) Change the mining code to group transactions together with their
mempool dependencies and then calculate all fees as a group. A tx with
a fee of 1 BTC that depends on 5 txns with zero fees would result in
all 6 transactions being considered to have a fee of 1BTC and
therefore become prioritized for inclusion. This allows a transition
to "receiver pays" model for fees. There are many advantages. One is
that it actually makes sense ... it's always the receiver who wants
confirmations because it's the receiver that fears double spends.
Senders never do. What's more, whilst Bitcoin is designed to operate
on a zero-trust model in the real world trust often exists and it can
be used to optimize by passing groups of transactions around with
their dependencies, until that group passes a trust boundary and gets
broadcast with a send-to-self tx to add fees. Another advantage is it
simplifies usage for end users who primarily buy rather than sell,
because it avoids the need to guess at fees, one of the most
problematic parts of Bitcoins design now.

The disadvantages are that it can result in extra transactions that
exist only for adding fees, and it requires a more modern payment
protocol than the direct-IP protocol Satoshi designed.

It would help address the current situation by avoiding angry users
who want to buy things, but don't know what fee to set and so their
transactions get stuck.

(2) SatoshiDice should use the same fee algorithms as Bitcoin-Qt to
avoid paying excessive fees and queue-jumping. Guess that's on my
plate.

(3) Scalability improvements seem like a no brainer to everyone, it's
just a case of how complicated they are.

(4) Making the block size limit float is better than picking a new
arbitrary threshold.

On the forums Matt stated that block chain pruning was a no-go because
"it makes bitcoin more centralized". I think we've thrashed this one
out sufficiently well by now that there should be a united opinion on
it. There are technical ways to implement it such that there is no
change of trust requirements. All the other issues (finding archival
nodes, etc) can be again addressed with sufficient programming.

For the case of huge blocks slowing down end user syncing and wasting
their resources, SPV clients like MultiBit and Android Wallet already
exist and will get better with time. If Jeff implements the bloom
filtering p2p commands I'll make bitcoinj use them and that'll knock
out excessive bandwidth usage and parse overheads from end users who
are on these clients. At some point Bitcoin-Qt can have a dual mode,
but who knows when that'll get implemented.

Does that all sound reasonable?



From mike at plan99.net  Fri Jun 15 11:52:56 2012
From: mike at plan99.net (Mike Hearn)
Date: Fri, 15 Jun 2012 13:52:56 +0200
Subject: [Bitcoin-development] New P2P commands for diagnostics,
	SPV clients
In-Reply-To: <CANEZrP0kNZDByHpK2=UjP+ag0X1KmqHxnJdm=e_pWMitP4QvvA@mail.gmail.com>
References: <CA+8xBpecVQcTTbPxUm_3_GWC99dEd4=-VFWb+QT6jUy4rg8U4w@mail.gmail.com>
	<CANEZrP0kNZDByHpK2=UjP+ag0X1KmqHxnJdm=e_pWMitP4QvvA@mail.gmail.com>
Message-ID: <CANEZrP0Tuzax2y9rjKmj12KP31ac96QaiuGYOxe2FnFBNu9jUA@mail.gmail.com>

> Need to specify the format of how these arrive. It means that when a
> new block is found instead of inv<->getdata<->block we'd see something
> like ?inv<->getdata<->merkleblock where a "merkleblock" structure is a
> header + list of transactions + list of merkle branches linking them
> to the root.

Thinking about it some more and re-reading the Scalability wiki page,
I remembered that a nice bandwidth optimization to the protocol is to
distribute blocks as header+list of tx hashes. If a node has already
seen that tx before (eg, it's in the mempool) there is no need to send
it again.

With the new command to download the contents of the mempool on
startup, this means that blocks could potentially propagate across the
network faster as download time is taken out of the equation, and
indeed, with the signature cache the hard work of verifying is already
done. So this could also help reduce orphan blocks and spurious chain
splits.

Are you planning on implementing any of this Jeff? I think we have the
opportunity to kill a few birds with one or two stones.



From mike at plan99.net  Fri Jun 15 13:23:11 2012
From: mike at plan99.net (Mike Hearn)
Date: Fri, 15 Jun 2012 15:23:11 +0200
Subject: [Bitcoin-development] New P2P commands for diagnostics,
	SPV clients
In-Reply-To: <1339766346.31489.49.camel@bmthinkpad>
References: <CA+8xBpecVQcTTbPxUm_3_GWC99dEd4=-VFWb+QT6jUy4rg8U4w@mail.gmail.com>
	<CANEZrP0kNZDByHpK2=UjP+ag0X1KmqHxnJdm=e_pWMitP4QvvA@mail.gmail.com>
	<1339766346.31489.49.camel@bmthinkpad>
Message-ID: <CANEZrP3jj2ymQPH50g2PvzZhRzTnUnCLUjvBYj8ndBCJsnGJ-w@mail.gmail.com>

> > Why not combine these two?
>
> I believe its because it allows the node which will have to use the
> bloom filter to scan transactions to chose how much effort it wants to
> put into each transaction on behalf of the SPV client.

If that's the case then the negotiation protocol needs to be specified
too. It seems heavy though. If a node is getting overloaded it could
just disconnect intensive peers or refuse new connections.



From bitcoin-list at bluematt.me  Fri Jun 15 13:08:55 2012
From: bitcoin-list at bluematt.me (Matt Corallo)
Date: Fri, 15 Jun 2012 15:08:55 +0200
Subject: [Bitcoin-development] Near-term scalability
In-Reply-To: <CANEZrP3w+AiTXmv9Wb3Zi5yyFmGPk82-ysVo4_DVvtg8HHBCdQ@mail.gmail.com>
References: <CANEZrP3w+AiTXmv9Wb3Zi5yyFmGPk82-ysVo4_DVvtg8HHBCdQ@mail.gmail.com>
Message-ID: <1339765735.31489.40.camel@bmthinkpad>

On Fri, 2012-06-15 at 13:29 +0200, Mike Hearn wrote:
> I had to hit the sack last night as it was 2am CET, but I'd like to
> sum up the discussion we had on IRC about scalability and SatoshiDice
> in particular.
> 
> I think we all agreed on the following:
> 
> - Having senders/buyers pay no fees is psychologically desirable even
> though we all understand that eventually, somebody, somewhere will be
> paying fees to use Bitcoin
> 
> - In the ideal world Bitcoin would scale perfectly and there would be
> no need for there to be some "winners" and some "losers" when it comes
> to confirmation time.
> 
> There was discussion of some one-off changes to address the current
> situation, namely de-ranking transactions that re-use addresses. Gavin
> and myself were not keen on this idea, primarily because it just
> avoids the real problem and Bitcoin already has a good way to
> prioritize transactions via the fees mechanism itself. The real issue
> is that SatoshiDice does indeed pay fees and generates a lot of
> transactions, pushing more traditional traffic out due to artificial
> throttles.
The idea can be more generalized in that there are many cases where the
generator of a transaction doesn't care about confirmation times, and
would really be willing to make their transaction lower priority than
other 0-fee transactions.  This enables the first point with lower
confirmation times for a while longer.
As it turns out, we already have an indication that someone is willing
to wait longer for confirmations - rapid reuse of an address.  
1) Green Addresses: The whole point of a green address is that you are
trusted based on your address, not necessarily based on confirmations of
your transactions.  In this case, you are generally willing to wait a
bit longer for confirmations than the average user depositing coins into
their Mt. Gox account.  
2) Donation Addresses: If you are using a publicized donation address,
you probably aren't depending on getting your coins *now* to turn around
and ship a product and, again, you are a bit more willing to tolerate
longer confirmation times.
3) Lazy (or overworked) coders: If, for whatever reason, someone
designing a bitcoin site decides that it is simply easier to make users
pay to a single address for everything, such actions should generally be
discouraged.  Such a setup is worse for end-user privacy.  Also, such
laziness (or likely just overworked and not having time to fix the
issue) is likely also laziness across the board including ignoring
multisend for payouts.  If you discourage such address use forcing site
designers to implement more sane policies, hopefully they will do enough
research to also do multisend.  Note that though this is where one
addresses sites like SatoshiDice, its also the one where we are likely
to have the least impact...

One of the ways to implement such deprioritization of rapidly-reused
addresses is to limit the count of address re-uses by default in memory
pool.  By limiting relaying of such transactions, you a) give nodes
across the network some small say in the transactions which they have to
deal with relaying outside of blocks, instead of relying on miners to
make decisions which are good for the total network load, but which are
worse for them.  b) You allow sites which wish to re-use addresses to do
so initially to keep the time-to-launch the same as it is today, but
force them to re-think their design decisions as they grow to
(hopefully) decrease their impact on the average Bitcoin full-node
operator.  Sites which begin to see their transactions rate-limited have
several options:
1) Make a deal with a miner to feed them their list of now-non-relayed
transactions outside of the regular p2p network and have them manually
added to blocks.  Id argue that such setups are going to become more
common in the future and such out-of-band transaction relaying should be
encouraged.  This also shifts the delay for other transactions from a
constant delay getting into blocks until there is room for additional
0-fee transactions to a spike on each block from the given miner.  I
highly prefer this, as you would see usually only one or two block delay
getting your transaction confirmed at the worst case, instead of a very
fuzzy unknown delay that could stretch on for some time.
2) Use rotating addresses.  This is likely the simplest to implement,
and I would absolutely think this is what most sites would end up doing.
Though it doesn't result in a decreased load on the transaction-relaying
nodes, it does at least allow for a minor improvement in user privacy.  

In the end, it boils down to an optional transaction deprioritization.
> 
> The following set of proposals were discussed:
> 
> (1) Change the mining code to group transactions together with their
> mempool dependencies and then calculate all fees as a group. A tx with
> a fee of 1 BTC that depends on 5 txns with zero fees would result in
> all 6 transactions being considered to have a fee of 1BTC and
> therefore become prioritized for inclusion. This allows a transition
> to "receiver pays" model for fees. There are many advantages. One is
> that it actually makes sense ... it's always the receiver who wants
> confirmations because it's the receiver that fears double spends.
> Senders never do. What's more, whilst Bitcoin is designed to operate
> on a zero-trust model in the real world trust often exists and it can
> be used to optimize by passing groups of transactions around with
> their dependencies, until that group passes a trust boundary and gets
> broadcast with a send-to-self tx to add fees. Another advantage is it
> simplifies usage for end users who primarily buy rather than sell,
> because it avoids the need to guess at fees, one of the most
> problematic parts of Bitcoins design now.
> 
> The disadvantages are that it can result in extra transactions that
> exist only for adding fees, and it requires a more modern payment
> protocol than the direct-IP protocol Satoshi designed.
> 
> It would help address the current situation by avoiding angry users
> who want to buy things, but don't know what fee to set and so their
> transactions get stuck.
> 
> (2) SatoshiDice should use the same fee algorithms as Bitcoin-Qt to
> avoid paying excessive fees and queue-jumping. Guess that's on my
> plate.
> 
> (3) Scalability improvements seem like a no brainer to everyone, it's
> just a case of how complicated they are.
I think all of the above are largely no brianers to everyone.
> 
> (4) Making the block size limit float is better than picking a new
> arbitrary threshold.
Definitely something that is very appealing as we need to scale up.
> 
> On the forums Matt stated that block chain pruning was a no-go because
> "it makes bitcoin more centralized". I think we've thrashed this one
> out sufficiently well by now that there should be a united opinion on
> it. There are technical ways to implement it such that there is no
> change of trust requirements. All the other issues (finding archival
> nodes, etc) can be again addressed with sufficient programming.
My point was that the easiest way to do it would be to ship a pruned
snapshot with Bitcoin, and such a system, while verifiable, would
increase Bitocin's centralization.  Though it is quite possible to prune
the chain while downloading at checkpoints or when blocks are N deep, it
complicates the initial download if no one has the chain to begin with. 

Another point I made was that by doing chain pruning by default, we may
see a decrease in non-fClient nodes (for compatibility, I would assume
pruned nodes have to set fClient) which is what old clients look for to
connect to, possibly complicating using Bitcoin for clients that either
wish to run a full IBD or older clients which need a non-fClient node
before they are happy (which could be an issue when you look at the very
poor "upgrade-apathy" in the Bitcoin community with people running
long-outdated versions because they don't feel like upgrading).

All that said, I do believe pruning will eventually have to come to
encourage p2pool and other getmemorypool-based pool mining, but
(obviously) its something that needs careful consideration in its
overall effects across the network before its applied.
> 
> For the case of huge blocks slowing down end user syncing and wasting
> their resources, SPV clients like MultiBit and Android Wallet already
> exist and will get better with time. If Jeff implements the bloom
> filtering p2p commands I'll make bitcoinj use them and that'll knock
> out excessive bandwidth usage and parse overheads from end users who
> are on these clients. At some point Bitcoin-Qt can have a dual mode,
> but who knows when that'll get implemented.
> 
> Does that all sound reasonable?





From bitcoin-list at bluematt.me  Fri Jun 15 13:19:06 2012
From: bitcoin-list at bluematt.me (Matt Corallo)
Date: Fri, 15 Jun 2012 15:19:06 +0200
Subject: [Bitcoin-development] New P2P commands for diagnostics,
 SPV clients
In-Reply-To: <CANEZrP0kNZDByHpK2=UjP+ag0X1KmqHxnJdm=e_pWMitP4QvvA@mail.gmail.com>
References: <CA+8xBpecVQcTTbPxUm_3_GWC99dEd4=-VFWb+QT6jUy4rg8U4w@mail.gmail.com>
	<CANEZrP0kNZDByHpK2=UjP+ag0X1KmqHxnJdm=e_pWMitP4QvvA@mail.gmail.com>
Message-ID: <1339766346.31489.49.camel@bmthinkpad>

On Thu, 2012-06-14 at 13:52 +0200, Mike Hearn wrote:
> > filterinit(false positive rate, number of elements): initialize
> > filterload(data): input a serialized bloom filter table metadata and data.
> 
> Why not combine these two?
I believe its because it allows the node which will have to use the
bloom filter to scan transactions to chose how much effort it wants to
put into each transaction on behalf of the SPV client.  Though its
generally a small amount of CPU time/memory, if we end up with a drastic
split between SPV nodes and only a few large network nodes, those nodes
may wish to limit the CPU/memory usage each node is allowed to use,
which may be important if you are serving 1000 SPV peers.  It offers a
sort of negotiation between SPV client and full node instead of letting
the client specify it outright.
> 
> > 'filterload' and 'filteradd' enable special behavior changes for
> > 'mempool' and existing P2P commands, whereby only transactions
> > matching the bloom filter will be announced to the connection, and
> > only matching transactions will be sent inside serialized blocks.
> 
> Need to specify the format of how these arrive. It means that when a
> new block is found instead of inv<->getdata<->block we'd see something
> like  inv<->getdata<->merkleblock where a "merkleblock" structure is a
> header + list of transactions + list of merkle branches linking them
> to the root. I think CMerkleTx already knows how to serialize this,
> but it redundantly includes the block hash which would not be
> necessary for a merkleblock message.
A series of CMerkleTx's might also end up redundantly encoding branches
of the merkle tree, so, yes as a part of the BIP/implementation, I would
say we probably want a CFilteredBlock or similar




From jgarzik at exmulti.com  Fri Jun 15 13:26:18 2012
From: jgarzik at exmulti.com (Jeff Garzik)
Date: Fri, 15 Jun 2012 09:26:18 -0400
Subject: [Bitcoin-development] New P2P commands for diagnostics,
	SPV clients
In-Reply-To: <CANEZrP0kNZDByHpK2=UjP+ag0X1KmqHxnJdm=e_pWMitP4QvvA@mail.gmail.com>
References: <CA+8xBpecVQcTTbPxUm_3_GWC99dEd4=-VFWb+QT6jUy4rg8U4w@mail.gmail.com>
	<CANEZrP0kNZDByHpK2=UjP+ag0X1KmqHxnJdm=e_pWMitP4QvvA@mail.gmail.com>
Message-ID: <CA+8xBpcwhQPQRe=stYb9xksLsTbiABKLS7PZnRtvPga6AmSg4Q@mail.gmail.com>

On Thu, Jun 14, 2012 at 7:52 AM, Mike Hearn <mike at plan99.net> wrote:
>> filterinit(false positive rate, number of elements): initialize
>> filterload(data): input a serialized bloom filter table metadata and data.
>
> Why not combine these two?

This is a fair point that sipa raised.

Consensus concluded that 'filterload' includes all necessary metadata
required to initialize a bloom filter.  That implies 'filterinit'
would only be needed for 'filteradd'.  If we don't think 'filteradd'
has a compelling use case, filterinit + filteradd can be dropped.

>> 'filterload' and 'filteradd' enable special behavior changes for
>> 'mempool' and existing P2P commands, whereby only transactions
>> matching the bloom filter will be announced to the connection, and
>> only matching transactions will be sent inside serialized blocks.
>
> Need to specify the format of how these arrive. It means that when a
> new block is found instead of inv<->getdata<->block we'd see something
> like ?inv<->getdata<->merkleblock where a "merkleblock" structure is a
> header + list of transactions + list of merkle branches linking them
> to the root. I think CMerkleTx already knows how to serialize this,
> but it redundantly includes the block hash which would not be
> necessary for a merkleblock message.

Yes, the format is something that must be hashed out (no pun
intended).  Need input from potential users about what information
they might need.

-- 
Jeff Garzik
exMULTI, Inc.
jgarzik at exmulti.com



From mike at plan99.net  Fri Jun 15 13:34:19 2012
From: mike at plan99.net (Mike Hearn)
Date: Fri, 15 Jun 2012 15:34:19 +0200
Subject: [Bitcoin-development] Near-term scalability
In-Reply-To: <1339765735.31489.40.camel@bmthinkpad>
References: <CANEZrP3w+AiTXmv9Wb3Zi5yyFmGPk82-ysVo4_DVvtg8HHBCdQ@mail.gmail.com>
	<1339765735.31489.40.camel@bmthinkpad>
Message-ID: <CANEZrP2rZEwQqkceTN3yOqx_Mo_8gyRyBUgv8NnfKd8ZWGYzww@mail.gmail.com>

> The idea can be more generalized in that there are many cases where the
> generator of a transaction doesn't care about confirmation times, and
> would really be willing to make their transaction lower priority than
> other 0-fee transactions.

Just to be clear, I think this solution is a hack and don't support it
because it's yet another change of network rules. Some random people
will get whacked because of a heuristic "rule of thumb".

If it's implemented, SD could/would switch to fresh addresses and
nothing would have been achieved except making an already complex
system more complex.

I disagree with the notion that you need "less important than free".
If you care about the confirmation time of a transaction that was sent
to you and you need space in a limited resource, you can pay for it.
It's an auction like any other. Besides, the idea that transactions
are free today is just a psychological trick befitting governments but
not us - transactions are funded by aggressive hyperinflation. I would
never describe Bitcoin as a free system and I suggest nobody else does
either.

If grouped fee calculations are implemented, we can keep the nice
property that the person who cares about double spending risk pays the
fees, and if you assume most transactions are hub-and-spoke from
buyers to merchants, rather than a pure p2p graph, in practice it'll
work out to seeming free most of the time even if seen globally it
doesn't make much difference.

> My point was that the easiest way to do it would be to ship a pruned
> snapshot with Bitcoin, and such a system, while verifiable, would
> increase Bitocin's centralization.

I'm not sure why. If you want to audit everything from scratch, after
checking the code you could just blow away the included files and then
"-connect=archive.bitcoin.org" or something like that. After
rebuilding the chain from scratch, check the databases for consistency
with the included data.

It reduces the number of nodes with full copies of the block chain,
yes, but as long as there's at least one copy of the old data in an
accessible location new nodes can still bootstrap just fine.

I'm sure we can find organizations willing to host full chains for
people who want to rebuild their databases from scratch, given how
cheap disk space is.

> connect to, possibly complicating using Bitcoin for clients that either
> wish to run a full IBD or older clients which need a non-fClient node

Yes, but old nodes probably have a copy of the chain already, so it
wouldn't affect them. New blocks would still be fully distributed,
right?

The only case where it'd cause issues is if you install a fresh copy
of a very old node. Not a common occurrence, and those nodes will have
to wait until they find an archival node announcing itself. Those
nodes could be made to announce more frequently than normal, if need
be.



From mike at plan99.net  Fri Jun 15 13:43:06 2012
From: mike at plan99.net (Mike Hearn)
Date: Fri, 15 Jun 2012 15:43:06 +0200
Subject: [Bitcoin-development] New P2P commands for diagnostics,
	SPV clients
In-Reply-To: <CA+8xBpcwhQPQRe=stYb9xksLsTbiABKLS7PZnRtvPga6AmSg4Q@mail.gmail.com>
References: <CA+8xBpecVQcTTbPxUm_3_GWC99dEd4=-VFWb+QT6jUy4rg8U4w@mail.gmail.com>
	<CANEZrP0kNZDByHpK2=UjP+ag0X1KmqHxnJdm=e_pWMitP4QvvA@mail.gmail.com>
	<CA+8xBpcwhQPQRe=stYb9xksLsTbiABKLS7PZnRtvPga6AmSg4Q@mail.gmail.com>
Message-ID: <CANEZrP39RHfCDX-x4ARo+oPphLv-70RxuMh3+AJzsNPxzOd=bA@mail.gmail.com>

> Yes, the format is something that must be hashed out (no pun
> intended). ?Need input from potential users about what information
> they might need.

Matts point that a branch-per-transaction may duplicate data is well
made, that said, I suspect a format that tries to fix this would be
much more complicated.

How about see this project as a three part change?

First step - add the mempool command and make nodes sync up their
mempools on startup.

Second step - if protocol version >= X, the "block" message consists
of a header + num transactions + vector<hash>  instead of the full
transactions themselves.

On receiving such a block, we go look to see which transactions we're
missing from the mempool and request them with getdata. Each time we
receive a tx message we check to see if it was one we were missing
from a block. Once all transactions in the block message are in
memory, we go ahead and assemble the block, then verify as per normal.
This should speed up block propagation. Miners have an incentive to
upgrade because it should reduce wasted work.

Third step - new message, getmerkletx takes a vector<hash> and returns
a merkletx message: "merkle branch missing the root + transaction data
itself" for each requested transaction. The filtering commands are
added, so the block message now only lists transaction hashes that
match the filter which can then be requested with getmerkletx.



From bitcoin-list at bluematt.me  Fri Jun 15 14:39:44 2012
From: bitcoin-list at bluematt.me (Matt Corallo)
Date: Fri, 15 Jun 2012 16:39:44 +0200
Subject: [Bitcoin-development] New P2P commands for diagnostics,
 SPV clients
In-Reply-To: <CANEZrP3jj2ymQPH50g2PvzZhRzTnUnCLUjvBYj8ndBCJsnGJ-w@mail.gmail.com>
References: <CA+8xBpecVQcTTbPxUm_3_GWC99dEd4=-VFWb+QT6jUy4rg8U4w@mail.gmail.com>
	<CANEZrP0kNZDByHpK2=UjP+ag0X1KmqHxnJdm=e_pWMitP4QvvA@mail.gmail.com>
	<1339766346.31489.49.camel@bmthinkpad>
	<CANEZrP3jj2ymQPH50g2PvzZhRzTnUnCLUjvBYj8ndBCJsnGJ-w@mail.gmail.com>
Message-ID: <1339771184.31489.53.camel@bmthinkpad>

On Fri, 2012-06-15 at 15:23 +0200, Mike Hearn wrote:
> > > Why not combine these two?
> >
> > I believe its because it allows the node which will have to use the
> > bloom filter to scan transactions to chose how much effort it wants to
> > put into each transaction on behalf of the SPV client.
> 
> If that's the case then the negotiation protocol needs to be specified
> too. It seems heavy though. If a node is getting overloaded it could
> just disconnect intensive peers or refuse new connections.
IMHO it already is.  A node requests a filter using filterinit by
specifying the false positive rate it wants and a guessed number of
items.  The node which will have to hold that filter then responds with
the closest filter to what the SPV node requested that it is willing to
provide.  If the SPV node responds with a filterload command, it has
accepted the offer, otherwise it will simply disconnect and find a
better full node.  
I'd much rather have an overloaded node respond with 50% fp rate filters
as an option if there aren't many full nodes available than simply
disconnect SPV clients.
At least thats my thinking, but you may be right that it is too heavy
for too little gain.




From bitcoin-list at bluematt.me  Fri Jun 15 14:56:52 2012
From: bitcoin-list at bluematt.me (Matt Corallo)
Date: Fri, 15 Jun 2012 16:56:52 +0200
Subject: [Bitcoin-development] New P2P commands for diagnostics,
 SPV clients
In-Reply-To: <CANEZrP39RHfCDX-x4ARo+oPphLv-70RxuMh3+AJzsNPxzOd=bA@mail.gmail.com>
References: <CA+8xBpecVQcTTbPxUm_3_GWC99dEd4=-VFWb+QT6jUy4rg8U4w@mail.gmail.com>
	<CANEZrP0kNZDByHpK2=UjP+ag0X1KmqHxnJdm=e_pWMitP4QvvA@mail.gmail.com>
	<CA+8xBpcwhQPQRe=stYb9xksLsTbiABKLS7PZnRtvPga6AmSg4Q@mail.gmail.com>
	<CANEZrP39RHfCDX-x4ARo+oPphLv-70RxuMh3+AJzsNPxzOd=bA@mail.gmail.com>
Message-ID: <1339772212.31489.62.camel@bmthinkpad>

On Fri, 2012-06-15 at 15:43 +0200, Mike Hearn wrote:
> > Yes, the format is something that must be hashed out (no pun
> > intended).  Need input from potential users about what information
> > they might need.
> 
> Matts point that a branch-per-transaction may duplicate data is well
> made, that said, I suspect a format that tries to fix this would be
> much more complicated.
> 
> How about see this project as a three part change?
> 
> First step - add the mempool command and make nodes sync up their
> mempools on startup.
ACK
> 
> Second step - if protocol version >= X, the "block" message consists
> of a header + num transactions + vector<hash>  instead of the full
> transactions themselves.
If vector<hash> is sorted in the order of the merkle tree, you dont need
to forward the merkle tree to non-filtered nodes, further saving some
small amount of bandwidth.  For filtered nodes, you would still need to
forward merkle branches anyway.
> 
> On receiving such a block, we go look to see which transactions we're
> missing from the mempool and request them with getdata. Each time we
> receive a tx message we check to see if it was one we were missing
> from a block. Once all transactions in the block message are in
> memory, we go ahead and assemble the block, then verify as per normal.
> This should speed up block propagation. Miners have an incentive to
> upgrade because it should reduce wasted work.
ACK
> 
> Third step - new message, getmerkletx takes a vector<hash> and returns
> a merkletx message: "merkle branch missing the root + transaction data
> itself" for each requested transaction. The filtering commands are
> added, so the block message now only lists transaction hashes that
> match the filter which can then be requested with getmerkletx.
I really dont think it would be /that/ difficult to make it getmerkletxs
vector<hashes>. And then respond with a partial merkle tree to those
transactions.

Matt




From peter at coinlab.com  Fri Jun 15 14:59:40 2012
From: peter at coinlab.com (Peter Vessenes)
Date: Fri, 15 Jun 2012 10:59:40 -0400
Subject: [Bitcoin-development] Suggestion for Simplifying development work
Message-ID: <CAMGNxUuPyS+NCfHaoBuc-dvm+m7wyqj2sUReyZiP3hbEQnNeRg@mail.gmail.com>

Hi all,

I've been wondering about whether it would be possible to wipe out the GUI
completely from the satoshi client, and reimplement any necessary data
requests as RPC calls, allowing us to fork -QT and other GUIs over and
(hopefully) dramatically simplifying the codebase that you all have to work
on.

Any thoughts about this? Once a week at least I find myself wanting to find
ways to help speed up development, this seems like it could be a big win.

Peter

-- 
Peter J. Vessenes
CEO, CoinLab
M: 206.595.9839
Skype: vessenes
Google+ <https://plus.google.com/112885659993091300749>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120615/79d6e61a/attachment.html>

From jgarzik at exmulti.com  Fri Jun 15 15:32:06 2012
From: jgarzik at exmulti.com (Jeff Garzik)
Date: Fri, 15 Jun 2012 11:32:06 -0400
Subject: [Bitcoin-development] New P2P commands for diagnostics,
	SPV clients
In-Reply-To: <CANEZrP39RHfCDX-x4ARo+oPphLv-70RxuMh3+AJzsNPxzOd=bA@mail.gmail.com>
References: <CA+8xBpecVQcTTbPxUm_3_GWC99dEd4=-VFWb+QT6jUy4rg8U4w@mail.gmail.com>
	<CANEZrP0kNZDByHpK2=UjP+ag0X1KmqHxnJdm=e_pWMitP4QvvA@mail.gmail.com>
	<CA+8xBpcwhQPQRe=stYb9xksLsTbiABKLS7PZnRtvPga6AmSg4Q@mail.gmail.com>
	<CANEZrP39RHfCDX-x4ARo+oPphLv-70RxuMh3+AJzsNPxzOd=bA@mail.gmail.com>
Message-ID: <CA+8xBpfHpMX5FUJz4HSYxOoeDEGZ0S4Ufz1pRYbwJLcPiVw_CA@mail.gmail.com>

On Fri, Jun 15, 2012 at 9:43 AM, Mike Hearn <mike at plan99.net> wrote:
> How about see this project as a three part change?
>
> First step - add the mempool command and make nodes sync up their
> mempools on startup.

Here's the "mempool" implementation:
https://github.com/bitcoin/bitcoin/pull/1470

SPV nodes would definitely want to sync up their mempool upon startup.
 As for full nodes... I like the organic growth and random nature of
the mempools.  On the fence, WRT full node mempool sync at startup.

-- 
Jeff Garzik
exMULTI, Inc.
jgarzik at exmulti.com



From simon at superduper.net  Fri Jun 15 15:43:04 2012
From: simon at superduper.net (Simon Barber)
Date: Fri, 15 Jun 2012 08:43:04 -0700
Subject: [Bitcoin-development] New P2P commands for diagnostics,
	SPV clients
In-Reply-To: <CANEZrP0kNZDByHpK2=UjP+ag0X1KmqHxnJdm=e_pWMitP4QvvA@mail.gmail.com>
References: <CA+8xBpecVQcTTbPxUm_3_GWC99dEd4=-VFWb+QT6jUy4rg8U4w@mail.gmail.com>
	<CANEZrP0kNZDByHpK2=UjP+ag0X1KmqHxnJdm=e_pWMitP4QvvA@mail.gmail.com>
Message-ID: <4FDB5808.2060506@superduper.net>

separate filterinit / filterload - so you can do a new filterload later 
on if your list changes, without the privacy implications of filteradd.

Simon


On Thu 14 Jun 2012 04:52:29 AM PDT, Mike Hearn wrote:
>> filterinit(false positive rate, number of elements): initialize
>> filterload(data): input a serialized bloom filter table metadata and data.
>
> Why not combine these two?
>
>> 'filterload' and 'filteradd' enable special behavior changes for
>> 'mempool' and existing P2P commands, whereby only transactions
>> matching the bloom filter will be announced to the connection, and
>> only matching transactions will be sent inside serialized blocks.
>
> Need to specify the format of how these arrive. It means that when a
> new block is found instead of inv<->getdata<->block we'd see something
> like  inv<->getdata<->merkleblock where a "merkleblock" structure is a
> header + list of transactions + list of merkle branches linking them
> to the root. I think CMerkleTx already knows how to serialize this,
> but it redundantly includes the block hash which would not be
> necessary for a merkleblock message.
>
> ------------------------------------------------------------------------------
> Live Security Virtual Conference
> Exclusive live event will cover all the ways today's security and
> threat landscape has changed and how IT managers can respond. Discussions
> will include endpoint security, mobile security and the latest in malware
> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
> _______________________________________________
> Bitcoin-development mailing list
> Bitcoin-development at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/bitcoin-development



From laanwj at gmail.com  Fri Jun 15 15:59:02 2012
From: laanwj at gmail.com (Wladimir)
Date: Fri, 15 Jun 2012 17:59:02 +0200
Subject: [Bitcoin-development] Suggestion for Simplifying development
	work
In-Reply-To: <CAMGNxUuPyS+NCfHaoBuc-dvm+m7wyqj2sUReyZiP3hbEQnNeRg@mail.gmail.com>
References: <CAMGNxUuPyS+NCfHaoBuc-dvm+m7wyqj2sUReyZiP3hbEQnNeRg@mail.gmail.com>
Message-ID: <CA+s+GJBrXL0QVPraM6mR1_ZpRtVri_kN6iy5Q8C0_EegrL+_6w@mail.gmail.com>

On Fri, Jun 15, 2012 at 4:59 PM, Peter Vessenes <peter at coinlab.com> wrote:

> Hi all,
>
> I've been wondering about whether it would be possible to wipe out the GUI
> completely from the satoshi client, and reimplement any necessary data
> requests as RPC calls, allowing us to fork -QT and other GUIs over and
> (hopefully) dramatically simplifying the codebase that you all have to work
> on.
>

Splitting the UI into a seperate *process* is a long-term goal. The UI code
is structured so that all communication with the core happens through a
"bottleneck" (consisting of the model classes), so preparation has been
under way.

However, the current RPC calls don't suffice to implement a full-featured,
responsive UI. I'm not even sure JSON-RPC is a good fit for a UI<->core
protocol, as it doesn't support bidirectional communication (at least
without pretty ugly hacks).

But what exactly is the problem with having a GUI as part of the main
client project? I don't see how it would "speed up development" to split
the project. By far most of the users use the program through the UI so it
is one of the drivers for requirements on the core, and I'd think it is
pretty important to keep it a first-class citizen.

Wladimir
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120615/f4a07b7b/attachment.html>

From bitcoin-list at bluematt.me  Fri Jun 15 16:18:36 2012
From: bitcoin-list at bluematt.me (Matt Corallo)
Date: Fri, 15 Jun 2012 18:18:36 +0200
Subject: [Bitcoin-development] Near-term scalability
In-Reply-To: <CANEZrP2rZEwQqkceTN3yOqx_Mo_8gyRyBUgv8NnfKd8ZWGYzww@mail.gmail.com>
References: <CANEZrP3w+AiTXmv9Wb3Zi5yyFmGPk82-ysVo4_DVvtg8HHBCdQ@mail.gmail.com>
	<1339765735.31489.40.camel@bmthinkpad>
	<CANEZrP2rZEwQqkceTN3yOqx_Mo_8gyRyBUgv8NnfKd8ZWGYzww@mail.gmail.com>
Message-ID: <1339777116.31489.87.camel@bmthinkpad>

On Fri, 2012-06-15 at 15:34 +0200, Mike Hearn wrote:
> > The idea can be more generalized in that there are many cases where the
> > generator of a transaction doesn't care about confirmation times, and
> > would really be willing to make their transaction lower priority than
> > other 0-fee transactions.
> 
> Just to be clear, I think this solution is a hack and don't support it
> because it's yet another change of network rules. Some random people
> will get whacked because of a heuristic "rule of thumb".
Its arguably not a change to network rules as its something that users
can already do today by patching their clients.  Obviously any
implementation would have sane defaults which allowed for a significant
number of transactions to/from a given address at a time, avoiding
whacking random people unless they are large enough that they should
really already be fully aware of how bitcoin works.
> 
> If it's implemented, SD could/would switch to fresh addresses and
> nothing would have been achieved except making an already complex
> system more complex.
I would think SD would switch to using fresh addresses for each bet.
But even that is a good thing, at least where user privacy is concerned.
However, I would hope that SD would see the rule tweak and, in order to
avoid having to generate a number of new addresses per second (or, if
they went the pool route, having a huge pool of many thousands of
addresses), they would consider implementing sendmulti support.
> 
> I disagree with the notion that you need "less important than free".
> If you care about the confirmation time of a transaction that was sent
> to you and you need space in a limited resource, you can pay for it.
> It's an auction like any other. Besides, the idea that transactions
> are free today is just a psychological trick befitting governments but
> not us - transactions are funded by aggressive hyperinflation. I would
> never describe Bitcoin as a free system and I suggest nobody else does
> either.
I agree, free transactions isnt something we should aggressively push as
a feature of Bitcoin, its simply not.  However, in the current system
free transactions are usually confirmed within a small number of blocks,
and for a number of users, that is an important feature that draws them
to get through the initial hurdles of converting money to Bitcoin and
understanding enough of the system to trust it.  I believe that if we
can incentive large transaction creators to avoid delaying free
transactions, we should and giving them the option to delay their own
transactions seems like a perfectly reasonable way to do so.  Even if
you drop all the per-address limit stuff, allowing transaction creators
to add a simple flag to transactions seems reasonable when they want to
encourage Bitcoin to continue to grow as it does today.  Obviously
keeping free transactions confirming won't be possible forever, but
hopefully that will be as a result of natural growth which can encourage
further growth without the need for free transactions and not as a
result of a few actors in the community creating a transaction volume
significantly greater than their user-base.
> 
> If grouped fee calculations are implemented, we can keep the nice
> property that the person who cares about double spending risk pays the
> fees, and if you assume most transactions are hub-and-spoke from
> buyers to merchants, rather than a pure p2p graph, in practice it'll
> work out to seeming free most of the time even if seen globally it
> doesn't make much difference.
ACK, thats an important thing to implement IMO, but I really dont see it
as something that replaces the option to deprioritize your own
transactions to below 0-fee transactions.  It could even allow users who
receive payouts which are below 0-fee transactions to place a fee on the
subsequent transactions to allow the payouts to confirm quicker (if done
right).
> 
> > My point was that the easiest way to do it would be to ship a pruned
> > snapshot with Bitcoin, and such a system, while verifiable, would
> > increase Bitocin's centralization.
> 
> I'm not sure why. If you want to audit everything from scratch, after
> checking the code you could just blow away the included files and then
> "-connect=archive.bitcoin.org" or something like that. After
> rebuilding the chain from scratch, check the databases for consistency
> with the included data.
I would be surprised if more than a handful of devs audit such a thing.
And I would say that does define an increase in centralization.
> 
> It reduces the number of nodes with full copies of the block chain,
> yes, but as long as there's at least one copy of the old data in an
> accessible location new nodes can still bootstrap just fine.
Sadly, old nodes do not know where to look for such data, and I'm fairly
certain people running old nodes don't read the forums enough to catch
when it is announced that old nodes should make sure to
-connect=archive.bitcoin.org in order to avoid initially having horrible
initial bootstrap times and eventually not being able to connect to
full-chain-serving nodes at all.
> 
> I'm sure we can find organizations willing to host full chains for
> people who want to rebuild their databases from scratch, given how
> cheap disk space is.
Sadly, disk space isnt the issue.  Each connection to bitcoind (not that
it cant be fixed, but currently) eats a nice chunk of memory.  An
organization that wants to provide nodes for old nodes to connect to
would need to have a significant number of open incoming connection
slots, have plenty of bandwidth for nodes that are in IBD and have
plenty of memory and CPU to manage all the connections.

> 
> > connect to, possibly complicating using Bitcoin for clients that either
> > wish to run a full IBD or older clients which need a non-fClient node
> 
> Yes, but old nodes probably have a copy of the chain already, so it
> wouldn't affect them. New blocks would still be fully distributed,
> right?
Sadly, BDB's infamous database corrupted messages appear all too often,
and the usual response is "delete the chain and resync."  I have a hard
time believing that old nodes will rarely be in IBD.  
> 
> The only case where it'd cause issues is if you install a fresh copy
> of a very old node. Not a common occurrence, and those nodes will have
> to wait until they find an archival node announcing itself. Those
> nodes could be made to announce more frequently than normal, if need
> be.
I agree that its very possible to have archival nodes available and to
make it work, but I have yet to see anyone doing any work to actually
get commitments to run archival nodes and I have yet to see any
discussion of what, exactly, that would entail.

Matt





From bitcoin-list at bluematt.me  Fri Jun 15 16:20:27 2012
From: bitcoin-list at bluematt.me (Matt Corallo)
Date: Fri, 15 Jun 2012 18:20:27 +0200
Subject: [Bitcoin-development] New P2P commands for diagnostics,
 SPV clients
In-Reply-To: <CA+8xBpfHpMX5FUJz4HSYxOoeDEGZ0S4Ufz1pRYbwJLcPiVw_CA@mail.gmail.com>
References: <CA+8xBpecVQcTTbPxUm_3_GWC99dEd4=-VFWb+QT6jUy4rg8U4w@mail.gmail.com>
	<CANEZrP0kNZDByHpK2=UjP+ag0X1KmqHxnJdm=e_pWMitP4QvvA@mail.gmail.com>
	<CA+8xBpcwhQPQRe=stYb9xksLsTbiABKLS7PZnRtvPga6AmSg4Q@mail.gmail.com>
	<CANEZrP39RHfCDX-x4ARo+oPphLv-70RxuMh3+AJzsNPxzOd=bA@mail.gmail.com>
	<CA+8xBpfHpMX5FUJz4HSYxOoeDEGZ0S4Ufz1pRYbwJLcPiVw_CA@mail.gmail.com>
Message-ID: <1339777227.31489.89.camel@bmthinkpad>

On Fri, 2012-06-15 at 11:32 -0400, Jeff Garzik wrote:
>  As for full nodes... I like the organic growth and random nature of
> the mempools.  On the fence, WRT full node mempool sync at startup.
> 
I dont particularly care either way, but I have a feeling miners will
really want that so that they can get fee-paying transactions right
away.

Matt




From jgarzik at exmulti.com  Fri Jun 15 16:40:46 2012
From: jgarzik at exmulti.com (Jeff Garzik)
Date: Fri, 15 Jun 2012 12:40:46 -0400
Subject: [Bitcoin-development] New P2P commands for diagnostics,
	SPV clients
In-Reply-To: <4FDB5808.2060506@superduper.net>
References: <CA+8xBpecVQcTTbPxUm_3_GWC99dEd4=-VFWb+QT6jUy4rg8U4w@mail.gmail.com>
	<CANEZrP0kNZDByHpK2=UjP+ag0X1KmqHxnJdm=e_pWMitP4QvvA@mail.gmail.com>
	<4FDB5808.2060506@superduper.net>
Message-ID: <CA+8xBpcOhdA=oOoPkNs3ingKkLMhM3g2a+JL3XDLHn5o=JrAFA@mail.gmail.com>

On Fri, Jun 15, 2012 at 11:43 AM, Simon Barber <simon at superduper.net> wrote:
> separate filterinit / filterload - so you can do a new filterload later on
> if your list changes, without the privacy implications of filteradd.

filterload loads a whole new bloom filter from scratch, in one atomic
operation.  Params set, table sized, data input into table.  A
separate filterinit does not make sense for filterload.

-- 
Jeff Garzik
exMULTI, Inc.
jgarzik at exmulti.com



From gmaxwell at gmail.com  Fri Jun 15 16:53:13 2012
From: gmaxwell at gmail.com (Gregory Maxwell)
Date: Fri, 15 Jun 2012 12:53:13 -0400
Subject: [Bitcoin-development]  Near-term scalability
In-Reply-To: <CAAS2fgTJ0UH0Gr6gVMNZwOiv41WzZVesyvNCULj8UfCPPGxQrw@mail.gmail.com>
References: <CANEZrP3w+AiTXmv9Wb3Zi5yyFmGPk82-ysVo4_DVvtg8HHBCdQ@mail.gmail.com>
	<CAAS2fgTJ0UH0Gr6gVMNZwOiv41WzZVesyvNCULj8UfCPPGxQrw@mail.gmail.com>
Message-ID: <CAAS2fgQ8Yo=t+owLWLXeqOKFXcaYcJA4dXube-z9Lh_UeQnuLw@mail.gmail.com>

[I originally sent an earlier version of this message to Mike off
list, but I figure it's worth adding to the public discussion]

On Fri, Jun 15, 2012 at 7:29 AM, Mike Hearn <mike at plan99.net> wrote:
> (4) Making the block size limit float is better than picking a new
> arbitrary threshold.
> On the forums Matt stated that block chain pruning was a no-go because
> "it makes bitcoin more centralized". I think we've thrashed this one
> out sufficiently well by now that there should be a united opinion on
> it.

By itself letting the size float has non-trivial existential risk. ?A
Bitcoin with expensive transactions due to competition for space in
blocks can be front-ended with fast payment systems and still provide
the promised decentralized currency. Bitcoin with a very large
blockchain and blocks does not. ?It would do the bitcoin users no good
to increase the transaction volume while concurrently making Bitcoin
more or less pointless over the alternatives.

Scalability must be improved, we can unite on that opinion. ?But
scalability can't come at the expense of what made Bitcoin worth
having in the first place.

Fortunately it appear to be possible to greatly increase the
scalability without compromising on keeping the costs of operating a
fully validating node very low, ?for example Pieter's experimentation
with txout+txid indexing (for the 'flip the chain' proposals)
indicates that the data required right now to validate further
transactions is only about 85MiB? and that would be somewhat smaller
with compression and with clients which intentionally try to reduce
the set of unspent transactions. ? Commitments to these indexes in the
chain would allow almost-full validating nodes with fairly limited
resources.  (Almost-full meaning they would not validate the history
long before they started, they'd trusted header difficulty for that. They
could still mine and otherwise act as full nodes).

Achieving scalability improvements without breaking the radical
decentralization will be a lot harder than just improving scalability
but it's effort that is justified if the scalability is actually
needed.

How much decentralization is needed in the end?  That isn't clear? "As
much as possible" should generally be the goal.  Modern currencies
aren't controlled by single parties but by tens of thousands of
parties locked in economic, legal, and political compromise that
limits their control.  In Bitcoin the traditional controls that keep
parties honest are non-existent and if they were just directly applied
we'd potentially lose the properties that make Bitcoin distinct and
useful (e.g. make all miners mine only with FED permission and you
just have a really bandwidth inefficient interface to the dollar).
Instead we have aggressive decentralization and autonomous rule
enforcement.

Mike pointed out that  "Before he left Satoshi made a comment saying
he used to think Bitcoin would need millions of nodes if it became
really popular, but in the end he thought it could do fine with just
tens of thousands"    I'm not so sure? and I think the truth is in
between.  Tens of thousands of nodes? run by a self-selecting bunch of
people who reap the greatest rewards from controlling the validation
of Bitcoin, who by that criteria necessarily have a lot in common with
each other and perhaps not with the regular users? could easily be an
outcome where control is _less_ publicly vested than popular
government controlled currencies.   We probably don't need the raw
numbers of nodes, but we need a distribution of ownership and a
distribution of interest (e.g. not a system by bankers for bankers) of
those nodes which I think can only be achieved by making them cheap to
operate and having a lot more than we actually need. ? though not so
much that it has to run on every laptop.

The core challenge is that the only obvious ways to justify the cost
of maintaining expensive validation infrastructure is because you
intend to manipulate the currency using it or because you intend to
prevent other people from manipulating the currency.  The latter
motivation is potentially subject to a tragedy of the commons? you
don't need to run a full validating node as long as 'enough' other
people do, and enough is a nice slippery slope to zero.   Right now
just the random computers I? some random geek? had at home prior to
Bitcoin could store over a hundred years of max size blocks and
process the maximum rate of transactions.   With the costs so low
there isn't any real question about a consolidation of validation
making Bitcoin pointless.  You could probably increase the scale 10x
without breaking that analysis  but beyond that unless the
cost-per-scale goes down a highly consolidated future seems likely.
40 years from now why would people use Bitcoin over centralized
private banknotes like paypal or democratic government controlled
currencies?

Perhaps Bitcoin transaction could transition to being more of the
same? controlled by a consortium of banks, exchanging gigabyte blocks
over terabit ethernet, but I think that would be sad.  An alternative
which was autonomous and decentralized even if the transactions were
somewhat slow or costly would be excellent competition for everything
else, and it's something I think man kind ought to have.



From moon at justmoon.de  Fri Jun 15 16:56:38 2012
From: moon at justmoon.de (Stefan Thomas)
Date: Fri, 15 Jun 2012 18:56:38 +0200
Subject: [Bitcoin-development] Near-term scalability
In-Reply-To: <CANEZrP3w+AiTXmv9Wb3Zi5yyFmGPk82-ysVo4_DVvtg8HHBCdQ@mail.gmail.com>
References: <CANEZrP3w+AiTXmv9Wb3Zi5yyFmGPk82-ysVo4_DVvtg8HHBCdQ@mail.gmail.com>
Message-ID: <4FDB6946.2020400@justmoon.de>

Thanks Mike for the writeup - I'm very sad to have missed the discussion
on IRC since fee economics are probably my favorite topic, but I'll try
to contribute to the email discussion instead.

> (4) Making the block size limit float is better than picking a new
> arbitrary threshold.

Fees are a product of both real and artificial limits to transaction
validation.

The artificial limits like the block size limit are essentially putting
a floor on prices by limiting supply beyond what it would otherwise be.
E.g. the network could confirm more transactions theoretically, but the
block size limit prevents it.

The real limits are the bandwidth, computing and memory resources of
participating nodes. For the sake of argument suppose a 1 TB block was
released into the network right now and we'll also assume there was no
block size limit of any kind. Many nodes would likely not be able to
successfully download this block in under 10-30 minutes, so there is a
very good chance that other miners will have generated two blocks before
this block makes its way to them.

What does this mean? The miner generating a 1 TB block knows this would
happen. So in terms of economic self interest he will generate the
largest possible block that he is still confident that other miners will
accept and process. A miner who receives a block will also consider
whether to build on it based on whether they think other miners will be
able to download it. In other words, if I receive a large block I may
decide not to mine on it, because I believe that the majority of mining
power will not mine on it - because it is either too large for them to
download or because their rules against large blocks reject it.

It's important to understand that in practice economic actors tend to
plan ahead. In other words, if there is no block size limit that doesn't
mean that there will be constant forks and total chaos. Rather, no miner
will ever want to have a block rejected due to size, there is plenty of
incentive to be conservative with your limits. Even if there are forks,
this simply means that miners have decided that they can make more money
by including more transactions at the cost of the occasional dud.

Therefore, from an economic perspective, we do not need a global block
size limit of any kind. As "guardians of the network" the only thing we
need to do is to let miners figure out what they wanna do.

HOWEVER, the existing economic incentives won't manifest unless somebody
translates them into code. We have to give our users (miners & endusers)
the tools to create a genuine fee-based verification market.

On the miner side: I would make the block size limit configurable with a
relatively high default. If the default is too low few people will
bother changing it, which means that it is not worth changing (because a
majority uses the default anyway), which means even fewer people will
change it and so on.

The block size limit should also be a soft rather than a hard limit -
here are some ideas for this:

- The default limit for accepting blocks from others should always be
significantly greater than the default limit for blocks that the client
itself will generate.

- There should be different size limits for side chains that are longer
than the currently active chain. In other words, I might reject a block
for being slightly too large, but if everyone else accepts it I should
eventually accept it too, and my client should also consider
automatically raising my size limit if this happens a lot.

The rationale for the soft limit is to allow for gradual upward
adjustment. It needs to be risky for individual miners to raise the size
of their blocks to new heights, but ideally there won't be one solid
wall for them to run into.

On the user side: I would display the fee on the Send Coins dialog and
allow users to choose a different fee per transaction. We also talked
about adding some UI feedback where the client tries to estimate how
long a transaction will take to confirm given a certain fee, based on
recent information about what it observed from the network. If the fee
can be changed on the Send Coins tab, then this could be a red, yellow,
green visual indication whether the fee is sufficient, adequate or
dangerously low.

A criticism one might raise is: "The block size limit is not to protect
miners, but to protect end users who may have less resources than miners
and can't download gigantic block chains." - That's a viewpoint that is
certainly valid. I believe that we will be able to do a lot just with
efficiency improvements, pruning, compression and whatnot. But when it
comes down to it, I'd prefer a large network with cheap
microtransactions even if that means that consumer hardware can't
operate as a standalone validating node anymore. Headers-only mode is
already a much-requested feature anyway and there are many ways of
improving the security of various header-only or lightweight protocols.

(I just saw Greg's message advocating the opposite viewpoint, I'll
respond to that as soon as I can.)


> (1) Change the mining code to group transactions together with their
> mempool dependencies and then calculate all fees as a group.

+1 Very good change. This would allow miners to maximize their revenue
and in doing so better represent the existing priorities that users
express through fees.


> There was discussion of some one-off changes to address the current
> situation, namely de-ranking transactions that re-use addresses.

Discouraging address reuse will not change the amount of transactions, I
think we all agree on that. As for whether it improves the
prioritization, I'm not sure. Use cases that we seek to discourage may
simply switch to random addresses and I don't agree in and of itself
this is a benefit (see item 4 below). Here are a few reasons one might
be against this proposal:

1) Certain use cases like green addresses will be forced to become more
complicated than they would otherwise need to be.

2) It will be harder to read information straight out of the block
chain, for example right now we can pretty easily see how much volume is
caused by Satoshi Dice, perhaps allowing us to make better decisions.

3) The address index that is used by block explorers and lightweight
client servers will grow unnecessarily (an address -> tx index will be
larger if the number of unique addresses increases given the same number
of txs), so for people like myself who work on that type of software
you're actually making our scalability equation slightly worse.

4) You're forcing people into privacy best practices which you think are
good, but others may not subscribe to. For example I have absolutely
zero interest in privacy, anyone who cares that I buy Bitcoins with my
salary and spend them on paragliding is welcome to know about it.
Frankly, if I cared about privacy I wouldn't be using Bitcoin. If other
people want to use mixing services and randomize their addresses and
communicate through Tor that's fine, but the client shouldn't force me
to do those things if I don't want to by "deprioritizing" my transactions.

5) We may not like firstbits, but the fact remains that for now they are
extremely popular, because they improve the user experience where we
failed to do so. If you deprioritize transactions to reused addresses
you'll for example deprioritize all/most of Girls Gone Bitcoin, which
(again, like it or not) is one of the few practical, sustainable niches
that Bitcoin has managed to carve out for itself so far.


> Having senders/buyers pay no fees is psychologically desirable even
> though we all understand that eventually, somebody, somewhere will be
> paying fees to use Bitcoin

Free is just an extreme form of cheap, so if we can make transactions
very cheap (through efficiency and very large blocks) then it will be
easier for charitable miners to include free transactions. In practice,
my prediction is that free transactions on the open network will simply
not be possible in the long run. Dirty hacks aside there is simply no
way of distinguishing a spam transaction from a charity-worthy
transaction. So the way I envision free transactions in the future is
that there may be miners in partnership with wallet providers like
BlockChain.info that let you submit feeless transactions straight to
them based on maybe a captcha or some ads. (For the purist, the captcha
challenge and response could be communicated across the bitcoin network,
but I think we agree that such things should ideally take place
out-of-band.)

That way, the available charity of miners who wish to include feeless
transactions would go to human users as opposed to the potentially
infinite demand of auto-generated feeless transactions.



On 6/15/2012 1:29 PM, Mike Hearn wrote:
> I had to hit the sack last night as it was 2am CET, but I'd like to
> sum up the discussion we had on IRC about scalability and SatoshiDice
> in particular.
>
> I think we all agreed on the following:
>
> - Having senders/buyers pay no fees is psychologically desirable even
> though we all understand that eventually, somebody, somewhere will be
> paying fees to use Bitcoin
>
> - In the ideal world Bitcoin would scale perfectly and there would be
> no need for there to be some "winners" and some "losers" when it comes
> to confirmation time.
>
> There was discussion of some one-off changes to address the current
> situation, namely de-ranking transactions that re-use addresses. Gavin
> and myself were not keen on this idea, primarily because it just
> avoids the real problem and Bitcoin already has a good way to
> prioritize transactions via the fees mechanism itself. The real issue
> is that SatoshiDice does indeed pay fees and generates a lot of
> transactions, pushing more traditional traffic out due to artificial
> throttles.
>
> The following set of proposals were discussed:
>
> (1) Change the mining code to group transactions together with their
> mempool dependencies and then calculate all fees as a group. A tx with
> a fee of 1 BTC that depends on 5 txns with zero fees would result in
> all 6 transactions being considered to have a fee of 1BTC and
> therefore become prioritized for inclusion. This allows a transition
> to "receiver pays" model for fees. There are many advantages. One is
> that it actually makes sense ... it's always the receiver who wants
> confirmations because it's the receiver that fears double spends.
> Senders never do. What's more, whilst Bitcoin is designed to operate
> on a zero-trust model in the real world trust often exists and it can
> be used to optimize by passing groups of transactions around with
> their dependencies, until that group passes a trust boundary and gets
> broadcast with a send-to-self tx to add fees. Another advantage is it
> simplifies usage for end users who primarily buy rather than sell,
> because it avoids the need to guess at fees, one of the most
> problematic parts of Bitcoins design now.
>
> The disadvantages are that it can result in extra transactions that
> exist only for adding fees, and it requires a more modern payment
> protocol than the direct-IP protocol Satoshi designed.
>
> It would help address the current situation by avoiding angry users
> who want to buy things, but don't know what fee to set and so their
> transactions get stuck.
>
> (2) SatoshiDice should use the same fee algorithms as Bitcoin-Qt to
> avoid paying excessive fees and queue-jumping. Guess that's on my
> plate.
>
> (3) Scalability improvements seem like a no brainer to everyone, it's
> just a case of how complicated they are.
>
> (4) Making the block size limit float is better than picking a new
> arbitrary threshold.
>
> On the forums Matt stated that block chain pruning was a no-go because
> "it makes bitcoin more centralized". I think we've thrashed this one
> out sufficiently well by now that there should be a united opinion on
> it. There are technical ways to implement it such that there is no
> change of trust requirements. All the other issues (finding archival
> nodes, etc) can be again addressed with sufficient programming.
>
> For the case of huge blocks slowing down end user syncing and wasting
> their resources, SPV clients like MultiBit and Android Wallet already
> exist and will get better with time. If Jeff implements the bloom
> filtering p2p commands I'll make bitcoinj use them and that'll knock
> out excessive bandwidth usage and parse overheads from end users who
> are on these clients. At some point Bitcoin-Qt can have a dual mode,
> but who knows when that'll get implemented.
>
> Does that all sound reasonable?
>
> ------------------------------------------------------------------------------
> Live Security Virtual Conference
> Exclusive live event will cover all the ways today's security and 
> threat landscape has changed and how IT managers can respond. Discussions 
> will include endpoint security, mobile security and the latest in malware 
> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
> _______________________________________________
> Bitcoin-development mailing list
> Bitcoin-development at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/bitcoin-development
>




From jgarzik at exmulti.com  Fri Jun 15 17:17:18 2012
From: jgarzik at exmulti.com (Jeff Garzik)
Date: Fri, 15 Jun 2012 13:17:18 -0400
Subject: [Bitcoin-development] SatoshiDice and Near-term scalability
Message-ID: <CA+8xBpde=y213zFeVjYoZgBstZ6GTnk84ADD8twaDJy2D6CHyw@mail.gmail.com>

On Fri, Jun 15, 2012 at 12:56 PM, Stefan Thomas <moon at justmoon.de> wrote:
> The artificial limits like the block size limit are essentially putting
[...]

Changing the block size is an item for the hard-fork list.  The chance
of the block size limit changing in the short term seems rather low...
 it is a "nuclear option."

Hard-fork requires a very high level of community buy-in, because it
shuts out older clients who will simply refuse to consider >1MB blocks
valid.

Anything approaching that level of change would need some good, hard
data indicating that SatoshiDice was shutting out the majority of
other traffic.  Does anyone measure mainnet "normal tx" confirmation
times on a regular basis?  Any other hard data?

Clearly SatoshiDice is a heavy user of the network, but there is a
vast difference between a good stress test and a network flood that is
shutting out non-SD users.

Can someone please help quantify the situation?  kthanks :)

-- 
Jeff Garzik
exMULTI, Inc.
jgarzik at exmulti.com



From moon at justmoon.de  Fri Jun 15 17:52:10 2012
From: moon at justmoon.de (Stefan Thomas)
Date: Fri, 15 Jun 2012 19:52:10 +0200
Subject: [Bitcoin-development] SatoshiDice and Near-term scalability
In-Reply-To: <CA+8xBpde=y213zFeVjYoZgBstZ6GTnk84ADD8twaDJy2D6CHyw@mail.gmail.com>
References: <CA+8xBpde=y213zFeVjYoZgBstZ6GTnk84ADD8twaDJy2D6CHyw@mail.gmail.com>
Message-ID: <4FDB764A.90909@justmoon.de>

I do agree that changing/lifting the block size limit is a hard fork
measure, but Mike raised the point and I do believe that whatever we
decide to do now will be informed by our long term plan as well. So I
think it is relevant to the discussion.

> Can someone please help quantify the situation?  kthanks :)

According to BlockChain.info we seem to have lots of small blocks of
0-50KB and some larger 200-300 KB blocks. So in terms of near term
measure one thing I would like to know is why miners (i.e. no miners at
all) are fully exhausting the available block size despite thousands of
transactions in the memory pool. I'm not too familiar with the default
inclusion rules, so that would certainly be interesting to understand.
There are probably some low hanging fruit here.

The fact that SatoshiDice is able to afford to pay 0.0005 BTC fees and
fill up the memory pool means that either users who care about speedy
confirmation have to pay higher fees, the average actual block size has
to go up or prioritization has to get smarter. If load increases more
then we need more of any of these three tendencies as well. (Note that
the last one is only a very limited fix, because as the high priority
transactions get confirmed faster, the low priority ones take even longer.)


On 6/15/2012 7:17 PM, Jeff Garzik wrote:
> On Fri, Jun 15, 2012 at 12:56 PM, Stefan Thomas <moon at justmoon.de> wrote:
>> The artificial limits like the block size limit are essentially putting
> [...]
>
> Changing the block size is an item for the hard-fork list.  The chance
> of the block size limit changing in the short term seems rather low...
>  it is a "nuclear option."
>
> Hard-fork requires a very high level of community buy-in, because it
> shuts out older clients who will simply refuse to consider >1MB blocks
> valid.
>
> Anything approaching that level of change would need some good, hard
> data indicating that SatoshiDice was shutting out the majority of
> other traffic.  Does anyone measure mainnet "normal tx" confirmation
> times on a regular basis?  Any other hard data?
>
> Clearly SatoshiDice is a heavy user of the network, but there is a
> vast difference between a good stress test and a network flood that is
> shutting out non-SD users.
>
> Can someone please help quantify the situation?  kthanks :)
>




From mike at coinlab.com  Fri Jun 15 17:37:04 2012
From: mike at coinlab.com (Mike Koss)
Date: Fri, 15 Jun 2012 10:37:04 -0700
Subject: [Bitcoin-development] Near-term scalability
In-Reply-To: <4FDB6946.2020400@justmoon.de>
References: <CANEZrP3w+AiTXmv9Wb3Zi5yyFmGPk82-ysVo4_DVvtg8HHBCdQ@mail.gmail.com>
	<4FDB6946.2020400@justmoon.de>
Message-ID: <CAErK2CgODFY7HMC-WZRAmts-6eOE074Tz4nX5Nr6EvB8o-QWJA@mail.gmail.com>

Grouping mempool transactions based on fees of the group seems
an unnecessary complexity; it makes it harder to predict if an isolated
transaction has enough "juice" to be included in the next Block.

Given your point about economic actors adapting to conditions, would it not
be simpler to use a individual "fee per byte" priority algorithm and let
transaction generators distribute their fees accordingly (and more
predictably)?

This simpler algorithm will prune arbitrary transactions sub-optimally, but
has the benefit of being more understandable and predictable from the point
of view of transaction generators.

On Fri, Jun 15, 2012 at 9:56 AM, Stefan Thomas <moon at justmoon.de> wrote:

> Thanks Mike for the writeup - I'm very sad to have missed the discussion
> on IRC since fee economics are probably my favorite topic, but I'll try
> to contribute to the email discussion instead.
>
> > (4) Making the block size limit float is better than picking a new
> > arbitrary threshold.
>
> Fees are a product of both real and artificial limits to transaction
> validation.
>
> The artificial limits like the block size limit are essentially putting
> a floor on prices by limiting supply beyond what it would otherwise be.
> E.g. the network could confirm more transactions theoretically, but the
> block size limit prevents it.
>
> The real limits are the bandwidth, computing and memory resources of
> participating nodes. For the sake of argument suppose a 1 TB block was
> released into the network right now and we'll also assume there was no
> block size limit of any kind. Many nodes would likely not be able to
> successfully download this block in under 10-30 minutes, so there is a
> very good chance that other miners will have generated two blocks before
> this block makes its way to them.
>
> What does this mean? The miner generating a 1 TB block knows this would
> happen. So in terms of economic self interest he will generate the
> largest possible block that he is still confident that other miners will
> accept and process. A miner who receives a block will also consider
> whether to build on it based on whether they think other miners will be
> able to download it. In other words, if I receive a large block I may
> decide not to mine on it, because I believe that the majority of mining
> power will not mine on it - because it is either too large for them to
> download or because their rules against large blocks reject it.
>
> It's important to understand that in practice economic actors tend to
> plan ahead. In other words, if there is no block size limit that doesn't
> mean that there will be constant forks and total chaos. Rather, no miner
> will ever want to have a block rejected due to size, there is plenty of
> incentive to be conservative with your limits. Even if there are forks,
> this simply means that miners have decided that they can make more money
> by including more transactions at the cost of the occasional dud.
>
> Therefore, from an economic perspective, we do not need a global block
> size limit of any kind. As "guardians of the network" the only thing we
> need to do is to let miners figure out what they wanna do.
>
> HOWEVER, the existing economic incentives won't manifest unless somebody
> translates them into code. We have to give our users (miners & endusers)
> the tools to create a genuine fee-based verification market.
>
> On the miner side: I would make the block size limit configurable with a
> relatively high default. If the default is too low few people will
> bother changing it, which means that it is not worth changing (because a
> majority uses the default anyway), which means even fewer people will
> change it and so on.
>
> The block size limit should also be a soft rather than a hard limit -
> here are some ideas for this:
>
> - The default limit for accepting blocks from others should always be
> significantly greater than the default limit for blocks that the client
> itself will generate.
>
> - There should be different size limits for side chains that are longer
> than the currently active chain. In other words, I might reject a block
> for being slightly too large, but if everyone else accepts it I should
> eventually accept it too, and my client should also consider
> automatically raising my size limit if this happens a lot.
>
> The rationale for the soft limit is to allow for gradual upward
> adjustment. It needs to be risky for individual miners to raise the size
> of their blocks to new heights, but ideally there won't be one solid
> wall for them to run into.
>
> On the user side: I would display the fee on the Send Coins dialog and
> allow users to choose a different fee per transaction. We also talked
> about adding some UI feedback where the client tries to estimate how
> long a transaction will take to confirm given a certain fee, based on
> recent information about what it observed from the network. If the fee
> can be changed on the Send Coins tab, then this could be a red, yellow,
> green visual indication whether the fee is sufficient, adequate or
> dangerously low.
>
> A criticism one might raise is: "The block size limit is not to protect
> miners, but to protect end users who may have less resources than miners
> and can't download gigantic block chains." - That's a viewpoint that is
> certainly valid. I believe that we will be able to do a lot just with
> efficiency improvements, pruning, compression and whatnot. But when it
> comes down to it, I'd prefer a large network with cheap
> microtransactions even if that means that consumer hardware can't
> operate as a standalone validating node anymore. Headers-only mode is
> already a much-requested feature anyway and there are many ways of
> improving the security of various header-only or lightweight protocols.
>
> (I just saw Greg's message advocating the opposite viewpoint, I'll
> respond to that as soon as I can.)
>
>
> > (1) Change the mining code to group transactions together with their
> > mempool dependencies and then calculate all fees as a group.
>
> +1 Very good change. This would allow miners to maximize their revenue
> and in doing so better represent the existing priorities that users
> express through fees.
>
>
> > There was discussion of some one-off changes to address the current
> > situation, namely de-ranking transactions that re-use addresses.
>
> Discouraging address reuse will not change the amount of transactions, I
> think we all agree on that. As for whether it improves the
> prioritization, I'm not sure. Use cases that we seek to discourage may
> simply switch to random addresses and I don't agree in and of itself
> this is a benefit (see item 4 below). Here are a few reasons one might
> be against this proposal:
>
> 1) Certain use cases like green addresses will be forced to become more
> complicated than they would otherwise need to be.
>
> 2) It will be harder to read information straight out of the block
> chain, for example right now we can pretty easily see how much volume is
> caused by Satoshi Dice, perhaps allowing us to make better decisions.
>
> 3) The address index that is used by block explorers and lightweight
> client servers will grow unnecessarily (an address -> tx index will be
> larger if the number of unique addresses increases given the same number
> of txs), so for people like myself who work on that type of software
> you're actually making our scalability equation slightly worse.
>
> 4) You're forcing people into privacy best practices which you think are
> good, but others may not subscribe to. For example I have absolutely
> zero interest in privacy, anyone who cares that I buy Bitcoins with my
> salary and spend them on paragliding is welcome to know about it.
> Frankly, if I cared about privacy I wouldn't be using Bitcoin. If other
> people want to use mixing services and randomize their addresses and
> communicate through Tor that's fine, but the client shouldn't force me
> to do those things if I don't want to by "deprioritizing" my transactions.
>
> 5) We may not like firstbits, but the fact remains that for now they are
> extremely popular, because they improve the user experience where we
> failed to do so. If you deprioritize transactions to reused addresses
> you'll for example deprioritize all/most of Girls Gone Bitcoin, which
> (again, like it or not) is one of the few practical, sustainable niches
> that Bitcoin has managed to carve out for itself so far.
>
>
> > Having senders/buyers pay no fees is psychologically desirable even
> > though we all understand that eventually, somebody, somewhere will be
> > paying fees to use Bitcoin
>
> Free is just an extreme form of cheap, so if we can make transactions
> very cheap (through efficiency and very large blocks) then it will be
> easier for charitable miners to include free transactions. In practice,
> my prediction is that free transactions on the open network will simply
> not be possible in the long run. Dirty hacks aside there is simply no
> way of distinguishing a spam transaction from a charity-worthy
> transaction. So the way I envision free transactions in the future is
> that there may be miners in partnership with wallet providers like
> BlockChain.info that let you submit feeless transactions straight to
> them based on maybe a captcha or some ads. (For the purist, the captcha
> challenge and response could be communicated across the bitcoin network,
> but I think we agree that such things should ideally take place
> out-of-band.)
>
> That way, the available charity of miners who wish to include feeless
> transactions would go to human users as opposed to the potentially
> infinite demand of auto-generated feeless transactions.
>
>
>
> On 6/15/2012 1:29 PM, Mike Hearn wrote:
> > I had to hit the sack last night as it was 2am CET, but I'd like to
> > sum up the discussion we had on IRC about scalability and SatoshiDice
> > in particular.
> >
> > I think we all agreed on the following:
> >
> > - Having senders/buyers pay no fees is psychologically desirable even
> > though we all understand that eventually, somebody, somewhere will be
> > paying fees to use Bitcoin
> >
> > - In the ideal world Bitcoin would scale perfectly and there would be
> > no need for there to be some "winners" and some "losers" when it comes
> > to confirmation time.
> >
> > There was discussion of some one-off changes to address the current
> > situation, namely de-ranking transactions that re-use addresses. Gavin
> > and myself were not keen on this idea, primarily because it just
> > avoids the real problem and Bitcoin already has a good way to
> > prioritize transactions via the fees mechanism itself. The real issue
> > is that SatoshiDice does indeed pay fees and generates a lot of
> > transactions, pushing more traditional traffic out due to artificial
> > throttles.
> >
> > The following set of proposals were discussed:
> >
> > (1) Change the mining code to group transactions together with their
> > mempool dependencies and then calculate all fees as a group. A tx with
> > a fee of 1 BTC that depends on 5 txns with zero fees would result in
> > all 6 transactions being considered to have a fee of 1BTC and
> > therefore become prioritized for inclusion. This allows a transition
> > to "receiver pays" model for fees. There are many advantages. One is
> > that it actually makes sense ... it's always the receiver who wants
> > confirmations because it's the receiver that fears double spends.
> > Senders never do. What's more, whilst Bitcoin is designed to operate
> > on a zero-trust model in the real world trust often exists and it can
> > be used to optimize by passing groups of transactions around with
> > their dependencies, until that group passes a trust boundary and gets
> > broadcast with a send-to-self tx to add fees. Another advantage is it
> > simplifies usage for end users who primarily buy rather than sell,
> > because it avoids the need to guess at fees, one of the most
> > problematic parts of Bitcoins design now.
> >
> > The disadvantages are that it can result in extra transactions that
> > exist only for adding fees, and it requires a more modern payment
> > protocol than the direct-IP protocol Satoshi designed.
> >
> > It would help address the current situation by avoiding angry users
> > who want to buy things, but don't know what fee to set and so their
> > transactions get stuck.
> >
> > (2) SatoshiDice should use the same fee algorithms as Bitcoin-Qt to
> > avoid paying excessive fees and queue-jumping. Guess that's on my
> > plate.
> >
> > (3) Scalability improvements seem like a no brainer to everyone, it's
> > just a case of how complicated they are.
> >
> > (4) Making the block size limit float is better than picking a new
> > arbitrary threshold.
> >
> > On the forums Matt stated that block chain pruning was a no-go because
> > "it makes bitcoin more centralized". I think we've thrashed this one
> > out sufficiently well by now that there should be a united opinion on
> > it. There are technical ways to implement it such that there is no
> > change of trust requirements. All the other issues (finding archival
> > nodes, etc) can be again addressed with sufficient programming.
> >
> > For the case of huge blocks slowing down end user syncing and wasting
> > their resources, SPV clients like MultiBit and Android Wallet already
> > exist and will get better with time. If Jeff implements the bloom
> > filtering p2p commands I'll make bitcoinj use them and that'll knock
> > out excessive bandwidth usage and parse overheads from end users who
> > are on these clients. At some point Bitcoin-Qt can have a dual mode,
> > but who knows when that'll get implemented.
> >
> > Does that all sound reasonable?
> >
> >
> ------------------------------------------------------------------------------
> > Live Security Virtual Conference
> > Exclusive live event will cover all the ways today's security and
> > threat landscape has changed and how IT managers can respond. Discussions
> > will include endpoint security, mobile security and the latest in malware
> > threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
> > _______________________________________________
> > Bitcoin-development mailing list
> > Bitcoin-development at lists.sourceforge.net
> > https://lists.sourceforge.net/lists/listinfo/bitcoin-development
> >
>
>
>
> ------------------------------------------------------------------------------
> Live Security Virtual Conference
> Exclusive live event will cover all the ways today's security and
> threat landscape has changed and how IT managers can respond. Discussions
> will include endpoint security, mobile security and the latest in malware
> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
> _______________________________________________
> Bitcoin-development mailing list
> Bitcoin-development at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/bitcoin-development
>



-- 
Mike Koss
CTO, CoinLab
(425) 246-7701 (m)

A Bitcoin Primer <http://coinlab.com/a-bitcoin-primer.pdf> - What you need
to know about Bitcoins.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120615/1bc049d6/attachment.html>

From zgenjix at yahoo.com  Fri Jun 15 18:38:20 2012
From: zgenjix at yahoo.com (Amir Taaki)
Date: Fri, 15 Jun 2012 11:38:20 -0700 (PDT)
Subject: [Bitcoin-development] Near-term scalability
In-Reply-To: <CAErK2CgODFY7HMC-WZRAmts-6eOE074Tz4nX5Nr6EvB8o-QWJA@mail.gmail.com>
References: <CANEZrP3w+AiTXmv9Wb3Zi5yyFmGPk82-ysVo4_DVvtg8HHBCdQ@mail.gmail.com>
	<4FDB6946.2020400@justmoon.de>
	<CAErK2CgODFY7HMC-WZRAmts-6eOE074Tz4nX5Nr6EvB8o-QWJA@mail.gmail.com>
Message-ID: <1339785500.74108.YahooMailNeo@web121006.mail.ne1.yahoo.com>

Forcing users to switch addresses per received payment to work around a bad fee system would be a braindead decision. You might love software and playing with web plugins, but not everyone does. Artists like Rap News can right now simply throw up an address and begin accepting donations. That's a hugely powerful and impactful selling point for Bitcoin.

I don't really see these problems as a concern. Stefan made an excellent post which touched on this, in that miners have an incentive to keep block sizes low so that their blocks propagate. The real problem here is not about block propagation but the user experience. The way I see it, Bitcoin is becoming more specialised over time and part of that process is abstraction. In the past we all used the Satoshi client for mining, merchant functions, validating blocks and personal uses. These are rapidly diverging, and managing the blockchain is not something that user clients should be doing.

Mike is right when he says the network only needs a few thousand nodes to function fairly. I am not worried about Bitcoin becoming corrupted because of it being a network "by bankers for bankers" because unlike the conventional finance industry, there are no artificial barriers to entry beyond the base cost. This network would always be competitive and strictly operate based on market dynamics.

Case in point: http://en.wikipedia.org/wiki/Coase_theorem

With strict property rights and zero (or low) transaction costs, the allocation of a system does not matter. The system will make efficient use of its resources. I don't see why a cabal would try to corrupt Bitcoin at expense to themselves when a new competitor can enter the market and undercut them. It's why we expect the ROI on mining to be 0 or negative.


I figured out that if you trust data from a blockchain service and only accept data with multiple confirms from each connected service, then you can trivially calculate the probability of being fed corrupt data (assuming a fixed chance per server). In this way, the model is a fault tolerant byzantine system. The chance of being manipulated falls expontentially as you add more servers. And these services can be made highly scalable if you see my BIP 33.

https://en.bitcoin.it/wiki/BIP_0033

________________________________
From: Mike Koss <mike at coinlab.com>
To: Stefan Thomas <moon at justmoon.de> 
Cc: bitcoin-development at lists.sourceforge.net 
Sent: Friday, June 15, 2012 7:37 PM
Subject: Re: [Bitcoin-development] Near-term scalability


Grouping mempool transactions based on fees of the group seems an?unnecessary?complexity; it makes it harder to predict if an isolated transaction has enough "juice" to be included in the next Block.

Given your point about economic actors adapting to conditions, would it not be simpler to use a individual "fee per byte" priority algorithm and let transaction generators distribute their fees accordingly (and more predictably)?

This simpler algorithm will prune arbitrary transactions sub-optimally, but has the benefit of being more understandable and predictable from the point of view of transaction generators.



On Fri, Jun 15, 2012 at 9:56 AM, Stefan Thomas <moon at justmoon.de> wrote:

Thanks Mike for the writeup - I'm very sad to have missed the discussion
>on IRC since fee economics are probably my favorite topic, but I'll try
>to contribute to the email discussion instead.
>
>
>> (4) Making the block size limit float is better than picking a new
>> arbitrary threshold.
>
>Fees are a product of both real and artificial limits to transaction
>validation.
>
>The artificial limits like the block size limit are essentially putting
>a floor on prices by limiting supply beyond what it would otherwise be.
>E.g. the network could confirm more transactions theoretically, but the
>block size limit prevents it.
>
>The real limits are the bandwidth, computing and memory resources of
>participating nodes. For the sake of argument suppose a 1 TB block was
>released into the network right now and we'll also assume there was no
>block size limit of any kind. Many nodes would likely not be able to
>successfully download this block in under 10-30 minutes, so there is a
>very good chance that other miners will have generated two blocks before
>this block makes its way to them.
>
>What does this mean? The miner generating a 1 TB block knows this would
>happen. So in terms of economic self interest he will generate the
>largest possible block that he is still confident that other miners will
>accept and process. A miner who receives a block will also consider
>whether to build on it based on whether they think other miners will be
>able to download it. In other words, if I receive a large block I may
>decide not to mine on it, because I believe that the majority of mining
>power will not mine on it - because it is either too large for them to
>download or because their rules against large blocks reject it.
>
>It's important to understand that in practice economic actors tend to
>plan ahead. In other words, if there is no block size limit that doesn't
>mean that there will be constant forks and total chaos. Rather, no miner
>will ever want to have a block rejected due to size, there is plenty of
>incentive to be conservative with your limits. Even if there are forks,
>this simply means that miners have decided that they can make more money
>by including more transactions at the cost of the occasional dud.
>
>Therefore, from an economic perspective, we do not need a global block
>size limit of any kind. As "guardians of the network" the only thing we
>need to do is to let miners figure out what they wanna do.
>
>HOWEVER, the existing economic incentives won't manifest unless somebody
>translates them into code. We have to give our users (miners & endusers)
>the tools to create a genuine fee-based verification market.
>
>On the miner side: I would make the block size limit configurable with a
>relatively high default. If the default is too low few people will
>bother changing it, which means that it is not worth changing (because a
>majority uses the default anyway), which means even fewer people will
>change it and so on.
>
>The block size limit should also be a soft rather than a hard limit -
>here are some ideas for this:
>
>- The default limit for accepting blocks from others should always be
>significantly greater than the default limit for blocks that the client
>itself will generate.
>
>- There should be different size limits for side chains that are longer
>than the currently active chain. In other words, I might reject a block
>for being slightly too large, but if everyone else accepts it I should
>eventually accept it too, and my client should also consider
>automatically raising my size limit if this happens a lot.
>
>The rationale for the soft limit is to allow for gradual upward
>adjustment. It needs to be risky for individual miners to raise the size
>of their blocks to new heights, but ideally there won't be one solid
>wall for them to run into.
>
>On the user side: I would display the fee on the Send Coins dialog and
>allow users to choose a different fee per transaction. We also talked
>about adding some UI feedback where the client tries to estimate how
>long a transaction will take to confirm given a certain fee, based on
>recent information about what it observed from the network. If the fee
>can be changed on the Send Coins tab, then this could be a red, yellow,
>green visual indication whether the fee is sufficient, adequate or
>dangerously low.
>
>A criticism one might raise is: "The block size limit is not to protect
>miners, but to protect end users who may have less resources than miners
>and can't download gigantic block chains." - That's a viewpoint that is
>certainly valid. I believe that we will be able to do a lot just with
>efficiency improvements, pruning, compression and whatnot. But when it
>comes down to it, I'd prefer a large network with cheap
>microtransactions even if that means that consumer hardware can't
>operate as a standalone validating node anymore. Headers-only mode is
>already a much-requested feature anyway and there are many ways of
>improving the security of various header-only or lightweight protocols.
>
>(I just saw Greg's message advocating the opposite viewpoint, I'll
>respond to that as soon as I can.)
>
>
>
>> (1) Change the mining code to group transactions together with their
>> mempool dependencies and then calculate all fees as a group.
>
>+1 Very good change. This would allow miners to maximize their revenue
>and in doing so better represent the existing priorities that users
>express through fees.
>
>
>
>> There was discussion of some one-off changes to address the current
>> situation, namely de-ranking transactions that re-use addresses.
>
>Discouraging address reuse will not change the amount of transactions, I
>think we all agree on that. As for whether it improves the
>prioritization, I'm not sure. Use cases that we seek to discourage may
>simply switch to random addresses and I don't agree in and of itself
>this is a benefit (see item 4 below). Here are a few reasons one might
>be against this proposal:
>
>1) Certain use cases like green addresses will be forced to become more
>complicated than they would otherwise need to be.
>
>2) It will be harder to read information straight out of the block
>chain, for example right now we can pretty easily see how much volume is
>caused by Satoshi Dice, perhaps allowing us to make better decisions.
>
>3) The address index that is used by block explorers and lightweight
>client servers will grow unnecessarily (an address -> tx index will be
>larger if the number of unique addresses increases given the same number
>of txs), so for people like myself who work on that type of software
>you're actually making our scalability equation slightly worse.
>
>4) You're forcing people into privacy best practices which you think are
>good, but others may not subscribe to. For example I have absolutely
>zero interest in privacy, anyone who cares that I buy Bitcoins with my
>salary and spend them on paragliding is welcome to know about it.
>Frankly, if I cared about privacy I wouldn't be using Bitcoin. If other
>people want to use mixing services and randomize their addresses and
>communicate through Tor that's fine, but the client shouldn't force me
>to do those things if I don't want to by "deprioritizing" my transactions.
>
>5) We may not like firstbits, but the fact remains that for now they are
>extremely popular, because they improve the user experience where we
>failed to do so. If you deprioritize transactions to reused addresses
>you'll for example deprioritize all/most of Girls Gone Bitcoin, which
>(again, like it or not) is one of the few practical, sustainable niches
>that Bitcoin has managed to carve out for itself so far.
>
>
>
>> Having senders/buyers pay no fees is psychologically desirable even
>> though we all understand that eventually, somebody, somewhere will be
>> paying fees to use Bitcoin
>
>Free is just an extreme form of cheap, so if we can make transactions
>very cheap (through efficiency and very large blocks) then it will be
>easier for charitable miners to include free transactions. In practice,
>my prediction is that free transactions on the open network will simply
>not be possible in the long run. Dirty hacks aside there is simply no
>way of distinguishing a spam transaction from a charity-worthy
>transaction. So the way I envision free transactions in the future is
>that there may be miners in partnership with wallet providers like
>BlockChain.info that let you submit feeless transactions straight to
>them based on maybe a captcha or some ads. (For the purist, the captcha
>challenge and response could be communicated across the bitcoin network,
>but I think we agree that such things should ideally take place
>out-of-band.)
>
>That way, the available charity of miners who wish to include feeless
>transactions would go to human users as opposed to the potentially
>infinite demand of auto-generated feeless transactions.
>
>
>
>
>On 6/15/2012 1:29 PM, Mike Hearn wrote:
>> I had to hit the sack last night as it was 2am CET, but I'd like to
>> sum up the discussion we had on IRC about scalability and SatoshiDice
>> in particular.
>>
>> I think we all agreed on the following:
>>
>> - Having senders/buyers pay no fees is psychologically desirable even
>> though we all understand that eventually, somebody, somewhere will be
>> paying fees to use Bitcoin
>>
>> - In the ideal world Bitcoin would scale perfectly and there would be
>> no need for there to be some "winners" and some "losers" when it comes
>> to confirmation time.
>>
>> There was discussion of some one-off changes to address the current
>> situation, namely de-ranking transactions that re-use addresses. Gavin
>> and myself were not keen on this idea, primarily because it just
>> avoids the real problem and Bitcoin already has a good way to
>> prioritize transactions via the fees mechanism itself. The real issue
>> is that SatoshiDice does indeed pay fees and generates a lot of
>> transactions, pushing more traditional traffic out due to artificial
>> throttles.
>>
>> The following set of proposals were discussed:
>>
>> (1) Change the mining code to group transactions together with their
>> mempool dependencies and then calculate all fees as a group. A tx with
>> a fee of 1 BTC that depends on 5 txns with zero fees would result in
>> all 6 transactions being considered to have a fee of 1BTC and
>> therefore become prioritized for inclusion. This allows a transition
>> to "receiver pays" model for fees. There are many advantages. One is
>> that it actually makes sense ... it's always the receiver who wants
>> confirmations because it's the receiver that fears double spends.
>> Senders never do. What's more, whilst Bitcoin is designed to operate
>> on a zero-trust model in the real world trust often exists and it can
>> be used to optimize by passing groups of transactions around with
>> their dependencies, until that group passes a trust boundary and gets
>> broadcast with a send-to-self tx to add fees. Another advantage is it
>> simplifies usage for end users who primarily buy rather than sell,
>> because it avoids the need to guess at fees, one of the most
>> problematic parts of Bitcoins design now.
>>
>> The disadvantages are that it can result in extra transactions that
>> exist only for adding fees, and it requires a more modern payment
>> protocol than the direct-IP protocol Satoshi designed.
>>
>> It would help address the current situation by avoiding angry users
>> who want to buy things, but don't know what fee to set and so their
>> transactions get stuck.
>>
>> (2) SatoshiDice should use the same fee algorithms as Bitcoin-Qt to
>> avoid paying excessive fees and queue-jumping. Guess that's on my
>> plate.
>>
>> (3) Scalability improvements seem like a no brainer to everyone, it's
>> just a case of how complicated they are.
>>
>> (4) Making the block size limit float is better than picking a new
>> arbitrary threshold.
>>
>> On the forums Matt stated that block chain pruning was a no-go because
>> "it makes bitcoin more centralized". I think we've thrashed this one
>> out sufficiently well by now that there should be a united opinion on
>> it. There are technical ways to implement it such that there is no
>> change of trust requirements. All the other issues (finding archival
>> nodes, etc) can be again addressed with sufficient programming.
>>
>> For the case of huge blocks slowing down end user syncing and wasting
>> their resources, SPV clients like MultiBit and Android Wallet already
>> exist and will get better with time. If Jeff implements the bloom
>> filtering p2p commands I'll make bitcoinj use them and that'll knock
>> out excessive bandwidth usage and parse overheads from end users who
>> are on these clients. At some point Bitcoin-Qt can have a dual mode,
>> but who knows when that'll get implemented.
>>
>> Does that all sound reasonable?
>>
>> ------------------------------------------------------------------------------
>> Live Security Virtual Conference
>> Exclusive live event will cover all the ways today's security and
>> threat landscape has changed and how IT managers can respond. Discussions
>> will include endpoint security, mobile security and the latest in malware
>> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
>> _______________________________________________
>> Bitcoin-development mailing list
>> Bitcoin-development at lists.sourceforge.net
>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development
>>
>
>
>------------------------------------------------------------------------------
>Live Security Virtual Conference
>Exclusive live event will cover all the ways today's security and
>threat landscape has changed and how IT managers can respond. Discussions
>will include endpoint security, mobile security and the latest in malware
>threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
>_______________________________________________
>Bitcoin-development mailing list
>Bitcoin-development at lists.sourceforge.net
>https://lists.sourceforge.net/lists/listinfo/bitcoin-development
>


-- 
Mike Koss
CTO, CoinLab
(425) 246-7701 (m)

A Bitcoin Primer?- What you need to know about Bitcoins.

------------------------------------------------------------------------------
Live Security Virtual Conference
Exclusive live event will cover all the ways today's security and 
threat landscape has changed and how IT managers can respond. Discussions 
will include endpoint security, mobile security and the latest in malware 
threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
_______________________________________________
Bitcoin-development mailing list
Bitcoin-development at lists.sourceforge.net
https://lists.sourceforge.net/lists/listinfo/bitcoin-development



From zgenjix at yahoo.com  Fri Jun 15 18:42:32 2012
From: zgenjix at yahoo.com (Amir Taaki)
Date: Fri, 15 Jun 2012 11:42:32 -0700 (PDT)
Subject: [Bitcoin-development] New P2P commands for diagnostics,
	SPV clients
In-Reply-To: <CANEZrP39RHfCDX-x4ARo+oPphLv-70RxuMh3+AJzsNPxzOd=bA@mail.gmail.com>
References: <CA+8xBpecVQcTTbPxUm_3_GWC99dEd4=-VFWb+QT6jUy4rg8U4w@mail.gmail.com>
	<CANEZrP0kNZDByHpK2=UjP+ag0X1KmqHxnJdm=e_pWMitP4QvvA@mail.gmail.com>
	<CA+8xBpcwhQPQRe=stYb9xksLsTbiABKLS7PZnRtvPga6AmSg4Q@mail.gmail.com>
	<CANEZrP39RHfCDX-x4ARo+oPphLv-70RxuMh3+AJzsNPxzOd=bA@mail.gmail.com>
Message-ID: <1339785752.91313.YahooMailNeo@web121004.mail.ne1.yahoo.com>

Why though? The bottleneck is not network traffic but disk space usage/blockchain validation time.



----- Original Message -----
From: Mike Hearn <mike at plan99.net>
To: Jeff Garzik <jgarzik at exmulti.com>
Cc: Bitcoin Development <bitcoin-development at lists.sourceforge.net>
Sent: Friday, June 15, 2012 3:43 PM
Subject: Re: [Bitcoin-development] New P2P commands for diagnostics, SPV clients

> Yes, the format is something that must be hashed out (no pun
> intended). ?Need input from potential users about what information
> they might need.

Matts point that a branch-per-transaction may duplicate data is well
made, that said, I suspect a format that tries to fix this would be
much more complicated.

How about see this project as a three part change?

First step - add the mempool command and make nodes sync up their
mempools on startup.

Second step - if protocol version >= X, the "block" message consists
of a header + num transactions + vector<hash>? instead of the full
transactions themselves.

On receiving such a block, we go look to see which transactions we're
missing from the mempool and request them with getdata. Each time we
receive a tx message we check to see if it was one we were missing
from a block. Once all transactions in the block message are in
memory, we go ahead and assemble the block, then verify as per normal.
This should speed up block propagation. Miners have an incentive to
upgrade because it should reduce wasted work.

Third step - new message, getmerkletx takes a vector<hash> and returns
a merkletx message: "merkle branch missing the root + transaction data
itself" for each requested transaction. The filtering commands are
added, so the block message now only lists transaction hashes that
match the filter which can then be requested with getmerkletx.

------------------------------------------------------------------------------
Live Security Virtual Conference
Exclusive live event will cover all the ways today's security and 
threat landscape has changed and how IT managers can respond. Discussions 
will include endpoint security, mobile security and the latest in malware 
threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
_______________________________________________
Bitcoin-development mailing list
Bitcoin-development at lists.sourceforge.net
https://lists.sourceforge.net/lists/listinfo/bitcoin-development




From zgenjix at yahoo.com  Fri Jun 15 18:50:47 2012
From: zgenjix at yahoo.com (Amir Taaki)
Date: Fri, 15 Jun 2012 11:50:47 -0700 (PDT)
Subject: [Bitcoin-development] Near-term scalability
In-Reply-To: <CAAS2fgSVbYFkkhP_0Ny5ULB-DJKN-3hZLkqWukrGL80-UenMwQ@mail.gmail.com>
References: <CANEZrP3w+AiTXmv9Wb3Zi5yyFmGPk82-ysVo4_DVvtg8HHBCdQ@mail.gmail.com>
	<4FDB6946.2020400@justmoon.de>
	<CAErK2CgODFY7HMC-WZRAmts-6eOE074Tz4nX5Nr6EvB8o-QWJA@mail.gmail.com>
	<1339785500.74108.YahooMailNeo@web121006.mail.ne1.yahoo.com>
	<CAAS2fgSVbYFkkhP_0Ny5ULB-DJKN-3hZLkqWukrGL80-UenMwQ@mail.gmail.com>
Message-ID: <1339786247.64852.YahooMailNeo@web121006.mail.ne1.yahoo.com>

> less expensive. This is no more "real" or less "artificial" then an
> imposed licensing fee or the like and it is not subject to market
> forces.

Sure, the market is not always efficient nor desirable. This seems more like a social question though about choice and information. I do strongly feel that users should have more control over their technology, and a say in how Bitcoin operates. It is our job to present the choices and inform them to make good decisions. If we think how to implement this with a social component of the users operating the network rather than hard and fast rules, I think that's the preferrable way.

Part of the problem is that Satoshi didn't totally anticipate the growth of the network. The block reward (the subsidy) is too high, which is why transactions can afford to be so cheap. What would happen if blocks required a cumulative fee of XN BTC for N transactions before being accepted?



----- Original Message -----
From: Gregory Maxwell <gmaxwell at gmail.com>
To: Amir Taaki <zgenjix at yahoo.com>
Cc: 
Sent: Friday, June 15, 2012 8:43 PM
Subject: Re: [Bitcoin-development] Near-term scalability

On Fri, Jun 15, 2012 at 2:38 PM, Amir Taaki <zgenjix at yahoo.com> wrote:
> Forcing users to switch addresses per received payment to work around a bad fee system would be a braindead decision. You might love software and playing with web plugins, but not everyone does. Artists like Rap News can right now simply throw up an address and begin accepting donations. That's a hugely powerful and impactful selling point for Bitcoin.

And that use case does not need fast confirmations!

This is making the point.

>there are no artificial barriers to entry beyond the base cost. This network would always be competitive and strictly operate based on market dynamics.

The users of bitcoin can collectively choose how expensive operating a
full node is by accepting validation rules that allow it to be more or
less expensive. This is no more "real" or less "artificial" then an
imposed licensing fee or the like and it is not subject to market
forces.




From gmaxwell at gmail.com  Fri Jun 15 18:55:52 2012
From: gmaxwell at gmail.com (Gregory Maxwell)
Date: Fri, 15 Jun 2012 14:55:52 -0400
Subject: [Bitcoin-development] Near-term scalability
In-Reply-To: <1339786247.64852.YahooMailNeo@web121006.mail.ne1.yahoo.com>
References: <CANEZrP3w+AiTXmv9Wb3Zi5yyFmGPk82-ysVo4_DVvtg8HHBCdQ@mail.gmail.com>
	<4FDB6946.2020400@justmoon.de>
	<CAErK2CgODFY7HMC-WZRAmts-6eOE074Tz4nX5Nr6EvB8o-QWJA@mail.gmail.com>
	<1339785500.74108.YahooMailNeo@web121006.mail.ne1.yahoo.com>
	<CAAS2fgSVbYFkkhP_0Ny5ULB-DJKN-3hZLkqWukrGL80-UenMwQ@mail.gmail.com>
	<1339786247.64852.YahooMailNeo@web121006.mail.ne1.yahoo.com>
Message-ID: <CAAS2fgQEFgLOEkSSTpn5OFuwa+AYA3LajTD-MqJ83rEPCW4u-Q@mail.gmail.com>

On Fri, Jun 15, 2012 at 2:50 PM, Amir Taaki <zgenjix at yahoo.com> wrote:
> Part of the problem is that Satoshi didn't totally anticipate the growth of the network. The block reward (the subsidy) is too high, which is why transactions can afford to be so cheap. What would happen if blocks required a cumulative fee of XN BTC for N transactions before being accepted?

I would take the last block I solved and use it to write a transaction
to nowhere which which gave all 50 BTC out in fee.  This pays for as
many transactions in the block as I like for any value of X you want
to choose.

You should read the bitcointalk forums more often: variants on that
idea are frequently suggested and dismantled. There is a lot of noise
there but also a lot of ideas and knowing what doesn't work is good
too.



From grarpamp at gmail.com  Fri Jun 15 20:55:41 2012
From: grarpamp at gmail.com (grarpamp)
Date: Fri, 15 Jun 2012 16:55:41 -0400
Subject: [Bitcoin-development] RPC and signals - processing priority
Message-ID: <CAD2Ti2-wqMwxJ6iU-z2kYjUjc4GkYMo0dWjL4rcPr2DjODfirQ@mail.gmail.com>

While happily processing these:
received block ...
SetBestChain: new best=...  height=...  work=...
ProcessBlock: ACCEPTED

bitcoind very often refuses to answer rpc queries such as getinfo/stop,
or signals such as kill/ctrl-c. It even registers:
 ThreadRPCServer method=getinfo/stop
in the debug log. But the action doesn't happen as expected.

Shouldn't it be checking and processing all user interrupts like
once per block and doing the chain in the background?

How do busy commerce servers deal with this poor rpc handling?

Is there a way to increase the priority of user scheduled tasks?
What's going on? Thanks.



From gavinandresen at gmail.com  Fri Jun 15 20:56:25 2012
From: gavinandresen at gmail.com (Gavin Andresen)
Date: Fri, 15 Jun 2012 16:56:25 -0400
Subject: [Bitcoin-development] Near-term scalability
In-Reply-To: <CANEZrP3w+AiTXmv9Wb3Zi5yyFmGPk82-ysVo4_DVvtg8HHBCdQ@mail.gmail.com>
References: <CANEZrP3w+AiTXmv9Wb3Zi5yyFmGPk82-ysVo4_DVvtg8HHBCdQ@mail.gmail.com>
Message-ID: <CABsx9T0Z3xKGOO=4Tr94cKTBpfUwHQ_qHJPidB_MPYAGCJV=VQ@mail.gmail.com>

> (1) Change the mining code to group transactions together with their
> mempool dependencies and then calculate all fees as a group.

I think there is general consensus this is a good idea.

> (2) SatoshiDice should use the same fee algorithms as Bitcoin-Qt to
> avoid paying excessive fees and queue-jumping. Guess that's on my
> plate.

I don't think the problem is with SatoshiDice, but is with the current
fixed/inflexible fee rules:

----------------------------

I've been thinking about fees a lot the last couple of days, and I'm
close to making a formal proposal. Here are my thoughts so far:

It seems to me there are two typical use cases:

Case 1:  I want my transaction to be confirmed quickly, and am willing
to pay a fee to make that happen.

Case 2: I want my transaction to be confirmed eventually. I'd rather
not pay, unless I have to.

I don't think the current code handles those two cases as well as it
could; here's a proposal to fix that:

o Let miners decide on how many free transactions they'll support, by
letting them specify how much of each block to set aside for 'free'
transactions (bytes) and what the threshold for 'free' is
(bitcoins/kilobyte). I think a new RPC call to get/set the parameters
dynamically is the right way to go.

o Change the block creation code to calculate a
bitcoin-fee-per-kilobyte for each transaction, where the fee and size
are both calculated based on the transaction and it's dependent
descendants (so we get the receiver-can-opt-to-pay-the-fee behavior we
want). Order transactions so highest-paying get into the non-free
space first.

o Fill up the "free" space (if any) with the highest-priority
transactions, where priority is a function of transaction size, age of
inputs, number of bitcoins... and ratio of inputs to outputs (to
encourage combining inputs so more pruning is possible).

The fee-paying space in a block lets Use Case #1 users compete to make
their fees high enough to get into the block.

The free space should let non-spamming Use Case #2 users (who don't
send a lot of transactions, and so have well-aged, high-priority
inputs) send transactions for free, at least as long as there are
miners willing to accept free transactions.

The question is: how do clients suggest fees to users if miners might
have very different fee policies?

I think full, transaction-verifying clients can watch how long
transactions stay in the memory pool to figure it out. I'm gathering
statistics right now to test a couple of simple heuristic algorithms
for reasonable fee/priority policies.

But that won't work for newly started clients that haven't seen a lot
of transactions enter/exit the memory pool, or SPV clients that can't
lookup transaction inputs (so can't calculate what fees are being paid
-- and once we have bloom filters may not be paying attention anything
but their own transactions, anyway).

I'm still thinking about that.

Maybe a new p2p network command: you give me a list of block hashes, I
tell you average fees paid per kilobyte for fee-paying transactions in
those blocks, and minimum and average priority of free transactions in
those blocks.

Maybe the big mining pools all publish their fee policies and that
information somehow gets to clients (encoded in the coinbase? ... but
they have a strong incentive to lie to try put upward pressure on
fees... ).

Maybe each client developer runs a "fee policy server" and clients
periodically ask it for reasonable fee rules (HTTP fetch info from a
web page that is updated as often or infrequently as is convenient,
maybe). I think I like this solution the best, it should let clients
compete to have the smartest/bestest algorithms for saving their
user's money on transaction fees.

-- 
--
Gavin Andresen



From grarpamp at gmail.com  Fri Jun 15 20:58:55 2012
From: grarpamp at gmail.com (grarpamp)
Date: Fri, 15 Jun 2012 16:58:55 -0400
Subject: [Bitcoin-development] Manual file cleanup on exit,
	safe? [coredump backtrace]
Message-ID: <CAD2Ti292gfPvxR42k-1dMRg_7=oU8WkWdYBxi0f4884vOQ-XCQ@mail.gmail.com>

When bitcoind exits cleanly, it does not seem safe for the blockchain
to clean up the following hierarchy with rm -r ?

database/
db.log
.lock
debug.log
addr.dat
wallet.dat

And what about adding to the above list the following files when
bitcoind crashes:

__db.*

Is there an option to make bitcoind roll/flush the above files on
exit so they can be removed/ported?

No matter the answers, bitcoind should not be dumping core.


Bitcoin version v0.6.2.2-unk-beta ()
Default data directory /.../.bitcoin
Loading addresses...
dbenv.open LogDir=/.../.bitcoin/database ErrorFile=/.../.bitcoin/db.log

************************
EXCEPTION: 11DbException
Db::open: Invalid argument
bitcoin in AppInit()
terminate called after throwing an instance of 'DbException'
  what():  Db::open: Invalid argument
sh: abort (core dumped)

file unknown has LSN 38/7968116, past end of log at 1/28
Commonly caused by moving a database from one database environment
to another without clearing the database LSNs, or by removing all of
the log files from a database environment
__db_meta_setup: /.../.bitcoin/addr.dat: unexpected file type or format


[New Thread 28801140 (LWP 100964/initial thread)]
(gdb) bt
#0  0x2873e9a7 in kill () from /lib/libc.so.7
#1  0x2852d397 in raise () from /lib/libthr.so.3
#2  0x2873d4da in abort () from /lib/libc.so.7
#3  0x285a0880 in __gnu_cxx::__verbose_terminate_handler () from
/usr/lib/libstdc++.so.6
#4  0x285a508a in std::set_unexpected () from /usr/lib/libstdc++.so.6
#5  0x285a50d2 in std::terminate () from /usr/lib/libstdc++.so.6
#6  0x285a4f58 in __cxa_rethrow () from /usr/lib/libstdc++.so.6
#7  0x0816d2ea in PrintException (pex=0x288251b0, pszThread=0x82f4cfa
"AppInit()") at util.cpp:792
#8  0x08087625 in AppInit (argc=2, argv=0xbfbfedf0) at init.cpp:113
#9  0x0808766d in main (argc=Cannot access memory at address 0x3) at init.cpp:96



From pieter.wuille at gmail.com  Fri Jun 15 23:11:39 2012
From: pieter.wuille at gmail.com (Pieter Wuille)
Date: Sat, 16 Jun 2012 01:11:39 +0200
Subject: [Bitcoin-development] Manual file cleanup on exit,
 safe? [coredump backtrace]
In-Reply-To: <CAD2Ti292gfPvxR42k-1dMRg_7=oU8WkWdYBxi0f4884vOQ-XCQ@mail.gmail.com>
References: <CAD2Ti292gfPvxR42k-1dMRg_7=oU8WkWdYBxi0f4884vOQ-XCQ@mail.gmail.com>
Message-ID: <20120615231138.GA30505@vps7135.xlshosting.net>

On Fri, Jun 15, 2012 at 04:58:55PM -0400, grarpamp wrote:
> When bitcoind exits cleanly, it does not seem safe for the blockchain
> to clean up the following hierarchy with rm -r ?

Use -detachdb if you want to detach the blockchain database files from the
database environment at exit. This was turned off by default in 0.6.0 to
speed up the shutdown process very significantly, and few people have a need
to manually fiddle with their blockchain database files.

-- 
Pieter



From jgarzik at exmulti.com  Sat Jun 16 00:13:21 2012
From: jgarzik at exmulti.com (Jeff Garzik)
Date: Fri, 15 Jun 2012 20:13:21 -0400
Subject: [Bitcoin-development] Proposed new P2P command and response:
	getcmds, cmdlist
Message-ID: <CA+8xBpdD31koaVBh1RuDZKH1sygr8z10K=bPz8DepqYOa8i6yg@mail.gmail.com>

Outside of major features advertised network-wide in nService bits,
P2P protocol lacks a good method of enumerating minor features or
extensions.  The version number increment is coarse-grained, and is
not self-documenting.  A simple extension which lists supported
commands is added, as demonstrated in this pull request:

     https://github.com/bitcoin/bitcoin/pull/1471

Another option is for verack to return this information at login,
eliminating the need for a separate command/response.

-- 
Jeff Garzik
exMULTI, Inc.
jgarzik at exmulti.com



From zgenjix at yahoo.com  Sat Jun 16 01:34:53 2012
From: zgenjix at yahoo.com (Amir Taaki)
Date: Fri, 15 Jun 2012 18:34:53 -0700 (PDT)
Subject: [Bitcoin-development] Proposed new P2P command and response:
	getcmds, cmdlist
In-Reply-To: <CA+8xBpdD31koaVBh1RuDZKH1sygr8z10K=bPz8DepqYOa8i6yg@mail.gmail.com>
References: <CA+8xBpdD31koaVBh1RuDZKH1sygr8z10K=bPz8DepqYOa8i6yg@mail.gmail.com>
Message-ID: <1339810493.15660.YahooMailNeo@web121004.mail.ne1.yahoo.com>

Introspection/command discovery is nice, but I would prefer it to be immediately done in the first version exchange so no assumptions as to how a network is operating need to be made.

I like the idea of a flat list of commands. It might make sense to have "meta"-commands that alias to groups of commands. i.e "original" for the current core subset up to (and including) "pong". The aliases could exist in a text definition file which is held on github or bitcoin.org/


----- Original Message -----
From: Jeff Garzik <jgarzik at exmulti.com>
To: Bitcoin Development <bitcoin-development at lists.sourceforge.net>
Cc: 
Sent: Saturday, June 16, 2012 2:13 AM
Subject: [Bitcoin-development] Proposed new P2P command and response: getcmds, cmdlist

Outside of major features advertised network-wide in nService bits,
P2P protocol lacks a good method of enumerating minor features or
extensions.? The version number increment is coarse-grained, and is
not self-documenting.? A simple extension which lists supported
commands is added, as demonstrated in this pull request:

? ? https://github.com/bitcoin/bitcoin/pull/1471

Another option is for verack to return this information at login,
eliminating the need for a separate command/response.

-- 
Jeff Garzik
exMULTI, Inc.
jgarzik at exmulti.com

------------------------------------------------------------------------------
Live Security Virtual Conference
Exclusive live event will cover all the ways today's security and 
threat landscape has changed and how IT managers can respond. Discussions 
will include endpoint security, mobile security and the latest in malware 
threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
_______________________________________________
Bitcoin-development mailing list
Bitcoin-development at lists.sourceforge.net
https://lists.sourceforge.net/lists/listinfo/bitcoin-development




From jonathan at bitcoinstats.org  Sat Jun 16 02:35:13 2012
From: jonathan at bitcoinstats.org (Jonathan Warren)
Date: Fri, 15 Jun 2012 22:35:13 -0400
Subject: [Bitcoin-development] SatoshiDice and Near-term scalability
In-Reply-To: <CA+8xBpde=y213zFeVjYoZgBstZ6GTnk84ADD8twaDJy2D6CHyw@mail.gmail.com>
References: <CA+8xBpde=y213zFeVjYoZgBstZ6GTnk84ADD8twaDJy2D6CHyw@mail.gmail.com>
Message-ID: <007701cd4b68$a88e6a40$f9ab3ec0$@bitcoinstats.org>

Yes, I measure mainnet confirmation times on a regular basis.
http://bitcoinstats.org/post/tx-confirmation-times-June2012.png

Before fairly recently, fee-paying transactions never took anywhere close to
this long to be confirmed. 

Jonathan Warren
(Bitcointalk: Atheros)

-----Original Message-----
From: Jeff Garzik [mailto:jgarzik at exmulti.com] 
Sent: Friday, June 15, 2012 1:17 PM
To: bitcoin-development at lists.sourceforge.net
Subject: [Bitcoin-development] SatoshiDice and Near-term scalability

Hard-fork requires a very high level of community buy-in, because it shuts
out older clients who will simply refuse to consider >1MB blocks valid.

Anything approaching that level of change would need some good, hard data
indicating that SatoshiDice was shutting out the majority of other traffic.
Does anyone measure mainnet "normal tx" confirmation times on a regular
basis?  Any other hard data?





From zgenjix at yahoo.com  Sat Jun 16 04:33:31 2012
From: zgenjix at yahoo.com (Amir Taaki)
Date: Fri, 15 Jun 2012 21:33:31 -0700 (PDT)
Subject: [Bitcoin-development] SatoshiDice and Near-term scalability
In-Reply-To: <007701cd4b68$a88e6a40$f9ab3ec0$@bitcoinstats.org>
References: <CA+8xBpde=y213zFeVjYoZgBstZ6GTnk84ADD8twaDJy2D6CHyw@mail.gmail.com>
	<007701cd4b68$a88e6a40$f9ab3ec0$@bitcoinstats.org>
Message-ID: <1339821211.57245.YahooMailNeo@web121006.mail.ne1.yahoo.com>

Did anyone try sending them an email asking them to stop or offering help to fix their site? What did they say? I'm sure they would try to be accomodating.



----- Original Message -----
From: Jonathan Warren <jonathan at bitcoinstats.org>
To: bitcoin-development at lists.sourceforge.net
Cc: 
Sent: Saturday, June 16, 2012 4:35 AM
Subject: Re: [Bitcoin-development] SatoshiDice and Near-term scalability

Yes, I measure mainnet confirmation times on a regular basis.
http://bitcoinstats.org/post/tx-confirmation-times-June2012.png

Before fairly recently, fee-paying transactions never took anywhere close to
this long to be confirmed. 

Jonathan Warren
(Bitcointalk: Atheros)

-----Original Message-----
From: Jeff Garzik [mailto:jgarzik at exmulti.com] 
Sent: Friday, June 15, 2012 1:17 PM
To: bitcoin-development at lists.sourceforge.net
Subject: [Bitcoin-development] SatoshiDice and Near-term scalability

Hard-fork requires a very high level of community buy-in, because it shuts
out older clients who will simply refuse to consider >1MB blocks valid.

Anything approaching that level of change would need some good, hard data
indicating that SatoshiDice was shutting out the majority of other traffic.
Does anyone measure mainnet "normal tx" confirmation times on a regular
basis?? Any other hard data?



------------------------------------------------------------------------------
Live Security Virtual Conference
Exclusive live event will cover all the ways today's security and 
threat landscape has changed and how IT managers can respond. Discussions 
will include endpoint security, mobile security and the latest in malware 
threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
_______________________________________________
Bitcoin-development mailing list
Bitcoin-development at lists.sourceforge.net
https://lists.sourceforge.net/lists/listinfo/bitcoin-development




From laanwj at gmail.com  Sat Jun 16 06:45:00 2012
From: laanwj at gmail.com (Wladimir)
Date: Sat, 16 Jun 2012 08:45:00 +0200
Subject: [Bitcoin-development] Proposed new P2P command and response:
 getcmds, cmdlist
In-Reply-To: <1339810493.15660.YahooMailNeo@web121004.mail.ne1.yahoo.com>
References: <CA+8xBpdD31koaVBh1RuDZKH1sygr8z10K=bPz8DepqYOa8i6yg@mail.gmail.com>
	<1339810493.15660.YahooMailNeo@web121004.mail.ne1.yahoo.com>
Message-ID: <CA+s+GJCKSrJv4L=4Nj4Hs+j2vfM-oWe5ayD_4NOUJMoXCkm3iA@mail.gmail.com>

As replied on the github issue:

Personally I still think it's better to have a clear standardized "protocol
version", that implies what capabilities are supported, instead of a
capability-based system that explicitly lists them.

Capability-based systems (just look at OpenGL) tend to become horrendously
complex, as you have to take into account all possible combinations of
possible interactions, and constantly check for support of specific
features instead of just comparing a version number.

Sure, it can be necessary to distinguish between different types of nodes,
but there is no need to make it this fine-grained.

Wladimir

On Sat, Jun 16, 2012 at 3:34 AM, Amir Taaki <zgenjix at yahoo.com> wrote:

> Introspection/command discovery is nice, but I would prefer it to be
> immediately done in the first version exchange so no assumptions as to how
> a network is operating need to be made.
>
> I like the idea of a flat list of commands. It might make sense to have
> "meta"-commands that alias to groups of commands. i.e "original" for the
> current core subset up to (and including) "pong". The aliases could exist
> in a text definition file which is held on github or bitcoin.org/
>
>
> ----- Original Message -----
> From: Jeff Garzik <jgarzik at exmulti.com>
> To: Bitcoin Development <bitcoin-development at lists.sourceforge.net>
> Cc:
> Sent: Saturday, June 16, 2012 2:13 AM
> Subject: [Bitcoin-development] Proposed new P2P command and response:
> getcmds, cmdlist
>
> Outside of major features advertised network-wide in nService bits,
> P2P protocol lacks a good method of enumerating minor features or
> extensions.  The version number increment is coarse-grained, and is
> not self-documenting.  A simple extension which lists supported
> commands is added, as demonstrated in this pull request:
>
>     https://github.com/bitcoin/bitcoin/pull/1471
>
> Another option is for verack to return this information at login,
> eliminating the need for a separate command/response.
>
> --
> Jeff Garzik
> exMULTI, Inc.
> jgarzik at exmulti.com
>
>
> ------------------------------------------------------------------------------
> Live Security Virtual Conference
> Exclusive live event will cover all the ways today's security and
> threat landscape has changed and how IT managers can respond. Discussions
> will include endpoint security, mobile security and the latest in malware
> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
> _______________________________________________
> Bitcoin-development mailing list
> Bitcoin-development at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/bitcoin-development
>
>
>
> ------------------------------------------------------------------------------
> Live Security Virtual Conference
> Exclusive live event will cover all the ways today's security and
> threat landscape has changed and how IT managers can respond. Discussions
> will include endpoint security, mobile security and the latest in malware
> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
> _______________________________________________
> Bitcoin-development mailing list
> Bitcoin-development at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/bitcoin-development
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120616/d990c59b/attachment.html>

From laanwj at gmail.com  Sat Jun 16 07:04:34 2012
From: laanwj at gmail.com (Wladimir)
Date: Sat, 16 Jun 2012 09:04:34 +0200
Subject: [Bitcoin-development] RPC and signals - processing priority
In-Reply-To: <CAD2Ti2-wqMwxJ6iU-z2kYjUjc4GkYMo0dWjL4rcPr2DjODfirQ@mail.gmail.com>
References: <CAD2Ti2-wqMwxJ6iU-z2kYjUjc4GkYMo0dWjL4rcPr2DjODfirQ@mail.gmail.com>
Message-ID: <CA+s+GJBqjvSkJp3dv_ZoeBh4QqF7z483PpCkCqsDZfQYGcJ65Q@mail.gmail.com>

On Fri, Jun 15, 2012 at 10:55 PM, grarpamp <grarpamp at gmail.com> wrote:

> While happily processing these:
> received block ...
> SetBestChain: new best=...  height=...  work=...
> ProcessBlock: ACCEPTED
>
> bitcoind very often refuses to answer rpc queries such as getinfo/stop,
> or signals such as kill/ctrl-c. It even registers:
>  ThreadRPCServer method=getinfo/stop
> in the debug log. But the action doesn't happen as expected.
>
> Shouldn't it be checking and processing all user interrupts like
> once per block and doing the chain in the background?
>


This has nothing to do with priority and "user interrupts", but with the
locks on the wallet and client. Every RPC command takes both locks, and
releases them only when finished.

Shutting down also requires both locks, so the operations will be
serialized.

This protects the database and critical data structures. Sure, there might
be some cases in which the locks are not necessary, or read/write locks
could be used instead to improve concurrency, but this has to be approached
really carefully.

Wladimir
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120616/aca1635e/attachment.html>

From mike at plan99.net  Sat Jun 16 07:55:55 2012
From: mike at plan99.net (Mike Hearn)
Date: Sat, 16 Jun 2012 09:55:55 +0200
Subject: [Bitcoin-development] Near-term scalability
In-Reply-To: <CABsx9T0Z3xKGOO=4Tr94cKTBpfUwHQ_qHJPidB_MPYAGCJV=VQ@mail.gmail.com>
References: <CANEZrP3w+AiTXmv9Wb3Zi5yyFmGPk82-ysVo4_DVvtg8HHBCdQ@mail.gmail.com>
	<CABsx9T0Z3xKGOO=4Tr94cKTBpfUwHQ_qHJPidB_MPYAGCJV=VQ@mail.gmail.com>
Message-ID: <CANEZrP2pgBQUj7TYhyAw38p5KKNjmg_mOG4+O-A0DENErYbk5Q@mail.gmail.com>

[resend, sorry gavin]

I think these ideas all make a ton of sense, some have been floating
around for a while in various forms but it's good to draw them
together coherently.

> o Fill up the "free" space (if any) with the highest-priority
> transactions, where priority is a function of transaction size, age of
> inputs, number of bitcoins... and ratio of inputs to outputs (to
> encourage combining inputs so more pruning is possible).

Is more incentive needed? If you have tons of tiny outputs you already
have incentives to merge them because otherwise your txns will become
large and the fees needed to overcome the DoS limits and gain priority
will rise.

The code to do it is a bit irritating as you really want to de-frag
wallets in the background when the user is not likely to need the
outputs quickly, and I suspect over time transaction volumes will
become diurnal so it'd be cheaper to do that at night time, but it's
all possible.

> But that won't work for newly started clients that haven't seen a lot
> of transactions enter/exit the memory pool

Peers could provide first-seen timestamps for transactions when
announced or when downloaded with Jeffs proposed command, but the
timestamps are not necessarily trustable. Not sure if that'd open up
new attacks.

> or SPV clients that can't lookup transaction inputs

SPV clients can do it by getdata-ing on the relevant inputs, but it's
very bandwidth intensive just to guesstimate fees.

> Maybe each client developer runs a "fee policy server"

That's reasonable. I don't believe this case is worth worrying about
right now. For the common cases of

a) Customer buys from merchant (runs full node)
b) Trusted person sends money to trusting person (does not need confirms)

it wouldn't matter after the changes to the block creation code. It's
only really an issue when a user running an SPV client wishes to
accept money from somebody they do not trust, and they want it to
confirm quick-ish (within an hour), but can tolerate delays up to
that. I think this is likely to be rare.

Much more common is that you want to accept the payment immediately,
which is an oft discussed but different problem.



From andyparkins at gmail.com  Sat Jun 16 08:16:24 2012
From: andyparkins at gmail.com (Andy Parkins)
Date: Sat, 16 Jun 2012 09:16:24 +0100
Subject: [Bitcoin-development] Proposed new P2P command and response:
	getcmds, cmdlist
In-Reply-To: <CA+s+GJCKSrJv4L=4Nj4Hs+j2vfM-oWe5ayD_4NOUJMoXCkm3iA@mail.gmail.com>
References: <CA+8xBpdD31koaVBh1RuDZKH1sygr8z10K=bPz8DepqYOa8i6yg@mail.gmail.com>
	<1339810493.15660.YahooMailNeo@web121004.mail.ne1.yahoo.com>
	<CA+s+GJCKSrJv4L=4Nj4Hs+j2vfM-oWe5ayD_4NOUJMoXCkm3iA@mail.gmail.com>
Message-ID: <201206160916.24485.andyparkins@gmail.com>

On Saturday 16 Jun 2012 07:45:00 Wladimir wrote:
> As replied on the github issue:
> 
> Personally I still think it's better to have a clear standardized
> "protocol version", that implies what capabilities are supported,
> instead of a capability-based system that explicitly lists them.
> 
> Capability-based systems (just look at OpenGL) tend to become
> horrendously complex, as you have to take into account all possible
> combinations of possible interactions, and constantly check for support
> of specific features instead of just comparing a version number.
> 
> Sure, it can be necessary to distinguish between different types of
> nodes, but there is no need to make it this fine-grained.

It's less of a problem in a (nearly) stateless protocol like Bitcoin.

I like the idea of a capabilities command; as time goes on and the ecosystem 
of thin/spv/semi-thin/headers-only/blocks-on-demand/reverse-search-
blockchain/memory-pool-query clients becomes more varied, it's going to be 
more an more important.  The particular example that occurs is thin clients 
connecting to the network are going to want to ensure they are connected to 
at least one non-thin client.



Andy

-- 
Dr Andy Parkins
andyparkins at gmail.com



From andyparkins at gmail.com  Sat Jun 16 08:17:39 2012
From: andyparkins at gmail.com (Andy Parkins)
Date: Sat, 16 Jun 2012 09:17:39 +0100
Subject: [Bitcoin-development] Proposed new P2P command and response:
	getcmds, cmdlist
In-Reply-To: <1339810493.15660.YahooMailNeo@web121004.mail.ne1.yahoo.com>
References: <CA+8xBpdD31koaVBh1RuDZKH1sygr8z10K=bPz8DepqYOa8i6yg@mail.gmail.com>
	<1339810493.15660.YahooMailNeo@web121004.mail.ne1.yahoo.com>
Message-ID: <201206160917.39752.andyparkins@gmail.com>

On Saturday 16 Jun 2012 02:34:53 Amir Taaki wrote:
> Introspection/command discovery is nice, but I would prefer it to be
> immediately done in the first version exchange so no assumptions as to
> how a network is operating need to be made.

That would need a change of the current version message.  So why not make 
the change be simply: one of the service bits indicates that "getcmds" is 
available?

Then the version message doesn't need any on-the-wire change.



Andy

-- 
Dr Andy Parkins
andyparkins at gmail.com



From mike at plan99.net  Sat Jun 16 08:25:48 2012
From: mike at plan99.net (Mike Hearn)
Date: Sat, 16 Jun 2012 10:25:48 +0200
Subject: [Bitcoin-development] New P2P commands for diagnostics,
	SPV clients
In-Reply-To: <1339785752.91313.YahooMailNeo@web121004.mail.ne1.yahoo.com>
References: <CA+8xBpecVQcTTbPxUm_3_GWC99dEd4=-VFWb+QT6jUy4rg8U4w@mail.gmail.com>
	<CANEZrP0kNZDByHpK2=UjP+ag0X1KmqHxnJdm=e_pWMitP4QvvA@mail.gmail.com>
	<CA+8xBpcwhQPQRe=stYb9xksLsTbiABKLS7PZnRtvPga6AmSg4Q@mail.gmail.com>
	<CANEZrP39RHfCDX-x4ARo+oPphLv-70RxuMh3+AJzsNPxzOd=bA@mail.gmail.com>
	<1339785752.91313.YahooMailNeo@web121004.mail.ne1.yahoo.com>
Message-ID: <CANEZrP037ZFr=NgVGQh3zU5fbEs9_03AdBjdNaUTwKjZvwoFmQ@mail.gmail.com>

The bottleneck for the android Bitcoin Wallet app is rapidly becoming
bandwidth and parse time.

On Fri, Jun 15, 2012 at 8:42 PM, Amir Taaki <zgenjix at yahoo.com> wrote:
> Why though? The bottleneck is not network traffic but disk space usage/blockchain validation time.
>
>
>
> ----- Original Message -----
> From: Mike Hearn <mike at plan99.net>
> To: Jeff Garzik <jgarzik at exmulti.com>
> Cc: Bitcoin Development <bitcoin-development at lists.sourceforge.net>
> Sent: Friday, June 15, 2012 3:43 PM
> Subject: Re: [Bitcoin-development] New P2P commands for diagnostics, SPV clients
>
>> Yes, the format is something that must be hashed out (no pun
>> intended). ?Need input from potential users about what information
>> they might need.
>
> Matts point that a branch-per-transaction may duplicate data is well
> made, that said, I suspect a format that tries to fix this would be
> much more complicated.
>
> How about see this project as a three part change?
>
> First step - add the mempool command and make nodes sync up their
> mempools on startup.
>
> Second step - if protocol version >= X, the "block" message consists
> of a header + num transactions + vector<hash>? instead of the full
> transactions themselves.
>
> On receiving such a block, we go look to see which transactions we're
> missing from the mempool and request them with getdata. Each time we
> receive a tx message we check to see if it was one we were missing
> from a block. Once all transactions in the block message are in
> memory, we go ahead and assemble the block, then verify as per normal.
> This should speed up block propagation. Miners have an incentive to
> upgrade because it should reduce wasted work.
>
> Third step - new message, getmerkletx takes a vector<hash> and returns
> a merkletx message: "merkle branch missing the root + transaction data
> itself" for each requested transaction. The filtering commands are
> added, so the block message now only lists transaction hashes that
> match the filter which can then be requested with getmerkletx.
>
> ------------------------------------------------------------------------------
> Live Security Virtual Conference
> Exclusive live event will cover all the ways today's security and
> threat landscape has changed and how IT managers can respond. Discussions
> will include endpoint security, mobile security and the latest in malware
> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
> _______________________________________________
> Bitcoin-development mailing list
> Bitcoin-development at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/bitcoin-development
>
>
> ------------------------------------------------------------------------------
> Live Security Virtual Conference
> Exclusive live event will cover all the ways today's security and
> threat landscape has changed and how IT managers can respond. Discussions
> will include endpoint security, mobile security and the latest in malware
> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
> _______________________________________________
> Bitcoin-development mailing list
> Bitcoin-development at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/bitcoin-development



From mike at plan99.net  Sat Jun 16 08:27:09 2012
From: mike at plan99.net (Mike Hearn)
Date: Sat, 16 Jun 2012 10:27:09 +0200
Subject: [Bitcoin-development] New P2P commands for diagnostics,
	SPV clients
In-Reply-To: <1339771184.31489.53.camel@bmthinkpad>
References: <CA+8xBpecVQcTTbPxUm_3_GWC99dEd4=-VFWb+QT6jUy4rg8U4w@mail.gmail.com>
	<CANEZrP0kNZDByHpK2=UjP+ag0X1KmqHxnJdm=e_pWMitP4QvvA@mail.gmail.com>
	<1339766346.31489.49.camel@bmthinkpad>
	<CANEZrP3jj2ymQPH50g2PvzZhRzTnUnCLUjvBYj8ndBCJsnGJ-w@mail.gmail.com>
	<1339771184.31489.53.camel@bmthinkpad>
Message-ID: <CANEZrP0hTRbE9+VEa3eCzJkbHqa3u8tpdw7eDLBQQR6DBf2adw@mail.gmail.com>

> I'd much rather have an overloaded node respond with 50% fp rate filters
> as an option if there aren't many full nodes available than simply
> disconnect SPV clients.

I don't think the bloom filter settings have any impact on server-side
load ... a node still has to check every transaction against the
filter regardless of how that filter is configured, which means the
same amount of disk io and processing.

How can you reduce load on a peer by negotiating different filter settings?



From mike at plan99.net  Sat Jun 16 08:30:30 2012
From: mike at plan99.net (Mike Hearn)
Date: Sat, 16 Jun 2012 10:30:30 +0200
Subject: [Bitcoin-development] SatoshiDice and Near-term scalability
In-Reply-To: <1339821211.57245.YahooMailNeo@web121006.mail.ne1.yahoo.com>
References: <CA+8xBpde=y213zFeVjYoZgBstZ6GTnk84ADD8twaDJy2D6CHyw@mail.gmail.com>
	<007701cd4b68$a88e6a40$f9ab3ec0$@bitcoinstats.org>
	<1339821211.57245.YahooMailNeo@web121006.mail.ne1.yahoo.com>
Message-ID: <CANEZrP2n0a37vRiyn3nMY92QaUYkmycVV59OObze3xF39u-mGg@mail.gmail.com>

Joseph is quite accommodating and doesn't want to hurt the network.
That said "asking him to stop" seems like the worst possible solution
possible. His site is quite reasonable.

I think if I fix bitcoinj to have smarter fee code he might stop
attaching a small fee to every TX, but I'm not sure.

On Sat, Jun 16, 2012 at 6:33 AM, Amir Taaki <zgenjix at yahoo.com> wrote:
> Did anyone try sending them an email asking them to stop or offering help to fix their site? What did they say? I'm sure they would try to be accomodating.
>
>
>
> ----- Original Message -----
> From: Jonathan Warren <jonathan at bitcoinstats.org>
> To: bitcoin-development at lists.sourceforge.net
> Cc:
> Sent: Saturday, June 16, 2012 4:35 AM
> Subject: Re: [Bitcoin-development] SatoshiDice and Near-term scalability
>
> Yes, I measure mainnet confirmation times on a regular basis.
> http://bitcoinstats.org/post/tx-confirmation-times-June2012.png
>
> Before fairly recently, fee-paying transactions never took anywhere close to
> this long to be confirmed.
>
> Jonathan Warren
> (Bitcointalk: Atheros)
>
> -----Original Message-----
> From: Jeff Garzik [mailto:jgarzik at exmulti.com]
> Sent: Friday, June 15, 2012 1:17 PM
> To: bitcoin-development at lists.sourceforge.net
> Subject: [Bitcoin-development] SatoshiDice and Near-term scalability
>
> Hard-fork requires a very high level of community buy-in, because it shuts
> out older clients who will simply refuse to consider >1MB blocks valid.
>
> Anything approaching that level of change would need some good, hard data
> indicating that SatoshiDice was shutting out the majority of other traffic.
> Does anyone measure mainnet "normal tx" confirmation times on a regular
> basis?? Any other hard data?
>
>
>
> ------------------------------------------------------------------------------
> Live Security Virtual Conference
> Exclusive live event will cover all the ways today's security and
> threat landscape has changed and how IT managers can respond. Discussions
> will include endpoint security, mobile security and the latest in malware
> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
> _______________________________________________
> Bitcoin-development mailing list
> Bitcoin-development at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/bitcoin-development
>
>
> ------------------------------------------------------------------------------
> Live Security Virtual Conference
> Exclusive live event will cover all the ways today's security and
> threat landscape has changed and how IT managers can respond. Discussions
> will include endpoint security, mobile security and the latest in malware
> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
> _______________________________________________
> Bitcoin-development mailing list
> Bitcoin-development at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/bitcoin-development



From laanwj at gmail.com  Sat Jun 16 08:42:21 2012
From: laanwj at gmail.com (Wladimir)
Date: Sat, 16 Jun 2012 10:42:21 +0200
Subject: [Bitcoin-development] Proposed new P2P command and response:
 getcmds, cmdlist
In-Reply-To: <201206160916.24485.andyparkins@gmail.com>
References: <CA+8xBpdD31koaVBh1RuDZKH1sygr8z10K=bPz8DepqYOa8i6yg@mail.gmail.com>
	<1339810493.15660.YahooMailNeo@web121004.mail.ne1.yahoo.com>
	<CA+s+GJCKSrJv4L=4Nj4Hs+j2vfM-oWe5ayD_4NOUJMoXCkm3iA@mail.gmail.com>
	<201206160916.24485.andyparkins@gmail.com>
Message-ID: <CA+s+GJA2-+HuRFfX3b=-4wv7u9iFCnfOMyDKwekxmipszt27Cw@mail.gmail.com>

On Sat, Jun 16, 2012 at 10:16 AM, Andy Parkins <andyparkins at gmail.com>wrote:

>
> It's less of a problem in a (nearly) stateless protocol like Bitcoin.
>

It's currently (nearly) stateless, however it would be short-sighted to
think it will stay that way. State is being introduced as we speak; for
example, connection-specific filters.

I like the idea of a capabilities command; as time goes on and the ecosystem
> of thin/spv/semi-thin/headers-only/blocks-on-demand/reverse-search-
> blockchain/memory-pool-query clients becomes more varied, it's going to be
> more an more important.  The particular example that occurs is thin clients
> connecting to the network are going to want to ensure they are connected to
> at least one non-thin client.
>

Which is a perfectly reasonable requirement. However, one could simply
standardize what a 'thin client' and what a 'thick client' does and offers
(at a certain version level), without having to explicitly enumerate
everything over the protocol.

This also makes it easier to deprecate (lack of) certain features later on.
You can simply drop support for protocol versions before a certain number
(which has happened before). With the extension system this is much harder,
which likely means you keep certain workarounds forever.

Letting the node know of each others capabilities at connection time helps
somewhat. It'd allow refusing clients that do not implement a certain
feature. Then again, to me it's unclear what this wins compared to
incremental protocol versions with clear requirements.

I'm just afraid that the currently simple P2P protocol will turn into a zoo
of complicated (and potentially buggy/insecure) interactions.

So maybe a capability system is a good idea but then the granularity should
be large, not command-level. The interaction between protocol versions and
capabilities needs to be defined as well. Does offering "getdata" at
protocol version 10 mean the same as offering it at protocol version 11"?
Probably not guaranteed. The arguments might have changed. So it's not
entirely self-documenting either.

Wladimir
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120616/2a6e50dc/attachment.html>

From zgenjix at yahoo.com  Sat Jun 16 09:16:46 2012
From: zgenjix at yahoo.com (Amir Taaki)
Date: Sat, 16 Jun 2012 02:16:46 -0700 (PDT)
Subject: [Bitcoin-development] Proposed new P2P command and response:
	getcmds, cmdlist
In-Reply-To: <CA+s+GJA2-+HuRFfX3b=-4wv7u9iFCnfOMyDKwekxmipszt27Cw@mail.gmail.com>
References: <CA+8xBpdD31koaVBh1RuDZKH1sygr8z10K=bPz8DepqYOa8i6yg@mail.gmail.com>
	<1339810493.15660.YahooMailNeo@web121004.mail.ne1.yahoo.com>
	<CA+s+GJCKSrJv4L=4Nj4Hs+j2vfM-oWe5ayD_4NOUJMoXCkm3iA@mail.gmail.com>
	<201206160916.24485.andyparkins@gmail.com>
	<CA+s+GJA2-+HuRFfX3b=-4wv7u9iFCnfOMyDKwekxmipszt27Cw@mail.gmail.com>
Message-ID: <1339838206.26361.YahooMailNeo@web121005.mail.ne1.yahoo.com>

> I'm just afraid that the currently simple P2P protocol will turn into a 
zoo of complicated (and potentially buggy/insecure) interactions. 


This is my biggest fear too. I would rather be extremely conservative in making any changes to the protocol unless absolutely needed. That includes the bloom filters which take away the fact that Bitcoin is stateless.

I was discussing this with another developer who mentioned something interesting: that always in the lifecycle of system's development, you see increasing complexity during its initial lifecycle as the field is being explored. At some later point, the technology matures and becomes standardised. At that point enough is known that the system snaps together and the cruft can be cut away to reduce the system down to core principles.

It's an interesting viewpoint to consider. I do however advise erring on the side of caution. Maybe there needs to a minimum schedule time before a new extension can be added to the protocol (except security fixes). If we're not careful, the protocol will become enormously huge and kludgy. However maybe as that developer pointed out, trying to stall the inevitable is slowing the long-term evolution of Bitcoin down.


________________________________
From: Wladimir <laanwj at gmail.com>
To: Andy Parkins <andyparkins at gmail.com> 
Cc: bitcoin-development at lists.sourceforge.net 
Sent: Saturday, June 16, 2012 10:42 AM
Subject: Re: [Bitcoin-development] Proposed new P2P command and response: getcmds, cmdlist


On Sat, Jun 16, 2012 at 10:16 AM, Andy Parkins <andyparkins at gmail.com> wrote:


>It's less of a problem in a (nearly) stateless protocol like Bitcoin.
>

It's currently (nearly) stateless, however it would be short-sighted to think it will stay that way. State is being introduced as we speak; for example, connection-specific filters.

I like the idea of a capabilities command; as time goes on and the ecosystem
>of thin/spv/semi-thin/headers-only/blocks-on-demand/reverse-search-
>blockchain/memory-pool-query clients becomes more varied, it's going to be
>more an more important. ?The particular example that occurs is thin clients
>connecting to the network are going to want to ensure they are connected to
>at least one non-thin client.
>

Which is a perfectly reasonable requirement. However, one could simply standardize what a 'thin client' and what a 'thick client' does and offers (at a certain version level), without having to explicitly enumerate everything over the protocol.?

This also makes it easier to deprecate (lack of) certain features later on. You can simply drop support for protocol versions before a certain number (which has happened before). With the extension system this is much harder, which likely means you keep certain workarounds forever.?

Letting the node know of each others capabilities at connection time helps somewhat. It'd allow refusing clients that do not implement a certain feature. Then again, to me it's unclear what this wins compared to incremental protocol versions with clear requirements.?

I'm just afraid that the currently simple P2P protocol will turn into a zoo of complicated (and potentially buggy/insecure) interactions.?

So maybe a capability system is a good idea but then the granularity should be large, not command-level. The interaction between protocol versions and capabilities needs to be defined as well. Does offering "getdata" at protocol version 10 mean the same as offering it at protocol version 11"? Probably not guaranteed. The arguments might have changed. So it's not entirely self-documenting either.

Wladimir

------------------------------------------------------------------------------
Live Security Virtual Conference
Exclusive live event will cover all the ways today's security and 
threat landscape has changed and how IT managers can respond. Discussions 
will include endpoint security, mobile security and the latest in malware 
threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
_______________________________________________
Bitcoin-development mailing list
Bitcoin-development at lists.sourceforge.net
https://lists.sourceforge.net/lists/listinfo/bitcoin-development



From andyparkins at gmail.com  Sat Jun 16 09:54:11 2012
From: andyparkins at gmail.com (Andy Parkins)
Date: Sat, 16 Jun 2012 10:54:11 +0100
Subject: [Bitcoin-development] Proposed new P2P command and response:
	getcmds, cmdlist
In-Reply-To: <CA+s+GJA2-+HuRFfX3b=-4wv7u9iFCnfOMyDKwekxmipszt27Cw@mail.gmail.com>
References: <CA+8xBpdD31koaVBh1RuDZKH1sygr8z10K=bPz8DepqYOa8i6yg@mail.gmail.com>
	<201206160916.24485.andyparkins@gmail.com>
	<CA+s+GJA2-+HuRFfX3b=-4wv7u9iFCnfOMyDKwekxmipszt27Cw@mail.gmail.com>
Message-ID: <201206161054.11537.andyparkins@gmail.com>

On Saturday 16 Jun 2012 09:42:21 Wladimir wrote:

> Which is a perfectly reasonable requirement. However, one could simply
> standardize what a 'thin client' and what a 'thick client' does and
> offers (at a certain version level), without having to explicitly
> enumerate everything over the protocol.

My problem is that that I suspect the spectrum of clients will be far more 
than simply "thin" or "thick".  What about thick-pruned, thick-full?  What 
about thin-blocks-on-demand and thin-headers-on-demand?  These are just what 
I can think of now; it seems unwise to limit the functionality of clients 
not yet designed with a binary designation.  So... we make a field that can 
hold more than just a bit; with each possible value representing a specific 
(possibly overlapping) set of features?  Why not just enumerate the features 
then?

I did write responses to each of your following points; but they just 
sounded like me being contrary.  The short version is that I think too much 
emphasis is being placed on defining a specific set of feature->version 
mapping.  That's going to make it hard for future clients that want to 
implement some of the features but not all, and yet still want to be good 
bitcoin citizens and be able to tell their peers what they don't support.  
For example, there is no easy way for a node to tell another that it doesn't 
have the whole block chain available, so requesting it from it will fail. 

> I'm just afraid that the currently simple P2P protocol will turn into a
> zoo of complicated (and potentially buggy/insecure) interactions.

Fair enough.

> So maybe a capability system is a good idea but then the granularity
> should be large, not command-level. The interaction between protocol
> versions and capabilities needs to be defined as well. Does offering
> "getdata" at protocol version 10 mean the same as offering it at
> protocol version 11"? Probably not guaranteed. The arguments might have
> changed. So it's not entirely self-documenting either.

That problem doesn't go away just because you don't have a capabilities 
system.  Either version 11 can speak version 10 or it can't.  I don't see 
how having a system for finding out that fact changes anything other than 
removing a load of protocol noise.

"I support getdata10" makes it far easier to discover that the peer supports 
getdata10 than sending getdata11 and watching it fail does.



Andy
-- 
Dr Andy Parkins
andyparkins at gmail.com



From pieter.wuille at gmail.com  Sat Jun 16 19:26:52 2012
From: pieter.wuille at gmail.com (Pieter Wuille)
Date: Sat, 16 Jun 2012 21:26:52 +0200
Subject: [Bitcoin-development] After compressed pubkeys: hybrid pubkeys
Message-ID: <20120616192651.GA13438@vps7135.xlshosting.net>

Hello all,

while OpenSSL's silent support for compressed public keys allowed us to
enable them in a fully backward-compatible way, it seems OpenSSL supports yet
another (and non-standard, and apparently useless) encoding for public keys.

As these are supported by (almost all?) fully validating clients on the
network, I believe alternative implementations should be willing to handle
them as well. No hybrid keys are used in the main chain, but I did test them
in testnet3, and they work as expected.

In total, the following encodings exist:
* 0x00: point at infinity; not a valid public key
* 0x02 [32-byte X coord]: compressed format for even Y coords
* 0x03 [32-byte X coord]: compressed format for odd Y coords
* 0x04 [32-byte X coord] [32-byte Y coord]: uncompressed format
* 0x06 [32-byte X coord] [32-byte Y coord]: hybrid format for even Y coords
* 0x07 [32-byte X coord] [32-byte Y coord]: hybrid format for odd Y coords

Handling them is trivial: if you see a public key starting with a 0x06 or
0x07, use it as if there was a 0x04 instead.

I suppose we could decide to forbid these after a certain date/block height,
and try to get sufficient mining power to enforce that before that date.
Any opinions? Forbidding it certainly makes alternative implementation
slightly easier in the future, but I'm not sure the hassle of a network
rule change is worth it.

-- 
Pieter



From gavinandresen at gmail.com  Sat Jun 16 21:41:52 2012
From: gavinandresen at gmail.com (Gavin Andresen)
Date: Sat, 16 Jun 2012 17:41:52 -0400
Subject: [Bitcoin-development] After compressed pubkeys: hybrid pubkeys
In-Reply-To: <20120616192651.GA13438@vps7135.xlshosting.net>
References: <20120616192651.GA13438@vps7135.xlshosting.net>
Message-ID: <CABsx9T1_R2RE0S=ygY18OyJ0W+Bxyt5Q75bF4J36r0C8ae4-Fw@mail.gmail.com>

RE: 0x06/0x07 'hybrid' public keys:

> Any opinions? Forbidding it certainly makes alternative implementation
> slightly easier in the future, but I'm not sure the hassle of a network
> rule change is worth it.

I say treat any transactions that use them as 'non-standard' -- don't
relay/mine them by default, but accept blocks that happen to contain
them.

I agree that a rule change isn't worth it right now, but making them
non-standard now is easy and should make a rule change in the future
easier.

-- 
--
Gavin Andresen



From gmaxwell at gmail.com  Sat Jun 16 23:39:00 2012
From: gmaxwell at gmail.com (Gregory Maxwell)
Date: Sat, 16 Jun 2012 19:39:00 -0400
Subject: [Bitcoin-development] After compressed pubkeys: hybrid pubkeys
In-Reply-To: <CABsx9T1_R2RE0S=ygY18OyJ0W+Bxyt5Q75bF4J36r0C8ae4-Fw@mail.gmail.com>
References: <20120616192651.GA13438@vps7135.xlshosting.net>
	<CABsx9T1_R2RE0S=ygY18OyJ0W+Bxyt5Q75bF4J36r0C8ae4-Fw@mail.gmail.com>
Message-ID: <CAAS2fgT=eLweqpmRGonuvEYfiGi4nbaKh-uw01PKyMC3qyY71A@mail.gmail.com>

On Sat, Jun 16, 2012 at 5:41 PM, Gavin Andresen <gavinandresen at gmail.com> wrote:
> RE: 0x06/0x07 'hybrid' public keys:
>
>> Any opinions? Forbidding it certainly makes alternative implementation
>> slightly easier in the future, but I'm not sure the hassle of a network
>> rule change is worth it.
>
> I say treat any transactions that use them as 'non-standard' -- don't
> relay/mine them by default, but accept blocks that happen to contain
> them.
>
> I agree that a rule change isn't worth it right now, but making them
> non-standard now is easy and should make a rule change in the future
> easier.

ACK.  Hopefully no one will mine these before we can merge denying
them into another rule change. But if they do, oh well.



From luke at dashjr.org  Sun Jun 17 01:15:54 2012
From: luke at dashjr.org (Luke-Jr)
Date: Sun, 17 Jun 2012 01:15:54 +0000
Subject: [Bitcoin-development] After compressed pubkeys: hybrid pubkeys
In-Reply-To: <CAAS2fgT=eLweqpmRGonuvEYfiGi4nbaKh-uw01PKyMC3qyY71A@mail.gmail.com>
References: <20120616192651.GA13438@vps7135.xlshosting.net>
	<CABsx9T1_R2RE0S=ygY18OyJ0W+Bxyt5Q75bF4J36r0C8ae4-Fw@mail.gmail.com>
	<CAAS2fgT=eLweqpmRGonuvEYfiGi4nbaKh-uw01PKyMC3qyY71A@mail.gmail.com>
Message-ID: <201206170115.56502.luke@dashjr.org>

On Saturday, June 16, 2012 11:39:00 PM Gregory Maxwell wrote:
> On Sat, Jun 16, 2012 at 5:41 PM, Gavin Andresen <gavinandresen at gmail.com> 
wrote:
> > RE: 0x06/0x07 'hybrid' public keys:
> >> Any opinions? Forbidding it certainly makes alternative implementation
> >> slightly easier in the future, but I'm not sure the hassle of a network
> >> rule change is worth it.
> > 
> > I say treat any transactions that use them as 'non-standard' -- don't
> > relay/mine them by default, but accept blocks that happen to contain
> > them.
> > 
> > I agree that a rule change isn't worth it right now, but making them
> > non-standard now is easy and should make a rule change in the future
> > easier.
> 
> ACK.  Hopefully no one will mine these before we can merge denying
> them into another rule change. But if they do, oh well.

I'm willing to make Eligius reject these as well, if someone provides a patch 
that doesn't depend on IsStandard being enforced...

Same goes for rejecting OP_NOP<n> - I can't see any legitimate reason we'd 
want these on mainnet right now.

Luke



From grarpamp at gmail.com  Sun Jun 17 09:22:20 2012
From: grarpamp at gmail.com (grarpamp)
Date: Sun, 17 Jun 2012 05:22:20 -0400
Subject: [Bitcoin-development] 0.6.x - detachdb in wrong place
Message-ID: <CAD2Ti2_Z-mzHu_VG7fq+sgQj7CfdZ_nKoa7Q6nDObwBSL6yXgQ@mail.gmail.com>

Well, detachdb doesn't appear in the -\? help
because it's stuffed under pnp, which is not set
in my build. please fix for people, tx :)

#ifdef USE_UPNP
#if USE_UPNP
            "  -upnp            \t  "   + _("Use Universal Plug and
Play to map the listening port (default: 1)") + "\n" +
#else
            "  -upnp            \t  "   + _("Use Universal Plug and
Play to map the listening port (default: 0)") + "\n" +
#endif
            "  -detachdb        \t  "   + _("Detach block and address
databases. Increases shutdown time (default: 0)") + "\n" +
#endif



From gmaxwell at gmail.com  Sun Jun 17 10:17:04 2012
From: gmaxwell at gmail.com (Gregory Maxwell)
Date: Sun, 17 Jun 2012 06:17:04 -0400
Subject: [Bitcoin-development] 0.6.x - detachdb in wrong place
In-Reply-To: <CAD2Ti2_Z-mzHu_VG7fq+sgQj7CfdZ_nKoa7Q6nDObwBSL6yXgQ@mail.gmail.com>
References: <CAD2Ti2_Z-mzHu_VG7fq+sgQj7CfdZ_nKoa7Q6nDObwBSL6yXgQ@mail.gmail.com>
Message-ID: <CAAS2fgQCc_-FgiXs0JABkAzZWNrbhhWYutvRgFLsnKYaEWDqwQ@mail.gmail.com>

On Sun, Jun 17, 2012 at 5:22 AM, grarpamp <grarpamp at gmail.com> wrote:
> Well, detachdb doesn't appear in the -\? help
> because it's stuffed under pnp, which is not set
> in my build. please fix for people, tx :)

It isn't inside the ifdef in bitcoin git master.

(For future reference this sort of request is probably best opened as
an issue in the github issue tracker instead of posted to the list).



From mike at plan99.net  Sun Jun 17 11:01:12 2012
From: mike at plan99.net (Mike Hearn)
Date: Sun, 17 Jun 2012 13:01:12 +0200
Subject: [Bitcoin-development] After compressed pubkeys: hybrid pubkeys
In-Reply-To: <20120616192651.GA13438@vps7135.xlshosting.net>
References: <20120616192651.GA13438@vps7135.xlshosting.net>
Message-ID: <CANEZrP0hGk63_9z3VLGGGw=VUZKC8tNorJF+2udmjZgCfDK-Tw@mail.gmail.com>

> * 0x04 [32-byte X coord] [32-byte Y coord]: uncompressed format
> * 0x06 [32-byte X coord] [32-byte Y coord]: hybrid format for even Y coords
> * 0x07 [32-byte X coord] [32-byte Y coord]: hybrid format for odd Y coords

So what's the actual difference in format? Is there any at all, or
it's just the first number that's different?



From pieter.wuille at gmail.com  Sun Jun 17 12:04:48 2012
From: pieter.wuille at gmail.com (Pieter Wuille)
Date: Sun, 17 Jun 2012 14:04:48 +0200
Subject: [Bitcoin-development] After compressed pubkeys: hybrid pubkeys
In-Reply-To: <CANEZrP0hGk63_9z3VLGGGw=VUZKC8tNorJF+2udmjZgCfDK-Tw@mail.gmail.com>
References: <20120616192651.GA13438@vps7135.xlshosting.net>
	<CANEZrP0hGk63_9z3VLGGGw=VUZKC8tNorJF+2udmjZgCfDK-Tw@mail.gmail.com>
Message-ID: <20120617120447.GA26357@vps7135.xlshosting.net>

On Sun, Jun 17, 2012 at 01:01:12PM +0200, Mike Hearn wrote:
> > * 0x04 [32-byte X coord] [32-byte Y coord]: uncompressed format
> > * 0x06 [32-byte X coord] [32-byte Y coord]: hybrid format for even Y coords
> > * 0x07 [32-byte X coord] [32-byte Y coord]: hybrid format for odd Y coords
> 
> So what's the actual difference in format? Is there any at all, or
> it's just the first number that's different?

>From what I understand, that is indeed the only difference.

-- 
Pieter




From laanwj at gmail.com  Sun Jun 17 15:16:13 2012
From: laanwj at gmail.com (Wladimir)
Date: Sun, 17 Jun 2012 17:16:13 +0200
Subject: [Bitcoin-development] After compressed pubkeys: hybrid pubkeys
In-Reply-To: <20120617120447.GA26357@vps7135.xlshosting.net>
References: <20120616192651.GA13438@vps7135.xlshosting.net>
	<CANEZrP0hGk63_9z3VLGGGw=VUZKC8tNorJF+2udmjZgCfDK-Tw@mail.gmail.com>
	<20120617120447.GA26357@vps7135.xlshosting.net>
Message-ID: <CA+s+GJAc8ZX9ziapULE7KJpJUonTv+sfncw0RGLwu_YPQrM=zw@mail.gmail.com>

On Sun, Jun 17, 2012 at 2:04 PM, Pieter Wuille <pieter.wuille at gmail.com>wrote:

> On Sun, Jun 17, 2012 at 01:01:12PM +0200, Mike Hearn wrote:
> > > * 0x04 [32-byte X coord] [32-byte Y coord]: uncompressed format
> > > * 0x06 [32-byte X coord] [32-byte Y coord]: hybrid format for even Y
> coords
> > > * 0x07 [32-byte X coord] [32-byte Y coord]: hybrid format for odd Y
> coords
> >
> > So what's the actual difference in format? Is there any at all, or
> > it's just the first number that's different?
>
> >From what I understand, that is indeed the only difference.
>
>
To prevent surprises in the future, in case OpenSSL decides to add more,
can we disable all other key formats in advance?

Wladimir
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120617/09b5f3b1/attachment.html>

From jgarzik at exmulti.com  Sun Jun 17 15:19:53 2012
From: jgarzik at exmulti.com (Jeff Garzik)
Date: Sun, 17 Jun 2012 11:19:53 -0400
Subject: [Bitcoin-development] Proposed new P2P command and response:
 getcmds, cmdlist
In-Reply-To: <CA+s+GJA2-+HuRFfX3b=-4wv7u9iFCnfOMyDKwekxmipszt27Cw@mail.gmail.com>
References: <CA+8xBpdD31koaVBh1RuDZKH1sygr8z10K=bPz8DepqYOa8i6yg@mail.gmail.com>
	<1339810493.15660.YahooMailNeo@web121004.mail.ne1.yahoo.com>
	<CA+s+GJCKSrJv4L=4Nj4Hs+j2vfM-oWe5ayD_4NOUJMoXCkm3iA@mail.gmail.com>
	<201206160916.24485.andyparkins@gmail.com>
	<CA+s+GJA2-+HuRFfX3b=-4wv7u9iFCnfOMyDKwekxmipszt27Cw@mail.gmail.com>
Message-ID: <CA+8xBpcvLsc+UyMT2LjrcuPf2Q+Rp8FQEZFKWddOxwyie0azkw@mail.gmail.com>

On Sat, Jun 16, 2012 at 4:42 AM, Wladimir <laanwj at gmail.com> wrote:
> Which is a perfectly reasonable requirement. However, one could simply
> standardize what a 'thin client' and what a 'thick client' does and offers
> (at a certain version level), without having to explicitly enumerate
> everything over the protocol.
>
> This also makes it easier to deprecate (lack of) certain features later on.
> You can simply drop support for protocol versions before a certain number
> (which has happened before). With the extension system this is much harder,
> which likely means you keep certain workarounds forever.
>
> Letting the node know of each others capabilities at connection time helps
> somewhat. It'd allow refusing clients that do not implement a certain
> feature. Then again, to me it's unclear what this wins compared to
> incremental protocol versions with clear requirements.
>
> I'm just afraid that the currently simple P2P protocol will turn into a zoo
> of complicated (and potentially buggy/insecure) interactions.

What is missing here is some perspective on the current situation.  It
is -very- easy to make a protocol change and bump PROTOCOL_VERSION in
the Satoshi client.

But for anyone maintaining a non-Satoshi codebase, the P2P protocol is
already filled with all sorts of magic numbers, arbitrarily versioned
binary data structures..  already an unfriendly zoo of complicated and
potentially buggy interactions.  There is scant, incomplete
documentation on the wiki -- the Satoshi source code is really the
only true reference.

I see these problems personally, trying to keep ArtForz' half-a-node
running on mainnet (distributed as 'blkmond' with pushpool).

In an era of HTTP and JSON, NFS and iSCSI, bitcoin's P2P protocol is
woefully backwards, fragile, limited and inflexible when it comes to
parameter/extension exchange and negotiation.  Even iSCSI, that which
is implemented on hard drive firmware, has the ability to exchange
key=value  parameters between local and remote sides of the RPC
connection.

Calling the current P2P protocol "simple" belies all the
implementation details you absolutely -must- get right, to run on
mainnet today.  Satoshi client devs almost never see the fragility and
complexity inherent in the current legacy codebase, built up over
time.

-- 
Jeff Garzik
exMULTI, Inc.
jgarzik at exmulti.com



From zgenjix at yahoo.com  Sun Jun 17 16:30:41 2012
From: zgenjix at yahoo.com (Amir Taaki)
Date: Sun, 17 Jun 2012 09:30:41 -0700 (PDT)
Subject: [Bitcoin-development] Proposed new P2P command and response:
	getcmds, cmdlist
In-Reply-To: <CA+8xBpcvLsc+UyMT2LjrcuPf2Q+Rp8FQEZFKWddOxwyie0azkw@mail.gmail.com>
References: <CA+8xBpdD31koaVBh1RuDZKH1sygr8z10K=bPz8DepqYOa8i6yg@mail.gmail.com>
	<1339810493.15660.YahooMailNeo@web121004.mail.ne1.yahoo.com>
	<CA+s+GJCKSrJv4L=4Nj4Hs+j2vfM-oWe5ayD_4NOUJMoXCkm3iA@mail.gmail.com>
	<201206160916.24485.andyparkins@gmail.com>
	<CA+s+GJA2-+HuRFfX3b=-4wv7u9iFCnfOMyDKwekxmipszt27Cw@mail.gmail.com>
	<CA+8xBpcvLsc+UyMT2LjrcuPf2Q+Rp8FQEZFKWddOxwyie0azkw@mail.gmail.com>
Message-ID: <1339950641.22050.YahooMailNeo@web121005.mail.ne1.yahoo.com>

As the only person to have created and maintaining a full reimplementation of the Bitcoin protocol/standard, I do think Bitcoin is filled with arbitrary endianness and magic numbers. However it is a tiny and simple protocol.

The big problem is not implementing the Bitcoin protocol, but the fact that once you have created a codebase, you want to perfect and fine-tune the design. During the meantime, the Bitcoin protocol is being changed. Change to the Bitcoin protocol is far more damaging to people that want to implement the protocol than any issues with the current protocol.

That's not to say, I disagree with changes to the protocol. I think changes should be a lot more conservative and have a longer time frame than they do currently. Usually changes suddenly get added to the Satoshi client and I notice them in the commit log or announcements. Then it's like "oh I have to add this" and I spend a week working to implement the change without proper consideration or reflection which ends up with me having to compromise on design choices. That is to remain compatible with the protocol.

However it is not my intent to slow down progress so I usually try to hedge against that kind of feeling towards conservatism.



----- Original Message -----
From: Jeff Garzik <jgarzik at exmulti.com>
To: Wladimir <laanwj at gmail.com>
Cc: bitcoin-development at lists.sourceforge.net
Sent: Sunday, June 17, 2012 5:19 PM
Subject: Re: [Bitcoin-development] Proposed new P2P command and response: getcmds, cmdlist

On Sat, Jun 16, 2012 at 4:42 AM, Wladimir <laanwj at gmail.com> wrote:
> Which is a perfectly reasonable requirement. However, one could simply
> standardize what a 'thin client' and what a 'thick client' does and offers
> (at a certain version level), without having to explicitly enumerate
> everything over the protocol.
>
> This also makes it easier to deprecate (lack of) certain features later on.
> You can simply drop support for protocol versions before a certain number
> (which has happened before). With the extension system this is much harder,
> which likely means you keep certain workarounds forever.
>
> Letting the node know of each others capabilities at connection time helps
> somewhat. It'd allow refusing clients that do not implement a certain
> feature. Then again, to me it's unclear what this wins compared to
> incremental protocol versions with clear requirements.
>
> I'm just afraid that the currently simple P2P protocol will turn into a zoo
> of complicated (and potentially buggy/insecure) interactions.

What is missing here is some perspective on the current situation.? It
is -very- easy to make a protocol change and bump PROTOCOL_VERSION in
the Satoshi client.

But for anyone maintaining a non-Satoshi codebase, the P2P protocol is
already filled with all sorts of magic numbers, arbitrarily versioned
binary data structures..? already an unfriendly zoo of complicated and
potentially buggy interactions.? There is scant, incomplete
documentation on the wiki -- the Satoshi source code is really the
only true reference.

I see these problems personally, trying to keep ArtForz' half-a-node
running on mainnet (distributed as 'blkmond' with pushpool).

In an era of HTTP and JSON, NFS and iSCSI, bitcoin's P2P protocol is
woefully backwards, fragile, limited and inflexible when it comes to
parameter/extension exchange and negotiation.? Even iSCSI, that which
is implemented on hard drive firmware, has the ability to exchange
key=value? parameters between local and remote sides of the RPC
connection.

Calling the current P2P protocol "simple" belies all the
implementation details you absolutely -must- get right, to run on
mainnet today.? Satoshi client devs almost never see the fragility and
complexity inherent in the current legacy codebase, built up over
time.

-- 
Jeff Garzik
exMULTI, Inc.
jgarzik at exmulti.com

------------------------------------------------------------------------------
Live Security Virtual Conference
Exclusive live event will cover all the ways today's security and 
threat landscape has changed and how IT managers can respond. Discussions 
will include endpoint security, mobile security and the latest in malware 
threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
_______________________________________________
Bitcoin-development mailing list
Bitcoin-development at lists.sourceforge.net
https://lists.sourceforge.net/lists/listinfo/bitcoin-development




From etotheipi at gmail.com  Sun Jun 17 18:39:28 2012
From: etotheipi at gmail.com (Alan Reiner)
Date: Sun, 17 Jun 2012 14:39:28 -0400
Subject: [Bitcoin-development] Ultimate Blockchain Compression w/ trust-free
	lite nodes
Message-ID: <4FDE2460.5080301@gmail.com>

All,

With the flurry of discussion about blockchain compression, I thought it 
was time to put forward my final, most-advanced idea, into a single, 
well-thought-out, *illustrated*, forum post.     Please check it out: 
https://bitcointalk.org/index.php?topic=88208.0

This is a huge undertaking, but it has some pretty huge benefits.  And 
it's actually feasible because it can be implemented without disrupting 
the main network.  I'm sure there's lots of issues with it, but I'm 
putting it out there to see how it might be improved and actually executed.

----
*Summary:

*/Use a special tree data structure to organize all unspent-TxOuts on 
the network, and use the root of this tree to communicate its 
"signature" between nodes.  The leaves of this tree actually correspond 
to addresses/scripts, and the data at the leaf is actually a root of the 
unspent-TxOut list for that address/script.  To maintain security of the 
tree signatures, it will be included in the header of an alternate 
blockchain, which will be secured by merged mining.

This provides the same compression as the simpler unspent-TxOut merkle 
tree, but also gives nodes a way to download just the unspent-TxOut list 
for each address in their wallet, and verify that list directly against 
the blockheaders.  Therefore, even lightweight nodes can get full 
address information, from any untrusted peer, and with only a tiny 
amount of downloaded data (a few kB). /*
*----

Alright, tear it up!
-Alan

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120617/c6a13a8c/attachment.html>

From pete at petertodd.org  Sun Jun 17 19:05:11 2012
From: pete at petertodd.org (Peter Todd)
Date: Sun, 17 Jun 2012 15:05:11 -0400
Subject: [Bitcoin-development] Ultimate Blockchain Compression w/
 trust-free lite nodes
In-Reply-To: <4FDE2460.5080301@gmail.com>
References: <4FDE2460.5080301@gmail.com>
Message-ID: <20120617190511.GA26047@savin>

On Sun, Jun 17, 2012 at 02:39:28PM -0400, Alan Reiner wrote:
> All,
> 
> With the flurry of discussion about blockchain compression, I
> thought it was time to put forward my final, most-advanced idea,
> into a single, well-thought-out, *illustrated*, forum post.
> Please check it out: https://bitcointalk.org/index.php?topic=88208.0
> 
> This is a huge undertaking, but it has some pretty huge benefits.
> And it's actually feasible because it can be implemented without
> disrupting the main network.  I'm sure there's lots of issues with
> it, but I'm putting it out there to see how it might be improved and
> actually executed.
> 
> ----
> *Summary:
> 
> */Use a special tree data structure to organize all unspent-TxOuts
> on the network, and use the root of this tree to communicate its
> "signature" between nodes.  The leaves of this tree actually
> correspond to addresses/scripts, and the data at the leaf is
> actually a root of the unspent-TxOut list for that address/script.
> To maintain security of the tree signatures, it will be included in
> the header of an alternate blockchain, which will be secured by
> merged mining.
> 
> This provides the same compression as the simpler unspent-TxOut
> merkle tree, but also gives nodes a way to download just the
> unspent-TxOut list for each address in their wallet, and verify that
> list directly against the blockheaders.  Therefore, even lightweight
> nodes can get full address information, from any untrusted peer, and
> with only a tiny amount of downloaded data (a few kB). /*

How are you going to prevent people from delibrately unbalancing the
tree with addresses with chosen hashes?

One idea that comes to mind, which unfortunately would make for a
pseudo-network rule, is to simply say that any *new* address whose hash
happens to be deeper in the tree than, say, 10*log(n), indicating it was
probably chosen to be unbalanced, gets discarded. The "new address" part
of the rule would be required, or else you could use the rule to get
other people's addresses discarded.

Having said that, such a rule just means that anyone playing games will
find they can't spend *their* money, and only with pruning clients.
Unrelated people will not be effected. The coins can also always be
spent with a non-pruning client to an acceptable address, which can
later re-spend on a pruning client.


It also comes to mind is that with the popularity of firstbits it may be
a good idea to use a comparison function that works last bit first...


It's merkles all the way down...

-- 
'peter'[:-1]@petertodd.org
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 490 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120617/f3a21565/attachment.sig>

From grarpamp at gmail.com  Sun Jun 17 21:35:46 2012
From: grarpamp at gmail.com (grarpamp)
Date: Sun, 17 Jun 2012 17:35:46 -0400
Subject: [Bitcoin-development] 0.6.x - detachdb in wrong place
In-Reply-To: <CAAS2fgQCc_-FgiXs0JABkAzZWNrbhhWYutvRgFLsnKYaEWDqwQ@mail.gmail.com>
References: <CAD2Ti2_Z-mzHu_VG7fq+sgQj7CfdZ_nKoa7Q6nDObwBSL6yXgQ@mail.gmail.com>
	<CAAS2fgQCc_-FgiXs0JABkAzZWNrbhhWYutvRgFLsnKYaEWDqwQ@mail.gmail.com>
Message-ID: <CAD2Ti29+=L+ss7fwivy+gfPFuE10sQdy68TGhL-30ngWKTLdQA@mail.gmail.com>

> It isn't inside the ifdef in bitcoin git master.

Oh, hmm, well then, what is the difference or usage
between these two repositories in regards to the project?

Which one are the formal releases tagged (tbz's cut) in?

Which one has the branches with the commits that will
make it into the next formal release? ie: tracking along
0.5.x, 0.6.x, HEAD/master (to be branched for 0.7.x).

https://github.com/bitcoin/bitcoin
https://git.gitorious.org/bitcoin/bitcoind-stable

I seem to be seeing more tags in the former, and
more maintained branches in the latter?



From gmaxwell at gmail.com  Sun Jun 17 21:52:18 2012
From: gmaxwell at gmail.com (Gregory Maxwell)
Date: Sun, 17 Jun 2012 17:52:18 -0400
Subject: [Bitcoin-development] 0.6.x - detachdb in wrong place
In-Reply-To: <CAD2Ti29+=L+ss7fwivy+gfPFuE10sQdy68TGhL-30ngWKTLdQA@mail.gmail.com>
References: <CAD2Ti2_Z-mzHu_VG7fq+sgQj7CfdZ_nKoa7Q6nDObwBSL6yXgQ@mail.gmail.com>
	<CAAS2fgQCc_-FgiXs0JABkAzZWNrbhhWYutvRgFLsnKYaEWDqwQ@mail.gmail.com>
	<CAD2Ti29+=L+ss7fwivy+gfPFuE10sQdy68TGhL-30ngWKTLdQA@mail.gmail.com>
Message-ID: <CAAS2fgTHo0U+2U5vtbmTEiKB6rBHbfuRHsm-bcnRhSrs-2jZpw@mail.gmail.com>

On Sun, Jun 17, 2012 at 5:35 PM, grarpamp <grarpamp at gmail.com> wrote:
>> It isn't inside the ifdef in bitcoin git master.
>
> Oh, hmm, well then, what is the difference or usage
> between these two repositories in regards to the project?
> Which one are the formal releases tagged (tbz's cut) in?
>
> Which one has the branches with the commits that will
> make it into the next formal release? ie: tracking along
> 0.5.x, 0.6.x, HEAD/master (to be branched for 0.7.x).
>
> https://github.com/bitcoin/bitcoin
> https://git.gitorious.org/bitcoin/bitcoind-stable

The latter is Luke's backports of security and stability fixes to
otherwise unmaintained old versions.



From kungfoobar at gmail.com  Sun Jun 17 22:46:47 2012
From: kungfoobar at gmail.com (Alberto Torres)
Date: Mon, 18 Jun 2012 00:46:47 +0200
Subject: [Bitcoin-development] Ultimate Blockchain Compression w/
 trust-free lite nodes
In-Reply-To: <20120617190511.GA26047@savin>
References: <4FDE2460.5080301@gmail.com> <20120617190511.GA26047@savin>
Message-ID: <CAE98tO2PcxKdz670ptHB=3Pc9JvV_+Wjdt1117M4SA2hANKH+w@mail.gmail.com>

Hi,

I did describe a very similar thing back in January (also illustrated,
and, if I'm not mistaken, more simple and efficient to recalculate),
and I wanted to do a prototype, but I have been very busy with other
projects since then.

https://en.bitcoin.it/wiki/User:DiThi/MTUT

I just saw Gavin left a comment in the talk page, I'm sorry I haven't
seen it earlier.

I think armory is the perfect client to implement such an idea. I sort
of waited it to be able to run in my laptop with 2 GB of RAM before
being sucked into other projects. I even lost track of its
development.

I hope this gets developed. I will be able to help after summer if
this is still not done.

DiThi

P.S: Sorry Peter, I've sent you the message privately by mistake.
Also, I don't quite understand your concern of "unbalancing" the tree.

2012/6/17 Peter Todd <pete at petertodd.org>:
> On Sun, Jun 17, 2012 at 02:39:28PM -0400, Alan Reiner wrote:
>> All,
>>
>> With the flurry of discussion about blockchain compression, I
>> thought it was time to put forward my final, most-advanced idea,
>> into a single, well-thought-out, *illustrated*, forum post.
>> Please check it out: https://bitcointalk.org/index.php?topic=88208.0
>>
>> This is a huge undertaking, but it has some pretty huge benefits.
>> And it's actually feasible because it can be implemented without
>> disrupting the main network. ?I'm sure there's lots of issues with
>> it, but I'm putting it out there to see how it might be improved and
>> actually executed.
>>
>> ----
>> *Summary:
>>
>> */Use a special tree data structure to organize all unspent-TxOuts
>> on the network, and use the root of this tree to communicate its
>> "signature" between nodes. ?The leaves of this tree actually
>> correspond to addresses/scripts, and the data at the leaf is
>> actually a root of the unspent-TxOut list for that address/script.
>> To maintain security of the tree signatures, it will be included in
>> the header of an alternate blockchain, which will be secured by
>> merged mining.
>>
>> This provides the same compression as the simpler unspent-TxOut
>> merkle tree, but also gives nodes a way to download just the
>> unspent-TxOut list for each address in their wallet, and verify that
>> list directly against the blockheaders. ?Therefore, even lightweight
>> nodes can get full address information, from any untrusted peer, and
>> with only a tiny amount of downloaded data (a few kB). /*
>
> How are you going to prevent people from delibrately unbalancing the
> tree with addresses with chosen hashes?
>
> One idea that comes to mind, which unfortunately would make for a
> pseudo-network rule, is to simply say that any *new* address whose hash
> happens to be deeper in the tree than, say, 10*log(n), indicating it was
> probably chosen to be unbalanced, gets discarded. The "new address" part
> of the rule would be required, or else you could use the rule to get
> other people's addresses discarded.
>
> Having said that, such a rule just means that anyone playing games will
> find they can't spend *their* money, and only with pruning clients.
> Unrelated people will not be effected. The coins can also always be
> spent with a non-pruning client to an acceptable address, which can
> later re-spend on a pruning client.
>
>
> It also comes to mind is that with the popularity of firstbits it may be
> a good idea to use a comparison function that works last bit first...
>
>
> It's merkles all the way down...
>
> --
> 'peter'[:-1]@petertodd.org
>
> ------------------------------------------------------------------------------
> Live Security Virtual Conference
> Exclusive live event will cover all the ways today's security and
> threat landscape has changed and how IT managers can respond. Discussions
> will include endpoint security, mobile security and the latest in malware
> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
> _______________________________________________
> Bitcoin-development mailing list
> Bitcoin-development at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/bitcoin-development
>



From grarpamp at gmail.com  Sun Jun 17 23:04:42 2012
From: grarpamp at gmail.com (grarpamp)
Date: Sun, 17 Jun 2012 19:04:42 -0400
Subject: [Bitcoin-development] 0.6.x - detachdb in wrong place
In-Reply-To: <CAAS2fgTHo0U+2U5vtbmTEiKB6rBHbfuRHsm-bcnRhSrs-2jZpw@mail.gmail.com>
References: <CAD2Ti2_Z-mzHu_VG7fq+sgQj7CfdZ_nKoa7Q6nDObwBSL6yXgQ@mail.gmail.com>
	<CAAS2fgQCc_-FgiXs0JABkAzZWNrbhhWYutvRgFLsnKYaEWDqwQ@mail.gmail.com>
	<CAD2Ti29+=L+ss7fwivy+gfPFuE10sQdy68TGhL-30ngWKTLdQA@mail.gmail.com>
	<CAAS2fgTHo0U+2U5vtbmTEiKB6rBHbfuRHsm-bcnRhSrs-2jZpw@mail.gmail.com>
Message-ID: <CAD2Ti28eC6A2+k8JfV3nXMLmye12K28rfCAorby87aXgV2ZGaQ@mail.gmail.com>

>> https://github.com/bitcoin/bitcoin
>> https://git.gitorious.org/bitcoin/bitcoind-stable
>
> The latter is Luke's backports of security and stability fixes to
> otherwise unmaintained old versions.

Ah ok, coming from cvs/svn, it's a bit different to find things.
There's something to be said for maintenance of pior branches.
Though I see some things I can use in github and my work would
be more useful there, so maybe I'll stwitch to that from gitorius/0.6.x.

Presumably the github/0.6.2 branch is safe for production?

What degree of caution about wallet eating should be
made for those using github/master?



From etotheipi at gmail.com  Sun Jun 17 23:17:23 2012
From: etotheipi at gmail.com (Alan Reiner)
Date: Sun, 17 Jun 2012 19:17:23 -0400
Subject: [Bitcoin-development] Ultimate Blockchain Compression w/
 trust-free lite nodes
In-Reply-To: <CAE98tO2PcxKdz670ptHB=3Pc9JvV_+Wjdt1117M4SA2hANKH+w@mail.gmail.com>
References: <4FDE2460.5080301@gmail.com> <20120617190511.GA26047@savin>
	<CAE98tO2PcxKdz670ptHB=3Pc9JvV_+Wjdt1117M4SA2hANKH+w@mail.gmail.com>
Message-ID: <4FDE6583.9020509@gmail.com>

Hi Alberto,

Your thread was part of the inspiration for the idea that I proposed.  
But as I read it more, I see that I originally misunderstood it 
(mistaking it for a simpler unspent-TxOut tree idea).  Even after 
reading it, I'm not entirely clear how your proposal would work, but I 
see that you proposed something very similar.  I just want to clarify 
that there are two, major orthogonal pieces to both proposals:

(1) The method for creating unspent-TxOut-tree roots/fingerprints for 
verification
(2) Using an alternate blockchain to maintain and distribute those 
fingerprints

There are multiple ways to do both of those.  You proposed a different 
tree structure (which I haven't entirely figured out, yet), and putting 
those "fingerprints" in the main chain header.

In my proposal, (2) is to avoid inducing a blockchain fork, or even 
changing the protocol at all.  By using a separate blockchain, it can be 
done non-disruptively, and could even be thrown out and re-worked if we 
were to find an issue with it later.  The availability of merged mining 
makes it possible to get [almost] the same security as changing the 
protocol, but without the disruption of hard-forking.  (I expect that if 
there's not too much computational overhead and the software is already 
written, most miners would sign on)

I'll read into your page a little more.  I don't want to take credit 
away from you, since you clearly had a comparable idea developed long 
before me :)

-Alan


On 06/17/2012 06:46 PM, Alberto Torres wrote:
> Hi,
>
> I did describe a very similar thing back in January (also illustrated,
> and, if I'm not mistaken, more simple and efficient to recalculate),
> and I wanted to do a prototype, but I have been very busy with other
> projects since then.
>
> https://en.bitcoin.it/wiki/User:DiThi/MTUT
>
> I just saw Gavin left a comment in the talk page, I'm sorry I haven't
> seen it earlier.
>
> I think armory is the perfect client to implement such an idea. I sort
> of waited it to be able to run in my laptop with 2 GB of RAM before
> being sucked into other projects. I even lost track of its
> development.
>
> I hope this gets developed. I will be able to help after summer if
> this is still not done.
>
> DiThi
>
> P.S: Sorry Peter, I've sent you the message privately by mistake.
> Also, I don't quite understand your concern of "unbalancing" the tree.
>
> 2012/6/17 Peter Todd<pete at petertodd.org>:
>> On Sun, Jun 17, 2012 at 02:39:28PM -0400, Alan Reiner wrote:
>>> All,
>>>
>>> With the flurry of discussion about blockchain compression, I
>>> thought it was time to put forward my final, most-advanced idea,
>>> into a single, well-thought-out, *illustrated*, forum post.
>>> Please check it out: https://bitcointalk.org/index.php?topic=88208.0
>>>
>>> This is a huge undertaking, but it has some pretty huge benefits.
>>> And it's actually feasible because it can be implemented without
>>> disrupting the main network.  I'm sure there's lots of issues with
>>> it, but I'm putting it out there to see how it might be improved and
>>> actually executed.
>>>
>>> ----
>>> *Summary:
>>>
>>> */Use a special tree data structure to organize all unspent-TxOuts
>>> on the network, and use the root of this tree to communicate its
>>> "signature" between nodes.  The leaves of this tree actually
>>> correspond to addresses/scripts, and the data at the leaf is
>>> actually a root of the unspent-TxOut list for that address/script.
>>> To maintain security of the tree signatures, it will be included in
>>> the header of an alternate blockchain, which will be secured by
>>> merged mining.
>>>
>>> This provides the same compression as the simpler unspent-TxOut
>>> merkle tree, but also gives nodes a way to download just the
>>> unspent-TxOut list for each address in their wallet, and verify that
>>> list directly against the blockheaders.  Therefore, even lightweight
>>> nodes can get full address information, from any untrusted peer, and
>>> with only a tiny amount of downloaded data (a few kB). /*
>> How are you going to prevent people from delibrately unbalancing the
>> tree with addresses with chosen hashes?
>>
>> One idea that comes to mind, which unfortunately would make for a
>> pseudo-network rule, is to simply say that any *new* address whose hash
>> happens to be deeper in the tree than, say, 10*log(n), indicating it was
>> probably chosen to be unbalanced, gets discarded. The "new address" part
>> of the rule would be required, or else you could use the rule to get
>> other people's addresses discarded.
>>
>> Having said that, such a rule just means that anyone playing games will
>> find they can't spend *their* money, and only with pruning clients.
>> Unrelated people will not be effected. The coins can also always be
>> spent with a non-pruning client to an acceptable address, which can
>> later re-spend on a pruning client.
>>
>>
>> It also comes to mind is that with the popularity of firstbits it may be
>> a good idea to use a comparison function that works last bit first...
>>
>>
>> It's merkles all the way down...
>>
>> --
>> 'peter'[:-1]@petertodd.org
>>
>> ------------------------------------------------------------------------------
>> Live Security Virtual Conference
>> Exclusive live event will cover all the ways today's security and
>> threat landscape has changed and how IT managers can respond. Discussions
>> will include endpoint security, mobile security and the latest in malware
>> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
>> _______________________________________________
>> Bitcoin-development mailing list
>> Bitcoin-development at lists.sourceforge.net
>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development
>>
> ------------------------------------------------------------------------------
> Live Security Virtual Conference
> Exclusive live event will cover all the ways today's security and
> threat landscape has changed and how IT managers can respond. Discussions
> will include endpoint security, mobile security and the latest in malware
> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
> _______________________________________________
> Bitcoin-development mailing list
> Bitcoin-development at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/bitcoin-development




From luke at dashjr.org  Mon Jun 18 00:02:41 2012
From: luke at dashjr.org (Luke-Jr)
Date: Mon, 18 Jun 2012 00:02:41 +0000
Subject: [Bitcoin-development] 0.6.x - detachdb in wrong place
In-Reply-To: <CAD2Ti28eC6A2+k8JfV3nXMLmye12K28rfCAorby87aXgV2ZGaQ@mail.gmail.com>
References: <CAD2Ti2_Z-mzHu_VG7fq+sgQj7CfdZ_nKoa7Q6nDObwBSL6yXgQ@mail.gmail.com>
	<CAAS2fgTHo0U+2U5vtbmTEiKB6rBHbfuRHsm-bcnRhSrs-2jZpw@mail.gmail.com>
	<CAD2Ti28eC6A2+k8JfV3nXMLmye12K28rfCAorby87aXgV2ZGaQ@mail.gmail.com>
Message-ID: <201206180002.43785.luke@dashjr.org>

On Sunday, June 17, 2012 11:04:42 PM grarpamp wrote:
> >> https://github.com/bitcoin/bitcoin
> >> https://git.gitorious.org/bitcoin/bitcoind-stable
> > 
> > The latter is Luke's backports of security and stability fixes to
> > otherwise unmaintained old versions.
> 
> Ah ok, coming from cvs/svn, it's a bit different to find things.
> There's something to be said for maintenance of pior branches.
> Though I see some things I can use in github and my work would
> be more useful there, so maybe I'll stwitch to that from gitorius/0.6.x.
> 
> Presumably the github/0.6.2 branch is safe for production?

No, that was a temporary branch of what became the stable 0.6.x branch.
GitHub/master is bleeding edge. For production, you usually want the stable 
branches/releases (which are on Gitorious).

The fix to -detachdb's position in -help was just merged to master, and should 
be backported sometime in the next few days.

Luke



From gmaxwell at gmail.com  Mon Jun 18 00:07:45 2012
From: gmaxwell at gmail.com (Gregory Maxwell)
Date: Sun, 17 Jun 2012 20:07:45 -0400
Subject: [Bitcoin-development] 0.6.x - detachdb in wrong place
In-Reply-To: <CAD2Ti28eC6A2+k8JfV3nXMLmye12K28rfCAorby87aXgV2ZGaQ@mail.gmail.com>
References: <CAD2Ti2_Z-mzHu_VG7fq+sgQj7CfdZ_nKoa7Q6nDObwBSL6yXgQ@mail.gmail.com>
	<CAAS2fgQCc_-FgiXs0JABkAzZWNrbhhWYutvRgFLsnKYaEWDqwQ@mail.gmail.com>
	<CAD2Ti29+=L+ss7fwivy+gfPFuE10sQdy68TGhL-30ngWKTLdQA@mail.gmail.com>
	<CAAS2fgTHo0U+2U5vtbmTEiKB6rBHbfuRHsm-bcnRhSrs-2jZpw@mail.gmail.com>
	<CAD2Ti28eC6A2+k8JfV3nXMLmye12K28rfCAorby87aXgV2ZGaQ@mail.gmail.com>
Message-ID: <CAAS2fgQkiB1b1sepCcZ_Fw3P4Dd0DQVb=oEFJdjos6YqDAKdJg@mail.gmail.com>

On Sun, Jun 17, 2012 at 7:04 PM, grarpamp <grarpamp at gmail.com> wrote:
> Presumably the github/0.6.2 branch is safe for production?

0.6.2 is very widely used, more so than the other acceptably updated backports.

> What degree of caution about wallet eating should be
> made for those using github/master?

I can't speak for anyone but myself:

I don't run master on wallets with large amounts of (non-testnet) coin
in them, except for a few times when I needed access to this feature
or that or just in a isolated capacity for testing.  In any use with
real wallets I'd be sure to have good backups that never touched the
new code.

We have at various times had bugs in master that would corrupt wallets
(though IIRC not too severely) and have bugs that would burn coin both
in mining and in transactions (though again, I think not too
severely).  My caution is not due to the risk being exceptionally
great but just because there is probably no remedy if things go wrong,
this caution is magnified by the fact that we don't currently have
enough testing activity on master.

Testnet exists so that people can test without fear of losing a lot of
funds and with the 0.7.0(git master) testnet reboot it should be more
usable than it has been.   It would be very helpful if anyone offering
bitcoin services would setup parallel toy versions of your sites on
testnet? it would bring more attention to your real services, it would
give you an opportunity to get more testing done of your real
services, it would show some more commitment to software quality, and
it would let you take a more active role in advancing bitcoin
development by doing a little testing yourself that you couldn't do on
your production systems.



From gmaxwell at gmail.com  Mon Jun 18 00:24:27 2012
From: gmaxwell at gmail.com (Gregory Maxwell)
Date: Sun, 17 Jun 2012 20:24:27 -0400
Subject: [Bitcoin-development] Block preview for faster relaying
Message-ID: <CAAS2fgR3SGeHOJ5CcnHhM-ETUA7=SwozfZH4yhYpnMFJ5Vm_XA@mail.gmail.com>

Right now we're seeing cases where block propagation is sometimes
taking minutes.
This doesn't cause much of a problem for general Bitcoin users but for
miners its problematic because it potentially increases the risk for
orphaning.

There are probably many contributing factors which can be improved
here but one of the most obvious is that nodes fully validate blocks
before relaying them. The validation is IO intensive and can currently
take a minute alone on sufficiently slow nodes with sufficiently large
blocks and larger blocks require more data to be transmitted.  Because
this slowness is proportional to the size of the block this risks
creating mismatched incentives where miners are better off not mining
(many) transactions in order to maximize their income.

The validation speed can and should be improved but there is at least
one short term improvement that can be made at the protocol level:
Make it possible to relay blocks to other nodes before fully
validating them.

This can be reasonable secure because basic validation (such as the
difficulty, previous block identity, and timestamps) can be done first
so an attacker would need to burn enormous amounts of computing power
just to make very modest trouble with it... and it's a change which
would be beneficial even after any other performance improvements were
made.

Luke has been working on a patch for this:

https://github.com/luke-jr/bitcoin/commit/0ce6f590dc2b9cbb46ceecd7320220f55d814bca

One aspect of it that I wanted to see more comments on was the use of
a new message for the preview-blocks instead of just announcing them
like normal. The reason for this is two-fold: To prevent existing full
nodes from blacklisting nodes sending a bad preview block due to the
existing misbehavior checks, otherwise an attacker could burn one
block to partition the network,  and also so that SPV nodes which
aren't able to fully validate the block themselves can opt-out or at
least know that the data is not yet validated by the peer.

I don't see any better way to address this but I thought other people
might have comments.



From mark at monetize.io  Mon Jun 18 01:27:39 2012
From: mark at monetize.io (Mark Friedenbach)
Date: Sun, 17 Jun 2012 18:27:39 -0700
Subject: [Bitcoin-development] Proposed new P2P command and response:
 getcmds, cmdlist
In-Reply-To: <1339950641.22050.YahooMailNeo@web121005.mail.ne1.yahoo.com>
References: <CA+8xBpdD31koaVBh1RuDZKH1sygr8z10K=bPz8DepqYOa8i6yg@mail.gmail.com>
	<1339810493.15660.YahooMailNeo@web121004.mail.ne1.yahoo.com>
	<CA+s+GJCKSrJv4L=4Nj4Hs+j2vfM-oWe5ayD_4NOUJMoXCkm3iA@mail.gmail.com>
	<201206160916.24485.andyparkins@gmail.com>
	<CA+s+GJA2-+HuRFfX3b=-4wv7u9iFCnfOMyDKwekxmipszt27Cw@mail.gmail.com>
	<CA+8xBpcvLsc+UyMT2LjrcuPf2Q+Rp8FQEZFKWddOxwyie0azkw@mail.gmail.com>
	<1339950641.22050.YahooMailNeo@web121005.mail.ne1.yahoo.com>
Message-ID: <CACh7GpHDnndEij5B24WVALKm1Ye+WXqVNx=2us1UhO5O4SBtYQ@mail.gmail.com>

Sorry for the duplication Amir, I meant to send this to everyone:

BitTorrent might be an example to look to here. It's a peer-to-peer network
that has undergone many significant protocol upgrades over the years while
maintaining compatibility. More recent clients have had the ability to
expose the capabilities of connected peers and modify behavior accordingly,
and overall it has worked very well.

Capability-based systems do work, and provide an excellent means of trying
out new algorithms, adding new features for upgraded clients, and when
necessary reverting protocol changes (by depreciating or removing
extensions).

The problem with OpenGL was and continues to be that the two superpowers of
that industry develop and maintain competing proposals for similar
functionality, which are thrust upon developers which must support both if
they want access to the latest and greatest features, until such time that
the ARB arbitrarily choses one to standardize upon (in the process creating
yet another extension of the form ARB_* that may be different and must be
explicitly supported by developers).

I think the BitTorrent example shows that a loosely organized, open-source
community *can* maintain a capability-based extension system without
falling into capability-hell.

Mark

On Sun, Jun 17, 2012 at 9:30 AM, Amir Taaki <zgenjix at yahoo.com> wrote:

> As the only person to have created and maintaining a full reimplementation
> of the Bitcoin protocol/standard, I do think Bitcoin is filled with
> arbitrary endianness and magic numbers. However it is a tiny and simple
> protocol.
>
> The big problem is not implementing the Bitcoin protocol, but the fact
> that once you have created a codebase, you want to perfect and fine-tune
> the design. During the meantime, the Bitcoin protocol is being changed.
> Change to the Bitcoin protocol is far more damaging to people that want to
> implement the protocol than any issues with the current protocol.
>
> That's not to say, I disagree with changes to the protocol. I think
> changes should be a lot more conservative and have a longer time frame than
> they do currently. Usually changes suddenly get added to the Satoshi client
> and I notice them in the commit log or announcements. Then it's like "oh I
> have to add this" and I spend a week working to implement the change
> without proper consideration or reflection which ends up with me having to
> compromise on design choices. That is to remain compatible with the
> protocol.
>
> However it is not my intent to slow down progress so I usually try to
> hedge against that kind of feeling towards conservatism.
>
>
>
> ----- Original Message -----
> From: Jeff Garzik <jgarzik at exmulti.com>
> To: Wladimir <laanwj at gmail.com>
> Cc: bitcoin-development at lists.sourceforge.net
> Sent: Sunday, June 17, 2012 5:19 PM
> Subject: Re: [Bitcoin-development] Proposed new P2P command and response:
> getcmds, cmdlist
>
> On Sat, Jun 16, 2012 at 4:42 AM, Wladimir <laanwj at gmail.com> wrote:
> > Which is a perfectly reasonable requirement. However, one could simply
> > standardize what a 'thin client' and what a 'thick client' does and
> offers
> > (at a certain version level), without having to explicitly enumerate
> > everything over the protocol.
> >
> > This also makes it easier to deprecate (lack of) certain features later
> on.
> > You can simply drop support for protocol versions before a certain number
> > (which has happened before). With the extension system this is much
> harder,
> > which likely means you keep certain workarounds forever.
> >
> > Letting the node know of each others capabilities at connection time
> helps
> > somewhat. It'd allow refusing clients that do not implement a certain
> > feature. Then again, to me it's unclear what this wins compared to
> > incremental protocol versions with clear requirements.
> >
> > I'm just afraid that the currently simple P2P protocol will turn into a
> zoo
> > of complicated (and potentially buggy/insecure) interactions.
>
> What is missing here is some perspective on the current situation.  It
> is -very- easy to make a protocol change and bump PROTOCOL_VERSION in
> the Satoshi client.
>
> But for anyone maintaining a non-Satoshi codebase, the P2P protocol is
> already filled with all sorts of magic numbers, arbitrarily versioned
> binary data structures..  already an unfriendly zoo of complicated and
> potentially buggy interactions.  There is scant, incomplete
> documentation on the wiki -- the Satoshi source code is really the
> only true reference.
>
> I see these problems personally, trying to keep ArtForz' half-a-node
> running on mainnet (distributed as 'blkmond' with pushpool).
>
> In an era of HTTP and JSON, NFS and iSCSI, bitcoin's P2P protocol is
> woefully backwards, fragile, limited and inflexible when it comes to
> parameter/extension exchange and negotiation.  Even iSCSI, that which
> is implemented on hard drive firmware, has the ability to exchange
> key=value  parameters between local and remote sides of the RPC
> connection.
>
> Calling the current P2P protocol "simple" belies all the
> implementation details you absolutely -must- get right, to run on
> mainnet today.  Satoshi client devs almost never see the fragility and
> complexity inherent in the current legacy codebase, built up over
> time.
>
> --
> Jeff Garzik
> exMULTI, Inc.
> jgarzik at exmulti.com
>
>
> ------------------------------------------------------------------------------
> Live Security Virtual Conference
> Exclusive live event will cover all the ways today's security and
> threat landscape has changed and how IT managers can respond. Discussions
> will include endpoint security, mobile security and the latest in malware
> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
> _______________________________________________
> Bitcoin-development mailing list
> Bitcoin-development at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/bitcoin-development
>
>
>
> ------------------------------------------------------------------------------
> Live Security Virtual Conference
> Exclusive live event will cover all the ways today's security and
> threat landscape has changed and how IT managers can respond. Discussions
> will include endpoint security, mobile security and the latest in malware
> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
> _______________________________________________
> Bitcoin-development mailing list
> Bitcoin-development at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/bitcoin-development
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120617/37f28afb/attachment.html>

From grarpamp at gmail.com  Mon Jun 18 03:27:52 2012
From: grarpamp at gmail.com (grarpamp)
Date: Sun, 17 Jun 2012 23:27:52 -0400
Subject: [Bitcoin-development] 0.6.x - detachdb in wrong place
In-Reply-To: <201206180002.43785.luke@dashjr.org>
References: <CAD2Ti2_Z-mzHu_VG7fq+sgQj7CfdZ_nKoa7Q6nDObwBSL6yXgQ@mail.gmail.com>
	<CAAS2fgTHo0U+2U5vtbmTEiKB6rBHbfuRHsm-bcnRhSrs-2jZpw@mail.gmail.com>
	<CAD2Ti28eC6A2+k8JfV3nXMLmye12K28rfCAorby87aXgV2ZGaQ@mail.gmail.com>
	<201206180002.43785.luke@dashjr.org>
Message-ID: <CAD2Ti28q2_c=AgUjapQQvLLoMcT4ay50q_PuYbwA3820747t1A@mail.gmail.com>

> be sure to have good backups that never touched the new code...
> We have at various times had bugs in master that would corrupt
> wallets

Sorry, that's to be expected, I shouldn't have asked.

> It would be very helpful if anyone offering bitcoin services would
> setup parallel toy versions of your sites on testnet...

Good point.

> we don't currently have enough testing activity on master.

I usually test compile / report current and stable of things I use.


So I get that github/master is the obvious top of things.
But in looking at where the tags are between repositories,
it's still not clear to me what the workflow is.

Example...

There are these release tarballs on sourceforge, which all have
tags in github, yet no tags in gitorious. There are no 'x' branches
on github, yet there is one release applicable branch on gitorious.

I guess I'd expect to see, that if as hinted by Luke that gitorious
has the stable trees, that there would be release tags there, laid
down at some comfy point in time on the 'x' stable branches there.
(The stable branches inheriting new code from master). But there
are no such tags.

And the releases/tags seem to magically appear from nowhere on
github :) Again, I'm trying to extricate myself from CVS here.


# sourceforge tarballs
0.6.0
0.6.1
0.6.2
0.6.2.2

# github branches
  remotes/origin/master        432d28d Merge pull request #1477 from
gmaxwell/master
  remotes/origin/0.6.2         40fd689 Bump version to 0.6.2.2 for
osx-special build
# github tags
v0.6.0
v0.6.1
v0.6.2
v0.6.2.1
v0.6.2.2

# gitorius branches
  remotes/origin/0.6.0.x d354f94 Merge branch '0.5.x' into 0.6.0.x
  remotes/origin/0.6.x   5e322a7 Merge branch '0.6.0.x' into 0.6.x
# gitorious tags
v0.6.0.7



From luke at dashjr.org  Mon Jun 18 03:57:11 2012
From: luke at dashjr.org (Luke-Jr)
Date: Mon, 18 Jun 2012 03:57:11 +0000
Subject: [Bitcoin-development] 0.6.x - detachdb in wrong place
In-Reply-To: <CAD2Ti28q2_c=AgUjapQQvLLoMcT4ay50q_PuYbwA3820747t1A@mail.gmail.com>
References: <CAD2Ti2_Z-mzHu_VG7fq+sgQj7CfdZ_nKoa7Q6nDObwBSL6yXgQ@mail.gmail.com>
	<201206180002.43785.luke@dashjr.org>
	<CAD2Ti28q2_c=AgUjapQQvLLoMcT4ay50q_PuYbwA3820747t1A@mail.gmail.com>
Message-ID: <201206180357.13430.luke@dashjr.org>

On Monday, June 18, 2012 3:27:52 AM grarpamp wrote:
> So I get that github/master is the obvious top of things.
> But in looking at where the tags are between repositories,
> it's still not clear to me what the workflow is.

Workflow is all new development takes place in master during release windows. 
Eventually, those windows close and master is cleaned up and bugfix'd for the 
next 0.x release. Occasionally, when 0.N.0 has some problem before the next 
release window opens, Gavin will use it to roll a 0.N.1 (and recently even a 
0.N.2 and 0.N.2.2). Once the release window for the next 0.N version opens,
I import the (last bugfix-only commit after the final 0.N.M release made in 
master) into the stable repository as the 0.N.x branch, and begin applying 
backports. When there's significant backports, I'll tag another 0.N.M from the 
branch and possibly release Windows binaries. Usually this happens around the 
same time as master becomes the next 0.N.0 release.

> Example...
> 
> There are these release tarballs on sourceforge, which all have
> tags in github, yet no tags in gitorious. There are no 'x' branches
> on github, yet there is one release applicable branch on gitorious.
> 
> I guess I'd expect to see, that if as hinted by Luke that gitorious
> has the stable trees, that there would be release tags there, laid
> down at some comfy point in time on the 'x' stable branches there.
> (The stable branches inheriting new code from master). But there
> are no such tags.

I guess I've been neglecting to update the stable repo with releases tagged in 
master. It should be fixed now.

Luke



From grarpamp at gmail.com  Mon Jun 18 08:09:56 2012
From: grarpamp at gmail.com (grarpamp)
Date: Mon, 18 Jun 2012 04:09:56 -0400
Subject: [Bitcoin-development] 0.6.x - detachdb in wrong place
In-Reply-To: <201206180357.13430.luke@dashjr.org>
References: <CAD2Ti2_Z-mzHu_VG7fq+sgQj7CfdZ_nKoa7Q6nDObwBSL6yXgQ@mail.gmail.com>
	<201206180002.43785.luke@dashjr.org>
	<CAD2Ti28q2_c=AgUjapQQvLLoMcT4ay50q_PuYbwA3820747t1A@mail.gmail.com>
	<201206180357.13430.luke@dashjr.org>
Message-ID: <CAD2Ti29_NcLyfEB3b=Xur=Q37SP6Cn34b5ZzOTmucLdni3of=w@mail.gmail.com>

> Workflow is ...

Thanks very much, I think that helps me/others. I did not realize
there were release windows in master and thought it more as the
typical full time dev slush. That also explains the presence of all
the release tags in github repo. And even, in a divergent way, the
presence of github/0.6.2 as path to gitorius/0.6.x. And I agree
with the (last bugfix after release) -> import/maintain model, it
would be similar in solo repo.

> I guess I've been neglecting to update the stable repo with
> releases tagged in master. It should be fixed now.

Yes, that has helped! Now git'ers can easily compare the release
tags to stable 'x' branches on gitorious. I don't know how to do
that across repos yet, save manuel diff of checkouts from each,
which would have been required prior to this update you made.

Also, these declarations of defunctness help sort out too.

# git branch -vv -a
"This stable branch is no longer maintained."


Ok, so for my works I will now track github/master (edge) and
gitorious/0.bigN(eg: 6).x (stable) against gitorious/bigTagRelease
(latest public). Thanks guys, and Luke :)

I hope other with similar questions find this thread. Apology for
subverting its subject somehows.



From pete at petertodd.org  Mon Jun 18 10:14:41 2012
From: pete at petertodd.org (Peter Todd)
Date: Mon, 18 Jun 2012 06:14:41 -0400
Subject: [Bitcoin-development] Ultimate Blockchain Compression w/
 trust-free lite nodes
In-Reply-To: <CAE98tO2PcxKdz670ptHB=3Pc9JvV_+Wjdt1117M4SA2hANKH+w@mail.gmail.com>
References: <4FDE2460.5080301@gmail.com> <20120617190511.GA26047@savin>
	<CAE98tO2PcxKdz670ptHB=3Pc9JvV_+Wjdt1117M4SA2hANKH+w@mail.gmail.com>
Message-ID: <20120618101441.GB11629@savin>

On Mon, Jun 18, 2012 at 12:46:47AM +0200, Alberto Torres wrote:
> Hi,
> 
> I did describe a very similar thing back in January (also illustrated,
> and, if I'm not mistaken, more simple and efficient to recalculate),
> and I wanted to do a prototype, but I have been very busy with other
> projects since then.
> 
> https://en.bitcoin.it/wiki/User:DiThi/MTUT
> 
> I just saw Gavin left a comment in the talk page, I'm sorry I haven't
> seen it earlier.
> 
> I think armory is the perfect client to implement such an idea. I sort
> of waited it to be able to run in my laptop with 2 GB of RAM before
> being sucked into other projects. I even lost track of its
> development.

I strongly disagree on that point. What you're proposing needs miner
support to work, and miners generally run either the satoshi client as a
daemon, or some other custom code. Implementing the idea in armory
doesn't give those miners a nice upgrade path.

That said, *using* the hash tree is something that can be implemented in
any client, but a lot of the code will be shared between calculating it
and using it anyway, so again implementing in the satoshi client makes
sense.

> I hope this gets developed. I will be able to help after summer if
> this is still not done.
> 
> DiThi
> 
> P.S: Sorry Peter, I've sent you the message privately by mistake.
> Also, I don't quite understand your concern of "unbalancing" the tree.

Lets suppose we're trying to make a tree consisting of real numbers:

    /\
   /  \
   *   \
  / \   \
 /   \   \
 *   *   *
/ \ / \ / \
1 2 3 4 5 6

If the numbers are evenly distributed, as will happen with hashes of
arbitrary data, any number will be at most log(n) steps away from the
head of the tree.

Suppose though some malicious actor adds the following numbers to that
tree: 3.001 3.002 3.003

    /\
   /  \
   *   \
  / \   \
 /   \   \
 *   *   *
/ \ / \ / \
1 2 * 4 5 6
   / \
   |  \
   *   *
  / \ / \
  0 1 2 3 <- (3.000 to 3.003)

Ooops, the tree suddenly became a lot higher, with an associated
decrease in retrieval performance and an increase in memory usage.

Of course the exact details depend on what rules there are for
constructing the tree, but essentially the attacker can either force the
a big increase in the depth of the tree, or a large number of vertexes
to be re-organizationed to create the tree, or both.

Now, to be exact, since the key of each vertex is a transaction hash,
this malicious actor will need to brute chosen prefix hash collisions,
but this is bitcoin: the whole system is about efficiently brute forcing
chosen prefix hash collisions. Besides, you would only need something
like k*n collisions to product an n increase in tree depth, with some
small k.


My solution was to simply state that vertexes that happened to cause the
tree to be unbalanced would be discarded, and set the depth of inbalance
such that this would be extremely unlikely to happen by accident. I'd
rather see someone come up with something better though.


Another naive option would be to hash each vertex key (the transaction
hash) with a nonce known only to the creator of that particular merkle
tree, but then the whole tree has to be recreatred from scratch each
time, which is worse than the problem... Interestingly in a
*non-distributed* system this idea is actually quite feasible feasible,
as the nonce could be kept secret.

-- 
'peter'[:-1]@petertodd.org
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 490 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120618/36d8e486/attachment.sig>

From luke at dashjr.org  Mon Jun 18 13:25:21 2012
From: luke at dashjr.org (Luke-Jr)
Date: Mon, 18 Jun 2012 13:25:21 +0000
Subject: [Bitcoin-development] 0.6.x - detachdb in wrong place
In-Reply-To: <CAD2Ti29_NcLyfEB3b=Xur=Q37SP6Cn34b5ZzOTmucLdni3of=w@mail.gmail.com>
References: <CAD2Ti2_Z-mzHu_VG7fq+sgQj7CfdZ_nKoa7Q6nDObwBSL6yXgQ@mail.gmail.com>
	<201206180357.13430.luke@dashjr.org>
	<CAD2Ti29_NcLyfEB3b=Xur=Q37SP6Cn34b5ZzOTmucLdni3of=w@mail.gmail.com>
Message-ID: <201206181325.23041.luke@dashjr.org>

On Monday, June 18, 2012 8:09:56 AM grarpamp wrote:
> > I guess I've been neglecting to update the stable repo with
> > releases tagged in master. It should be fixed now.
> 
> Yes, that has helped! Now git'ers can easily compare the release
> tags to stable 'x' branches on gitorious. I don't know how to do
> that across repos yet, save manuel diff of checkouts from each,
> which would have been required prior to this update you made.

This is the work model I use:
  git clone git://github.com/bitcoin/bitcoin.git
  cd bitcoin
  git remote add stable git://gitorious.org/+bitcoin-stable-developers/bitcoin/bitcoind-stable.git
  git remote add personal git at github.com:YOURNAME/bitcoin.git

With this, you can use "git fetch --all" to update your copy of the remote
branches, and access them as "origin/master", "stable/0.6.x", etc; and push
personal branches using "git push personal <branch>"

Luke



From mike at plan99.net  Mon Jun 18 18:41:43 2012
From: mike at plan99.net (Mike Hearn)
Date: Mon, 18 Jun 2012 20:41:43 +0200
Subject: [Bitcoin-development] LevelDB benchmarking
Message-ID: <CANEZrP2xnsOHyH+a1g6qSNSx_g+TW-yvL0Due7PVr421U6kRLw@mail.gmail.com>

I switched the transaction database to use the Google LevelDB library,
which is a refactored out part of BigTable.

Here are my results. All tests are done on this hard disk:

  http://wdc.custhelp.com/app/answers/detail/a_id/1409/~/specifications-for-the-500-gb-caviar-blue-and-caviar-se16-serial-ata-drive

which has an average 8.9msec seek time. It is a 6 core Ubuntu machine.

I used -loadblock on a chain with with 185127 blocks in it, so it has
lots of SatoshiDice traffic.

8.9 ms (average) seek time

>> Regular BDB as we have today:
real	96m6.836s
user	49m55.220s
sys	2m29.850s

Throughput usually 4-5MB/sec according to iotop, pauses of 8-10
seconds for ?Flushing wallet ...?. 611mb of blkindex.dat

>> BDB without sig checking
Throughput, 12-17mb/sec
real	42m51.508s
user	11m52.700s
sys	2m36.590s

Disabling EC verification halves running time.

>> LevelDB no customized options
(I ran the wrong time command here, hence the different format)
3184.73user 181.02system 51:20.81elapsed 109%CPU (0avgtext+0avgdata
1220096maxresident)k
1104inputs+125851776outputs (293569major+37436202minor)pagefaults 0swaps

So, 50 minutes. Throughput often in range of 20-30mb/sec. 397MB of data files.

>> LevelDB w/ 10 bit per key bloom filter
real	50m52.740s
user	53m38.870s
sys	3m4.990s

424mb of data files

No change.

>> LevelDB w/ 10 bit per key bloom filter + 30mb cache (up from 8mb by default)
real	50m53.054s
user	53m26.910s
sys	3m10.720s

No change. The reason is, signature checking is the bottleneck not IO.

>> LevelDB w/10 bit per key bloom filter, 30mb cache, no sigs
real	12m58.998s
user	11m42.330s
sys	2m5.670s

12 minutes vs 42 minutes for BDB on the same benchmark.


Conclusion: LevelDB is a clear win, taking a sync in the absence of
network delays from 95 minutes to 50, at which point signature
checking becomes the bottleneck. It is nearly 4x as fast when
signature checks are not done (ie, when receiving a block containing
only mempool transactions you already verified).



From mike at plan99.net  Tue Jun 19 09:05:20 2012
From: mike at plan99.net (Mike Hearn)
Date: Tue, 19 Jun 2012 11:05:20 +0200
Subject: [Bitcoin-development] LevelDB benchmarking
In-Reply-To: <CAAS2fgTNqUeYy+oEFyQWrfs4Xyb=3NXutvCmLusknF-18JmFQg@mail.gmail.com>
References: <CANEZrP2xnsOHyH+a1g6qSNSx_g+TW-yvL0Due7PVr421U6kRLw@mail.gmail.com>
	<CAAS2fgTNqUeYy+oEFyQWrfs4Xyb=3NXutvCmLusknF-18JmFQg@mail.gmail.com>
Message-ID: <CANEZrP2q9a_0rFh+oo6iUFF1goWs0OJO1xPvxC9zqNA-6VnFAQ@mail.gmail.com>

+list

On Mon, Jun 18, 2012 at 9:07 PM, Gregory Maxwell <gmaxwell at gmail.com> wrote:
> In addition to the ECDSA caching, ?ECDSA can can easily be run on
> multiple cores for basically a linear speedup.. so even with the
> checking in place once ECDSA was using multiple threads we'd be back
> to the DB being the bottleneck for this kind of case.

Maybe ... looking again I think I may be wrong about being IO bound in
the last benchmark. The core running the main Bitcoin thread is still
pegged and the LevelDB background thread is only spending around 20%
of its time in iowait. An oprofile shows most of the time being spent
inside a std::map.

OK, to make progress on this work I need a few decisions (Gavin?)

1) Shall we do it?

2) LevelDB is obscure, new and has a very minimalist build system. It
supports "make" but not "make install", for example, and is unlikely
to be packaged. It's also not very large. I suggest we just check the
source into the main Bitcoin tree and link it statically rather than
complicate the build.

3) As the DB format would change and a slow migration period
necessary, any other tweaks to db format we could make at the same
time? Right now the key/values are the same as before, though using
satoshi serialization for everything is a bit odd.

We'd need UI for migration as well.



From pieter.wuille at gmail.com  Tue Jun 19 11:38:59 2012
From: pieter.wuille at gmail.com (Pieter Wuille)
Date: Tue, 19 Jun 2012 13:38:59 +0200
Subject: [Bitcoin-development] LevelDB benchmarking
In-Reply-To: <CANEZrP2q9a_0rFh+oo6iUFF1goWs0OJO1xPvxC9zqNA-6VnFAQ@mail.gmail.com>
References: <CANEZrP2xnsOHyH+a1g6qSNSx_g+TW-yvL0Due7PVr421U6kRLw@mail.gmail.com>
	<CAAS2fgTNqUeYy+oEFyQWrfs4Xyb=3NXutvCmLusknF-18JmFQg@mail.gmail.com>
	<CANEZrP2q9a_0rFh+oo6iUFF1goWs0OJO1xPvxC9zqNA-6VnFAQ@mail.gmail.com>
Message-ID: <20120619113857.GA29542@vps7135.xlshosting.net>

On Tue, Jun 19, 2012 at 11:05:20AM +0200, Mike Hearn wrote:
> OK, to make progress on this work I need a few decisions (Gavin?)
> 
> 1) Shall we do it?

I'm all for moving away from BDB. It's a very good system for what it is
intended for, but that is not how we use it. The fact that it is tied to
a database environment (but people want to copy the files themselves
between systems), that is provides consistency in case of failures (but
because we remove old log files, we still see very frequent corrupted
systems), the fact that its environments are sometimes not even forward-
compatible, ...

Assuming LevelDB is an improvement in these areas as well as resulting in
a speed improvement, I like it.

> 2) LevelDB is obscure, new and has a very minimalist build system. It
> supports "make" but not "make install", for example, and is unlikely
> to be packaged. It's also not very large. I suggest we just check the
> source into the main Bitcoin tree and link it statically rather than
> complicate the build.

How portable is LevelDB? How well tested is it? What compatibility
guarantees exist between versions of the system?

I don't mind including the source code; it doesn't seem particularly
large, and the 2-clause BSD license shouldn't be a problem.

> 3) As the DB format would change and a slow migration period
> necessary, any other tweaks to db format we could make at the same
> time? Right now the key/values are the same as before, though using
> satoshi serialization for everything is a bit odd.
> 
> We'd need UI for migration as well.

Jeff was working on splitting the database into several files earlier, and
I'm working on the database/validation logic as well. Each of these will
require a rebuild of the databases anyway. If possible, we should try to
get them in a single release, so people only need to rebuild once. 

PS: can we see the code?

-- 
Pieter



From gavinandresen at gmail.com  Tue Jun 19 15:05:21 2012
From: gavinandresen at gmail.com (Gavin Andresen)
Date: Tue, 19 Jun 2012 11:05:21 -0400
Subject: [Bitcoin-development] LevelDB benchmarking
In-Reply-To: <CANEZrP2q9a_0rFh+oo6iUFF1goWs0OJO1xPvxC9zqNA-6VnFAQ@mail.gmail.com>
References: <CANEZrP2xnsOHyH+a1g6qSNSx_g+TW-yvL0Due7PVr421U6kRLw@mail.gmail.com>
	<CAAS2fgTNqUeYy+oEFyQWrfs4Xyb=3NXutvCmLusknF-18JmFQg@mail.gmail.com>
	<CANEZrP2q9a_0rFh+oo6iUFF1goWs0OJO1xPvxC9zqNA-6VnFAQ@mail.gmail.com>
Message-ID: <CABsx9T3pQFqL0xsvRfnixYEATO61qMCCDdLmtqZkbVLW0Vxytg@mail.gmail.com>

> OK, to make progress on this work I need a few decisions (Gavin?)
>
> 1) Shall we do it?

What problem does it solve?

If the problem it will solve is "it will only take 4 hours to download
the entire blockchain next year instead of taking 16 hours" then no, I
don't think we should do it, both 4 and 16 hours to get fully up and
running is too long.

If the problem it will solve is the "too easy to get a DB_RUNRECOVERY
error" because bdb is fragile when it comes to its environment... then
LevelDB looks very interesting.

If the problem is bdb is creaky and old and has obscure semantics and
a hard-to-work-with API, then yes, lets switch (I'm easily seduced by
a pretty API and blazing fast performance).

> 2) LevelDB is obscure, new and has a very minimalist build system. It
> supports "make" but not "make install", for example, and is unlikely
> to be packaged. It's also not very large. I suggest we just check the
> source into the main Bitcoin tree and link it statically rather than
> complicate the build.

As long as it compiles and runs on mac/windows/linux that doesn't
really worry me. I just tried it, and it compiled quickly with no
complaints on my mac.

Lack of infrastructure because it is new does worry me; for example,
could I rework bitcointools to read the LevelDB blockchain?  (are
there python bindings for LevelDB?)

> 3) As the DB format would change and a slow migration period
> necessary, any other tweaks to db format we could make at the same
> time? Right now the key/values are the same as before, though using
> satoshi serialization for everything is a bit odd.

Satoshi rolled his own network serialization because he didn't trust
existing serialization solutions to be 100% secure against remote
exploits. Then it made sense to use the same solution for disk
serialization; I don't see a compelling reason to switch to some other
serialization scheme.

Modifying the database schema during migration to better support
applications like InstaWallet (tens of thousands of separate wallets)
or something like Pieter's ultra-pruning makes sense.

-- 
--
Gavin Andresen



From mike at plan99.net  Tue Jun 19 16:06:30 2012
From: mike at plan99.net (Mike Hearn)
Date: Tue, 19 Jun 2012 18:06:30 +0200
Subject: [Bitcoin-development] LevelDB benchmarking
In-Reply-To: <CABsx9T3pQFqL0xsvRfnixYEATO61qMCCDdLmtqZkbVLW0Vxytg@mail.gmail.com>
References: <CANEZrP2xnsOHyH+a1g6qSNSx_g+TW-yvL0Due7PVr421U6kRLw@mail.gmail.com>
	<CAAS2fgTNqUeYy+oEFyQWrfs4Xyb=3NXutvCmLusknF-18JmFQg@mail.gmail.com>
	<CANEZrP2q9a_0rFh+oo6iUFF1goWs0OJO1xPvxC9zqNA-6VnFAQ@mail.gmail.com>
	<CABsx9T3pQFqL0xsvRfnixYEATO61qMCCDdLmtqZkbVLW0Vxytg@mail.gmail.com>
Message-ID: <CANEZrP08NrCJM2gxNitXrLjuY6AusNULvkcheN_0MbgFQV_QXw@mail.gmail.com>

> What problem does it solve?

Primarily that block verification and therefore propagation is too
slow because it's very CPU and IO intensive. The CPU work can be
multi-threaded. The IO work, not as much. As Bitcoin grows we need to
scale the nodes. Eventually there may be multi-machine nodes, but for
now we can buy more time by making the existing nodes faster.

I don't see this as a replacement for moving users to SPV clients.
Obviously, otherwise I would not be writing one ;)

> If the problem it will solve is the "too easy to get a DB_RUNRECOVERY
> error" because bdb is fragile when it comes to its environment... then
> LevelDB looks very interesting.

I have no experience with how robust LevelDB is. It has an API call to
try and repair the database and I know from experience that BigTable
is pretty solid. But that doesn't mean LevelDB is.

> If the problem is bdb is creaky and old and has obscure semantics and
> a hard-to-work-with API, then yes, lets switch (I'm easily seduced by
> a pretty API and blazing fast performance).

The code is a lot simpler for sure.

> As long as it compiles and runs on mac/windows/linux that doesn't
> really worry me.

It was refactored out of BigTable and made standalone for usage in
Chrome. Therefore it's as portable as Chrome is. Mac/Windows/Linux
should all work. Solaris, I believe, may need 64 bit binaries to avoid
low FD limits.

> Lack of infrastructure because it is new does worry me; for example,
> could I rework bitcointools to read the LevelDB blockchain? ?(are
> there python bindings for LevelDB?)

Yes: http://code.google.com/p/py-leveldb/

First look at the code is here, but it's not ready for a pull req yet,
and I'll force push over it a few times to get it into shape. So don't
branch:

https://github.com/mikehearn/bitcoin/commit/2b601dd4a0093f834084241735d84d84e484f183

It has misc other changes I made whilst profiling, isn't well
commented enough, etc.



From amiller at cs.ucf.edu  Tue Jun 19 16:46:52 2012
From: amiller at cs.ucf.edu (Andrew Miller)
Date: Tue, 19 Jun 2012 12:46:52 -0400
Subject: [Bitcoin-development] Ultimate Blockchain Compression w/
	trust-free lite node
Message-ID: <CAF7tpEyEWCbcB+jSpWOMyeZUBjQ=FbVEC8kLt3j2Yzv3YJOgiA@mail.gmail.com>

> Peter Todd wrote:
> My solution was to simply state that vertexes that happened to cause the
> tree to be unbalanced would be discarded, and set the depth of inbalance
> such that this would be extremely unlikely to happen by accident. I'd
> rather see someone come up with something better though.

Here is a simpler solution. (most of this message repeats the content
of my reply to the forum)

Suppose we were talking about a binary search tree, rather than a
Merkle tree. It's important to balance a binary search tree, so that
the worst-case maximum length from the root to a leaf is bounded by
O(log N). AVL trees were the original algorithm to do this, Red-Black
trees are also popular, and there are many similar methods. All
involve storing some form of 'balancing metadata' at each node. In a
RedBlack tree, this is a single bit (red or black). Every operation on
these trees, including search, inserting, deleting, and rebalancing,
requires a worst-case effort of O(log N).

Any (acyclic) recursive data structure can be Merkle-ized, simply by
adding a hash of the child node alongside each link/pointer. This way,
you can verify the data for each node very naturally, as you traverse
the structure.

In fact, as long as a lite-client knows the O(1) root hash, the rest
of the storage burden can be delegated to an untrusted helper server.
Suppose a lite-client wants to insert and rebalance its tree. This
requires accessing at most O(log N) nodes. The client can request only
the data relevant to these nodes, and it knows the hash for each chunk
of data in advance of accessing it. After computing the updated root
hash, the client can even discard the data it processed.

This technique has been well discussed in the academic literature,
e.g. [1,2], although since I am not aware of any existing
implementation, I made my own, intended as an explanatory aid:
https://github.com/amiller/redblackmerkle/blob/master/redblack.py


[1] Certificate Revocation and Update
    Naor and Nissim. 1998
    http://static.usenix.org/publications/library/proceedings/sec98/full_papers/nissim/nissim.pdf

[2] A General Model for Authenticated Data Structures
    Martel, Nuckolls, Devanbu, Michael Gertz, Kwong, Stubblebine. 2004
    http://truthsayer.cs.ucdavis.edu/algorithmica.pdf

--
Andrew Miller



From etotheipi at gmail.com  Tue Jun 19 17:33:31 2012
From: etotheipi at gmail.com (Alan Reiner)
Date: Tue, 19 Jun 2012 13:33:31 -0400
Subject: [Bitcoin-development] Ultimate Blockchain Compression w/
 trust-free lite node
In-Reply-To: <CAF7tpEyEWCbcB+jSpWOMyeZUBjQ=FbVEC8kLt3j2Yzv3YJOgiA@mail.gmail.com>
References: <CAF7tpEyEWCbcB+jSpWOMyeZUBjQ=FbVEC8kLt3j2Yzv3YJOgiA@mail.gmail.com>
Message-ID: <4FE0B7EB.6000100@gmail.com>


I hope that someone else here would chime in on the issue raised in the 
thread, about using a tree-structure that has multiple valid 
configurations for the same set of unspent-TxOuts.  If you use any 
binary tree, you must replay the entire history of insertions and 
deletions in the correct order to get the tree structure and correct 
root.  Along those lines, using something like a red-black tree, while 
theoretically well-known, could be subject to implementation errors.  
One implementation of a red-black tree may do the rebalancing 
differently, and still work for it's intended purpose in the majority of 
applications where it doesn't matter.  One app developer updates their 
RB tree code which updated the RB-tree optimizations/rebalancing, and 
now a significant portion of the network can't agree on the correct 
root.  Not only would that be disruptive, it would be a disaster to 
track down.

If we were to use a raw trie structure, then we'd have all the above 
issues solved:  a trie has the same configuration no matter how elements 
are inserted or deleted, and accesses to elements in the tree are 
constant time -- O(1).  There is no such thing as an unbalanced trie.  
But overall space-efficiency is an issue.

A PATRICIA tree/trie would be ideal, in my mind, as it also has a 
completely deterministic structure, and is an order-of-magnitude more 
space-efficient.  Insert, delete and query times are still O(1).    
However, it is not a trivial implementation.  I have occasionally looked 
for implementations, but not found any that were satisfactory.

So, I don't have a good all-around solution, within my own stated 
constraints. But perhaps I'm being too demanding of this solution.

-Alan



On 06/19/2012 12:46 PM, Andrew Miller wrote:
>> Peter Todd wrote:
>> My solution was to simply state that vertexes that happened to cause the
>> tree to be unbalanced would be discarded, and set the depth of inbalance
>> such that this would be extremely unlikely to happen by accident. I'd
>> rather see someone come up with something better though.
> Here is a simpler solution. (most of this message repeats the content
> of my reply to the forum)
>
> Suppose we were talking about a binary search tree, rather than a
> Merkle tree. It's important to balance a binary search tree, so that
> the worst-case maximum length from the root to a leaf is bounded by
> O(log N). AVL trees were the original algorithm to do this, Red-Black
> trees are also popular, and there are many similar methods. All
> involve storing some form of 'balancing metadata' at each node. In a
> RedBlack tree, this is a single bit (red or black). Every operation on
> these trees, including search, inserting, deleting, and rebalancing,
> requires a worst-case effort of O(log N).
>
> Any (acyclic) recursive data structure can be Merkle-ized, simply by
> adding a hash of the child node alongside each link/pointer. This way,
> you can verify the data for each node very naturally, as you traverse
> the structure.
>
> In fact, as long as a lite-client knows the O(1) root hash, the rest
> of the storage burden can be delegated to an untrusted helper server.
> Suppose a lite-client wants to insert and rebalance its tree. This
> requires accessing at most O(log N) nodes. The client can request only
> the data relevant to these nodes, and it knows the hash for each chunk
> of data in advance of accessing it. After computing the updated root
> hash, the client can even discard the data it processed.
>
> This technique has been well discussed in the academic literature,
> e.g. [1,2], although since I am not aware of any existing
> implementation, I made my own, intended as an explanatory aid:
> https://github.com/amiller/redblackmerkle/blob/master/redblack.py
>
>
> [1] Certificate Revocation and Update
>      Naor and Nissim. 1998
>      http://static.usenix.org/publications/library/proceedings/sec98/full_papers/nissim/nissim.pdf
>
> [2] A General Model for Authenticated Data Structures
>      Martel, Nuckolls, Devanbu, Michael Gertz, Kwong, Stubblebine. 2004
>      http://truthsayer.cs.ucdavis.edu/algorithmica.pdf
>
> --
> Andrew Miller
>
> ------------------------------------------------------------------------------
> Live Security Virtual Conference
> Exclusive live event will cover all the ways today's security and
> threat landscape has changed and how IT managers can respond. Discussions
> will include endpoint security, mobile security and the latest in malware
> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
> _______________________________________________
> Bitcoin-development mailing list
> Bitcoin-development at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/bitcoin-development




From gmaxwell at gmail.com  Tue Jun 19 17:59:04 2012
From: gmaxwell at gmail.com (Gregory Maxwell)
Date: Tue, 19 Jun 2012 13:59:04 -0400
Subject: [Bitcoin-development] Ultimate Blockchain Compression w/
 trust-free lite node
In-Reply-To: <4FE0B7EB.6000100@gmail.com>
References: <CAF7tpEyEWCbcB+jSpWOMyeZUBjQ=FbVEC8kLt3j2Yzv3YJOgiA@mail.gmail.com>
	<4FE0B7EB.6000100@gmail.com>
Message-ID: <CAAS2fgRFFtdsuaS+ZoFLYcnUxLBufA8aMV=_sHca5ZOi-3viTw@mail.gmail.com>

On Tue, Jun 19, 2012 at 1:33 PM, Alan Reiner <etotheipi at gmail.com> wrote:
>?One app developer updates their
> RB tree code which updated the RB-tree optimizations/rebalancing, and
> now a significant portion of the network can't agree on the correct
> root. ?Not only would that be disruptive, it would be a disaster to
> track down.

This is why good comprehensive tests and a well specified algorithim
are important. The tree update algorithm would be normative in that
scheme. Worrying that implementers might get it wrong would be like
worrying that they'd get SHA256 wrong.

> A PATRICIA tree/trie would be ideal, in my mind, as it also has a
> completely deterministic structure, and is an order-of-magnitude more

Provable libJudy trees. Oh boy.



From etotheipi at gmail.com  Tue Jun 19 18:12:19 2012
From: etotheipi at gmail.com (Alan Reiner)
Date: Tue, 19 Jun 2012 14:12:19 -0400
Subject: [Bitcoin-development] Ultimate Blockchain Compression w/
 trust-free lite node
In-Reply-To: <CAAS2fgRFFtdsuaS+ZoFLYcnUxLBufA8aMV=_sHca5ZOi-3viTw@mail.gmail.com>
References: <CAF7tpEyEWCbcB+jSpWOMyeZUBjQ=FbVEC8kLt3j2Yzv3YJOgiA@mail.gmail.com>
	<4FE0B7EB.6000100@gmail.com>
	<CAAS2fgRFFtdsuaS+ZoFLYcnUxLBufA8aMV=_sHca5ZOi-3viTw@mail.gmail.com>
Message-ID: <4FE0C103.3070304@gmail.com>

On 06/19/2012 01:59 PM, Gregory Maxwell wrote:
> On Tue, Jun 19, 2012 at 1:33 PM, Alan Reiner<etotheipi at gmail.com>  wrote:
>>   One app developer updates their
>> RB tree code which updated the RB-tree optimizations/rebalancing, and
>> now a significant portion of the network can't agree on the correct
>> root.  Not only would that be disruptive, it would be a disaster to
>> track down.
> This is why good comprehensive tests and a well specified algorithim
> are important. The tree update algorithm would be normative in that
> scheme. Worrying that implementers might get it wrong would be like
> worrying that they'd get SHA256 wrong.

The point is not that they get it *wrong*, it's that the implement it 
*differently*.  Given a set of 100 TxOuts, there's a seemingly-infinite 
number of ways to construct a binary tree.  Put them in in a different 
order, and you get a different tree. *They're all correct and legal* in 
terms of satisfying expectations of insert, delete and query runtime -- 
but they will produce different root hashes.   And the differences in 
underlying structure are completely transparent to the calling code.

I'm extremely uncomfortable with the idea the you can have all the nodes 
in the tree, but have to replay X years of blockchain history just to 
get the same tree configuration as someone else.  However, a trie 
configuration is history-independent -- given an unspent-TxOut list, 
there's only one way to construct that tree.  That's an important 
property to me.

I can't tell if you're joking about Judy structures: I've never heard of 
them.  But I'll look into it anyway...

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120619/5426be15/attachment.html>

From mark at monetize.io  Tue Jun 19 18:18:07 2012
From: mark at monetize.io (Mark Friedenbach)
Date: Tue, 19 Jun 2012 11:18:07 -0700
Subject: [Bitcoin-development] Ultimate Blockchain Compression w/
 trust-free lite node
In-Reply-To: <4FE0B7EB.6000100@gmail.com>
References: <CAF7tpEyEWCbcB+jSpWOMyeZUBjQ=FbVEC8kLt3j2Yzv3YJOgiA@mail.gmail.com>
	<4FE0B7EB.6000100@gmail.com>
Message-ID: <CACh7GpEehHFEJGRTtijgM7UAa2jeEWRKrQo5dym8F_YgXAEhFA@mail.gmail.com>

On Tue, Jun 19, 2012 at 10:33 AM, Alan Reiner <etotheipi at gmail.com> wrote:

> I hope that someone else here would chime in on the issue raised in the
> thread, about using a tree-structure that has multiple valid
> configurations for the same set of unspent-TxOuts.  If you use any
> binary tree, you must replay the entire history of insertions and
> deletions in the correct order to get the tree structure and correct
> root.  Along those lines, using something like a red-black tree, while
> theoretically well-known, could be subject to implementation errors.
> One implementation of a red-black tree may do the rebalancing
> differently, and still work for it's intended purpose in the majority of
> applications where it doesn't matter.  One app developer updates their
> RB tree code which updated the RB-tree optimizations/rebalancing, and
> now a significant portion of the network can't agree on the correct
> root.  Not only would that be disruptive, it would be a disaster to
> track down.
>

Then use a 2-3-4 tree (aka self-balancing B-tree of order 4), which is a
generalization of RB-trees that doesn't allow for implementation choices in
balancing (assuming ordered insertion and deletion).

As gmaxwell points out, this is an trivially fixable 'problem'. Choose a
standard, mandate it, and write test cases.

If we were to use a raw trie structure, then we'd have all the above
> issues solved:  a trie has the same configuration no matter how elements
> are inserted or deleted, and accesses to elements in the tree are
> constant time -- O(1).  There is no such thing as an unbalanced trie.
> But overall space-efficiency is an issue.
>
> A PATRICIA tree/trie would be ideal, in my mind, as it also has a
> completely deterministic structure, and is an order-of-magnitude more
> space-efficient.  Insert, delete and query times are still O(1).
> However, it is not a trivial implementation.  I have occasionally looked
> for implementations, but not found any that were satisfactory.
>

No, a trie of any sort is dependent upon distribution of input data for
balancing. As Peter Todd points out, a malicious actor could construct
transaction or address hashes in such a way as to grow some segment of the
trie in an unbalanced fashion. It's not much of an attack, but in principle
exploitable under particular timing-sensitive circumstances.

Self-balancing search trees (KVL, RB, 2-3-4, whatever) don't suffer from
this problem.

Mark
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120619/8d09b88b/attachment.html>

From amiller at cs.ucf.edu  Tue Jun 19 18:29:45 2012
From: amiller at cs.ucf.edu (Andrew Miller)
Date: Tue, 19 Jun 2012 14:29:45 -0400
Subject: [Bitcoin-development] Ultimate Blockchain Compression w/
	trust-free lite node
Message-ID: <CAF7tpEzi80MT5Ud46_BgvnLKXrcWhkNZNOiuY7NMnL-z0C0GqA@mail.gmail.com>

Alan Reiner wrote:
> A PATRICIA tree/trie would be ideal, in my mind, as it also has a
> completely deterministic structure, and is an order-of-magnitude more
> space-efficient. ?Insert, delete and query times are still O(1).
> However, it is not a trivial implementation. ?I have occasionally looked
> for implementations, but not found any that were satisfactory.

PATRICIA Tries (aka Radix trees) have worst-case O(k), where k is the
number of bits in the key. Notice that since we would storing k-bit
hashes, the number of elements must be less than 2^k, or else by
birthday paradox we would have a hash collision! So O(log N) <= O(k).

You're right, though, that such a trie would have the property that
any two trees containing the same data (leaves) will be identical. I
can't think of any reason why this is useful, although I am hoping we
can figure out what is triggering your intuition to desire this! I am
indeed assuming that the tree will be incrementally constructed
according to the canonical (blockchain) ordering of transactions, and
that the balancing rules are agreed on as part of the protocol.

-- 
Andrew Miller



From etotheipi at gmail.com  Tue Jun 19 18:30:16 2012
From: etotheipi at gmail.com (Alan Reiner)
Date: Tue, 19 Jun 2012 14:30:16 -0400
Subject: [Bitcoin-development] Ultimate Blockchain Compression w/
 trust-free lite node
In-Reply-To: <CACh7GpEehHFEJGRTtijgM7UAa2jeEWRKrQo5dym8F_YgXAEhFA@mail.gmail.com>
References: <CAF7tpEyEWCbcB+jSpWOMyeZUBjQ=FbVEC8kLt3j2Yzv3YJOgiA@mail.gmail.com>
	<4FE0B7EB.6000100@gmail.com>
	<CACh7GpEehHFEJGRTtijgM7UAa2jeEWRKrQo5dym8F_YgXAEhFA@mail.gmail.com>
Message-ID: <4FE0C538.3090001@gmail.com>

On 06/19/2012 02:18 PM, Mark Friedenbach wrote:
> On Tue, Jun 19, 2012 at 10:33 AM, Alan Reiner <etotheipi at gmail.com 
> <mailto:etotheipi at gmail.com>> wrote:
>
>     If we were to use a raw trie structure, then we'd have all the above
>     issues solved:  a trie has the same configuration no matter how
>     elements
>     are inserted or deleted, and accesses to elements in the tree are
>     constant time -- O(1).  There is no such thing as an unbalanced trie.
>     But overall space-efficiency is an issue.
>
>     A PATRICIA tree/trie would be ideal, in my mind, as it also has a
>     completely deterministic structure, and is an order-of-magnitude more
>     space-efficient.  Insert, delete and query times are still O(1).
>     However, it is not a trivial implementation.  I have occasionally
>     looked
>     for implementations, but not found any that were satisfactory.
>
>
> No, a trie of any sort is dependent upon distribution of input data 
> for balancing. As Peter Todd points out, a malicious actor could 
> construct transaction or address hashes in such a way as to grow some 
> segment of the trie in an unbalanced fashion. It's not much of an 
> attack, but in principle exploitable under particular timing-sensitive 
> circumstances.
>
> Self-balancing search trees (KVL, RB, 2-3-4, whatever) don't suffer 
> from this problem.
>
> Mark

I was using "unbalanced" to refer to "query time" (and also 
insert/delete time).  If your trie nodes branch based on the next byte 
of your key hash, then the max depth of your trie is 32.  Period.  No 
one can do anything to ever make you do more than 32 hops to 
find/insert/delete your data.   And if you're using a raw trie, you'll 
always use /exactly/ 32 hops regardless of the distribution of the 
underlying data.  Hence, the trie structure is deterministic 
(history-independent) and cannot become unbalanced in terms of access time.

My first concern was that a malicious actor could linearize parts of the 
tree and cause access requests to take much longer than log(N) time.  
With the trie, that's not only impossible, you're actually accessing in 
O(1) time.

However, you are right that disk space can be affected by a malicious 
actor.  The more branching he can induce, the more branch nodes that are 
created to support branches with only one leaf.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120619/2548286d/attachment.html>

From bitcoin-list at bluematt.me  Tue Jun 19 19:09:58 2012
From: bitcoin-list at bluematt.me (Matt Corallo)
Date: Tue, 19 Jun 2012 21:09:58 +0200
Subject: [Bitcoin-development] New P2P commands for diagnostics,
 SPV clients
In-Reply-To: <CANEZrP0hTRbE9+VEa3eCzJkbHqa3u8tpdw7eDLBQQR6DBf2adw@mail.gmail.com>
References: <CA+8xBpecVQcTTbPxUm_3_GWC99dEd4=-VFWb+QT6jUy4rg8U4w@mail.gmail.com>
	<CANEZrP0kNZDByHpK2=UjP+ag0X1KmqHxnJdm=e_pWMitP4QvvA@mail.gmail.com>
	<1339766346.31489.49.camel@bmthinkpad>
	<CANEZrP3jj2ymQPH50g2PvzZhRzTnUnCLUjvBYj8ndBCJsnGJ-w@mail.gmail.com>
	<1339771184.31489.53.camel@bmthinkpad>
	<CANEZrP0hTRbE9+VEa3eCzJkbHqa3u8tpdw7eDLBQQR6DBf2adw@mail.gmail.com>
Message-ID: <1340132998.6065.7.camel@bmthinkpad>

On Sat, 2012-06-16 at 10:27 +0200, Mike Hearn wrote:
> > I'd much rather have an overloaded node respond with 50% fp rate filters
> > as an option if there aren't many full nodes available than simply
> > disconnect SPV clients.
> 
> I don't think the bloom filter settings have any impact on server-side
> load ... a node still has to check every transaction against the
> filter regardless of how that filter is configured, which means the
> same amount of disk io and processing.
> 
> How can you reduce load on a peer by negotiating different filter settings?
Agreed, I was largely giving a reason why one may want to negotiate the
filter settings in response to your question as to why it was done.  As
long as there are sane limits (you cant make a 1GB filter by specifying
0% fp and some crazy number of entires), filter negotiation largely isnt
worth it (also prevents any floats from appearing in the p2p protocol,
though in either case it shouldn't be able to cause issues).

Matt




From moon at justmoon.de  Tue Jun 19 19:22:15 2012
From: moon at justmoon.de (Stefan Thomas)
Date: Tue, 19 Jun 2012 21:22:15 +0200
Subject: [Bitcoin-development] LevelDB benchmarking
In-Reply-To: <CANEZrP08NrCJM2gxNitXrLjuY6AusNULvkcheN_0MbgFQV_QXw@mail.gmail.com>
References: <CANEZrP2xnsOHyH+a1g6qSNSx_g+TW-yvL0Due7PVr421U6kRLw@mail.gmail.com>
	<CAAS2fgTNqUeYy+oEFyQWrfs4Xyb=3NXutvCmLusknF-18JmFQg@mail.gmail.com>
	<CANEZrP2q9a_0rFh+oo6iUFF1goWs0OJO1xPvxC9zqNA-6VnFAQ@mail.gmail.com>
	<CABsx9T3pQFqL0xsvRfnixYEATO61qMCCDdLmtqZkbVLW0Vxytg@mail.gmail.com>
	<CANEZrP08NrCJM2gxNitXrLjuY6AusNULvkcheN_0MbgFQV_QXw@mail.gmail.com>
Message-ID: <4FE0D167.7030506@justmoon.de>

Here are my 2 cents after using LevelDB as the default backend for
BitcoinJS for about a year.

LevelDB was written to power IndexedDB in Chrome which is a JavaScript
API. That means that LevelDB doesn't really give you a lot of options,
because they assume that on the C++ layer you don't know any more than
they do, because the actual application is on the JavaScript layer. For
example whereas BDB supports hashtables, b-trees, queues, etc., LevelDB
uses one database type, LSM trees which is an ordered data structure
that is pretty good at everything.

Another gotcha was the number of file descriptors, LevelDB defaults to
1000 per DB. We originally used multiple DBs, one for each of the
indices, but it was easy enough to combine everything into one table,
thereby solving the fd issue. (Lowering the file descriptor limit also
works of course, but if you lower it too much, LevelDB will start to
spend a lot of time opening and closing files, so I believe combining
your tables into one is the better option.)

Overall, LevelDB is a fantastic solution for desktop software that is
faced with multiple use cases that aren't known at compile time. It
isn't really designed for something like Bitcoin which doesn't need
ordered access, has relatively predictable characteristics and - at
least some of the time - runs on servers.

That said, it does seem to work well for the Bitcoin use case anyway.
Thanks to the LSM trees, It's very quick at doing bulk inserts and we
don't seem to need any of the bells and whistles that BDB offers. So I
can't think of a reason not to switch, just make sure you all understand
the deal, LevelDB unlike Tokyo/Kyoto Cabinet is *not* intended as a
competitor or replacement for BDB, it's something quite different.



On 6/19/2012 6:06 PM, Mike Hearn wrote:
>> What problem does it solve?
> Primarily that block verification and therefore propagation is too
> slow because it's very CPU and IO intensive. The CPU work can be
> multi-threaded. The IO work, not as much. As Bitcoin grows we need to
> scale the nodes. Eventually there may be multi-machine nodes, but for
> now we can buy more time by making the existing nodes faster.
>
> I don't see this as a replacement for moving users to SPV clients.
> Obviously, otherwise I would not be writing one ;)
>
>> If the problem it will solve is the "too easy to get a DB_RUNRECOVERY
>> error" because bdb is fragile when it comes to its environment... then
>> LevelDB looks very interesting.
> I have no experience with how robust LevelDB is. It has an API call to
> try and repair the database and I know from experience that BigTable
> is pretty solid. But that doesn't mean LevelDB is.
>
>> If the problem is bdb is creaky and old and has obscure semantics and
>> a hard-to-work-with API, then yes, lets switch (I'm easily seduced by
>> a pretty API and blazing fast performance).
> The code is a lot simpler for sure.
>
>> As long as it compiles and runs on mac/windows/linux that doesn't
>> really worry me.
> It was refactored out of BigTable and made standalone for usage in
> Chrome. Therefore it's as portable as Chrome is. Mac/Windows/Linux
> should all work. Solaris, I believe, may need 64 bit binaries to avoid
> low FD limits.
>
>> Lack of infrastructure because it is new does worry me; for example,
>> could I rework bitcointools to read the LevelDB blockchain?  (are
>> there python bindings for LevelDB?)
> Yes: http://code.google.com/p/py-leveldb/
>
> First look at the code is here, but it's not ready for a pull req yet,
> and I'll force push over it a few times to get it into shape. So don't
> branch:
>
> https://github.com/mikehearn/bitcoin/commit/2b601dd4a0093f834084241735d84d84e484f183
>
> It has misc other changes I made whilst profiling, isn't well
> commented enough, etc.
>
> ------------------------------------------------------------------------------
> Live Security Virtual Conference
> Exclusive live event will cover all the ways today's security and 
> threat landscape has changed and how IT managers can respond. Discussions 
> will include endpoint security, mobile security and the latest in malware 
> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
> _______________________________________________
> Bitcoin-development mailing list
> Bitcoin-development at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/bitcoin-development





From grarpamp at gmail.com  Wed Jun 20 09:06:53 2012
From: grarpamp at gmail.com (grarpamp)
Date: Wed, 20 Jun 2012 05:06:53 -0400
Subject: [Bitcoin-development] 0.6.x - detachdb in wrong place
In-Reply-To: <201206181325.23041.luke@dashjr.org>
References: <CAD2Ti2_Z-mzHu_VG7fq+sgQj7CfdZ_nKoa7Q6nDObwBSL6yXgQ@mail.gmail.com>
	<201206180357.13430.luke@dashjr.org>
	<CAD2Ti29_NcLyfEB3b=Xur=Q37SP6Cn34b5ZzOTmucLdni3of=w@mail.gmail.com>
	<201206181325.23041.luke@dashjr.org>
Message-ID: <CAD2Ti2_v3LpYMCoH+ATDCPE6KFhTXWic4ZdatTWB9L_1p5qL6g@mail.gmail.com>

> This is the work model I use:

Will try all these things out this weekend. Thanks.



From mike at plan99.net  Wed Jun 20 09:44:48 2012
From: mike at plan99.net (Mike Hearn)
Date: Wed, 20 Jun 2012 11:44:48 +0200
Subject: [Bitcoin-development] LevelDB benchmarking
In-Reply-To: <4FE0D167.7030506@justmoon.de>
References: <CANEZrP2xnsOHyH+a1g6qSNSx_g+TW-yvL0Due7PVr421U6kRLw@mail.gmail.com>
	<CAAS2fgTNqUeYy+oEFyQWrfs4Xyb=3NXutvCmLusknF-18JmFQg@mail.gmail.com>
	<CANEZrP2q9a_0rFh+oo6iUFF1goWs0OJO1xPvxC9zqNA-6VnFAQ@mail.gmail.com>
	<CABsx9T3pQFqL0xsvRfnixYEATO61qMCCDdLmtqZkbVLW0Vxytg@mail.gmail.com>
	<CANEZrP08NrCJM2gxNitXrLjuY6AusNULvkcheN_0MbgFQV_QXw@mail.gmail.com>
	<4FE0D167.7030506@justmoon.de>
Message-ID: <CANEZrP3WHA7P+t2Jk+w1kBMk0QuqN+gBXeNMoAnj_rK=nor-qg@mail.gmail.com>

Thanks, I didn't realize BitcoinJS used LevelDB already.

Just one minor thing - LevelDB was definitely designed for servers, as
it comes from BigTable. It happens to be used in Chrome today, and
that was the motivation for open sourcing it, but that's not where the
design came from.

If anything it's going to get less and less optimal for desktops and
laptops over time because they're moving towards SSDs, where the
minimal-seeks design of LevelDB doesn't necessarily help. Servers are
moving too of course but I anticipate most Bitcoin nodes on servers to
be HDD based for the forseeable future.

Also, Satoshis code does use ordered access/iteration in at least one
place, where it looks up the "owner transactions" of a tx. I'm not
totally sure what that code is used for, but it's there. Whether it's
actually the best way to solve the problem is another question :-)



From mike at plan99.net  Wed Jun 20 09:53:32 2012
From: mike at plan99.net (Mike Hearn)
Date: Wed, 20 Jun 2012 11:53:32 +0200
Subject: [Bitcoin-development] LevelDB benchmarking
In-Reply-To: <CANEZrP3WHA7P+t2Jk+w1kBMk0QuqN+gBXeNMoAnj_rK=nor-qg@mail.gmail.com>
References: <CANEZrP2xnsOHyH+a1g6qSNSx_g+TW-yvL0Due7PVr421U6kRLw@mail.gmail.com>
	<CAAS2fgTNqUeYy+oEFyQWrfs4Xyb=3NXutvCmLusknF-18JmFQg@mail.gmail.com>
	<CANEZrP2q9a_0rFh+oo6iUFF1goWs0OJO1xPvxC9zqNA-6VnFAQ@mail.gmail.com>
	<CABsx9T3pQFqL0xsvRfnixYEATO61qMCCDdLmtqZkbVLW0Vxytg@mail.gmail.com>
	<CANEZrP08NrCJM2gxNitXrLjuY6AusNULvkcheN_0MbgFQV_QXw@mail.gmail.com>
	<4FE0D167.7030506@justmoon.de>
	<CANEZrP3WHA7P+t2Jk+w1kBMk0QuqN+gBXeNMoAnj_rK=nor-qg@mail.gmail.com>
Message-ID: <CANEZrP2ztFOxVrUdewL-pM4Xy=kkU3WBUqdB9ixk05YGwFz6WQ@mail.gmail.com>

There's an interesting post here about block propagation times:

https://bitcointalk.org/index.php?topic=88302.msg975343#msg975343

Looks like the regular network is reliably 0-60 seconds behind p2pool
in propagating new blocks.

So optimizing IO load (and after that, threading tx verification)
seems like an important win. Lukes preview functionality would also be
useful.



From pieter.wuille at gmail.com  Wed Jun 20 11:37:46 2012
From: pieter.wuille at gmail.com (Pieter Wuille)
Date: Wed, 20 Jun 2012 13:37:46 +0200
Subject: [Bitcoin-development] LevelDB benchmarking
In-Reply-To: <CANEZrP3WHA7P+t2Jk+w1kBMk0QuqN+gBXeNMoAnj_rK=nor-qg@mail.gmail.com>
References: <CANEZrP2xnsOHyH+a1g6qSNSx_g+TW-yvL0Due7PVr421U6kRLw@mail.gmail.com>
	<CAAS2fgTNqUeYy+oEFyQWrfs4Xyb=3NXutvCmLusknF-18JmFQg@mail.gmail.com>
	<CANEZrP2q9a_0rFh+oo6iUFF1goWs0OJO1xPvxC9zqNA-6VnFAQ@mail.gmail.com>
	<CABsx9T3pQFqL0xsvRfnixYEATO61qMCCDdLmtqZkbVLW0Vxytg@mail.gmail.com>
	<CANEZrP08NrCJM2gxNitXrLjuY6AusNULvkcheN_0MbgFQV_QXw@mail.gmail.com>
	<4FE0D167.7030506@justmoon.de>
	<CANEZrP3WHA7P+t2Jk+w1kBMk0QuqN+gBXeNMoAnj_rK=nor-qg@mail.gmail.com>
Message-ID: <20120620113744.GA14695@vps7135.xlshosting.net>

On Wed, Jun 20, 2012 at 11:44:48AM +0200, Mike Hearn wrote:
> Also, Satoshis code does use ordered access/iteration in at least one
> place, where it looks up the "owner transactions" of a tx. I'm not
> totally sure what that code is used for, but it's there. Whether it's
> actually the best way to solve the problem is another question :-)

Two days ago on #bitcoin-dev:
21:01:19< sipa> what was CTxDB::ReadOwnerTxes ever used for?
21:01:31< sipa> maybe it predates the wallet logic

(read: it's not used anywhere in the code, and apparently wasn't ever, even in 0.1.5)

-- 
Pieter




From mike at plan99.net  Wed Jun 20 12:41:30 2012
From: mike at plan99.net (Mike Hearn)
Date: Wed, 20 Jun 2012 14:41:30 +0200
Subject: [Bitcoin-development] LevelDB benchmarking
In-Reply-To: <20120620113744.GA14695@vps7135.xlshosting.net>
References: <CANEZrP2xnsOHyH+a1g6qSNSx_g+TW-yvL0Due7PVr421U6kRLw@mail.gmail.com>
	<CAAS2fgTNqUeYy+oEFyQWrfs4Xyb=3NXutvCmLusknF-18JmFQg@mail.gmail.com>
	<CANEZrP2q9a_0rFh+oo6iUFF1goWs0OJO1xPvxC9zqNA-6VnFAQ@mail.gmail.com>
	<CABsx9T3pQFqL0xsvRfnixYEATO61qMCCDdLmtqZkbVLW0Vxytg@mail.gmail.com>
	<CANEZrP08NrCJM2gxNitXrLjuY6AusNULvkcheN_0MbgFQV_QXw@mail.gmail.com>
	<4FE0D167.7030506@justmoon.de>
	<CANEZrP3WHA7P+t2Jk+w1kBMk0QuqN+gBXeNMoAnj_rK=nor-qg@mail.gmail.com>
	<20120620113744.GA14695@vps7135.xlshosting.net>
Message-ID: <CANEZrP3=0cJLjyUZ8FqmWXBRoiP-bzPzm14O=unmbDQ_=hHwVA@mail.gmail.com>

> Two days ago on #bitcoin-dev:
> 21:01:19< sipa> what was CTxDB::ReadOwnerTxes ever used for?
> 21:01:31< sipa> maybe it predates the wallet logic
>
> (read: it's not used anywhere in the code, and apparently wasn't ever, even in 0.1.5)

Great, in that case Stefan is right and I'll delete that code when I
next work on the patch.



From mike at coinlab.com  Thu Jun 21 21:42:58 2012
From: mike at coinlab.com (Mike Koss)
Date: Thu, 21 Jun 2012 14:42:58 -0700
Subject: [Bitcoin-development] Ultimate Blockchain Compression w/
 trust-free lite node
In-Reply-To: <4FE0C538.3090001@gmail.com>
References: <CAF7tpEyEWCbcB+jSpWOMyeZUBjQ=FbVEC8kLt3j2Yzv3YJOgiA@mail.gmail.com>
	<4FE0B7EB.6000100@gmail.com>
	<CACh7GpEehHFEJGRTtijgM7UAa2jeEWRKrQo5dym8F_YgXAEhFA@mail.gmail.com>
	<4FE0C538.3090001@gmail.com>
Message-ID: <CAErK2CgH1k2oosn2a7HyvDxvasw1pHh6jPVWG0JUORMVHettOQ@mail.gmail.com>

Are we just talking about pruning the spent transactions from an old block?
 We already have a data structure that allows us to replace any un-needed
transaction by just it's hash - and possibly a whole sub-tree if we get
lucky in that the un-needed transaction all fall within a common node of
the merkle tree.

If a lite client only cares to retain a single transaction in a block (the
most common case) - it will only need O(log2(T)) merkle hashes plus the
transaction it cares about.

Does it really make sense to adopt a more complex data-structure than the
merkle tree for inclusing in the bticoin protocol?  And we're not talking
about blocks with millions of transactions in them - I don't understand the
relevance of Order statistics for random access to a transaction given its
block.

On Tue, Jun 19, 2012 at 11:30 AM, Alan Reiner <etotheipi at gmail.com> wrote:

>  On 06/19/2012 02:18 PM, Mark Friedenbach wrote:
>
> On Tue, Jun 19, 2012 at 10:33 AM, Alan Reiner <etotheipi at gmail.com> wrote:
>
>  If we were to use a raw trie structure, then we'd have all the above
>> issues solved:  a trie has the same configuration no matter how elements
>> are inserted or deleted, and accesses to elements in the tree are
>> constant time -- O(1).  There is no such thing as an unbalanced trie.
>> But overall space-efficiency is an issue.
>>
>> A PATRICIA tree/trie would be ideal, in my mind, as it also has a
>> completely deterministic structure, and is an order-of-magnitude more
>> space-efficient.  Insert, delete and query times are still O(1).
>> However, it is not a trivial implementation.  I have occasionally looked
>> for implementations, but not found any that were satisfactory.
>>
>
>  No, a trie of any sort is dependent upon distribution of input data for
> balancing. As Peter Todd points out, a malicious actor could construct
> transaction or address hashes in such a way as to grow some segment of the
> trie in an unbalanced fashion. It's not much of an attack, but in principle
> exploitable under particular timing-sensitive circumstances.
>
>  Self-balancing search trees (KVL, RB, 2-3-4, whatever) don't suffer from
> this problem.
>
>  Mark
>
>
> I was using "unbalanced" to refer to "query time" (and also insert/delete
> time).  If your trie nodes branch based on the next byte of your key hash,
> then the max depth of your trie is 32.  Period.  No one can do anything to
> ever make you do more than 32 hops to find/insert/delete your data.   And
> if you're using a raw trie, you'll always use *exactly* 32 hops
> regardless of the distribution of the underlying data.  Hence, the trie
> structure is deterministic (history-independent) and cannot become
> unbalanced in terms of access time.
>
> My first concern was that a malicious actor could linearize parts of the
> tree and cause access requests to take much longer than log(N) time.  With
> the trie, that's not only impossible, you're actually accessing in O(1)
> time.
>
> However, you are right that disk space can be affected by a malicious
> actor.  The more branching he can induce, the more branch nodes that are
> created to support branches with only one leaf.
>
>
>
>
> ------------------------------------------------------------------------------
> Live Security Virtual Conference
> Exclusive live event will cover all the ways today's security and
> threat landscape has changed and how IT managers can respond. Discussions
> will include endpoint security, mobile security and the latest in malware
> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
> _______________________________________________
> Bitcoin-development mailing list
> Bitcoin-development at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/bitcoin-development
>
>


-- 
Mike Koss
CTO, CoinLab
(425) 246-7701 (m)

A Bitcoin Primer <http://coinlab.com/a-bitcoin-primer.pdf> - What you need
to know about Bitcoins.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120621/84612184/attachment.html>

From gmaxwell at gmail.com  Thu Jun 21 22:02:27 2012
From: gmaxwell at gmail.com (Gregory Maxwell)
Date: Thu, 21 Jun 2012 18:02:27 -0400
Subject: [Bitcoin-development] Ultimate Blockchain Compression w/
 trust-free lite node
In-Reply-To: <CAErK2CgH1k2oosn2a7HyvDxvasw1pHh6jPVWG0JUORMVHettOQ@mail.gmail.com>
References: <CAF7tpEyEWCbcB+jSpWOMyeZUBjQ=FbVEC8kLt3j2Yzv3YJOgiA@mail.gmail.com>
	<4FE0B7EB.6000100@gmail.com>
	<CACh7GpEehHFEJGRTtijgM7UAa2jeEWRKrQo5dym8F_YgXAEhFA@mail.gmail.com>
	<4FE0C538.3090001@gmail.com>
	<CAErK2CgH1k2oosn2a7HyvDxvasw1pHh6jPVWG0JUORMVHettOQ@mail.gmail.com>
Message-ID: <CAAS2fgTBH4bo5UebOcjHgJL+mf_020SobR=WME_haufnmv-j2g@mail.gmail.com>

On Thu, Jun 21, 2012 at 5:42 PM, Mike Koss <mike at coinlab.com> wrote:
> Are we just talking about pruning the spent transactions from an old block?

No.

We're talking about commitments to the state of _unspent_ transactions
which would allow ~memoryless nodes to engage in full validation
without having to trust anything with the help of some untrusted
non-memoryless peers.  Also, talking about being able to securely
initialize new pruned nodes (not memoryless but reduced memory)
without exposing them to the old history of the chain. In both cases
this is possible without substantially degrading the full node
security model (rule violations prior to where they begin are only
undetectable with a conspiracy of the entire network).

But it requires a new data structure for managing these trees of
unspent transactions in a secure, scalable, and DOS resistant manner.
Fortunately there are lots of possibilities here.

> Does it really make sense to adopt a more complex data-structure than the merkle tree for inclusing in the bticoin protocol?

Yes. Though this is obviously not an ultra short term thing.



From zgenjix at yahoo.com  Thu Jun 21 23:03:58 2012
From: zgenjix at yahoo.com (Amir Taaki)
Date: Thu, 21 Jun 2012 16:03:58 -0700 (PDT)
Subject: [Bitcoin-development] Berlin Bitcoin Hackathon
Message-ID: <1340319838.17167.YahooMailNeo@web121003.mail.ne1.yahoo.com>

This is happening in Berlin if anyone is around: http://bitcoin-hackathon.com/

I am happy to host if space is needed.




From moon at justmoon.de  Fri Jun 22 06:39:24 2012
From: moon at justmoon.de (Stefan Thomas)
Date: Fri, 22 Jun 2012 08:39:24 +0200
Subject: [Bitcoin-development] Berlin Bitcoin Hackathon
In-Reply-To: <1340319838.17167.YahooMailNeo@web121003.mail.ne1.yahoo.com>
References: <1340319838.17167.YahooMailNeo@web121003.mail.ne1.yahoo.com>
Message-ID: <4FE4131C.6070605@justmoon.de>

Flights booked. Mike Hearn and I will be there. :)

On 6/22/2012 1:03 AM, Amir Taaki wrote:
> This is happening in Berlin if anyone is around: http://bitcoin-hackathon.com/
>
> I am happy to host if space is needed.
>
>
> ------------------------------------------------------------------------------
> Live Security Virtual Conference
> Exclusive live event will cover all the ways today's security and 
> threat landscape has changed and how IT managers can respond. Discussions 
> will include endpoint security, mobile security and the latest in malware 
> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
> _______________________________________________
> Bitcoin-development mailing list
> Bitcoin-development at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/bitcoin-development
>





From grarpamp at gmail.com  Fri Jun 22 11:01:19 2012
From: grarpamp at gmail.com (grarpamp)
Date: Fri, 22 Jun 2012 07:01:19 -0400
Subject: [Bitcoin-development] Wallet related bug?
Message-ID: <CAD2Ti2_JL0HoNwkJJhzJ2CehPA0Ai0X7+CgF_n9-+H=vCU1F9A@mail.gmail.com>

I think there may be an ideal order of ops bug around rescan,
wallet upgrades/import and last block markers.

I dropped an old wallet in a current blockchain.

First ran - in rescan mode.
It said old walletver.
Then rescanned whole chain.
AddToWallet some blockhash, blocks out of range, invalid/nonwallet txid's,
which were already in there as legit ones in the old logs.

Second run in plain mode.
New wallet ver logged.
Rescanned  the last 20k blocks or so,
which might have been the marker last time the old wallet was used.

Third and later runs... duplicates the second.

Never did say 'upgrading wallet' as it sometimes does.

Running detach=1 always.

Why scan the last 20k every time? Shouldn't have to if
whole chain was scanned. And certainly no more than
once if not.

Also...
Dumping the run params (bitcoin.conf, cmdline) to the log would be good.

And not automatically truncate the log when big but just append or roll it.



From grarpamp at gmail.com  Sat Jun 23 00:10:57 2012
From: grarpamp at gmail.com (grarpamp)
Date: Fri, 22 Jun 2012 20:10:57 -0400
Subject: [Bitcoin-development] Wallet related bug?
Message-ID: <CAD2Ti29zRc38K1DGfe=NC2PMZwCQs55LV1pkN4wuLntrgdchiw@mail.gmail.com>

I had previously commented on this.
My references to wallet ver were really to nFileVersion.
And I've since been able to make that, and the
real walletversion become current.

However it is still doing this every invocation...
 Rescanning last 14xxx blocks (from block 170xxx)...
Which seems unneeded more than 1x. I cannot yet explain.

So I expect to avoid all by send the balance from old
wallets to new wallet soon instead.

The old wallets were ver 10500.



From mike at plan99.net  Sun Jun 24 12:45:53 2012
From: mike at plan99.net (Mike Hearn)
Date: Sun, 24 Jun 2012 14:45:53 +0200
Subject: [Bitcoin-development] Enforcing inflation rules for SPV clients
Message-ID: <CANEZrP0nagdAXyMEY5yxyXBjdo78YC16mjUG9=b0AMe4qOS=fA@mail.gmail.com>

I've been having a discussion with d'aniel from the forums?about how
to handle the possibility of a majority-miner conspiracy to raise
inflation, if most economic actors use SPV clients.

Because of how blocks are formatted you cannot check the coinbase of a
transaction without knowing the fees in the block, and the fees can
only be calculated if you have all the input transactions for every
transaction in that block. Because the attack scenario is an attempted
takeover of the economy by miners, attempting to put hints into the
blocks won't work - we have to assume the hardest chain is in fact
wrong according to the rules signed up to by the Bitcoin user.

The most obvious goal for a cartel of miners is to change the
inflation formula, either for purely selfish reasons (they want more
money than can be obtained by fees) or due to coercion by
governments/central banks who still subscribe to the "inflation is
good" idea.

Whilst "good" nodes (still on the old ruleset) won't relay blocks that
violate the rules no matter how hard they are, in a situation where an
SPV client DOES hear about the bad best chain, it would switch to it
automatically. And who knows how the network might look in future -
perhaps most nodes would end up run by miners, or other entities that
upgrade to the new ruleset for other reasons.

d'aniel made a good proposal - having good nodes broadcast
announcements when they detect a rule that breaks the rules, along
with a proof that it did so. Checking the proof might be very
expensive, but it would only have to be done for split points,
limiting the potential for DoS. If a node announces that it has a
weaker chain and that the split point is a rule-breaker, the SPV
client would download the headers for the side chain to verify the
split, then download all the transactions in the split block along
with all their inputs, and the merkle branches linking the inputs to
the associated block headers. In this way the fee can be calculated,
the inflation formula applied and the coinbase value checked.

If the block is indeed found to be a rule-breaker, it'd be blacklisted
and chains from that point forward ignored.

Miners may decide to allow themselves to create money with
non-index-zero transactions to work around this. In that case the good
node can announce that a given tx in the rule-breaker block is
invalid. The SPV node would then challenge nodes announcing the longer
chain to provide the inputs for the bad tx all the way back to a
pre-split coinbase.

Doing these checks would be rather time consuming with huge blocks,
but it's a last resort only. In the absence of bugs, the mere presence
of the mechanism should ensure it never has to be used.



From moon at justmoon.de  Sun Jun 24 16:51:26 2012
From: moon at justmoon.de (Stefan Thomas)
Date: Sun, 24 Jun 2012 18:51:26 +0200
Subject: [Bitcoin-development] Enforcing inflation rules for SPV clients
In-Reply-To: <CANEZrP0nagdAXyMEY5yxyXBjdo78YC16mjUG9=b0AMe4qOS=fA@mail.gmail.com>
References: <CANEZrP0nagdAXyMEY5yxyXBjdo78YC16mjUG9=b0AMe4qOS=fA@mail.gmail.com>
Message-ID: <4FE7458E.2020908@justmoon.de>

Very interesting for you to bring this up. I had a similar idea for a
totally different use case. Greg recently pointed out an interesting
dilemma saying that (significantly) larger blocks would lead to
centralization. So I've been working on a design for a decentralized
pool that can handle gigabyte sized blocks by splitting up the work
among its members.

At the moment P2Pool nodes all verify all transactions in all blocks.
But it seems feasible to create a system where miners who have over the
last 10000 blocks contributed to the pool's proof-of-work are allocated
a proportional piece of verification work with redundancy and
deterministic randomness that makes manipulation of the allocation
extremely difficult. Such a pool would be very unlikely to accept an
invalid block or transaction in practice.

However, with these block sizes obviously non miners are going to have
to be SPV, so even just a 0.0001% chance of an invalid block being
accepted has profound implications for the network. If a decentralized
pool like that had more than 50% of the hashing power and it accepted a
single invalid block, that tainted chain would be forever regarded as
valid by SPV clients. There needs to be some way to recover once an
invalid block has been accidentally accepted by an imperfect miner.

Based on that I also started to think about proofs of invalidity that
would circulate. Basically you would add a new network message that
would contain the proof that a specific signature and therefore the
whole block is invalid.

As long as the block's proof-of-work is valid and the block's parent is
one of the last n = 50000 blocks, the message is relayed (subject to a
cooldown, warnings would be less frequently relayed the older the
offending block is.)

The mechanism works in exactly the way Mike mentions: It allows even SPV
clients to punish any miner who is dishonest or negligent with their
verification work. That gives miners a good reason not to be dishonest
or negligent in the first place.


(Motivation:

Processing more transactions means that hashing is a smaller part of the
overall cost for miners. For example, paying for 50 BTC worth of hashing
per block costs 0.05 BTC per tx at 1000 tx/block, but only 0.0005 BTC at
100000 tx/block.

Number of transactions is a lever that lets us have lower fees and more
network security at the same time. Like Greg correctly pointed out, this
is not worth having if we have to sacrifice decentralization. But if we
don't, it becomes a no-brainer.

My IMTUO proposal [1] showed a way where miners don't need a copy of the
set of unspent outputs at all. This means the minimum storage
requirements per node no longer grow with the number of transactions.

However, the price for this was about five times greater bandwidth usage
per verified transaction. Since every miner still had to verify every
transaction it looked like bandwidth would become an even bigger problem
with IMTUO than storage would have been without. However, if a small
miner can do less than 100% verifications and still contribute, suddenly
IMTUO may become viable. That would accomplish the holy grail of Bitcoin
scalability where the network successfully runs on trust-atomic entities
all of which can choose to store only a small fraction of the block
chain, verify a small fraction of transactions and perform a small
fraction of the hashing.)


[1] https://en.bitcoin.it/wiki/User:Justmoon/IMTUO

On 6/24/2012 2:45 PM, Mike Hearn wrote:
> I've been having a discussion with d'aniel from the forums about how
> to handle the possibility of a majority-miner conspiracy to raise
> inflation, if most economic actors use SPV clients.
>
> Because of how blocks are formatted you cannot check the coinbase of a
> transaction without knowing the fees in the block, and the fees can
> only be calculated if you have all the input transactions for every
> transaction in that block. Because the attack scenario is an attempted
> takeover of the economy by miners, attempting to put hints into the
> blocks won't work - we have to assume the hardest chain is in fact
> wrong according to the rules signed up to by the Bitcoin user.
>
> The most obvious goal for a cartel of miners is to change the
> inflation formula, either for purely selfish reasons (they want more
> money than can be obtained by fees) or due to coercion by
> governments/central banks who still subscribe to the "inflation is
> good" idea.
>
> Whilst "good" nodes (still on the old ruleset) won't relay blocks that
> violate the rules no matter how hard they are, in a situation where an
> SPV client DOES hear about the bad best chain, it would switch to it
> automatically. And who knows how the network might look in future -
> perhaps most nodes would end up run by miners, or other entities that
> upgrade to the new ruleset for other reasons.
>
> d'aniel made a good proposal - having good nodes broadcast
> announcements when they detect a rule that breaks the rules, along
> with a proof that it did so. Checking the proof might be very
> expensive, but it would only have to be done for split points,
> limiting the potential for DoS. If a node announces that it has a
> weaker chain and that the split point is a rule-breaker, the SPV
> client would download the headers for the side chain to verify the
> split, then download all the transactions in the split block along
> with all their inputs, and the merkle branches linking the inputs to
> the associated block headers. In this way the fee can be calculated,
> the inflation formula applied and the coinbase value checked.
>
> If the block is indeed found to be a rule-breaker, it'd be blacklisted
> and chains from that point forward ignored.
>
> Miners may decide to allow themselves to create money with
> non-index-zero transactions to work around this. In that case the good
> node can announce that a given tx in the rule-breaker block is
> invalid. The SPV node would then challenge nodes announcing the longer
> chain to provide the inputs for the bad tx all the way back to a
> pre-split coinbase.
>
> Doing these checks would be rather time consuming with huge blocks,
> but it's a last resort only. In the absence of bugs, the mere presence
> of the mechanism should ensure it never has to be used.
>
> ------------------------------------------------------------------------------
> Live Security Virtual Conference
> Exclusive live event will cover all the ways today's security and 
> threat landscape has changed and how IT managers can respond. Discussions 
> will include endpoint security, mobile security and the latest in malware 
> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/
> _______________________________________________
> Bitcoin-development mailing list
> Bitcoin-development at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/bitcoin-development





From gmaxwell at gmail.com  Sun Jun 24 18:03:10 2012
From: gmaxwell at gmail.com (Gregory Maxwell)
Date: Sun, 24 Jun 2012 14:03:10 -0400
Subject: [Bitcoin-development] Enforcing inflation rules for SPV clients
In-Reply-To: <CANEZrP0nagdAXyMEY5yxyXBjdo78YC16mjUG9=b0AMe4qOS=fA@mail.gmail.com>
References: <CANEZrP0nagdAXyMEY5yxyXBjdo78YC16mjUG9=b0AMe4qOS=fA@mail.gmail.com>
Message-ID: <CAAS2fgSWb2v6F2xbK0ZFNPz5cwgf6zwB8QbRYgyS1u8xmgAUTQ@mail.gmail.com>

On Sun, Jun 24, 2012 at 8:45 AM, Mike Hearn <mike at plan99.net> wrote:
> d'aniel made a good proposal - having good nodes broadcast
> announcements when they detect a rule that breaks the rules, along
> with a proof that it did so. Checking the proof might be very

Link?

I also proposed this on this list (see the response in the tree
datastructures thread) along with more elaboration on IRC. If multiple
people are coming up with it thats a good sign that it it might
actually be viable. :)

I was going for a slightly different angle and pointing out that the
proofs would mean that a node doing validation with TxOUT tree which
hasn't personally wittnessed the complete history of Bitcoin actually
has basically the same security? including resistance to miners
creating fake coin in the past? as a full node today because in order
to get away with a lie every single node must conspire: It's adequate
that only one honest node wittness the lie because once it has the
proof information is hard to suppress.

To save people from having to dig through the public IRC logs for what
I wrote there:

--- Day changed Thu Jun 21 2012
15:10 < gmaxwell> etotheipi_: amiller: an interesting point with all
this txout tree stuff is that if you join the network late and just
trust that the history is correct based on the headers, any other node
who has witnessed a rule violation in the past can prepare a small
message which you would take to be conclusive proof of a rule
violation and then ignore that chain.
15:11 < gmaxwell> e.g. if someone doublespends I just take the
conflicting transactions out and the segments connecting them to the
chain... and show them to you. And without trusting me you can now
ignore the entire child chain past that point.
15:13 < gmaxwell> This fits nicely with the Satoshi comment "It takes
advantage of the nature of information being easy to spread but hard
to stifle" ...  it would be safe to late-join a txout tree chain,
because if there is only a single other honest node in the world who
was around long enough to wittness the cheating, he could still tell
you and it would be as good as if you saw it yourself.
15:17 < gmaxwell> (this is akin to the provable doublespend alert
stuff we talked about before, but applied to blocks)



From mike at plan99.net  Mon Jun 25 08:42:02 2012
From: mike at plan99.net (Mike Hearn)
Date: Mon, 25 Jun 2012 10:42:02 +0200
Subject: [Bitcoin-development] Enforcing inflation rules for SPV clients
In-Reply-To: <CAAS2fgSWb2v6F2xbK0ZFNPz5cwgf6zwB8QbRYgyS1u8xmgAUTQ@mail.gmail.com>
References: <CANEZrP0nagdAXyMEY5yxyXBjdo78YC16mjUG9=b0AMe4qOS=fA@mail.gmail.com>
	<CAAS2fgSWb2v6F2xbK0ZFNPz5cwgf6zwB8QbRYgyS1u8xmgAUTQ@mail.gmail.com>
Message-ID: <CANEZrP0QK3gGEz6qvWbQ17uv7rdch-u9Cs1U7itAh2a=MiNtDA@mail.gmail.com>

> Link?

It was a private conversation for some reason.

> I also proposed this on this list (see the response in the tree
> datastructures thread) along with more elaboration on IRC.

Ah OK. I wasn't paying much attention to those threads.



From mike at plan99.net  Mon Jun 25 08:44:50 2012
From: mike at plan99.net (Mike Hearn)
Date: Mon, 25 Jun 2012 10:44:50 +0200
Subject: [Bitcoin-development] Enforcing inflation rules for SPV clients
In-Reply-To: <4FE7458E.2020908@justmoon.de>
References: <CANEZrP0nagdAXyMEY5yxyXBjdo78YC16mjUG9=b0AMe4qOS=fA@mail.gmail.com>
	<4FE7458E.2020908@justmoon.de>
Message-ID: <CANEZrP1DyCn0cFdPjohgFoyux17_RmNh6Fk=8kpO1R1Cr0YFGg@mail.gmail.com>

> Very interesting for you to bring this up. I had a similar idea for a
> totally different use case. Greg recently pointed out an interesting
> dilemma saying that (significantly) larger blocks would lead to
> centralization.

Yeah. I am still unsure that this really holds. Bitcoin moves fast,
but even so, unless there are a few more SatoshiDice-like events and
the way people use transactions changes dramatically we're a long way
from gigabyte sized blocks.  And once we get there, technology will
probably have improved to the point where it doesn't seem like a big
deal anymore.

Of course we have debated this many times already. Maybe again at the
next meetup :-)



From mike at plan99.net  Mon Jun 25 16:32:56 2012
From: mike at plan99.net (Mike Hearn)
Date: Mon, 25 Jun 2012 18:32:56 +0200
Subject: [Bitcoin-development] LevelDB benchmarking
In-Reply-To: <CANEZrP3=0cJLjyUZ8FqmWXBRoiP-bzPzm14O=unmbDQ_=hHwVA@mail.gmail.com>
References: <CANEZrP2xnsOHyH+a1g6qSNSx_g+TW-yvL0Due7PVr421U6kRLw@mail.gmail.com>
	<CAAS2fgTNqUeYy+oEFyQWrfs4Xyb=3NXutvCmLusknF-18JmFQg@mail.gmail.com>
	<CANEZrP2q9a_0rFh+oo6iUFF1goWs0OJO1xPvxC9zqNA-6VnFAQ@mail.gmail.com>
	<CABsx9T3pQFqL0xsvRfnixYEATO61qMCCDdLmtqZkbVLW0Vxytg@mail.gmail.com>
	<CANEZrP08NrCJM2gxNitXrLjuY6AusNULvkcheN_0MbgFQV_QXw@mail.gmail.com>
	<4FE0D167.7030506@justmoon.de>
	<CANEZrP3WHA7P+t2Jk+w1kBMk0QuqN+gBXeNMoAnj_rK=nor-qg@mail.gmail.com>
	<20120620113744.GA14695@vps7135.xlshosting.net>
	<CANEZrP3=0cJLjyUZ8FqmWXBRoiP-bzPzm14O=unmbDQ_=hHwVA@mail.gmail.com>
Message-ID: <CANEZrP2H4kutDaaXMKczGGwN8-BbQzNifUtP04COoe3ikyKE0A@mail.gmail.com>

I've added some more commits:

https://github.com/mikehearn/bitcoin/commits/leveldb

It's still not ready for a pull req but is a lot closer:

1) Auto-migration is there but not well tested enough (I only tested
with empty wallets).
2) Migration progress UI is there so you have something to watch for
the few minutes it takes. Script execution is disabled during
migration
3) LevelDB source is checked in to the main tree, bitcoin-qt.pro
updated to use it
4) LevelDB is conditionally compiled so if there's some unexpected
issue or regression on some platform it can be switched back to BDB

Still to go:

1) More testing, eg, with actual wallets :-)
2) Update the non-Qt makefiles
3) On Windows it's currently de-activated due to some missing files
from leveldb + I didn't test it

If you want to help out, some testing and makefile work would be
useful. I may not get a chance to work on this again until next week.

On Wed, Jun 20, 2012 at 2:41 PM, Mike Hearn <mike at plan99.net> wrote:
>> Two days ago on #bitcoin-dev:
>> 21:01:19< sipa> what was CTxDB::ReadOwnerTxes ever used for?
>> 21:01:31< sipa> maybe it predates the wallet logic
>>
>> (read: it's not used anywhere in the code, and apparently wasn't ever, even in 0.1.5)
>
> Great, in that case Stefan is right and I'll delete that code when I
> next work on the patch.



From lidstrom83 at gmail.com  Mon Jun 25 23:21:14 2012
From: lidstrom83 at gmail.com (Daniel Lidstrom)
Date: Mon, 25 Jun 2012 16:21:14 -0700
Subject: [Bitcoin-development] Enforcing inflation rules for SPV clients
Message-ID: <4FE8F26A.6050103@gmail.com>

Here's the conversation I had with Mike that Gregory requested a link to:


Thanks!

Bad or hacked client devs is indeed a huge, worrying problem. The 
official client is addressing this with a system called gitian, where 
multiple developers all compile the same source to the same binary and 
then sign the results. Multi-signatures raise the bar for releasing 
hacked clients a lot. We're starting to investigate this with bitcoinj 
too, but it's a lot of work.

Generally, the more people you have to involve in a conspiracy, the less 
likely it is to succeed. If a few miners started to dominate the system 
they have strong financial incentives to cheat, alternatively, they may 
be subjected to government pressure. Having to get the client developers 
involved too makes it much harder, especially as users have to actually 
upgrade.

I started a thread on the development mailing list with your suggestion, 
by the way.

On Mon, Jun 25, 2012 at 1:00 AM, Daniel Lidstrom <lidstrom83 at gmail.com 
<mailto:lidstrom83 at gmail.com>> wrote:

    Hey Mike,

    I put our conversation in the email for easy reference.

    In the unlikely event of a miner conspiracy to print money, is it
    really so much of a further stretch to think the developers of a
    widely used client could also be involved?  (Well, maybe, since
    miners are unaccountable and developers are not.  OTOH if most users
    are apathetic...)  Also, isn't the advantage for lightweight clients
    of SPV over the server-client model that you don't have to trust any
    operator?  Maybe I'm being too much of a purist here...

    Regarding errors being cheap to send and expensive to verify,
    compartmentalizing them the way I suggested before would make them
    individually cheaper to verify.  Just throwing around ideas:
    requiring the error message be received by a quorum of peers before
    checking, and dropping misbehaving or unreliable peers could help.
    Also, not verifying error messages unless the peers relaying them
    are willing to send all the data necessary to do so would help. 
    Hashcash could also be used to balance the costs to send and to
    verify a given type of error message.  I like your idea to only
    check errors in blocks that are split points, and the length of the
    split could also be a consideration.

>     Can we move further conversations to email please? SMF kind of
>     sucks as an inbox.
>
>     Anyway, yes, your proposal makes a lot of sense, although I think
>     in practice this is unlikely to be an issue. If a majority of
>     miners did start mining on a chain with new rules, even if SPV
>     clients couldn't detect the switch automatically it's very likely
>     the developers of those clients would notify the users out of band
>     in some way. For example, by pushing an update to users that
>     explains the new rules to them and tells them how they can cash
>     out of the Bitcoin economy if they disagree with the new consensus.
>
>     If users are on the losing side of a rule change and want to stay
>     there (eg, maybe most non-miners want to stay on the slower
>     chain), then the client can just checkpoint the first block after
>     the rule change occurred. Now even though there's a harder chain
>     with the new rules, the client will stay with the old rules
>     despite being blind to them. There's nothing that says checkpoints
>     have to be hard coded - clients could poll the client developers
>     every day to get new ones. So as long as the SPV devs are on the
>     ball, most users would stay on the old rules even if the software
>     can't do it by itself.
>
>     All that said, broadcasting messages proving a block broke the
>     rules is a nice backstop if it can be done without excessive
>     complexity. There are some details to think about. These messages
>     would be cheap to create and expensive to verify. There has to be
>     something that stops me claiming to SPV clients that every single
>     block is invalid and forcing them to do tons of useless work.
>     Perhaps only blocks that are split points would be eligible. Also,
>     currently, SPV clients do not form their own P2P network. They are
>     always leaves of full nodes. So propagation of the messages might
>     prove difficult unless that was changed.

>     Hi Mike,
>
>     Thanks for your reply.  It was actually an old post of yours on
>     the forum that made me understand the importance of lightweight
>     clients being able to audit the coinbase tx.
>
>     Re: input tx download, I think that splitting the "invalid
>     coinbase" error notification into separate "input n in tx m is
>     invalid" and "invalid fee arithmetic" errors would mean mobile
>     clients would only ever have to download at most one input tx to
>     verify an invalid coinbase.
>
>     The "invalid fee arithmetic" error could also be compartmentalized
>     into "invalid fee arithmetic in tx batch n", where the fee
>     subtotals are recorded in the block, so as to be separately
>     verifiable.  Then the necessary tx downloads would be completely
>     capped.
>
>     Anyway, I think most of my fears about lifting the block size
>     limit are put to rest by these lines of thinking Smiley

>     Hey Daniel,
>
>     I think you're thinking along the right lines here. We should be
>     looking at cheap and backwards compatible ways to upgrade the SPV
>     trust model to more than just going along with the majority consensus.
>
>     For the coinbase issue, it's actually really important because if
>     you can't calculate fees, you can't check the size of the coinbase
>     value and therefore a conspiracy of miners could try and change
>     the inflation schedule. The worst that can happen today if miners
>     decide to try and fork the ruleset against the best interests of
>     other users, is that they produce chains that are not accepted by
>     regular merchant/exchange/end-user nodes and tx confirmation slows
>     down. But if most users are on SPV nodes they will happily accept
>     the new blocks and the miner conspiracy will have successfully
>     seized control of the money supply for the majority of end users.
>
>     The problem with calculating fees is you actually need not only
>     every tx in a block, but all the input txns too! You can't know
>     the fee of a transaction without the input transactions being
>     available too.
>
>     So there's good news and bad news. The good news is you don't have
>     to actually store every transaction on disk or check signatures,
>     which is the expensive part, if you're an SPV node - you can still
>     check the coinbase value asynchronously after receiving a block by
>     requesting the entire contents and all the input transactions
>     (+branches, of course). If you are in an environment that is
>     always-on and not bandwidth constrained this can make a lot of
>     sense. For example, if you're running an SPV client on a regular
>     PC that is sitting in a shop somewhere, just downloading a lot of
>     data is still very cheap compared to indexing it all and verifying
>     all the signatures. I'm implementing support for this kind of
>     async verification in bitcoinj at the moment.
>
>     Where it doesn't really make sense is mobile clients, and your
>     ideas of notification can help a lot there. The first type of
>     error broadcast I'd add is actually for detected double spends
>     because there's a paper from researchers at ETH which show this is
>     needed to address a feasible attack on todays infrastructure. But
>     adding more later to detect invalid blocks is not a bad idea.

>     Hi Mike,
>
>     I wanted to post this on the forum and figured that since, based
>     on the subject, you'd probably end up reading it anyway, I thought
>     I'd run it by you first to see if there's any merit to it.
>
>     Quote
>     I think similar ideas are currently being tossed around.
>
>     The motivation here is to keep rule enforcement maximally
>     decentralized while scaling Bitcoin. Basically, SPV clients could
>     subscribe to peers' announcements of /invalid/ blocks + their
>     specific failure mode.  AFAICT, most of the possible failure modes
>     are easily verifiable by smart phone SPV clients, even at scale. 
>     For example, inclusion of double spends or otherwise invalid txs
>     are easy to verify /given the necessary data/, and Merkle tree
>     nodes can be deemed invalid /if no peer can produce the data that
>     hashes to it/ (gotta be careful to detect and deal with false
>     positives here).  The only failure mode I can think of that isn't
>     easily verifiable at scale is an invalid quantity of fees spent in
>     the coinbase tx, since that currently requires the download of the
>     whole block. (I think I have an idea to fix this below.)
>
>     This trust model relies upon having a single fully validating peer
>     that will reliably announce invalid blocks.  To help this, one or
>     more peers could be periodically cycled, and new ones asked if the
>     main chain contains any invalid blocks.  Worst case scenario, an
>     SPV client reverts to the current trust model, so this idea can
>     only improve the situation. Best case, it takes advantage of the
>     fact that public information is hard to suppress.
>
>     *Verifying coinbase tx invalidity at scale with SPV clients*
>
>     While this is currently not prohibitive for a smart phone if done
>     only occasionally, this will not be the case if the block size
>     limit is lifted.  The idea is to split up the summing of the
>     spendable fees into verifiable pieces.  A new merkle tree is
>     calculated where the i'th leaf's data is the total fees in the
>     i'th chunk of 1024 txs, along with these txs' Merkle root (to
>     prevent collisions).  The Merkle root of this new tree is then
>     included somewhere in the block.
>
>     To verify a claim of invalidity of one of these leaves requires
>     the download of ~1MB of tx data (along with a small amount of
>     Merkle tree data to verify inclusion).  If no claims are made
>     against it, the leaf data is assumed to be valid.  If this is the
>     case, but shenanigans are called on the coinbase tx, downloading
>     all of the leaf data, verifying inclusion, and calculating the
>     total spendable fees is easily doable for a smart phone, even at
>     very large tx volumes.
>
>     Thanks for any thoughts/suggestions!
>
>     Cheers,
>     Daniel




-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120625/bf142529/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/gif
Size: 382 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120625/bf142529/attachment.gif>

From pieter.wuille at gmail.com  Tue Jun 26 14:11:29 2012
From: pieter.wuille at gmail.com (Pieter Wuille)
Date: Tue, 26 Jun 2012 16:11:29 +0200
Subject: [Bitcoin-development] Tor hidden service support
Message-ID: <20120626141129.GA30240@vps7135.xlshosting.net>

Hello everyone,

a few days ago we merged Tor hidden service support in mainline. This means
that it's now possible to run a hidden service bitcoin node, and connect to
other bitcoin hidden services (via a Tor proxy) when running git HEAD. See
doc/Tor.txt for more information. This is expected to be included in the 0.7
release.

Additionally, such addresses are exchanged and relayed via the P2P network.
To do so, we reused the fd87:d87e:eb43::/48 IPv6 range. Each address in this
80-bit range is mapped to an onion address, and treated as belonging to a
separate network. This network range is the same as used by the OnionCat
application (though we do not use OnionCat in any way), and is part of the
RFC4193 Unique Local IPv6 range, which is normally not globally routable.

Other clients that wish to implement similar functionality, can use this
test case: 5wyqrzbvrdsumnok.onion == FD87:D87E:EB43:edb1:8e4:3588:e546:35ca.
The conversion is simply decoding the base32 onion address, and storing the
resulting 80 bits of data as low-order bits of an IPv6 address, prefixed by
fd87:d87e:eb43:. As this range is not routable, there should be no
compatibility problems: any unaware IPv6-capable code will immediately fail
when trying to connect.

-- 
Pieter

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120626/9b5e2a09/attachment.html>

From grarpamp at gmail.com  Tue Jun 26 23:01:14 2012
From: grarpamp at gmail.com (grarpamp)
Date: Tue, 26 Jun 2012 19:01:14 -0400
Subject: [Bitcoin-development] Tor hidden service support
In-Reply-To: <20120626141129.GA30240@vps7135.xlshosting.net>
References: <20120626141129.GA30240@vps7135.xlshosting.net>
Message-ID: <CAD2Ti2_7dc00bad0stAzYHgPG9f6Y91fYodyczTch73-psk7Sw@mail.gmail.com>

> Additionally, such addresses are exchanged and relayed via the P2P network.
> To do so, we reused the fd87:d87e:eb43::/48 IPv6 range. Each address in this
> 80-bit range is mapped to an onion address, and treated as belonging to a
> separate network. This network range is the same as used by the OnionCat
> application (though we do not use OnionCat in any way), and is part of the
> RFC4193 Unique Local IPv6 range, which is normally not globally routable.
>
> Other clients that wish to implement similar functionality, can use this
> test case: 5wyqrzbvrdsumnok.onion == FD87:D87E:EB43:edb1:8e4:3588:e546:35ca.
> The conversion is simply decoding the base32 onion address, and storing the
> resulting 80 bits of data as low-order bits of an IPv6 address, prefixed by
> fd87:d87e:eb43:. As this range is not routable, there should be no
> compatibility problems: any unaware IPv6-capable code will immediately fail
> when trying to connect.

You are going to want to include the block of the Phatom project as well:
https://code.google.com/p/phantom/
fd00:2522:3493::/48

And the one for 'garlicat' for I2P, which might be more complex due
to I2P's addressing:
fd60:db4d:ddb5::/48

Note that while these blocks are not expected to be routable, that
people may in fact have interfaces, routing tables and packet filters
on their machines configured with up to all three of those networks
for the purposes therein.



From gmaxwell at gmail.com  Wed Jun 27 00:14:08 2012
From: gmaxwell at gmail.com (Gregory Maxwell)
Date: Tue, 26 Jun 2012 20:14:08 -0400
Subject: [Bitcoin-development] Tor hidden service support
In-Reply-To: <CAD2Ti2_7dc00bad0stAzYHgPG9f6Y91fYodyczTch73-psk7Sw@mail.gmail.com>
References: <20120626141129.GA30240@vps7135.xlshosting.net>
	<CAD2Ti2_7dc00bad0stAzYHgPG9f6Y91fYodyczTch73-psk7Sw@mail.gmail.com>
Message-ID: <CAAS2fgQEcNOwfw9YkN-jJZi_pb4Ex3ujY4KgWzL07=BVVknt_w@mail.gmail.com>

On Tue, Jun 26, 2012 at 7:01 PM, grarpamp <grarpamp at gmail.com> wrote:
> You are going to want to include the block of the Phatom project as well:
> https://code.google.com/p/phantom/
> fd00:2522:3493::/48

Perhaps some argument to add blocks to the IsRoutable check is in
order?  Then people who use overlay networks that are actually
routable but which use otherwise private space can just add the
relevant blocks.

> Note that while these blocks are not expected to be routable, that
> people may in fact have interfaces, routing tables and packet filters
> on their machines configured with up to all three of those networks
> for the purposes therein.

Note that while the hidden service support in bitcoin uses a
compatible IPv6 mapping with onioncat,  it is _not_ onioncat, does not
use onioncat, does not need onioncat, and wouldn't benefit from
onioncat.  The onioncat style advertisement is used because our
protocol already relays IPv6 addresses. The connections are regular
tor hidden service connections, not the more-risky and low performance
ip in tcp onioncat stuff.



From andyparkins at gmail.com  Wed Jun 27 08:47:01 2012
From: andyparkins at gmail.com (Andy Parkins)
Date: Wed, 27 Jun 2012 09:47:01 +0100
Subject: [Bitcoin-development] Tor hidden service support
In-Reply-To: <20120626141129.GA30240@vps7135.xlshosting.net>
References: <20120626141129.GA30240@vps7135.xlshosting.net>
Message-ID: <201206270947.07921.andyparkins@gmail.com>

On 2012 June 26 Tuesday, Pieter Wuille wrote:

> Additionally, such addresses are exchanged and relayed via the P2P network.
> To do so, we reused the fd87:d87e:eb43::/48 IPv6 range. Each address in

Yuck.  Can't we pinch a few of the addr.services bits to store an address 
family?  AF_INET, AF_INET6, AF_CUSTOM_TOR, and leave space for a few more 
would be, say, four bits out of 64 mostly unused.


Andy

-- 
Dr Andy Parkins
andyparkins at gmail.com
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 198 bytes
Desc: This is a digitally signed message part.
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120627/00ea91b2/attachment.sig>

From grarpamp at gmail.com  Wed Jun 27 09:25:15 2012
From: grarpamp at gmail.com (grarpamp)
Date: Wed, 27 Jun 2012 05:25:15 -0400
Subject: [Bitcoin-development] [tor-talk]  Tor hidden service support
In-Reply-To: <CAD2Ti28hu6PccXpu4ObcbzWwFq+tchYaCoVY7S=9yakaB-nKjQ@mail.gmail.com>
References: <20120626141129.GA30240@vps7135.xlshosting.net>
	<CAD2Ti2_7dc00bad0stAzYHgPG9f6Y91fYodyczTch73-psk7Sw@mail.gmail.com>
	<CAD2Ti29tMRCoW0rBSH738=0LpMfnGSJoSGYubV2-qQ9a6SC=zQ@mail.gmail.com>
	<4FEAA936.10300@infosecurity.ch>
	<CAD2Ti28hu6PccXpu4ObcbzWwFq+tchYaCoVY7S=9yakaB-nKjQ@mail.gmail.com>
Message-ID: <CAD2Ti2_+4YzoO-9Qg8c68WkTkU39_Z1taBHCKuG6UJYL1vaZ4A@mail.gmail.com>

GregM, wasn't sure how to answer your question, and as to
conflicts [1]. I think I grasped it in my reply to something on
tor-talk, which is on its way here pending moderation due to bcc.
I put that part below. The FYI referred to seednodes as
they exist on Tor / I2P today.

> You are going to want to include the block of the Phatom project as well:
>> https://code.google.com/p/phantom/
>> fd00:2522:3493::/48

> Perhaps some argument to add blocks to the IsRoutable check is in
> order?  Then people who use overlay networks that are actually
> routable but which use otherwise private space can just add the
> relevant blocks.

/ [1] Well bitcoin wouldn't know to offload traffic to any of those
/ blocks, or a specific host on them, if you had them set up locally
/ via *Cat or Phantom... for bitcoin use. It would probably end up
/ half useful similar to the above FYI. But that would just affect
/ bitcoin, not whatever else you were running on them.



From grarpamp at gmail.com  Wed Jun 27 19:51:08 2012
From: grarpamp at gmail.com (grarpamp)
Date: Wed, 27 Jun 2012 15:51:08 -0400
Subject: [Bitcoin-development] Tor hidden service support
Message-ID: <CAD2Ti28rUMwEOfHwkdoUOewHKr851BetC=o_=gcvEKrAZtysQQ@mail.gmail.com>

Forward past automoderation...


> Reading https://github.com/bitcoin/bitcoin/blob/master/doc/Tor.txt

> Is bitcoin software going to incorporate tor binaries within the
> application standard application and automatically create a Tor Hidden
> Service on behalf of end-user?
>
> Are there any direction regarding this kind of integration?

The document (Tor.txt) assumes the bitcoin user has taken care of
that. So no bi-direction needed (I am not TorProject).

> Regarding the addressing, why not use directly the .onion address?
> They represent in parallel:
> - Routing information (providing a path to the destination)
> - Proof of identity (owning the private RSA key)
> Which is the reason to map it to an IPv6 address?

Seems it's used only within bitcoin code to distinguish which proxy
or native IPvN path to send bitcoin traffic to (or receive from).
It might be simpler than managing onions, i2p's and whatever else
throughout the code and the private bitcoin p2p mesh.

Though I don't suspect it will conflict [1] with anyone's use of
OnionCat, GarliCat, or Phantom... it would just feel odd configuring
bitcoin to use Tor or I2P proxy ports (or Phantom native) when you
could conceivably just dump the IPv6 traffic to the OS stack for
handling once you have the *Cat shims and Phantom set up. They do
have a point about about ocat as a shim for their purposes. And
Phantom is a special case in that it's all native IPv6 interface,
no proxy or shim needed or provided.

I will quote an additional note from bitcoin-devel...

"Note that while the hidden service support in bitcoin uses a
compatible IPv6 mapping with onioncat, it is _not_ onioncat, does not
use onioncat, does not need onioncat, and wouldn't benefit from
onioncat. The onioncat style advertisement is used because our
protocol already relays IPv6 addresses. The connections are regular
tor hidden service connections, not the more-risky and low performance
ip in tcp onioncat stuff."

FYI. There have been a dozen or so onion:8333 nodes and maybe some
on I2P long before this work. But I think could only be used as
-connect or -addnode seeds with some extra host setup. Never tried
it since -proxy was sufficient. Seems this is a simpler and full
solution.

[1] Well bitcoin wouldn't know to offload traffic to any of those
blocks, or a specific host on them, if you had them set up locally
via *Cat or Phantom... for bitcoin use. It would probably end up
half useful similar to the above FYI. But that would just affect
bitcoin, not whatever else you were running on them.



