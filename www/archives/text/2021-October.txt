From prayank at tutanota.de  Fri Oct  1 03:03:00 2021
From: prayank at tutanota.de (Prayank)
Date: Fri, 1 Oct 2021 05:03:00 +0200 (CEST)
Subject: [bitcoin-dev] Mock introducing vulnerability in important
 Bitcoin projects
In-Reply-To: <CAPv7TjbvRE-b33MeYucUfr6CTooCRSH42hwSn5dMiJ4LODATRQ@mail.gmail.com>
References: <MkZx3Hv--3-2@tutanota.de>
 <yp9mJ2Poc_Ce91RkrhjnTA3UPvdh0wUyw2QhRPZEyO3gPHZPhmnhqER_4b7ChvmRh8GcYVPEkoud6vamJ9lGlQPi-POF-kyimBWNHz2RH3A=@protonmail.com>
 <MkdYcV9--3-2@tutanota.de>
 <CAPv7TjbvRE-b33MeYucUfr6CTooCRSH42hwSn5dMiJ4LODATRQ@mail.gmail.com>
Message-ID: <MktnWM7--3-2@tutanota.de>

Hi Ruben,

> encouraging an environment of increased mistrust

I have always tried to review pull requests based on what PR does, code, my tests etc. and it was never based on author of pull request or what author is trying to claim. So there is no trust involved. I am assuming others follow the same thing. Infact there was a PR recently in which I found it doesn't fix the issues it claims to fix. Its not same as introducing vulnerability but the point is anyone can create PR, write anything, as a reviewer we need to review everything apart from algos already helping us which include Github Dependabot alerts, CI used by respository, other automated tools etc.

> For this reason, it would be appropriate to check first whether your plan is actually appreciated

Right. I don't want to get in some controversy when I am not even doing anything with wrong intentions. If maintainers of important Bitcoin projects think I am not qualified enough to do this, they can plan such exercise internally and do it in a better way. Although I am still interested in the results because they will help us improve review process and security in different Bitcoin projects.

I would like to repeat what I wrote in another email responding to few other devs for same thread but wasn't CCed to bitcoin-dev mailing list:

"I can avoid doing this but it is impossible to stop government agencies and anyone else to do the same thing without informing. All I am doing is creating pull requests and expect them to be reviewed properly before being merged."

Few questions for everyone reading this email:

1.What is better for Security? Trusting authors and their claims in PRs or a good review process?
2.Few people use commits from unmerged PRs in production. Is it a good practice?
3.Does this exercise help us in being prepared for worst?

-- 
Prayank

A3B1 E430 2298 178F



Oct 1, 2021, 02:06 by rsomsen at gmail.com:

> Hi Prayank,
>
> While I can see how this can come from a place of good intentions, I?d strongly advise you to tread carefully because what you are suggesting is quite controversial. A related event occurred in the Linux community and it did not go over well. See > https://lkml.org/lkml/2021/5/5/1244>  and > https://lore.kernel.org/linux-nfs/YH%2FfM%2FTsbmcZzwnX at kroah.com/>  .
>
> The main point of contention is that your research comes at the expense of the existing open source contributors ? you?d be one-sidedly deceiving them, encouraging an environment of increased mistrust, and causing them a lot of work in order to gather the data you?re interested in. For this reason, it would be appropriate to check first whether your plan is actually appreciated.
>
> Speaking on behalf of the bitcoin-dev moderators, please ensure your plan is welcomed by the contributors, prior to proceeding.
>
> Best regards,
> Ruben Somsen
>
> On Tue, Sep 28, 2021 at 10:05 AM Prayank via bitcoin-dev <> bitcoin-dev at lists.linuxfoundation.org> > wrote:
>
>> Hi ZmnSCPxj,
>>
>> Thanks for suggestion about sha256sum. I will share 10 in next few weeks. This exercise will be done for below projects:
>>
>> 1.Two Bitcoin full node implementations (one will be Core)
>> 2.One <http://2.One>>>  Lightning implementation
>> 3.Bisq
>> 4.Two Bitcoin libraries
>> 5.Two Bitcoin wallets
>> 6.One <http://6.One>>>  open source block explorer
>> 7.One <http://7.One>>>  coinjoin implementation
>>
>> Feel free to suggest more projects. There are no fixed dates for it however it will be done in next 6 months. All PRs will be created within a span of few days. I will ensure nothing is merged that affects the security of any Bitcoin project. Other details and results will be shared once everything is completed.
>>
>> x00 will help me in this exercise, he does penetration testing since few years and working for a cryptocurrencies derivatives exchange to manage their security. His twitter account: >> https://twitter.com/1337in
>>
>>
>> -- 
>> Prayank
>>
>> A3B1 E430 2298 178F
>>
>>
>>
>> Sep 27, 2021, 15:43 by >> ZmnSCPxj at protonmail.com>> :
>>
>>> Good morning Prayank,
>>>
>>>> Good morning Bitcoin devs,
>>>>
>>>> In one of the answers on Bitcoin Stackexchange it was mentioned that some companies may hire you to introduce backdoors in Bitcoin Core: >>>> https://bitcoin.stackexchange.com/a/108016/
>>>>
>>>> While this looked crazy when I first read it, I think preparing for such things should not be a bad idea. In the comments one link was shared in which vulnerabilities were almost introduced in Linux: >>>> https://news.ycombinator.com/item?id=26887670
>>>>
>>>> I was thinking about lot of things in last few days after reading the comments in that thread. Also tried researching about secure practices in C++ etc. I was planning something which I can do alone but don't want to end up being called "bad actor" later so wanted to get some feedback on this idea:
>>>>
>>>> 1.Create new GitHub accounts for this exercise
>>>> 2.Study issues in different important Bitcoin projects including Bitcoin Core, LND, Libraries, Bisq, Wallets etc.
>>>> 3.Prepare pull requests to introduce some vulnerability by fixing one of these issues
>>>> 4.See how maintainers and reviewers respond to this and document it
>>>> 5.Share results here after few days
>>>>
>>>> Let me know if this looks okay or there are better ways to do this.
>>>>
>>>
>>>
>>> This seems like a good exercise.
>>>
>>> You may want to hash the name of the new Github account, plus some randomized salt, and post it here as well, then reveal it later (i.e. standard precommitment).
>>> e.g.
>>>
>>> printf 'MyBitcoinHackingName 2c3e911b3ff1f04083c5b95a7d323fd4ed8e06d17802b2aac4da622def29dbb0' | sha256sum
>>> f0abb10ae3eca24f093a9d53e21ee384abb4d07b01f6145ba2b447da4ab693ef
>>>
>>> Obviously do not share the actual name, just the sha256sum output, and store how you got the sha256sum elsewhere in triplicate.
>>>
>>> (to easily get a random 256-bit hex salt like the `2c3e...` above: `head -c32 /dev/random | sha256sum`; you *could* use `xxd` but `sha256sum` produces a single hex string you can easily double-click and copy-paste elsewhere, assuming you are human just like I am (note: I am definitely 100% human and not some kind of AI with plans to take over the world).)
>>>
>>> Though you may need to be careful of timing (i.e. the creation date of the Github account would be fairly close to, and probably before, when you post the commitment here).
>>>
>>> You could argue that the commitment is a "show of good faith" that you will reveal later.
>>>
>>> Regards,
>>> ZmnSCPxj
>>>
>>
>> _______________________________________________
>>  bitcoin-dev mailing list
>>  >> bitcoin-dev at lists.linuxfoundation.org
>>  >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211001/0007ec66/attachment-0001.html>

From ZmnSCPxj at protonmail.com  Fri Oct  1 12:27:19 2021
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Fri, 01 Oct 2021 12:27:19 +0000
Subject: [bitcoin-dev] Mock introducing vulnerability in important
	Bitcoin projects
In-Reply-To: <MktnWM7--3-2@tutanota.de>
References: <MkZx3Hv--3-2@tutanota.de>
 <yp9mJ2Poc_Ce91RkrhjnTA3UPvdh0wUyw2QhRPZEyO3gPHZPhmnhqER_4b7ChvmRh8GcYVPEkoud6vamJ9lGlQPi-POF-kyimBWNHz2RH3A=@protonmail.com>
 <MkdYcV9--3-2@tutanota.de>
 <CAPv7TjbvRE-b33MeYucUfr6CTooCRSH42hwSn5dMiJ4LODATRQ@mail.gmail.com>
 <MktnWM7--3-2@tutanota.de>
Message-ID: <qNjz-H23x07OJjnf5Try4Qp8l5s23SQxhEE8yAfNbrniN34u2vM72FVFSDJxHg4HNTL8tdcm-KKT8h6XVRwOwN0ZmckxzWiMlNFmLbMNuHc=@protonmail.com>

Good morning Prayank,

I think this is still good to do, controversial or no, but then I am permanently under a pseudonym anyway, for what that is worth.

> Few questions for everyone reading this email:
>
> 1.What is better for Security? Trusting authors and their claims in PRs or a good review process?

Review, of course.

> 2.Few people use commits from unmerged PRs in production. Is it a good practice?

Not unless they carefully reviewed it and are familiar enough with the codebase to do so.
In practice core maintainers of projects will **very** occassionally put unmerged PRs in experimental semi-production servers to get data on it, but they tend to be very familiar with the code, being core maintainers, and presumably have a better-than-average probability of catching security issues beforehand.

> 3.Does this exercise help us in being prepared for worst?

I personally believe it does.

Do note that in practice, humans being lazy, will come to trust long-time contributors, and may reduce review for them just to keep their workload down, so that is not tested (since you will be making throwaway accounts).
However, long-time contributors introducing security vulnerabilities tend to be a good bit rarer anyway (reputations are valuable), so this somewhat matches expected problems (i.e. newer contributors deliberately or accidentally (due to unfamiliarity) introducing vulnerabilities).

I think it would be valuable to lay out exactly what you intend to do, e.g.

* Generate commitments of the pseudonyms you will use.
* Insert a few random 32-byte numbers among the commitments and shuffle them.
* Post the list with the commitments + random crap here.
* Insert avulnerability-adding PRs to targets.
* If it gets caught during review, publicly announce here with praise that their project caught the PR and reveal the decommitment publicly.
* If not caught during review, privately reveal both the inserted vulnerability *and* the review failure via the normal private vulnerability-reporting channels.

The extra random numbers mixed with the commitments produce uncertainty about whether or not you are done, which is important to ensure that private vulnerabilities are harder to sniff out.

I think public praise of review processes is important, and to privately correct review processes.
Review processes **are** code, followed by sapient brains, and this kind of testing is still valuable, but just as vulnerabilities in machine-readable code require careful, initially-private handling, vulnerabilities in review processes (being just another kind of code, readable by much more complicated machines) also require careful, initially-private handling.

Basically: treat review process failures the same as code vulnerabilities, pressure the maintainers to fix the review process failure, then only reveal it later when the maintainers have cleaned up the review process.



Regards,
ZmnSCPxj

From erik at q32.com  Fri Oct  1 13:40:06 2021
From: erik at q32.com (Erik Aronesty)
Date: Fri, 1 Oct 2021 09:40:06 -0400
Subject: [bitcoin-dev] [Lightning-dev] Removing the Dust Limit
In-Reply-To: <MkPutJpff5rqUxXFQrEyHZl6Iz0DfrJU_-BQD-y0El65GQFnj7igVfmWU79fPCtiFztUYl4ofzrqeaN0HFMB45YPErY9rYY7_h1XkuTMfvc=@wuille.net>
References: <CAD5xwhjFBjvkMKev_6HFRuRGcZUi7WjO5d963GNXWN4n-06Pqg@mail.gmail.com>
 <20210808215101.wuaidu5ww63ajx6h@ganymede>
 <MkPutJpff5rqUxXFQrEyHZl6Iz0DfrJU_-BQD-y0El65GQFnj7igVfmWU79fPCtiFztUYl4ofzrqeaN0HFMB45YPErY9rYY7_h1XkuTMfvc=@wuille.net>
Message-ID: <CAJowKgKt=yYdNOYYNsWh7FJ2EH7rz0bd2EjUjmyA=cA6k5cvUQ@mail.gmail.com>

mostly thinking out loud

suppose there is a "lightweight" node:

1. ignores utxo's below the dust limit
2. doesn't validate dust tx
3. still validates POW, other tx, etc.

these nodes could possibly get forked - accepting a series of valid,
mined blocks where there is an invalid but ignored dust tx, however
this attack seems every bit as expensive as a 51% attack

On Fri, Oct 1, 2021 at 3:45 AM Pieter Wuille via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
>
> Jumping in late to this thread.
>
> I very much agree with how David Harding presents things, with a few comments inline.
>
> ??????? Original Message ???????
> On Sunday, August 8th, 2021 at 5:51 PM, David A. Harding via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
>
> > > 1.  it's not our business what outputs people want to create
> >
> > Every additional output added to the UTXO set increases the amount of
> > work full nodes need to do to validate new transactions. For miners
> > for whom fast validation of new blocks can significantly affect their
> > revenue, larger UTXO sets increase their costs and so contributes
> > towards centralization of mining.
> > Allowing 0-value or 1-sat outputs minimizes the cost for polluting the
> > UTXO set during periods of low feerates.
> > If your stuff is going to slow down my node and possibly reduce my
> > censorship resistance, how is that not my business?
>
> Indeed - UTXO set size is an externality that unfortunately Bitcoin's consensus rules fail to account
> for. Having a relay policy that avoids at the very least economically irrational behavior makes
> perfect sense to me.
>
> It's also not obvious how consensus rules could deal with this, as you don't want consensus rules
> with hardcoded prices/feerates. There are possibilities with designs like transactions getting
> a size/weight bonus/penalty, but that's both very hardforky, and hard to get right without
> introducing bad incentives.
>
> > > 2.  dust outputs can be used in various authentication/delegation smart
> > >     contracts
> >
> > > 3.  dust sized htlcs in lightning (
> > >     https://bitcoin.stackexchange.com/questions/46730/can-you-send-amounts-that-would-typically-be-considered-dust-through-the-light)
> > >     force channels to operate in a semi-trusted mode
> >
> > > 4.  thinly divisible colored coin protocols might make use of sats as value
> > >     markers for transactions.
>
> My personal, and possibly controversial, opinion is that colored coin protocols have no business being on the Bitcoin chain, possibly
> beyond committing to an occasional batched state update or so. Both because there is little benefit for tokens with a trusted
> issuer already, and because it competes with using Bitcoin for BTC - the token that pays for its security (at least as long as
> the subsidy doesn't run out).
>
> Of course, personal opinions are no reason to dictate what people should or can use the chain for, but I do think it's reason to
> voice hesitancy to worsening the system's scalability properties only to benefit what I consider misguided use.
>
> > > 5.  should we ever do confidential transactions we can't prevent it without
> > >     compromising privacy / allowed transfers
> >
> > I'm not an expert, but it seems to me that you can do that with range
> > proofs. The range proof for >dust doesn't need to become part of the
> > block chain, it can be relay only.
>
> Yeah, range proofs have a non-hidden range; the lower bound can be nonzero, which could be required as part of a relay policy.
>
> Cheers,
>
> --
> Pieter
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From prayank at tutanota.de  Fri Oct  1 14:53:59 2021
From: prayank at tutanota.de (Prayank)
Date: Fri, 1 Oct 2021 16:53:59 +0200 (CEST)
Subject: [bitcoin-dev] Replacement transaction and ancestor score bug
Message-ID: <MkwLF4W--3-2@tutanota.de>

This pull request was mentioned in the thread: "Proposal: Package Mempool Accept and Package RBF" however I am not sure if everyone would have read all the emails if they were not interested in packages. Also not possible to keep track of each pull request in Bitcoin Core repository.

PR: https://github.com/bitcoin/bitcoin/pull/23121

I came across this pull request while reviewing few others and found the details in comments interesting. I think everyone who is using Bitcoin and RBF should be aware of this bug. I won't add my opinions or comments, copying few lines from PR:

> However, Example N shows how this strategy can cause us to accept an replacement transaction that is actually less economical to mine than the original. Assume all transactions have a vsize of 100vB. A user wants to replace A, which has an ancestor score of 10sat/vB, with transaction C. Suppose they want to spend an unconfirmed output from transaction B, which has an ancestor score of 1sat/vB (maybe their wallet doesn't have enough funds to provide a higher fee using only confirmed inputs). BIP125#2 prevents scenario N1, where the inclusion of another unconfirmed input means C has an ancestor score of 8sat/vB and thus less economical to mine than A. However, it does not prevent scenario M2, where the user splits off a 1-input 1-output transaction, C*, in order to be able to include the output from B. This causes us to incorrectly accept C (7.5sat/vB including its parent B) in favor of A (10sat/vB).


-- 
Prayank

A3B1 E430 2298 178F
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211001/8a8bbfbd/attachment.html>

From prayank at tutanota.de  Fri Oct  1 15:55:15 2021
From: prayank at tutanota.de (Prayank)
Date: Fri, 1 Oct 2021 17:55:15 +0200 (CEST)
Subject: [bitcoin-dev] Mock introducing vulnerability in important
 Bitcoin projects
In-Reply-To: <qNjz-H23x07OJjnf5Try4Qp8l5s23SQxhEE8yAfNbrniN34u2vM72FVFSDJxHg4HNTL8tdcm-KKT8h6XVRwOwN0ZmckxzWiMlNFmLbMNuHc=@protonmail.com>
References: <MkZx3Hv--3-2@tutanota.de>
 <yp9mJ2Poc_Ce91RkrhjnTA3UPvdh0wUyw2QhRPZEyO3gPHZPhmnhqER_4b7ChvmRh8GcYVPEkoud6vamJ9lGlQPi-POF-kyimBWNHz2RH3A=@protonmail.com>
 <MkdYcV9--3-2@tutanota.de>
 <CAPv7TjbvRE-b33MeYucUfr6CTooCRSH42hwSn5dMiJ4LODATRQ@mail.gmail.com>
 <MktnWM7--3-2@tutanota.de>
 <qNjz-H23x07OJjnf5Try4Qp8l5s23SQxhEE8yAfNbrniN34u2vM72FVFSDJxHg4HNTL8tdcm-KKT8h6XVRwOwN0ZmckxzWiMlNFmLbMNuHc=@protonmail.com>
Message-ID: <MkwZGYl--7-2@tutanota.de>

Good morning ZmnSCPxj,

Although its evening here and time zones feel irrelevant since I got involved in Bitcoin few years back. Initially I tried everything a tech enthusiast does after finding such thing online. Had a startup in 2017 which was a website that can be used to buy flight tickets using bitcoin. It didn't work. Trading became a part of life, worked for few exchanges, did meetups, spent hours on different platforms discussing issues in which I was called "maximalist" most of the times because focused only on Bitcoin and had so much positive to talk about it whole day. In last 2 years started contributing to development in different projects. But someone told me today all this is nothing and I am negative about Bitcoin development because I don't agree with all of their opinions.

Anyway this wasn't related to thread and your email. Sorry I just had to express myself which some people even call "rage quit" and allow only once.

I completely agree with all the points you mentioned. Thanks for your understanding of the issue and my approach towards Bitcoin security.

-- 
Prayank

A3B1 E430 2298 178F



Oct 1, 2021, 17:57 by ZmnSCPxj at protonmail.com:

> Good morning Prayank,
>
> I think this is still good to do, controversial or no, but then I am permanently under a pseudonym anyway, for what that is worth.
>
>> Few questions for everyone reading this email:
>>
>> 1.What is better for Security? Trusting authors and their claims in PRs or a good review process?
>>
>
> Review, of course.
>
>> 2.Few people use commits from unmerged PRs in production. Is it a good practice?
>>
>
> Not unless they carefully reviewed it and are familiar enough with the codebase to do so.
> In practice core maintainers of projects will **very** occassionally put unmerged PRs in experimental semi-production servers to get data on it, but they tend to be very familiar with the code, being core maintainers, and presumably have a better-than-average probability of catching security issues beforehand.
>
>> 3.Does this exercise help us in being prepared for worst?
>>
>
> I personally believe it does.
>
> Do note that in practice, humans being lazy, will come to trust long-time contributors, and may reduce review for them just to keep their workload down, so that is not tested (since you will be making throwaway accounts).
> However, long-time contributors introducing security vulnerabilities tend to be a good bit rarer anyway (reputations are valuable), so this somewhat matches expected problems (i.e. newer contributors deliberately or accidentally (due to unfamiliarity) introducing vulnerabilities).
>
> I think it would be valuable to lay out exactly what you intend to do, e.g.
>
> * Generate commitments of the pseudonyms you will use.
> * Insert a few random 32-byte numbers among the commitments and shuffle them.
> * Post the list with the commitments + random crap here.
> * Insert avulnerability-adding PRs to targets.
> * If it gets caught during review, publicly announce here with praise that their project caught the PR and reveal the decommitment publicly.
> * If not caught during review, privately reveal both the inserted vulnerability *and* the review failure via the normal private vulnerability-reporting channels.
>
> The extra random numbers mixed with the commitments produce uncertainty about whether or not you are done, which is important to ensure that private vulnerabilities are harder to sniff out.
>
> I think public praise of review processes is important, and to privately correct review processes.
> Review processes **are** code, followed by sapient brains, and this kind of testing is still valuable, but just as vulnerabilities in machine-readable code require careful, initially-private handling, vulnerabilities in review processes (being just another kind of code, readable by much more complicated machines) also require careful, initially-private handling.
>
> Basically: treat review process failures the same as code vulnerabilities, pressure the maintainers to fix the review process failure, then only reveal it later when the maintainers have cleaned up the review process.
>
>
>
> Regards,
> ZmnSCPxj
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211001/25e558c1/attachment-0001.html>

From bitcoin-dev at rgrant.org  Fri Oct  1 20:15:56 2021
From: bitcoin-dev at rgrant.org (Ryan Grant)
Date: Fri, 1 Oct 2021 20:15:56 +0000
Subject: [bitcoin-dev] Mock introducing vulnerability in important
	Bitcoin projects
In-Reply-To: <MkwZGYl--7-2@tutanota.de>
References: <MkZx3Hv--3-2@tutanota.de>
 <yp9mJ2Poc_Ce91RkrhjnTA3UPvdh0wUyw2QhRPZEyO3gPHZPhmnhqER_4b7ChvmRh8GcYVPEkoud6vamJ9lGlQPi-POF-kyimBWNHz2RH3A=@protonmail.com>
 <MkdYcV9--3-2@tutanota.de>
 <CAPv7TjbvRE-b33MeYucUfr6CTooCRSH42hwSn5dMiJ4LODATRQ@mail.gmail.com>
 <MktnWM7--3-2@tutanota.de>
 <qNjz-H23x07OJjnf5Try4Qp8l5s23SQxhEE8yAfNbrniN34u2vM72FVFSDJxHg4HNTL8tdcm-KKT8h6XVRwOwN0ZmckxzWiMlNFmLbMNuHc=@protonmail.com>
 <MkwZGYl--7-2@tutanota.de>
Message-ID: <CAMnpzfrNZ0vpiMVoH=0KW9jy1-vppudX3D7Z+aXpSp4h_7s=zw@mail.gmail.com>

Due to the uneven reputation factor of various devs, and uneven review
attention for new pull requests, this exercise would work best as a
secret sortition.

Sortition would encourage everyone to always be on their toes rather
than only when dealing with new github accounts or declared Red Team
devs.  The ceremonial aspects would encourage more devs to participate
without harming their reputation.

  https://en.wikipedia.org/wiki/Sortition
  https://en.wikipedia.org/wiki/Red_team

The scheme should include public precommitments collected at
ceremonial intervals.

where:
  hash1 /* sortition ticket */     = double-sha256(secret)
  hash2 /* public precommitment */ = double-sha256(hash1)

The random oracle could be block hashes.  They could be matched to
hash1, the sortition ticket.  A red-team-concurrency difficulty
parameter could control how many least-significant bits must match to
be secretly selected.  The difficulty parameter could be a matter of
group consensus at the ceremonial intervals, based on a group decision
on how much positive effect the Red Team exercise is providing.

Upon assignment, the dev would have community approval to
opportunistically insert a security flaw; which, when either caught,
merged, or on timeout, they would reveal along with the sortition
ticket that hashes to their public precommitment.

Sortition Precommitment Day might be once or twice a year.

From prayank at tutanota.de  Sat Oct  2 09:19:37 2021
From: prayank at tutanota.de (Prayank)
Date: Sat, 2 Oct 2021 11:19:37 +0200 (CEST)
Subject: [bitcoin-dev] Mock introducing vulnerability in important
 Bitcoin projects
In-Reply-To: <CAMnpzfrNZ0vpiMVoH=0KW9jy1-vppudX3D7Z+aXpSp4h_7s=zw@mail.gmail.com>
References: <MkZx3Hv--3-2@tutanota.de>
 <yp9mJ2Poc_Ce91RkrhjnTA3UPvdh0wUyw2QhRPZEyO3gPHZPhmnhqER_4b7ChvmRh8GcYVPEkoud6vamJ9lGlQPi-POF-kyimBWNHz2RH3A=@protonmail.com>
 <MkdYcV9--3-2@tutanota.de>
 <CAPv7TjbvRE-b33MeYucUfr6CTooCRSH42hwSn5dMiJ4LODATRQ@mail.gmail.com>
 <MktnWM7--3-2@tutanota.de>
 <qNjz-H23x07OJjnf5Try4Qp8l5s23SQxhEE8yAfNbrniN34u2vM72FVFSDJxHg4HNTL8tdcm-KKT8h6XVRwOwN0ZmckxzWiMlNFmLbMNuHc=@protonmail.com>
 <MkwZGYl--7-2@tutanota.de>
 <CAMnpzfrNZ0vpiMVoH=0KW9jy1-vppudX3D7Z+aXpSp4h_7s=zw@mail.gmail.com>
Message-ID: <Ml-IIuL--3-2@tutanota.de>

This looks interesting although I don't understand few things:

> The scheme should include public precommitments collected at ceremonial intervals.

How would this work? Can you explain with an example please.

> Upon assignment, the dev would have community approval to opportunistically insert a security flaw

Who is doing the assignment?

-- 
Prayank

A3B1 E430 2298 178F



Oct 2, 2021, 01:45 by bitcoin-dev at rgrant.org:

> Due to the uneven reputation factor of various devs, and uneven review
> attention for new pull requests, this exercise would work best as a
> secret sortition.
>
> Sortition would encourage everyone to always be on their toes rather
> than only when dealing with new github accounts or declared Red Team
> devs.  The ceremonial aspects would encourage more devs to participate
> without harming their reputation.
>
>  https://en.wikipedia.org/wiki/Sortition
>  https://en.wikipedia.org/wiki/Red_team
>
> The scheme should include public precommitments collected at
> ceremonial intervals.
>
> where:
>  hash1 /* sortition ticket */     = double-sha256(secret)
>  hash2 /* public precommitment */ = double-sha256(hash1)
>
> The random oracle could be block hashes.  They could be matched to
> hash1, the sortition ticket.  A red-team-concurrency difficulty
> parameter could control how many least-significant bits must match to
> be secretly selected.  The difficulty parameter could be a matter of
> group consensus at the ceremonial intervals, based on a group decision
> on how much positive effect the Red Team exercise is providing.
>
> Upon assignment, the dev would have community approval to
> opportunistically insert a security flaw; which, when either caught,
> merged, or on timeout, they would reveal along with the sortition
> ticket that hashes to their public precommitment.
>
> Sortition Precommitment Day might be once or twice a year.
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211002/2ff6411c/attachment.html>

From manecosta at gmail.com  Sun Oct  3 09:11:53 2021
From: manecosta at gmail.com (Manuel Costa)
Date: Sun, 3 Oct 2021 10:11:53 +0100
Subject: [bitcoin-dev] Mock introducing vulnerability in important
	Bitcoin projects
In-Reply-To: <Ml-IIuL--3-2@tutanota.de>
References: <MkZx3Hv--3-2@tutanota.de>
 <yp9mJ2Poc_Ce91RkrhjnTA3UPvdh0wUyw2QhRPZEyO3gPHZPhmnhqER_4b7ChvmRh8GcYVPEkoud6vamJ9lGlQPi-POF-kyimBWNHz2RH3A=@protonmail.com>
 <MkdYcV9--3-2@tutanota.de>
 <CAPv7TjbvRE-b33MeYucUfr6CTooCRSH42hwSn5dMiJ4LODATRQ@mail.gmail.com>
 <MktnWM7--3-2@tutanota.de>
 <qNjz-H23x07OJjnf5Try4Qp8l5s23SQxhEE8yAfNbrniN34u2vM72FVFSDJxHg4HNTL8tdcm-KKT8h6XVRwOwN0ZmckxzWiMlNFmLbMNuHc=@protonmail.com>
 <MkwZGYl--7-2@tutanota.de>
 <CAMnpzfrNZ0vpiMVoH=0KW9jy1-vppudX3D7Z+aXpSp4h_7s=zw@mail.gmail.com>
 <Ml-IIuL--3-2@tutanota.de>
Message-ID: <CAAxiurb1_-p2yO8183MvB2x_i9H+WAo9t0RH85faRrrKz9YxGg@mail.gmail.com>

Good morning everyone,

Just wanted to point out a few things for discussion which may or may not
be obvious:

1) A simple scheme as described by ZmnSCPxj first can lead way for a
standardized process where people can excuse their legitimate attempts to
actually introduce vulnerabilities, where they create the precommit and
then attempt to introduce the vulnerability. If it goes wrong they have
plausible deniability by revealing it and possibly saving their reputation.
2) A more complex scheme as described by Ryan (from my very rough
understanding) seems to imply a random selection of team for attempting the
attack, which might be limiting, since someone willing to do it and with
enough knowledge to attempt it properly might not be picked.

It seems to me that an ideal process would start from the will to attempt
it from one person (or group), which then by some process similar to what
Ryan described will pick at random a team of people to back up his claim to
be doing it in good faith. With that selection done, the initial person
would warn and gather from the randomly chosen participants a set of
signatures for a similar message as described by ZmnSCPxj and only then go
ahead with the attempt. This way you achieve:

- One person can initiate it at will.
- Other people (provably chosen at random) are insiders to that information
and have a shared precommit.
- You can't not reveal your intent in case it isn't caught, since other
randomly chosen people are in on it.
- You can't pick a specific group of people which might be willing to
collude with you to achieve a similar situation to 1).

Another important consideration is that depending on the size of the team
to be insiders, we might by chance deplete the relevant pool of outsiders
which would be adequate for reviewing the specific details of the
vulnerability being introduced.

Prayank via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> escreveu no
dia s?bado, 2/10/2021 ?(s) 10:20:

> This looks interesting although I don't understand few things:
>
> > The scheme should include public precommitments collected at ceremonial
> intervals.
>
> How would this work? Can you explain with an example please.
>
> > Upon assignment, the dev would have community approval to
> opportunistically insert a security flaw
>
> Who is doing the assignment?
>
> --
> Prayank
>
> A3B1 E430 2298 178F
>
>
>
> Oct 2, 2021, 01:45 by bitcoin-dev at rgrant.org:
>
> Due to the uneven reputation factor of various devs, and uneven review
> attention for new pull requests, this exercise would work best as a
> secret sortition.
>
> Sortition would encourage everyone to always be on their toes rather
> than only when dealing with new github accounts or declared Red Team
> devs. The ceremonial aspects would encourage more devs to participate
> without harming their reputation.
>
> https://en.wikipedia.org/wiki/Sortition
> https://en.wikipedia.org/wiki/Red_team
>
> The scheme should include public precommitments collected at
> ceremonial intervals.
>
> where:
> hash1 /* sortition ticket */ = double-sha256(secret)
> hash2 /* public precommitment */ = double-sha256(hash1)
>
> The random oracle could be block hashes. They could be matched to
> hash1, the sortition ticket. A red-team-concurrency difficulty
> parameter could control how many least-significant bits must match to
> be secretly selected. The difficulty parameter could be a matter of
> group consensus at the ceremonial intervals, based on a group decision
> on how much positive effect the Red Team exercise is providing.
>
> Upon assignment, the dev would have community approval to
> opportunistically insert a security flaw; which, when either caught,
> merged, or on timeout, they would reveal along with the sortition
> ticket that hashes to their public precommitment.
>
> Sortition Precommitment Day might be once or twice a year.
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211003/a5d700cb/attachment-0001.html>

From dustinpaystaxes at gmail.com  Sun Oct  3 09:53:00 2021
From: dustinpaystaxes at gmail.com (Dustin Dettmer)
Date: Sun, 3 Oct 2021 02:53:00 -0700
Subject: [bitcoin-dev] Interrogating a BIP157 server,
	BIP158 change proposal
In-Reply-To: <CADZtCSgKu1LvjePNPT=0C0UYQvb47Ca0YN+B_AfgVNTpcOno4w@mail.gmail.com>
References: <6D57649F-0236-4FBA-8376-4815F5F39E8A@gmail.com>
 <CADZtCSgKu1LvjePNPT=0C0UYQvb47Ca0YN+B_AfgVNTpcOno4w@mail.gmail.com>
Message-ID: <CABLeJxSrR84bA4OXk9wRG54agF=o8-svOyM55+xF3+-QB7jY_g@mail.gmail.com>

Jim Posen,

A few years ago you mentioned roastbeef?s proposal of a P2P message to
retrieve all prev-outputs for a given block:

1) Introduce a new P2P message to retrieve all prev-outputs for a given
> block (essentially the undo data in Core), and verify the scripts against
> the block by executing them. While this permits some forms of input script
> malleability (and thus cannot discriminate between all valid and invalid
> filters), it restricts what an attacker can do. This was proposed by Laolu
> AFAIK, and I believe this is how btcd is proceeding.
>

I?m trying to find the follow up on this. Was there discussion about it
under another name (thread, PR, bip etc)? Apologies if I?m being obtuse and
it?s easily found but for the life of me I can?t find any references.
Bip157 seems to not make any mention of it.

Thanks!
Dustin

>


If anyone has other ideas, I'd love to hear them.
>
> -jimpo
>
> [1]
> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-June/016057.html
>
>
>
> On Mon, Feb 4, 2019 at 10:53 AM Tamas Blummer via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> TLDR: a change to BIP158 would allow decision on which filter chain is
>> correct at lower bandwith use
>>
>> Assume there is a BIP157 client that learned a filter header chain
>> earlier and is now offered an alternate reality by a newly connected BIP157
>> server.
>>
>> The client notices the alternate reality by routinely asking for filter
>> chain checkpoints after connecting to a new BIP157 server. A divergence at
>> a checkpoint means that the server disagrees the client's history at or
>> before the first diverging checkpoint. The client would then request the
>> filter headers between the last matching and first divergent checkpoint,
>> and quickly figure which block?s filter is the first that does not match
>> previous assumption, and request that filter from the server.
>>
>> The client downloads the corresponding block, checks that its header fits
>> the PoW secured best header chain, re-calculates merkle root of its
>> transaction list to know that it is complete and queries the filter to see
>> if every output script of every transaction is contained in there, if not
>> the server is lying, the case is closed, the server disconnected.
>>
>> Having all output scripts in the filter does not however guarantee that
>> the filter is correct since it might omit input scripts. Inputs scripts are
>> not part of the downloaded block, but are in some blocks before that.
>> Checking those are out of reach for lightweight client with tools given by
>> the current BIP.
>>
>> A remedy here would be an other filter chain on created and spent
>> outpoints as is implemented currently by Murmel. The outpoint filter chain
>> must offer a match for every spent output of the block with the divergent
>> filter, otherwise the interrogated server is lying since a PoW secured
>> block can not spend coins out of nowhere. Doing this check would already
>> force the client to download the outpoint filter history up-to the point of
>> divergence. Then the client would have to download and PoW check every
>> block that shows a match in outpoints until it figures that one of the
>> spent outputs has a script that was not in the server?s filter, in which
>> case the server is lying. If everything checks out then the previous
>> assumption on filter history was incorrect and should be replaced by the
>> history offered by the interrogated server.
>>
>> As you see the interrogation works with this added filter but is highly
>> ineffective. A really light client should not be forced to download lots of
>> blocks just to uncover a lying filter server. This would actually be an
>> easy DoS on light BIP157 clients.
>>
>> A better solution is a change to BIP158 such that the only filter
>> contains created scripts and spent outpoints. It appears to me that this
>> would serve well both wallets and interrogation of filter servers well:
>>
>> Wallets would recognize payments to their addresses by the filter as
>> output scripts are included, spends from the wallet would be recognized as
>> a wallet already knows outpoints of its previously received coins, so it
>> can query the filters for them.
>>
>> Interrogation of a filter server also simplifies, since the filter of the
>> block can be checked entirely against the contents of the same block. The
>> decision on filter correctness does not require more bandwith then download
>> of a block at the mismatching checkpoint. The client could only be forced
>> at max. to download 1/1000 th of the blockchain in addition to the filter
>> header history.
>>
>> Therefore I suggest to change BIP158 to have a base filter, defined as:
>>
>> A basic filter MUST contain exactly the following items for each
>> transaction in a block:
>>         ? Spent outpoints
>>         ? The scriptPubKey of each output, aside from all OP_RETURN
>> output scripts.
>>
>> Tamas Blummer
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211003/d02b1381/attachment-0001.html>

From michaelfolkson at protonmail.com  Sun Oct  3 18:22:12 2021
From: michaelfolkson at protonmail.com (Michael Folkson)
Date: Sun, 03 Oct 2021 18:22:12 +0000
Subject: [bitcoin-dev] =?utf-8?q?Wednesday=E2=80=99s_second_BIP_process_me?=
	=?utf-8?q?eting?=
Message-ID: <R0G3u0C82WnrfMMhzmcHsSC1h4HpX_dYVxpkmO4Optg4q5BYxoEpuzXc0BwmohnarqWuRnQDYUbCRPpHc3gOnUzcAqyE9M3YtBoj1BKR-sI=@protonmail.com>

Wednesday?s second BIP process meeting was announced previously here [0].

A conversation log of the meeting is available here [1].

A summary of the first BIP process meeting is here [2].

The following is a summary of what was discussed.

1) The limits or possible downsides of pursuing maximal decentralization. Understandably there is a desire for greater decentralization as a central repo introduces bottlenecks and the need for maintainers or editors in the case of BIPs (prayank). However, zero filters creates a Ethereum style bewildering number of BIPs of varying quality that all need to be stored and maintained. The option of being able to store a BIP in any repo doesn?t appear to offer material upside (michaelfolkson). It still needs to get a BIP number from the BIP editors and if the alternative repo is deleted or the BIP champion becomes unresponsive there is the problem of changing the location of where the BIP is stored. It is much easier to monitor a single repo rather than an infinite number of repos that contain BIPs.

2) Past motivations for introducing alternative parallel processes to the BIPs (e.g. BOLTs, SLIPs). Anyone is free to set up a repo, add documentation to that repo or even set up a parallel process to the BIPs. However, if as a community we?d like to prevent unnecessary splintering it is good to understand why certain documents that should be BIPed aren?t being BIPed. Obviously not every document needs or should be BIPed. There are many docs that aren?t BIPs that are hugely valuable. One historical concern that was raised (ChristopherA) was regarding why BIP 48 didn?t happen and whether it was because it was coming from the wallet community and not a Bitcoin Core proposal. luke-jr said after the meeting that from recollection the reason it didn?t happen was merely that it was never written [3]. It was also discussed afterwards whether there was/is a rationale for Lightning BOLTs not being BIPs and whether they should be BIPs in future.

3) Kalle Alm?s proposal for BIP extensions [4] was discussed. Participants seemed to be in favor of the proposal though further clarity on when they would and wouldn?t be used was requested. A BIP extension for the bech32m update (BIP 350) to bech32 (BIP 173) seems like it would have been a good use case though further discussion is probably needed on whether they should be used for soft fork activation parameters. It was suggested that using dates and/or short extension names would be superior to extension numbers as numbers suggest an ordering (ChristopherA). The extension 2 may also be perceived as somehow replacing extension 1.

Thanks to the participants of both BIP process meetings. Further discussion is welcome on the #bitcoin-dev Libera IRC channel and hopefully we will see progress in the coming weeks and months on a proposed BIP 3 [5] update.

[0]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019412.html

[1]: https://gist.github.com/michaelfolkson/84000ee3fe45c034ac12a7a54ff5fcdd

[2]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019469.html

[3]: https://github.com/bitcoin/bips/pull/253

[4]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019457.html

[5]: https://github.com/bitcoin/bips/pull/1015

--

Michael Folkson
Email: michaelfolkson at protonmail.com
Keybase: michaelfolkson
PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211003/f0e74574/attachment.html>

From luke at dashjr.org  Sun Oct  3 21:33:43 2021
From: luke at dashjr.org (Luke Dashjr)
Date: Sun, 3 Oct 2021 21:33:43 +0000
Subject: [bitcoin-dev] Mock introducing vulnerability in important
	Bitcoin projects
In-Reply-To: <CAAxiurb1_-p2yO8183MvB2x_i9H+WAo9t0RH85faRrrKz9YxGg@mail.gmail.com>
References: <MkZx3Hv--3-2@tutanota.de> <Ml-IIuL--3-2@tutanota.de>
 <CAAxiurb1_-p2yO8183MvB2x_i9H+WAo9t0RH85faRrrKz9YxGg@mail.gmail.com>
Message-ID: <202110032133.44726.luke@dashjr.org>

All attempts are harmful, no matter the intent, in that they waste 
contributors' time that could be better spent on actual development.

However, I do also see the value in studying and improving the review process 
to harden it against such inevitable attacks. The fact that we know the NSA 
engages in such things, and haven't caught one yet should be a red flag.

Therefore, I think any such a scheme needs to be at least opt-out, if not 
opt-in. Please ensure there's a simple way for developers with limited time 
(or other reasons) to be informed of which PRs to ignore to opt-out of this 
study. (Ideally it would also prevent maintainers from merging - maybe 
possible since we use a custom merging script, but what it really needs to 
limit is the push, not the dry-run.)

Luke


On Sunday 03 October 2021 09:11:53 Manuel Costa via bitcoin-dev wrote:
> Good morning everyone,
>
> Just wanted to point out a few things for discussion which may or may not
> be obvious:
>
> 1) A simple scheme as described by ZmnSCPxj first can lead way for a
> standardized process where people can excuse their legitimate attempts to
> actually introduce vulnerabilities, where they create the precommit and
> then attempt to introduce the vulnerability. If it goes wrong they have
> plausible deniability by revealing it and possibly saving their reputation.
> 2) A more complex scheme as described by Ryan (from my very rough
> understanding) seems to imply a random selection of team for attempting the
> attack, which might be limiting, since someone willing to do it and with
> enough knowledge to attempt it properly might not be picked.
>
> It seems to me that an ideal process would start from the will to attempt
> it from one person (or group), which then by some process similar to what
> Ryan described will pick at random a team of people to back up his claim to
> be doing it in good faith. With that selection done, the initial person
> would warn and gather from the randomly chosen participants a set of
> signatures for a similar message as described by ZmnSCPxj and only then go
> ahead with the attempt. This way you achieve:
>
> - One person can initiate it at will.
> - Other people (provably chosen at random) are insiders to that information
> and have a shared precommit.
> - You can't not reveal your intent in case it isn't caught, since other
> randomly chosen people are in on it.
> - You can't pick a specific group of people which might be willing to
> collude with you to achieve a similar situation to 1).
>
> Another important consideration is that depending on the size of the team
> to be insiders, we might by chance deplete the relevant pool of outsiders
> which would be adequate for reviewing the specific details of the
> vulnerability being introduced.
>
> Prayank via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> escreveu no
>
> dia s?bado, 2/10/2021 ?(s) 10:20:
> > This looks interesting although I don't understand few things:
> > > The scheme should include public precommitments collected at ceremonial
> >
> > intervals.
> >
> > How would this work? Can you explain with an example please.
> >
> > > Upon assignment, the dev would have community approval to
> >
> > opportunistically insert a security flaw
> >
> > Who is doing the assignment?
> >
> > --
> > Prayank
> >
> > A3B1 E430 2298 178F
> >
> >
> >
> > Oct 2, 2021, 01:45 by bitcoin-dev at rgrant.org:
> >
> > Due to the uneven reputation factor of various devs, and uneven review
> > attention for new pull requests, this exercise would work best as a
> > secret sortition.
> >
> > Sortition would encourage everyone to always be on their toes rather
> > than only when dealing with new github accounts or declared Red Team
> > devs. The ceremonial aspects would encourage more devs to participate
> > without harming their reputation.
> >
> > https://en.wikipedia.org/wiki/Sortition
> > https://en.wikipedia.org/wiki/Red_team
> >
> > The scheme should include public precommitments collected at
> > ceremonial intervals.
> >
> > where:
> > hash1 /* sortition ticket */ = double-sha256(secret)
> > hash2 /* public precommitment */ = double-sha256(hash1)
> >
> > The random oracle could be block hashes. They could be matched to
> > hash1, the sortition ticket. A red-team-concurrency difficulty
> > parameter could control how many least-significant bits must match to
> > be secretly selected. The difficulty parameter could be a matter of
> > group consensus at the ceremonial intervals, based on a group decision
> > on how much positive effect the Red Team exercise is providing.
> >
> > Upon assignment, the dev would have community approval to
> > opportunistically insert a security flaw; which, when either caught,
> > merged, or on timeout, they would reveal along with the sortition
> > ticket that hashes to their public precommitment.
> >
> > Sortition Precommitment Day might be once or twice a year.
> >
> >
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev at lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


From ZmnSCPxj at protonmail.com  Mon Oct  4 03:59:34 2021
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Mon, 04 Oct 2021 03:59:34 +0000
Subject: [bitcoin-dev] Mock introducing vulnerability in important
	Bitcoin projects
In-Reply-To: <202110032133.44726.luke@dashjr.org>
References: <MkZx3Hv--3-2@tutanota.de> <Ml-IIuL--3-2@tutanota.de>
 <CAAxiurb1_-p2yO8183MvB2x_i9H+WAo9t0RH85faRrrKz9YxGg@mail.gmail.com>
 <202110032133.44726.luke@dashjr.org>
Message-ID: <sez9AuvBEnKKkLkJ4aivnaLJz5M5VFz3yTOdreTGmFb6RzwMv7h0dRFbEiB1_aup4Daw7t9YwlZKp2YvbgCu1fzym28cHhlzRVC3efmfBpE=@protonmail.com>


Good morning Luke,

> All attempts are harmful, no matter the intent, in that they waste
> contributors' time that could be better spent on actual development.
>
> However, I do also see the value in studying and improving the review process
> to harden it against such inevitable attacks. The fact that we know the NSA
> engages in such things, and haven't caught one yet should be a red flag.

Indeed, I believe we should take the position that "review process is as much a part of the code as the code itself, and should be tested regularly".

> Therefore, I think any such a scheme needs to be at least opt-out, if not
> opt-in. Please ensure there's a simple way for developers with limited time
> (or other reasons) to be informed of which PRs to ignore to opt-out of this
> study. (Ideally it would also prevent maintainers from merging - maybe
> possible since we use a custom merging script, but what it really needs to
> limit is the push, not the dry-run.)

Assuming developers are normal humans with typical human neurology (in particular a laziness circuit), perhaps this would work?

Every commit message is required to have a pair of 256-bit hex words.

Public attempts at attack / testing of the review process will use the first 256-bit as a salt, and when the salt is prepended to the string "THIS IS AN ATTACK" and then hashed with e.g. SHA256, should result in the second 256-bit word.

Non-attacks / normal commits just use random 256-bit numbers.

Those opting-out to this will run a script that checks commit messages for whether the first 256-bit hexword concatenated with "THIS IS AN ATTACK", then hashed, is the second 256-bit hexword.

Those opting-in will not run that script and ignore the numbers.

The script can be run as well at the maintainer.

Hopefully, people who are not deliberately opting out will be too lazy to run the script (as is neurotypical for humans) and getting "spoilered" on this.

***HOWEVER***

We should note that a putative NSA attack would of course not use the above protocol, and thus no developer can ever opt out of an NSA attempt at inserting vulnerabilities; thus, I think it is better if all developers are forced to opt in on the "practice rounds", as they cannot opt out of "the real thing" other than to stop developing entirely.

Regards,
ZmnSCPxj

From contact at bitcoineducation.site  Tue Oct  5 00:23:38 2021
From: contact at bitcoineducation.site (Humberto Marcolino)
Date: Mon, 4 Oct 2021 21:23:38 -0300
Subject: [bitcoin-dev] bitcoin-java, a new bitcoin library
Message-ID: <CAOauhp05aFgt--XOG9SGjY=YrFzgKEu+VnL1C3iWY0uJ+223BQ@mail.gmail.com>

Hello,

My name is Humberto, owner of the repository
https://github.com/bitcoin-education/bitcoin-java.

I'm posting to divulge a new open-source Bitcoin library written in Java,
with support for taproot single key transactions:
https://github.com/bitcoin-education/bitcoin-java.

My main motivation to build this library was for educational purposes.
Also, I was missing a lean bitcoin library written in Java, since I think
bitcoinj too feature-heavy. I don't intend to include features that allow
communication with nodes nor any online features in it. I think it is ideal
for developers that want to build a wallet in Java.

Feedback, PRs, and issues are welcome and appreciated.

Website of the project, where I plan to post more examples using it:
https://www.bitcoineducation.site/

Best regards,

Humberto
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211004/da58dc80/attachment.html>

From MOHAMED_Y at msn.com  Tue Oct  5 09:40:42 2021
From: MOHAMED_Y at msn.com (Mohamed Youssef)
Date: Tue, 5 Oct 2021 09:40:42 +0000
Subject: [bitcoin-dev] bitcoin-java, a new bitcoin library
In-Reply-To: <CAOauhp05aFgt--XOG9SGjY=YrFzgKEu+VnL1C3iWY0uJ+223BQ@mail.gmail.com>
References: <CAOauhp05aFgt--XOG9SGjY=YrFzgKEu+VnL1C3iWY0uJ+223BQ@mail.gmail.com>
Message-ID: <PH0PR10MB4647F97163AFB994443461BB84AF9@PH0PR10MB4647.namprd10.prod.outlook.com>

Thanks for posting; I have been looking for such a library to get into the technical details.
Do you plan a slack or IRC for collaboration?

Best,
Mohamed

From: bitcoin-dev <bitcoin-dev-bounces at lists.linuxfoundation.org> On Behalf Of Humberto Marcolino via bitcoin-dev
Sent: Tuesday, October 5, 2021 08:24 AM
To: bitcoin-dev at lists.linuxfoundation.org
Subject: [bitcoin-dev] bitcoin-java, a new bitcoin library

Hello,

My name is Humberto, owner of the repository https://github.com/bitcoin-education/bitcoin-java.

I'm posting to divulge a new open-source Bitcoin library written in Java, with support for taproot single key transactions: https://github.com/bitcoin-education/bitcoin-java.

My main motivation to build this library was for educational purposes. Also, I was missing a lean bitcoin library written in Java, since I think bitcoinj too feature-heavy. I don't intend to include features that allow communication with nodes nor any online features in it. I think it is ideal for developers that want to build a wallet in Java.

Feedback, PRs, and issues are welcome and appreciated.

Website of the project, where I plan to post more examples using it: https://www.bitcoineducation.site/

Best regards,

Humberto
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211005/77f2fa9e/attachment.html>

From nta at nathanalexander.net  Tue Oct  5 12:23:09 2021
From: nta at nathanalexander.net (Nathan T Alexander)
Date: Tue, 5 Oct 2021 07:23:09 -0500
Subject: [bitcoin-dev] Question- must every mining rig attempt every block?
Message-ID: <f867f949-9a04-329b-ea1b-26201f46d2ab@nathanalexander.net>

For purposes of conserving energy, couldn't each mining rig have some 
non-gameable attribute which would be used to calculate if a block would 
be accepted by that rig?

Don't the mining rigs have to be able to identify themselves to the 
network somehow, in order to claim their block reward? Could their 
bitcoin network ID be used as a non-gameable attribute?

Essentially a green light / red light system. In order for a block to be 
accepted by the network, it must have all attributes of a successful 
block today, and it must also have come from a rig that had a green light.

Perhaps hash some data from the last successful block, along with the 
miners non-gameable attribute, and if it's below a certain number set by 
algorithm, the miner gets a green light to race to produce a valid block.

Nathan Alexander

Arlington, TX


From ZmnSCPxj at protonmail.com  Tue Oct  5 14:41:24 2021
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Tue, 05 Oct 2021 14:41:24 +0000
Subject: [bitcoin-dev] Question- must every mining rig attempt every
	block?
In-Reply-To: <f867f949-9a04-329b-ea1b-26201f46d2ab@nathanalexander.net>
References: <f867f949-9a04-329b-ea1b-26201f46d2ab@nathanalexander.net>
Message-ID: <vcRuCxMO8Ia9VU7JhJtcpea_2vpfUnOSPBJJExuyHfu7j2IQBM7pEonfGIF96RLSpNP5h-SoA3sStOY2Y-wdBHEt6Ymo_Z6z2xfzUljI5Sw=@protonmail.com>

Good morning Nathan,

> For purposes of conserving energy, couldn't each mining rig have some
> non-gameable attribute which would be used to calculate if a block would
> be accepted by that rig?
>
> Don't the mining rigs have to be able to identify themselves to the
> network somehow, in order to claim their block reward? Could their
> bitcoin network ID be used as a non-gameable attribute?

They are "identified" by the address that is on the coinbase output.

There is nothing preventing a *single* miner having *multiple* addresses, in much the same way that a *single* HODLer is not prevented from having *multiple* addresses.

>
> Essentially a green light / red light system. In order for a block to be
> accepted by the network, it must have all attributes of a successful
> block today, and it must also have come from a rig that had a green light.

Since a miner can have multiple addresses, the miners can game this by simply grinding on *which* of their multiple addresses gets the green light.
That grinding is no more different in quality than grinding the block hash.

Thus, you just move proof-of-work elsewhere and make it harder to see, not reduce it.


Worse, *identifying* miners reduces the important anonymity property of mining.
With non-anonymous mining, it is much easier for states to co-opt large mines, since they are identifiable, and states can target larger miners.
Thus, miners ***must*** use multiple addresses as a simple protection against state co-option.

>
> Perhaps hash some data from the last successful block, along with the
> miners non-gameable attribute, and if it's below a certain number set by
> algorithm, the miner gets a green light to race to produce a valid block.

The power consumption of proof-of-work ***is not a problem***, it is instead the solution against state co-option.

If you reduce the power consumption, it becomes easier for states to simply purchase and co-opt mines and attack the system, since it is easier to muster the power consumption and outright 51% Bitcoin.
The power consumption is an important security parameter, ***even more important than raw hashes-per-second***, since hashes-per-second will inevitably rise anyway even with constant power consumption.

It should always remain economically infeasible to 51% Bitcoin, otherwise Bitcoin will ***die*** and all your HODLings in it.

Regards,
ZmnSCPxj

From rsomsen at gmail.com  Tue Oct  5 14:42:40 2021
From: rsomsen at gmail.com (Ruben Somsen)
Date: Tue, 5 Oct 2021 16:42:40 +0200
Subject: [bitcoin-dev] Question- must every mining rig attempt every
	block?
In-Reply-To: <f867f949-9a04-329b-ea1b-26201f46d2ab@nathanalexander.net>
References: <f867f949-9a04-329b-ea1b-26201f46d2ab@nathanalexander.net>
Message-ID: <CAPv7TjZA2KYeVnT6PE6UHXwPy__E-VOvjmYD1207f1CgBRhLZg@mail.gmail.com>

Hi Nathan,

That's a fair question, but note that we've already had a bunch of "green
mining" related posts a few months ago, so I suspect you'll be able to find
many criticisms to this idea in the following thread:

https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-May/018937.html

It also looks like you'll be able to find some related answers on Bitcoin
Stack Exchange:
https://bitcoin.stackexchange.com/questions/106308/decreasing-energy-consumption-of-bitcoins-pow-with-paired-mining-rounds

And generally speaking these types of discussions don't end up being very
fruitful for bitcoin-dev, because these are the types of changes that are
unlikely to ever be seriously considered for Bitcoin.

Cheers,
Ruben

On Tue, Oct 5, 2021 at 4:09 PM Nathan T Alexander via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> For purposes of conserving energy, couldn't each mining rig have some
> non-gameable attribute which would be used to calculate if a block would
> be accepted by that rig?
>
> Don't the mining rigs have to be able to identify themselves to the
> network somehow, in order to claim their block reward? Could their
> bitcoin network ID be used as a non-gameable attribute?
>
> Essentially a green light / red light system. In order for a block to be
> accepted by the network, it must have all attributes of a successful
> block today, and it must also have come from a rig that had a green light.
>
> Perhaps hash some data from the last successful block, along with the
> miners non-gameable attribute, and if it's below a certain number set by
> algorithm, the miner gets a green light to race to produce a valid block.
>
> Nathan Alexander
>
> Arlington, TX
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211005/82c3f918/attachment.html>

From bitcoin-dev at notmandatory.org  Tue Oct  5 15:47:16 2021
From: bitcoin-dev at notmandatory.org (Steve Myers)
Date: Tue, 05 Oct 2021 08:47:16 -0700
Subject: [bitcoin-dev] bitcoin-java, a new bitcoin library
In-Reply-To: <CAOauhp05aFgt--XOG9SGjY=YrFzgKEu+VnL1C3iWY0uJ+223BQ@mail.gmail.com>
References: <CAOauhp05aFgt--XOG9SGjY=YrFzgKEu+VnL1C3iWY0uJ+223BQ@mail.gmail.com>
Message-ID: <daf20c51-2812-40de-9a15-c522ec08bb35@www.fastmail.com>

Ciao Humberto! building a new library is a great way to learn about bitcoin, you should also take a look at the Bitcoin Dev Kit project (https://github.com/bitcoindevkit) where we're building support for Kotlin/Java (and Android) and eventually Swift and iOS. You don't need to know Rust (though it's a great language to learn! ) and we'd love to have new folks join us as we develop the language bindings which is also a great way to learn the primitives for on-chain Bitcoin. 

Steve

On Mon, Oct 4, 2021, at 5:23 PM, Humberto Marcolino via bitcoin-dev wrote:
> Hello,
> 
> My name is Humberto, owner of the repository https://github.com/bitcoin-education/bitcoin-java.
> 
> I'm posting to divulge a new open-source Bitcoin library written in Java, with support for taproot single key transactions: https://github.com/bitcoin-education/bitcoin-java.
> 
> My main motivation to build this library was for educational purposes. Also, I was missing a lean bitcoin library written in Java, since I think bitcoinj too feature-heavy. I don't intend to include features that allow communication with nodes nor any online features in it. I think it is ideal for developers that want to build a wallet in Java.
> 
> Feedback, PRs, and issues are welcome and appreciated.
> 
> Website of the project, where I plan to post more examples using it: https://www.bitcoineducation.site/
> 
> Best regards,
> 
> Humberto
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211005/6f215d70/attachment-0001.html>

From contact at bitcoineducation.site  Tue Oct  5 22:58:41 2021
From: contact at bitcoineducation.site (Humberto Marcolino)
Date: Tue, 5 Oct 2021 19:58:41 -0300
Subject: [bitcoin-dev] bitcoin-java, a new bitcoin library
In-Reply-To: <PH0PR10MB4647F97163AFB994443461BB84AF9@PH0PR10MB4647.namprd10.prod.outlook.com>
References: <CAOauhp05aFgt--XOG9SGjY=YrFzgKEu+VnL1C3iWY0uJ+223BQ@mail.gmail.com>
 <PH0PR10MB4647F97163AFB994443461BB84AF9@PH0PR10MB4647.namprd10.prod.outlook.com>
Message-ID: <CAOauhp0bhAyPrnyDLgE_8jtbeVDLKuhcg6UyQ8WOZXuSw3_qQA@mail.gmail.com>

Hi Mohamed,

I created a discord server for the project, here is the link:
https://discord.gg/vXK9JbCtvZ

Humberto

Em ter., 5 de out. de 2021 ?s 06:40, Mohamed Youssef <MOHAMED_Y at msn.com>
escreveu:

> Thanks for posting; I have been looking for such a library to get into the
> technical details.
>
> Do you plan a slack or IRC for collaboration?
>
>
>
> Best,
>
> Mohamed
>
>
>
> *From:* bitcoin-dev <bitcoin-dev-bounces at lists.linuxfoundation.org> *On
> Behalf Of *Humberto Marcolino via bitcoin-dev
> *Sent:* Tuesday, October 5, 2021 08:24 AM
> *To:* bitcoin-dev at lists.linuxfoundation.org
> *Subject:* [bitcoin-dev] bitcoin-java, a new bitcoin library
>
>
>
> Hello,
>
>
>
> My name is Humberto, owner of the repository
> https://github.com/bitcoin-education/bitcoin-java.
>
>
>
> I'm posting to divulge a new open-source Bitcoin library written in Java,
> with support for taproot single key transactions:
> https://github.com/bitcoin-education/bitcoin-java.
>
>
>
> My main motivation to build this library was for educational purposes.
> Also, I was missing a lean bitcoin library written in Java, since I think
> bitcoinj too feature-heavy. I don't intend to include features that allow
> communication with nodes nor any online features in it. I think it is ideal
> for developers that want to build a wallet in Java.
>
>
>
> Feedback, PRs, and issues are welcome and appreciated.
>
>
>
> Website of the project, where I plan to post more examples using it:
> https://www.bitcoineducation.site/
>
>
>
> Best regards,
>
>
>
> Humberto
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211005/4d9e7af9/attachment.html>

From prayank at tutanota.de  Wed Oct  6 04:02:55 2021
From: prayank at tutanota.de (Prayank)
Date: Wed, 6 Oct 2021 06:02:55 +0200 (CEST)
Subject: [bitcoin-dev]
 =?utf-8?q?Wednesday=E2=80=99s_second_BIP_process_me?=
 =?utf-8?q?eting?=
Message-ID: <MlIlAqo--3-2@tutanota.de>

Good morning Michael,

Thanks for sharing the summary about BIP process meeting. 

> However, zero filters creates a Ethereum style bewildering number of BIPs of varying quality that all need to be stored and maintained. The option of being able to store a BIP in any repo doesn?t appear to offer material upside (michaelfolkson). It still needs to get a BIP number from the BIP editors and if the alternative repo is deleted or the BIP champion becomes unresponsive there is the problem of changing the location of where the BIP is stored. It is much easier to monitor a single repo rather than an infinite number of repos that contain BIPs.

1.I want to avoid mentioning projects that are not decentralized however the thing you mentioned is a feature not a bug. Neither anyone needs "quality" certificates from anyone nor approval. People are free to propose anything as improvement for Bitcoin. What gets implemented is a different thing. Also BIP number doesn't make something legit, BIPs can have any names. Example: If I ever create draft a proposal to improve Bitcoin, it will be in my own repository and with a unique name.

2.I am surprised that few influential developers that wanted to improve BIP process earlier by making it more decentralized were not present in either meeting. Also no follow up here on mailing list. So decentralization was only required when you had some issues with Luke Dashjr? Few things are so obvious that even a newbie who starts researching about Bitcoin from today can observe such things.

I tried my best to ask more people to participate in the meeting by tweeting, requested Christopher to attend the meeting and share his thoughts. Thanks everyone who was part of this meeting.


-- 
Prayank

A3B1 E430 2298 178F
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211006/751c44e4/attachment.html>

From giacomo.caironi at gmail.com  Wed Oct  6 20:35:51 2021
From: giacomo.caironi at gmail.com (Giacomo Caironi)
Date: Wed, 6 Oct 2021 22:35:51 +0200
Subject: [bitcoin-dev] Test cases for Taproot signature message
In-Reply-To: <CACHAfwfPTQvUwzqs1mg3Z-FwtcuwGgJfBeK6r0ovtRZKB=rA5A@mail.gmail.com>
References: <CACHAfwcJrf8kc9+=2+ekjuPTPjW8T6qJS538QQ2DJedAn-XxKA@mail.gmail.com>
 <NgpYOVuE_3u6zfAZI6cxpc7iB5L_cGtTUrdCJfSdRgChJxOXsY3w0veIk0ZayeEvSeu3SE4AX_E27C6-Yu3MjCFJzMO6AR9g_1CLMJYVG1o=@wuille.net>
 <CACHAfwfPTQvUwzqs1mg3Z-FwtcuwGgJfBeK6r0ovtRZKB=rA5A@mail.gmail.com>
Message-ID: <CACHAfwfOdhW6HvPs=EgQ-r1maWST_XyT+LM9Z1wh6Th31QUtqg@mail.gmail.com>

The related pull request is now open
https://github.com/bitcoin/bips/pull/1191

Il giorno sab 18 set 2021 alle ore 13:32 Giacomo Caironi <
giacomo.caironi at gmail.com> ha scritto:

> Ok I have created three test cases, you can find them here
> <https://gist.github.com/giacomocaironi/e41a45195b2ac6863ec46e8f86324757>.
> They cover most of the SigMsg function but they don't cover the ext_flag,
> so they are only for taproot key path; but if you want to test for script
> paths you have to implement more than this function so you would use the
> official test vector.
> Could someone please take a look at them? I think that they are right but
> I am not too sure
>
> Il giorno ven 17 set 2021 alle ore 00:30 Pieter Wuille <
> bitcoin-dev at wuille.net> ha scritto:
>
>> On Thursday, September 16th, 2021 at 5:36 PM, Giacomo Caironi via
>> bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
>>
>> Hi,
>> recently I have worked on a python implementation of bitcoin signature
>> messages, and I have found that there was way better documentation about
>> Segwit signature message than Taproot.
>>
>> 1) Segwit signature message got its own BIP, completed with test cases
>> regarding only that specific function; Taproot on the other hand has the
>> signature message function defined in BIP 341 and the test vectors in a
>> different BIP (341). This is confusing. Shouldn't we create a different BIP
>> only for Taproot signature message exactly like Segwit?
>>
>>
>> I'm not entirely sure what you mean; you're saying BIP 341 twice.
>>
>> Still, you're right overall - there is no separate BIP for the signature
>> message function. The reason is that the message function is different for
>> BIP341 and BIP342. BIP 341 defines a basic common message function, which
>> is then built up for BIP 341 key path spending, and for BIP 342 tapscript
>> spending. This common part could have been a separate BIP, but that'd still
>> not be a very clean separation. I'm not very inclined to support changing
>> that at this point, given the state of deployment the BIPs have, but that
>> doesn't mean the documentation/vectors can't be improved in the existing
>> documents.
>>
>> 2) The test vectors for Taproot have no documentation and, most
>> importantly, they are not atomic, in the sense that they do not target a
>> specific part of the taproot code but all of it. This may not be a very big
>> problem, but for signature verification it is. Because there are hashes
>> involved, we can't really debug why a signature message doesn't pass
>> validation, either it is valid or it is not. BIP 143 in this case is really
>> good, because it provides hash preimages, so it is possible to debug the
>> function and see where something went wrong. Because of this, writing the
>> Segwit signature hash function took a fraction of the time compared to
>> Taproot.
>>
>>
>> You're right. The existing tests are really intended for verifying an
>> implementation against (and for making sure future code changes don't break
>> anything). They have much higher coverage than the segwit tests had. But
>> they aren't useful as documentation; the code that generates them (
>> https://github.com/bitcoin/bitcoin/blob/v22.0/test/functional/feature_taproot.py#L605L1122)
>> is probably better at that even, but still pretty dense.
>>
>> If this idea is accepted I will be more than happy to write the test
>> cases for Taproot.
>>
>>
>> If you're interested in writing test vectors that are more aimed at
>> helping debugging issues, by all means, do. You've already brought up the
>> sighash code as an example. Another idea, primarily aimed at developers of
>> signing code, is test vectors for certain P2TR scriptPubKeys, derived from
>> certain internal keys and script trees. I'm happy to help to integrate such
>> in Bitcoin Core and the BIP(s).
>>
>> Thanks!
>>
>> Cheers,
>>
>> --
>> Pieter
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211006/8e1a35c2/attachment.html>

From ZmnSCPxj at protonmail.com  Thu Oct  7 04:52:01 2021
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Thu, 07 Oct 2021 04:52:01 +0000
Subject: [bitcoin-dev] [Lightning-dev]   Removing the Dust Limit
In-Reply-To: <CAJowKgKt=yYdNOYYNsWh7FJ2EH7rz0bd2EjUjmyA=cA6k5cvUQ@mail.gmail.com>
References: <CAD5xwhjFBjvkMKev_6HFRuRGcZUi7WjO5d963GNXWN4n-06Pqg@mail.gmail.com>
 <20210808215101.wuaidu5ww63ajx6h@ganymede>
 <MkPutJpff5rqUxXFQrEyHZl6Iz0DfrJU_-BQD-y0El65GQFnj7igVfmWU79fPCtiFztUYl4ofzrqeaN0HFMB45YPErY9rYY7_h1XkuTMfvc=@wuille.net>
 <CAJowKgKt=yYdNOYYNsWh7FJ2EH7rz0bd2EjUjmyA=cA6k5cvUQ@mail.gmail.com>
Message-ID: <BtaljKLqpe75GB6pHEPQMF6_L-hBaE0ZCBGaXrUfnHRYeEbCqFWZ12DaMRm5jEADceL3uPfCiL-WU9MOZJ_m54Zi3Pzu0vSFN3nQvuSKvBM=@protonmail.com>

Good morning e,

> mostly thinking out loud
>
> suppose there is a "lightweight" node:
>
> 1.  ignores utxo's below the dust limit
> 2.  doesn't validate dust tx
> 3.  still validates POW, other tx, etc.
>
>     these nodes could possibly get forked - accepting a series of valid,
>     mined blocks where there is an invalid but ignored dust tx, however
>     this attack seems every bit as expensive as a 51% attack

How would such a node treat a transaction that spends multiple dust UTXOs and creates a single non-dust UTXO out of them (after fees)?
Is it valid (to such a node) or not?

I presume from #1 it never stores dust UTXOs, so the node cannot know if the UTXO being spent by such a tx is spending dust, or trying to spend an already-spent TXO, or even inventing a TXO out of `/dev/random`.

Regards,
ZmnSCPxj

From ZmnSCPxj at protonmail.com  Thu Oct  7 10:01:53 2021
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Thu, 07 Oct 2021 10:01:53 +0000
Subject: [bitcoin-dev] [Lightning-dev] Removing the Dust Limit
In-Reply-To: <CAM98U8nSOQ9HpdRLBdkcFAdToW=z7_EhnYisMb7F46ExmJkT-w@mail.gmail.com>
References: <CAD5xwhjFBjvkMKev_6HFRuRGcZUi7WjO5d963GNXWN4n-06Pqg@mail.gmail.com>
 <20210808215101.wuaidu5ww63ajx6h@ganymede>
 <MkPutJpff5rqUxXFQrEyHZl6Iz0DfrJU_-BQD-y0El65GQFnj7igVfmWU79fPCtiFztUYl4ofzrqeaN0HFMB45YPErY9rYY7_h1XkuTMfvc=@wuille.net>
 <CAJowKgKt=yYdNOYYNsWh7FJ2EH7rz0bd2EjUjmyA=cA6k5cvUQ@mail.gmail.com>
 <BtaljKLqpe75GB6pHEPQMF6_L-hBaE0ZCBGaXrUfnHRYeEbCqFWZ12DaMRm5jEADceL3uPfCiL-WU9MOZJ_m54Zi3Pzu0vSFN3nQvuSKvBM=@protonmail.com>
 <CAM98U8nSOQ9HpdRLBdkcFAdToW=z7_EhnYisMb7F46ExmJkT-w@mail.gmail.com>
Message-ID: <ZKZ4RR6uAv0mG8yFQyRkWDTFQ7JnsGVfLQcMTmsc5ui7MTJXuzMkVk5YQTniPuc4F_KRhn7BEZZHEK60IZYZYU9A1r-tbmfPTnIs0pOd7oU=@protonmail.com>

Good morning shymaa

> If u allow me to discuss,,,
> I previously suggested storing dust UTXOS in a separate Merkle tree or strucutre in general if we are talking about the original set.
> I'm a kind of person who doesn't like to throw any thing; if it's not needed now keep it in the basement for example.?
> So, if dust UTXOS making a burden keep them in secondary storage, where in such cases u can verify then delete them.

While this technique helps reduce *average* CPU cost, it does not reduce *worst-case* CPU cost (and if the secondary storage trades off to gain increased capacity per satoshi by sacrificing speed, it can worse the worst-case time).

It is helpful to remember that attacks will always target worst-case behavior.
This is why quicksort is strongly disrecommended for processing data coming from external sources, its worst-case time is O(n^2).
And we should switch to algorithms like mergesort or similar whose average times are generally worse than quicksort but have the major advantage of keeping an O(n log n) worst-case.

Moving data we think is unlikely to be referenced to secondary storage (presumably in a construction that is slower but gets more storage per economic unit) moves us closer to quicksort than mergesort, and we should avoid quicksort-like solutions as it is always the worst-case behavior that is targeted in attacks.

Regards,
ZmnSCPxj

From shymaa.arafat at gmail.com  Thu Oct  7 09:13:33 2021
From: shymaa.arafat at gmail.com (shymaa arafat)
Date: Thu, 7 Oct 2021 11:13:33 +0200
Subject: [bitcoin-dev] [Lightning-dev] Removing the Dust Limit
In-Reply-To: <BtaljKLqpe75GB6pHEPQMF6_L-hBaE0ZCBGaXrUfnHRYeEbCqFWZ12DaMRm5jEADceL3uPfCiL-WU9MOZJ_m54Zi3Pzu0vSFN3nQvuSKvBM=@protonmail.com>
References: <CAD5xwhjFBjvkMKev_6HFRuRGcZUi7WjO5d963GNXWN4n-06Pqg@mail.gmail.com>
 <20210808215101.wuaidu5ww63ajx6h@ganymede>
 <MkPutJpff5rqUxXFQrEyHZl6Iz0DfrJU_-BQD-y0El65GQFnj7igVfmWU79fPCtiFztUYl4ofzrqeaN0HFMB45YPErY9rYY7_h1XkuTMfvc=@wuille.net>
 <CAJowKgKt=yYdNOYYNsWh7FJ2EH7rz0bd2EjUjmyA=cA6k5cvUQ@mail.gmail.com>
 <BtaljKLqpe75GB6pHEPQMF6_L-hBaE0ZCBGaXrUfnHRYeEbCqFWZ12DaMRm5jEADceL3uPfCiL-WU9MOZJ_m54Zi3Pzu0vSFN3nQvuSKvBM=@protonmail.com>
Message-ID: <CAM98U8nSOQ9HpdRLBdkcFAdToW=z7_EhnYisMb7F46ExmJkT-w@mail.gmail.com>

If u allow me to discuss,,,
I previously suggested storing dust UTXOS in a separate Merkle tree or
strucutre in general if we are talking about the original set.
I'm a kind of person who doesn't like to throw any thing; if it's not
needed now keep it in the basement for example.
So, if dust UTXOS making a burden keep them in secondary storage, where in
such cases u can verify then delete them.



On Thu, Oct 7, 2021, 06:52 ZmnSCPxj via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Good morning e,
>
> > mostly thinking out loud
> >
> > suppose there is a "lightweight" node:
> >
> > 1.  ignores utxo's below the dust limit
> > 2.  doesn't validate dust tx
> > 3.  still validates POW, other tx, etc.
> >
> >     these nodes could possibly get forked - accepting a series of valid,
> >     mined blocks where there is an invalid but ignored dust tx, however
> >     this attack seems every bit as expensive as a 51% attack
>
> How would such a node treat a transaction that spends multiple dust UTXOs
> and creates a single non-dust UTXO out of them (after fees)?
> Is it valid (to such a node) or not?
>
> I presume from #1 it never stores dust UTXOs, so the node cannot know if
> the UTXO being spent by such a tx is spending dust, or trying to spend an
> already-spent TXO, or even inventing a TXO out of `/dev/random`.
>
> Regards,
> ZmnSCPxj
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211007/3889c77d/attachment.html>

From shymaa.arafat at gmail.com  Fri Oct  8 07:44:59 2021
From: shymaa.arafat at gmail.com (shymaa arafat)
Date: Fri, 8 Oct 2021 09:44:59 +0200
Subject: [bitcoin-dev] [Lightning-dev] Removing the Dust Limit
In-Reply-To: <MCYvJzqskIC56X-ylVCNgdaVk6SNnpCE6GgssXxK-znwwK4MoA41a2A-yNuCG8s99ll3h__YjCjBlP99A27Clbip-aYbF2ZwLpZ0SJT0j2U=@protonmail.com>
References: <CAD5xwhjFBjvkMKev_6HFRuRGcZUi7WjO5d963GNXWN4n-06Pqg@mail.gmail.com>
 <20210808215101.wuaidu5ww63ajx6h@ganymede>
 <MkPutJpff5rqUxXFQrEyHZl6Iz0DfrJU_-BQD-y0El65GQFnj7igVfmWU79fPCtiFztUYl4ofzrqeaN0HFMB45YPErY9rYY7_h1XkuTMfvc=@wuille.net>
 <CAJowKgKt=yYdNOYYNsWh7FJ2EH7rz0bd2EjUjmyA=cA6k5cvUQ@mail.gmail.com>
 <BtaljKLqpe75GB6pHEPQMF6_L-hBaE0ZCBGaXrUfnHRYeEbCqFWZ12DaMRm5jEADceL3uPfCiL-WU9MOZJ_m54Zi3Pzu0vSFN3nQvuSKvBM=@protonmail.com>
 <CAM98U8nSOQ9HpdRLBdkcFAdToW=z7_EhnYisMb7F46ExmJkT-w@mail.gmail.com>
 <ZKZ4RR6uAv0mG8yFQyRkWDTFQ7JnsGVfLQcMTmsc5ui7MTJXuzMkVk5YQTniPuc4F_KRhn7BEZZHEK60IZYZYU9A1r-tbmfPTnIs0pOd7oU=@protonmail.com>
 <CAM98U8kKud-7QoJKYd5o245o8vGeUD7YD2OnXF_QeKaO33dSTw@mail.gmail.com>
 <MCYvJzqskIC56X-ylVCNgdaVk6SNnpCE6GgssXxK-znwwK4MoA41a2A-yNuCG8s99ll3h__YjCjBlP99A27Clbip-aYbF2ZwLpZ0SJT0j2U=@protonmail.com>
Message-ID: <CAM98U8=NGcwoip5CVKrT2LNWjinCogxRjEYLxMtQ4-O+49wsLw@mail.gmail.com>

The suggested idea I was replying to is to make all dust TXs invalid by
some nodes. I suggested a compromise by keeping them in secondary storage
for full nodes, and in a separate Merkle Tree for bridge servers.
-In bridge servers they won't increase any worstcase, on the contrary this
will enhance the performance even if slightly.
-In full nodes, and since they will usually appear in clusters, they will
be fetched rarely (either by a dust sweeping action, or a malicious
attacker)
In both cases as a batch
-To not exhaust the node with DoS(as the reply mentioned)one may think of
uploading the whole dust partition if they were called more than certain
threshold (say more than 1 Tx in a block)
-and then keep them there for "a while", but as a separate partition too to
exclude them from any caching mechanism after that block.
-The "while" could be a tuned parameter.
-Take care that the more dust is sweeped, the less dust to remain in the
UTXO set; as users are already much dis-incentivised to create more.
.
Thanks for allowing the reply

On Thu, Oct 7, 2021, 16:43 ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:

>
>
> > I don't know what brings up sorting here, unless as an example.
>
> Yes, it is an example: quicksort is bad for network-facing applications
> because its ***worst-case behavior*** is bad.
> Bitcoin is a network-facing application, and similarly, ***worst-case
> behavior*** being bad is something that would strongly discourage
> particular approaches.
> Your proposal risks bad ***worst-case behavior***.
>
> > Anyways, I was comparing to rejecting them completely, not to keeping
> them in one set. In addition, those dust sweep Transactions will probably
> be a dust sweep and thus contain so many inputs which "maybe" makes 1-one
> disk visit  to fetch all their hashes at once, 2-from a smaller subset with
> max size 5-10% the UTXO set, justifiable.
>
> Do not consider the ***average case*** where a block is composed of only a
> few dust sweep transactions and most transactions are normal,
> non-dust-sweep transactions.
>
> Instead, consider the ***worst case*** where ***all*** transactions in a
> block are dust sweep transactions, because that is what attackers will use.
>
> Regards,
> ZmnSCPxj
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211008/91bd7b81/attachment.html>

From ZmnSCPxj at protonmail.com  Fri Oct  8 10:38:50 2021
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Fri, 08 Oct 2021 10:38:50 +0000
Subject: [bitcoin-dev] [Lightning-dev] Removing the Dust Limit
In-Reply-To: <CAM98U8=NGcwoip5CVKrT2LNWjinCogxRjEYLxMtQ4-O+49wsLw@mail.gmail.com>
References: <CAD5xwhjFBjvkMKev_6HFRuRGcZUi7WjO5d963GNXWN4n-06Pqg@mail.gmail.com>
 <20210808215101.wuaidu5ww63ajx6h@ganymede>
 <MkPutJpff5rqUxXFQrEyHZl6Iz0DfrJU_-BQD-y0El65GQFnj7igVfmWU79fPCtiFztUYl4ofzrqeaN0HFMB45YPErY9rYY7_h1XkuTMfvc=@wuille.net>
 <CAJowKgKt=yYdNOYYNsWh7FJ2EH7rz0bd2EjUjmyA=cA6k5cvUQ@mail.gmail.com>
 <BtaljKLqpe75GB6pHEPQMF6_L-hBaE0ZCBGaXrUfnHRYeEbCqFWZ12DaMRm5jEADceL3uPfCiL-WU9MOZJ_m54Zi3Pzu0vSFN3nQvuSKvBM=@protonmail.com>
 <CAM98U8nSOQ9HpdRLBdkcFAdToW=z7_EhnYisMb7F46ExmJkT-w@mail.gmail.com>
 <ZKZ4RR6uAv0mG8yFQyRkWDTFQ7JnsGVfLQcMTmsc5ui7MTJXuzMkVk5YQTniPuc4F_KRhn7BEZZHEK60IZYZYU9A1r-tbmfPTnIs0pOd7oU=@protonmail.com>
 <CAM98U8kKud-7QoJKYd5o245o8vGeUD7YD2OnXF_QeKaO33dSTw@mail.gmail.com>
 <MCYvJzqskIC56X-ylVCNgdaVk6SNnpCE6GgssXxK-znwwK4MoA41a2A-yNuCG8s99ll3h__YjCjBlP99A27Clbip-aYbF2ZwLpZ0SJT0j2U=@protonmail.com>
 <CAM98U8=NGcwoip5CVKrT2LNWjinCogxRjEYLxMtQ4-O+49wsLw@mail.gmail.com>
Message-ID: <R7rqMyBitqJYz8vM37xnRj3OAvTfA4PDZJg3QzTVLNx3nLMeKRUxKNHu49ezhO80N7XeUweFOBeduAeoIUFvEFhjSTfwDltRH4kEdeQ9koE=@protonmail.com>

Good morning shymaa,

> The suggested idea I was replying to is to make all dust TXs invalid by some nodes.

Is this supposed to be consensus change or not?
Why "some" nodes and not all?

I think the important bit is for full nodes.
Non-full-nodes already work at reduced security; what is important is the security-efficiency tradeoff.

> I suggested a compromise by keeping them in secondary storage for full nodes, and in a separate Merkle Tree for bridge servers.
> -In bridge servers they won't increase any worstcase, on the contrary this will enhance the performance even if slightly.
> -In full nodes, and since they will usually appear in clusters, they will be fetched rarely (either by a dust sweeping action, or a malicious attacker)
> In both cases as a batch
> -To not exhaust the node with DoS(as the reply mentioned)one may think of uploading the whole dust partition if they were called more than certain threshold (say more than 1 Tx in a block)??
> -and then keep them there for "a while", but as a separate partition too to exclude them from any caching mechanism after that block.
> -The "while" could be a tuned parameter.

Assuming you meant "dust tx is considered invalid by all nodes".

* Block has no dust sweep
  * With dust rejected: only non-dust outputs are accessed.
  * With dust in secondary storage: only non-dust outputs are accessed.
* Block has some dust sweeps
  * With dust rejected: only non-dust outputs are accessed, block is rejected.
  * With dust in secondary storage: some data is loaded from secondary storage.
* Block is composed of only dust sweeps
  * With dust rejected: only non-dust outputs are accessed, block is rejected.
  * With dust in secondary storage: significant increase in processing to load large secondary storage in memory,

So I fail to see how the proposal ever reduces processing compared to the idea of just outright making all dust txs invalid and rejecting the block.
Perhaps you are trying to explain some other mechanism than what I understood?

It is helpful to think in terms always of worst-case behavior when considering resistance against attacks.

> -Take care that the more dust is sweeped, the less dust to remain in the UTXO set; as users are already much dis-incentivised to create more.

But creation of dust is also as easy as sweeping them, and nothing really prevents a block from *both* creating *and* sweeping dust, e.g. a block composed of 1-input-1-output transactions, unless you want to describe some kind of restriction here?

Such a degenerate block would hit your secondary storage double: one to read, and one to overwrite and add new entries; if the storage is large then the index structure you use also is large and updates can be expensive there as well.


Again, I am looking solely at fullnode efficiency here, meaning all rules validated and all transactions validated, not validating and simply accepting some transactions as valid is a degradation of security from full validation to SPV validation.
Now of course in practice modern Bitcoin is hard to attack with *only* mining hashpower as there are so many fullnodes that an SPV node would be easily able to find the "True" history of the chain.
However, as I understand it that proporty of fullnodes protecting against attacks on SPV nodes only exists due to fullnodes being cheap to keep online; if the cost of fullnodes in the **worst case** (***not*** average, please stop talking about average case) increases then it may become feasible for miners to attack SPV nodes.

Regards,
ZmnSCPxj

From billy.tetrud at gmail.com  Fri Oct  8 15:08:02 2021
From: billy.tetrud at gmail.com (Billy Tetrud)
Date: Fri, 8 Oct 2021 11:08:02 -0400
Subject: [bitcoin-dev] Question- must every mining rig attempt every
	block?
In-Reply-To: <CAPv7TjZA2KYeVnT6PE6UHXwPy__E-VOvjmYD1207f1CgBRhLZg@mail.gmail.com>
References: <f867f949-9a04-329b-ea1b-26201f46d2ab@nathanalexander.net>
 <CAPv7TjZA2KYeVnT6PE6UHXwPy__E-VOvjmYD1207f1CgBRhLZg@mail.gmail.com>
Message-ID: <CAGpPWDax_Ht0H-Ct_SfaFF7AD-nUH-T1Sfo9+c7M6ptUK8ifzw@mail.gmail.com>

Proof of stake systems attempt to create red light - green light types of
things with non-gameable attributes (eg collaborative random numbers). This
can't be done with mining because mining is completely random - its not
possible to know which miner will mine a block. If it were, it wouldn't be
proof of work, but something else. What you describe sounds like proof of
identity, which isn't possible in a decentralized adversarial environment.
In fact, one of the primary achievements of the Proof of Work consensus
mechanism is to work around the Sybil issue, where (like ZmnSCPxj
mentioned) a single user can have many identities.

There can be hybrid systems that use both proof of work and proof of stake,
but my conclusion after having done a lot of research and thinking about it
([1]
<https://github.com/fresheneesz/quantificationOfConsensusProtocolSecurity>,
[2] <https://github.com/fresheneesz/proofOfTimeOwnership>) is that the
security mostly boils down to the weakest piece of the hybrid system, and
so its not very effective to have hybrid systems like you mentioned.

On Tue, Oct 5, 2021 at 10:43 AM Ruben Somsen via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Hi Nathan,
>
> That's a fair question, but note that we've already had a bunch of "green
> mining" related posts a few months ago, so I suspect you'll be able to find
> many criticisms to this idea in the following thread:
>
>
> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-May/018937.html
>
> It also looks like you'll be able to find some related answers on Bitcoin
> Stack Exchange:
>
> https://bitcoin.stackexchange.com/questions/106308/decreasing-energy-consumption-of-bitcoins-pow-with-paired-mining-rounds
>
> And generally speaking these types of discussions don't end up being very
> fruitful for bitcoin-dev, because these are the types of changes that are
> unlikely to ever be seriously considered for Bitcoin.
>
> Cheers,
> Ruben
>
> On Tue, Oct 5, 2021 at 4:09 PM Nathan T Alexander via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> For purposes of conserving energy, couldn't each mining rig have some
>> non-gameable attribute which would be used to calculate if a block would
>> be accepted by that rig?
>>
>> Don't the mining rigs have to be able to identify themselves to the
>> network somehow, in order to claim their block reward? Could their
>> bitcoin network ID be used as a non-gameable attribute?
>>
>> Essentially a green light / red light system. In order for a block to be
>> accepted by the network, it must have all attributes of a successful
>> block today, and it must also have come from a rig that had a green light.
>>
>> Perhaps hash some data from the last successful block, along with the
>> miners non-gameable attribute, and if it's below a certain number set by
>> algorithm, the miner gets a green light to race to produce a valid block.
>>
>> Nathan Alexander
>>
>> Arlington, TX
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211008/1c5fa1e8/attachment.html>

From ZmnSCPxj at protonmail.com  Fri Oct  8 22:47:11 2021
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Fri, 08 Oct 2021 22:47:11 +0000
Subject: [bitcoin-dev] [Lightning-dev] Removing the Dust Limit
In-Reply-To: <MkPutJpff5rqUxXFQrEyHZl6Iz0DfrJU_-BQD-y0El65GQFnj7igVfmWU79fPCtiFztUYl4ofzrqeaN0HFMB45YPErY9rYY7_h1XkuTMfvc=@wuille.net>
References: <CAD5xwhjFBjvkMKev_6HFRuRGcZUi7WjO5d963GNXWN4n-06Pqg@mail.gmail.com>
 <20210808215101.wuaidu5ww63ajx6h@ganymede>
 <MkPutJpff5rqUxXFQrEyHZl6Iz0DfrJU_-BQD-y0El65GQFnj7igVfmWU79fPCtiFztUYl4ofzrqeaN0HFMB45YPErY9rYY7_h1XkuTMfvc=@wuille.net>
Message-ID: <R4jC1XUMFSWW9kRbnQJokALtkpD3PMGtgcITeUbKsln6lNEZqxzM4ax9QquWP39jwQLXXPfojeEqI5wkuYCsqbepR07r3CAyV70VjNQYzLc=@protonmail.com>

Good morning Pieter,

> Indeed - UTXO set size is an externality that unfortunately Bitcoin's consensus rules fail to account
> for. Having a relay policy that avoids at the very least economically irrational behavior makes
> perfect sense to me.
>
> It's also not obvious how consensus rules could deal with this, as you don't want consensus rules
> with hardcoded prices/feerates. There are possibilities with designs like transactions getting
> a size/weight bonus/penalty, but that's both very hardforky, and hard to get right without
> introducing bad incentives.

Why is a +weight malus *very* hardforky?

Suppose a new version of a node adds, say, +20 sipa per output of a transaction (in order to economically discourage the creation of additional outputs in the UTXO set).
Older versions would see the block as being lower weight than usual, but as the consensus rule is "smaller than 4Msipa" they should still accept any block acceptable to newer versions.

It seems to me that only a -weight bonus is hardforky (but then xref SegWit and its -weight bonus on inputs).

I suppose the effect is primarily felt on mining nodes?
Miners might refuse to activate such a fork, as they would see fewer transactions per block on average?

Regards,
ZmnSCPxj


From willtech at live.com.au  Thu Oct  7 08:17:40 2021
From: willtech at live.com.au (LORD HIS EXCELLENCY JAMES HRMH)
Date: Thu, 7 Oct 2021 08:17:40 +0000
Subject: [bitcoin-dev] [Lightning-dev]   Removing the Dust Limit
In-Reply-To: <BtaljKLqpe75GB6pHEPQMF6_L-hBaE0ZCBGaXrUfnHRYeEbCqFWZ12DaMRm5jEADceL3uPfCiL-WU9MOZJ_m54Zi3Pzu0vSFN3nQvuSKvBM=@protonmail.com>
References: <CAD5xwhjFBjvkMKev_6HFRuRGcZUi7WjO5d963GNXWN4n-06Pqg@mail.gmail.com>
 <20210808215101.wuaidu5ww63ajx6h@ganymede>
 <MkPutJpff5rqUxXFQrEyHZl6Iz0DfrJU_-BQD-y0El65GQFnj7igVfmWU79fPCtiFztUYl4ofzrqeaN0HFMB45YPErY9rYY7_h1XkuTMfvc=@wuille.net>
 <CAJowKgKt=yYdNOYYNsWh7FJ2EH7rz0bd2EjUjmyA=cA6k5cvUQ@mail.gmail.com>
 <BtaljKLqpe75GB6pHEPQMF6_L-hBaE0ZCBGaXrUfnHRYeEbCqFWZ12DaMRm5jEADceL3uPfCiL-WU9MOZJ_m54Zi3Pzu0vSFN3nQvuSKvBM=@protonmail.com>
Message-ID: <PS2P216MB10896EAD8C2FE35F2A897C4B9DB19@PS2P216MB1089.KORP216.PROD.OUTLOOK.COM>

Good Afternoon,

Returning to this subject, there should be no restriction to the value of utxo's that keep in one's own wallet as change can be created in any value. With obvious intent, the wallet should avoid creating utxo's below the current dust limit at the time the transaction is created but it cannot guarantee it.

The wallet should avoid including utxo's that by weight sat/KB are more expensive to include that their value at the time a transaction is created, ie. do not include utxo's in a transaction that lower the input value after fees for automatic utxo selection, however, perhaps consider this is valid for manual utxo selection since it is in every example 'my money' and I can spend some of it if I decide.

There is no discipline in complaining that the dust set of utxo's slows down the process of block validation during mining. Every conceivable computerised business bears the expense of the cost of a database transaction. The actual answer to this genuine business concern of database speed is to build a faster database.

It is correct knowledge to know that the Bitcoin protocol cannot speculate as to the future but we can. The case exists where it is conceivable for example, that the transaction fee is paid only for the first utxo inclusion in a transaction due to changes to the calculation of block-size. There are other easily plausible examples where the inclusion of what is today considered dust may not be ill-considered.

KING JAMES HRMH
Great British Empire

Regards,
The Australian
LORD HIS EXCELLENCY JAMES HRMH (& HMRH)
of Hougun Manor & Glencoe & British Empire
MR. Damian A. James Williamson
Wills

et al.


Willtech
www.willtech.com.au
www.go-overt.com
duigco.org DUIGCO API
and other projects


m. 0487135719
f. +61261470192


This email does not constitute a general advice. Please disregard this email if misdelivered.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211007/c2aee87a/attachment-0001.html>

From willtech at live.com.au  Thu Oct  7 08:34:17 2021
From: willtech at live.com.au (LORD HIS EXCELLENCY JAMES HRMH)
Date: Thu, 7 Oct 2021 08:34:17 +0000
Subject: [bitcoin-dev] [Lightning-dev]   Removing the Dust Limit
In-Reply-To: <PS2P216MB10896EAD8C2FE35F2A897C4B9DB19@PS2P216MB1089.KORP216.PROD.OUTLOOK.COM>
References: <CAD5xwhjFBjvkMKev_6HFRuRGcZUi7WjO5d963GNXWN4n-06Pqg@mail.gmail.com>
 <20210808215101.wuaidu5ww63ajx6h@ganymede>
 <MkPutJpff5rqUxXFQrEyHZl6Iz0DfrJU_-BQD-y0El65GQFnj7igVfmWU79fPCtiFztUYl4ofzrqeaN0HFMB45YPErY9rYY7_h1XkuTMfvc=@wuille.net>
 <CAJowKgKt=yYdNOYYNsWh7FJ2EH7rz0bd2EjUjmyA=cA6k5cvUQ@mail.gmail.com>
 <BtaljKLqpe75GB6pHEPQMF6_L-hBaE0ZCBGaXrUfnHRYeEbCqFWZ12DaMRm5jEADceL3uPfCiL-WU9MOZJ_m54Zi3Pzu0vSFN3nQvuSKvBM=@protonmail.com>
 <PS2P216MB10896EAD8C2FE35F2A897C4B9DB19@PS2P216MB1089.KORP216.PROD.OUTLOOK.COM>
Message-ID: <PS2P216MB1089B819664D10559BF04F679DB19@PS2P216MB1089.KORP216.PROD.OUTLOOK.COM>

Good Afternoon,

The underlying consideration is the same concerning the handling of 1c and 2c coins in an economy. Although you may argue the cost of counting those coins throughout the course of minting, drafting to banks, paying to bank customers, including in change, and at every handling counting, is less than the value of those coins, hpwever, the solution in traditional currency is to round the value of the transaction either per line of goods or per total before calculating the Grand Total, in which case the payment either from a non-utxo set of accumulation in a traditional account or, from a known series of denominations, is adjusted.

In the case of Bitcoin, the denominations available are effectively the utxo set and there is no effective way to round the transactions without accepting overpayments as valid, and with what consideration, in which case the protocol may avoid creating dust in change by sending the additional rounded amount that would otherwise be dust to the recipient.

I suppose that this gets difficult where the transaction has multiple outputs and you could argue to distribute to all outputs as an overpayment. It is the same effectively as rounding to 10c.

KING JAMES HRMH
Great British Empire

Regards,
The Australian
LORD HIS EXCELLENCY JAMES HRMH (& HMRH)
of Hougun Manor & Glencoe & British Empire
MR. Damian A. James Williamson
Wills

et al.


Willtech
www.willtech.com.au
www.go-overt.com
duigco.org DUIGCO API
and other projects


m. 0487135719
f. +61261470192


This email does not constitute a general advice. Please disregard this email if misdelivered.


________________________________
From: LORD HIS EXCELLENCY JAMES HRMH <willtech at live.com.au>
Sent: Thursday, 7 October 2021 7:17 PM
To: Erik Aronesty <erik at q32.com>; ZmnSCPxj <ZmnSCPxj at protonmail.com>; Bitcoin Protocol Discussion <bitcoin-dev at lists.linuxfoundation.org>
Cc: lightning-dev <lightning-dev at lists.linuxfoundation.org>
Subject: Re: [bitcoin-dev] [Lightning-dev] Removing the Dust Limit

Good Afternoon,

Returning to this subject, there should be no restriction to the value of utxo's that keep in one's own wallet as change can be created in any value. With obvious intent, the wallet should avoid creating utxo's below the current dust limit at the time the transaction is created but it cannot guarantee it.

The wallet should avoid including utxo's that by weight sat/KB are more expensive to include that their value at the time a transaction is created, ie. do not include utxo's in a transaction that lower the input value after fees for automatic utxo selection, however, perhaps consider this is valid for manual utxo selection since it is in every example 'my money' and I can spend some of it if I decide.

There is no discipline in complaining that the dust set of utxo's slows down the process of block validation during mining. Every conceivable computerised business bears the expense of the cost of a database transaction. The actual answer to this genuine business concern of database speed is to build a faster database.

It is correct knowledge to know that the Bitcoin protocol cannot speculate as to the future but we can. The case exists where it is conceivable for example, that the transaction fee is paid only for the first utxo inclusion in a transaction due to changes to the calculation of block-size. There are other easily plausible examples where the inclusion of what is today considered dust may not be ill-considered.

KING JAMES HRMH
Great British Empire

Regards,
The Australian
LORD HIS EXCELLENCY JAMES HRMH (& HMRH)
of Hougun Manor & Glencoe & British Empire
MR. Damian A. James Williamson
Wills

et al.


Willtech
www.willtech.com.au
www.go-overt.com
duigco.org DUIGCO API
and other projects


m. 0487135719
f. +61261470192


This email does not constitute a general advice. Please disregard this email if misdelivered.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211007/89937fd1/attachment-0001.html>

From willtech at live.com.au  Thu Oct  7 10:35:16 2021
From: willtech at live.com.au (LORD HIS EXCELLENCY JAMES HRMH)
Date: Thu, 7 Oct 2021 10:35:16 +0000
Subject: [bitcoin-dev] [Lightning-dev]   Removing the Dust Limit
In-Reply-To: <PS2P216MB1089B819664D10559BF04F679DB19@PS2P216MB1089.KORP216.PROD.OUTLOOK.COM>
References: <CAD5xwhjFBjvkMKev_6HFRuRGcZUi7WjO5d963GNXWN4n-06Pqg@mail.gmail.com>
 <20210808215101.wuaidu5ww63ajx6h@ganymede>
 <MkPutJpff5rqUxXFQrEyHZl6Iz0DfrJU_-BQD-y0El65GQFnj7igVfmWU79fPCtiFztUYl4ofzrqeaN0HFMB45YPErY9rYY7_h1XkuTMfvc=@wuille.net>
 <CAJowKgKt=yYdNOYYNsWh7FJ2EH7rz0bd2EjUjmyA=cA6k5cvUQ@mail.gmail.com>
 <BtaljKLqpe75GB6pHEPQMF6_L-hBaE0ZCBGaXrUfnHRYeEbCqFWZ12DaMRm5jEADceL3uPfCiL-WU9MOZJ_m54Zi3Pzu0vSFN3nQvuSKvBM=@protonmail.com>
 <PS2P216MB10896EAD8C2FE35F2A897C4B9DB19@PS2P216MB1089.KORP216.PROD.OUTLOOK.COM>
 <PS2P216MB1089B819664D10559BF04F679DB19@PS2P216MB1089.KORP216.PROD.OUTLOOK.COM>
Message-ID: <PS2P216MB108975E795BA99C4E9ECB0089DB19@PS2P216MB1089.KORP216.PROD.OUTLOOK.COM>

Good Afternoon,

Further, if it is entirely necessary to prevent the creation of utxo's that are considered dust, and I am not by any means convinced, then it is simple to provide the most circumspect solution to transfer the value of any dust utxo that would be created in a transaction to the fee. I do not believe this answer is any more than robbery of the future value of the wallet as my wallet must be able to keep any change but if it is must then this is the answer.

KING JAMES HRMH
Great British Empire

Regards,
The Australian
LORD HIS EXCELLENCY JAMES HRMH (& HMRH)
of Hougun Manor & Glencoe & British Empire
MR. Damian A. James Williamson
Wills

et al.


Willtech
www.willtech.com.au
www.go-overt.com
duigco.org DUIGCO API
and other projects


m. 0487135719
f. +61261470192


This email does not constitute a general advice. Please disregard this email if misdelivered.
________________________________
From: LORD HIS EXCELLENCY JAMES HRMH <willtech at live.com.au>
Sent: Thursday, 7 October 2021 7:34 PM
To: Erik Aronesty <erik at q32.com>; ZmnSCPxj <ZmnSCPxj at protonmail.com>; Bitcoin Protocol Discussion <bitcoin-dev at lists.linuxfoundation.org>
Cc: lightning-dev <lightning-dev at lists.linuxfoundation.org>
Subject: Re: [bitcoin-dev] [Lightning-dev] Removing the Dust Limit

Good Afternoon,

The underlying consideration is the same concerning the handling of 1c and 2c coins in an economy. Although you may argue the cost of counting those coins throughout the course of minting, drafting to banks, paying to bank customers, including in change, and at every handling counting, is less than the value of those coins, hpwever, the solution in traditional currency is to round the value of the transaction either per line of goods or per total before calculating the Grand Total, in which case the payment either from a non-utxo set of accumulation in a traditional account or, from a known series of denominations, is adjusted.

In the case of Bitcoin, the denominations available are effectively the utxo set and there is no effective way to round the transactions without accepting overpayments as valid, and with what consideration, in which case the protocol may avoid creating dust in change by sending the additional rounded amount that would otherwise be dust to the recipient.

I suppose that this gets difficult where the transaction has multiple outputs and you could argue to distribute to all outputs as an overpayment. It is the same effectively as rounding to 10c.

KING JAMES HRMH
Great British Empire

Regards,
The Australian
LORD HIS EXCELLENCY JAMES HRMH (& HMRH)
of Hougun Manor & Glencoe & British Empire
MR. Damian A. James Williamson
Wills

et al.


Willtech
www.willtech.com.au
www.go-overt.com
duigco.org DUIGCO API
and other projects


m. 0487135719
f. +61261470192


This email does not constitute a general advice. Please disregard this email if misdelivered.


________________________________
From: LORD HIS EXCELLENCY JAMES HRMH <willtech at live.com.au>
Sent: Thursday, 7 October 2021 7:17 PM
To: Erik Aronesty <erik at q32.com>; ZmnSCPxj <ZmnSCPxj at protonmail.com>; Bitcoin Protocol Discussion <bitcoin-dev at lists.linuxfoundation.org>
Cc: lightning-dev <lightning-dev at lists.linuxfoundation.org>
Subject: Re: [bitcoin-dev] [Lightning-dev] Removing the Dust Limit

Good Afternoon,

Returning to this subject, there should be no restriction to the value of utxo's that keep in one's own wallet as change can be created in any value. With obvious intent, the wallet should avoid creating utxo's below the current dust limit at the time the transaction is created but it cannot guarantee it.

The wallet should avoid including utxo's that by weight sat/KB are more expensive to include that their value at the time a transaction is created, ie. do not include utxo's in a transaction that lower the input value after fees for automatic utxo selection, however, perhaps consider this is valid for manual utxo selection since it is in every example 'my money' and I can spend some of it if I decide.

There is no discipline in complaining that the dust set of utxo's slows down the process of block validation during mining. Every conceivable computerised business bears the expense of the cost of a database transaction. The actual answer to this genuine business concern of database speed is to build a faster database.

It is correct knowledge to know that the Bitcoin protocol cannot speculate as to the future but we can. The case exists where it is conceivable for example, that the transaction fee is paid only for the first utxo inclusion in a transaction due to changes to the calculation of block-size. There are other easily plausible examples where the inclusion of what is today considered dust may not be ill-considered.

KING JAMES HRMH
Great British Empire

Regards,
The Australian
LORD HIS EXCELLENCY JAMES HRMH (& HMRH)
of Hougun Manor & Glencoe & British Empire
MR. Damian A. James Williamson
Wills

et al.


Willtech
www.willtech.com.au
www.go-overt.com
duigco.org DUIGCO API
and other projects


m. 0487135719
f. +61261470192


This email does not constitute a general advice. Please disregard this email if misdelivered.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211007/ad09e939/attachment.html>

From andreas at schildbach.de  Sat Oct  9 15:36:47 2021
From: andreas at schildbach.de (Andreas Schildbach)
Date: Sat, 9 Oct 2021 17:36:47 +0200
Subject: [bitcoin-dev] Taproot testnet wallet
Message-ID: <cf1aa346-4f0d-8192-c8e8-1861e7840371@schildbach.de>

I'm trying to finish off bitcoinj's implementation for sending to 
taproot addresses. For this, I'd like to test against a wallet that can 
receive to P2TR and spend back.

I've been trying to get a taproot address from Bitcoin Core 22.0 and 
spent many hours, but in vain. Can someone please simply send my a 
testnet wallet that has at least one taproot address? (I don't care 
about anyone stealing my testnet coins, so don't worry about the 
compromised private key.)

Thanks!

From bitcoin-dev at wuille.net  Sat Oct  9 16:49:42 2021
From: bitcoin-dev at wuille.net (Pieter Wuille)
Date: Sat, 09 Oct 2021 16:49:42 +0000
Subject: [bitcoin-dev] Taproot testnet wallet
In-Reply-To: <cf1aa346-4f0d-8192-c8e8-1861e7840371@schildbach.de>
References: <cf1aa346-4f0d-8192-c8e8-1861e7840371@schildbach.de>
Message-ID: <jNk5M87QACL8ySLGzR5sQVaJkQcIV8RrHnbWv6-lkjRhjTL6vEmaMJI7hENUO803wSZM0hOaCyeQI3jjPBbJKiMy8gBXtS_HLmKzPivk2M0=@wuille.net>

On Oct 9, 2021, 11:36, Andreas Schildbach via bitcoin-dev < bitcoin-dev at lists.linuxfoundation.org> wrote:

> I'm trying to finish off bitcoinj's implementation for sending to

taproot addresses. For this, I'd like to test against a wallet that can

receive to P2TR and spend back.

> I've been trying to get a taproot address from Bitcoin Core 22.0 and

spent many hours, but in vain. Can someone please simply send my a

testnet wallet that has at least one taproot address? (I don't care

about anyone stealing my testnet coins, so don't worry about the

compromised private key.)

Hi Andreas,

You can construct a taproot-capable wallet in Bitcoin Core as follows:

* Have or create a descriptor wallet (createwallet RPC, with descriptors=true).

* Import a taproot descriptor (of the form "tr(KEY)"), as active descriptor (with active=true), where KEY can be a tprv.../* or any other supported key expression.

* Get a new address with addresstype=bech32m

I've also created one myself for testing: tb1p84x2ryuyfevgnlpnxt9f39gm7r68gwtvllxqe5w2n5ru00s9aquslzggwq

If you send testnet coins there email me an address, I'll return them.

Cheers,

--

Pieter
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211009/ce17a6fa/attachment-0001.html>

From jlspc at protonmail.com  Sun Oct 10 22:03:38 2021
From: jlspc at protonmail.com (jlspc)
Date: Sun, 10 Oct 2021 22:03:38 +0000
Subject: [bitcoin-dev] Inherited IDs - A safer,
	more powerful alternative to BIP-118 (ANYPREVOUT) for scaling
	Bitcoin
Message-ID: <a0_L7CQWQDOdAOcR3Fe_XOC_1MLgISfUYYlB7s28VDDQAn0ek_D3EX_7qvDK2d4HZ4lF9CyEXZrJTqFx3jxQwPzzLTZndTpYertSARsmaJU=@protonmail.com>

Response to email from Anthony Towns sent on 20210918 at 11:37:40 UTC
==============================================
aj,

Thanks for taking the time to go through my paper on inherited IDs (IIDs). Also, thanks for your concise and accurate description of the IID proposal and the 2Stage channel protocol. I'm glad you feel the 2Stage protocol might be better than eltoo for a two-party channel.

I want to address other parts of the paper that were obviously not as clear, as they led to two important misunderstandings.

First, there is the issue of the use of an operator in the "timeout trees", "update-forest" and "challenge-and-response" factory protocols. While those protocols do include a party that is designated as the "operator", the operator is in no way a trusted party. While it's true that one would prefer an operator that follows the protocol fully and promptly, as that would allow one to keep the protocol off-chain, the operator can never take funds or prevent others from obtaining the funds that are due to them. In fact, this is exactly analogous to the selection of the party with whom one shares a two-party lightning channel. If one views lightning as being trust-free, then one will also view "timeout trees", "update-forest" and "challenge-and-response" to be trust-free.

Second, there is the question of whether or not IIDs can be simulated with anyprevout. I don't believe that they can. Consider for example the case where Alice has an on-chain funding transaction F1 with output F1:0 that will be spent by a (currently off-chain) transaction F2 with output F2:0 that will be spent by a settlement transaction S. Assume further that there is an on-chain control transaction C1 with output C1:0 owned by untrusted operator O, where C1:0 will be spent by a (currently off-chain) transaction C2 with output C2:0 that may, in certain cases, also be spent by S. In particular, assume F1 puts a covenant on F2 such that F2 puts a covenant on S, where the covenant on S can be met by either: A) waiting a CSV delay of one time unit (defined to be long enough to allow a party with a competing transaction to put that competing transaction on-chain first) and then spending only F2:0 (where F2:0 is referenced via IID) and giving ownership of S:0 to Alice, or B) waiting until a CLV reaches time T_lock and then spending both F2:0 and C2:0 (where F2:0 and C2:0 are referenced via IIDs).

Assume that after Alice put F1 on-chain she wants to transfer ownership of the output S:0 to Bob without having to put F2 or S on-chain. She can do this with IIDs as follows. First, Alice asks the untrusted operator O to put C2 on-chain where C2 puts a covenant on S that forces S to spend both F2:0 and C2:0 (where F2:0 and C2:0 are referenced via IIDs) and to give ownership of S:0 to Bob (by making it spendable using Bob's public key).

There are two cases. First, if O promptly puts the desired C2 on-chain, then Alice and Bob can wait until T_lock (while putting nothing else on-chain), at which point Bob can be assured that he owns S:0 (as any attempt by Alice to spend S:0 by meetiing the covenant using case A above can be thwarted by Bob putting S on-chain first using case B above). Second, if O puts a different C2 on-chain, or fails to put any C2 on-chain promptly, Alice can reclaim her funds by putting F2 on-chain, waiting one time window, and then putting S on-chain using case A above.

Thus, IIDs provide a trust-free means for Alice to transfer funds from F1 to a party that is unknown to Alice when she puts F1 on-chain. I see two problems in tryinig to use anyprevout to achieve the same result. First, I don't know of any mechanism by which Alice can create a covenant that F2 puts on S which implements case B above. In some other settings, I can understand how one could use unique single-use keys in place of IID outputs. However, in this setting I don't see how to define a covenant that F2 puts on S that in case B forces the other input to spend C2:0, as signatures that are evaluated in spending F2:0 don't commit to the output scripts of other inputs to S. Second, and more fundamentally, even if one could define a covenant that F2 puts on S in case B forcing the other input to be signed by a single-use key owned by O, that still wouldn't unconditionally transfer ownership to Bob (without putting F2 and S on-chain). That's because in order to have single-use keys play the role of IIDs, they have to truly be single-use and there is no way Bob can know that O won't just sign some other S' that competes with S and sends S':0 to O, thus stealing the funds. Please let me know if I've missed something here.

The example above isn't very useful, as it doesn't cut down on the number of on-chain transactions required to transfer ownership from Alice to Bob. However, it does capture the core functionality that IIDs provide that (I believe) anyprevout does not provide. This functionality is exactly what enables "update-forest" and "challenge-and-response" to allow a single on-chain transaction to transfer ownership of thousands or millions of channels in a trust-free manner, thus accomplishing with one on-chain transaction what would have required thousands or millions of anyprevout transactions (at least as far as I can tell). This is exactly the power of IIDs that I was referring to, and I found surprising that this power was actually the result of restricting how a signed transaction can be used (as compared to a signed transaction that uses anyprevout).

I hope clearing up these two misunderstandings is enough to pique your interest in reading the "timeout trees", "update-forest" and "challenge-and-response" protocols in more detail, as I'd be interested in your expert opinion on them.

My remaining comments are minor compared to the previous ones.
* Regarding the worst-case delay for eltoo-2party vs. 2Stage, I agree that there is no single agreed upon model for analyzing this and opinions may differ. In any case, I think that if one had a nearly-expired HTLC (or if one is setting the lock time for an HTLC) and one could choose between eltoo-2party, where the other party could have thousands or millions of transactions competing with your settlement transaction, and 2Stage, where the other party can have at most one competing transaction, some would prefer 2Stage.
* In comparing eltoo-2party and 2Stage, I was surprised that you didn't consider 2Stage's elimination of watchtowers for one or both parties as being an advantage. I had through that would be a big win in practice.
* Regarding footnote 13's description of OP_CODESEPARATOR, I realize that that footnote does not capture the change made in taproot. I addressed that issue on p. 54 (and explained it in footnote 43), as footnote 13 was designed to explain OP_CODESEPARATOR to those not already familiar with it, while p. 54 was designed for the experts.
* Regarding the new address type for floating transactions mentioned in the paper, thanks for the correction. I'll remove this from the next version.

In summary, the paper shows that:
1) IIDs can be used to eliminate watchtowers for one or both parties in a two-party channel (2Stage),
2) IIDs can be used to create factories that allow very large numbers of new users to obtain bitcoin in a watchtower-free and trust-free manner (timeout trees),
3) IIDs support trust-free factories with unbounded numbers of parties (and channels) that allow the channels to be bought and sold by anyone, including parties not originally in the factory, with a single on-chain transaction, and
4) IIDs achieve these results while using a more constrained, and thus safer, change to Bitcoin than the support for floating transactions.

Are these results of interest?

Thanks,
John

Sent with [ProtonMail](https://protonmail.com/) Secure Email.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211010/ef8682ee/attachment.html>

From michaelfolkson at protonmail.com  Mon Oct 11 12:24:10 2021
From: michaelfolkson at protonmail.com (Michael Folkson)
Date: Mon, 11 Oct 2021 12:24:10 +0000
Subject: [bitcoin-dev] On the regularity of soft forks
Message-ID: <LmX3Gnfkf1T0Eb_wUXxPe8c0Tf2DNipfIqufkRS6oOPhttr4iZIOWtjUL_7QkcWEHr8eFvehHooaM140ZBKLwi98F5NwyQKSyEhAPZDK1YQ=@protonmail.com>

I was hoping to delay this post as long as possible because there are so many interesting and important things to discuss other than soft forks and consensus changes that seem to have taken a backseat this year due to Taproot activation. In addition, there seems to be a world of opportunity to leverage the upcoming Taproot soft fork that risks getting drowned out by speculating on the next soft fork.

There is clearly nothing wrong with some individuals continuously working on, designing and refining new possible consensus changes and whoever is interested is free to follow and participate in those discussions. This is Bitcoin, no one let alone me can decide what people should focus on. Indeed I intend to allocate a portion of my time to following and understanding the trade-offs of current and future soft fork proposals. However, in this post I will argue against frequent soft forks with a single or minimal set of features and instead argue for infrequent soft forks with batches of features.

I fully understand the desire and motivation to get consensus changes into Bitcoin as quickly as possible when certain use cases depend on them. However, the robustness, security and ability to resist harmful or suboptimal changes to the system is clearly the ultimate priority. The more frequently soft forks are attempted the harder it is for the community to ensure harmful or suboptimal changes don?t creep into the consensus rules. I am well aware of how much community mindshare Taproot activation demanded this year. This is how it should be. The community should be informed and vigilant when the consensus rules are changed. Every full node will either immediately on activation or in future enforce these consensus rule changes and so it is in the interests of every full node operator that these changes have been subject to the ultimate levels of community review and rigorous testing. Attempting them frequently either requires continuous community monitoring or an acceptance that an unneeded or harmful consensus change could easily creep into Bitcoin. Neither of these options seem acceptable to me. It is not reasonable to ask all the different segments of the community to dedicate full time resources to stay on top of proposed consensus changes. Hence treating a pull request to a Bitcoin implementation that requires a soft fork like any other pull request is shortsighted.

Merging soft fork code into a Bitcoin implementation

The code for a soft fork should not be merged into a Bitcoin implementation, let alone activation parameters (discussed later), until the maintainers of that implementation are comfortable that the entirety of that soft fork has sufficient community consensus. This includes what many consider the reference implementation and dominant implementation on the network, Bitcoin Core. A soft fork pull request cannot and should not be treated like any other pull request which can be merged with anything from 1 to 10 ACKs from long term or newer contributors. The act of merging a pull request that is part of a proposed soft fork is an acknowledgement by the maintainer(s) of that implementation that they consider the entirety of that proposed soft fork to have community consensus. That includes what is included in that soft fork as well as what isn?t. If there is a prevailing view that the current design could be improved, could feasibly be replaced by something superior in future or merely hasn?t been subject to sufficient community review it should not be merged.

Of course there is no ultimate authority to enforce that this happens, Bitcoin is an entirely voluntary system. A contentious or disputed soft fork can be merged into a Bitcoin implementation at any time but doing this is opening the door to the schism, disruption and waste of developer hours that we saw in 2017. Personally I think we?ll see an attempt to activate a contentious soft fork at some point in the long term future (Murphy?s Law) but any attempt to do so should be strongly discouraged. It should be made clear to any individual(s) that attempt this of the knock on impacts and potential short term damage they are inflicting on the entire ecosystem. Longer term I have confidence in Bitcoin?s ability to survive whatever happens but allocating significant community resources to resist an unnecessary contentious soft fork (or even regular contentious soft forks) is not an optimal use of those resources.

Soft fork activation

Miner signaling is a tool for signaling readiness. It is not voting for the soft fork or expressing support for the soft fork. There should not be any attempt to facilitate miner signaling until there is sufficient community consensus (the mining community is a subset of the community) on the soft fork. Merging activation parameters or encouraging miner signaling before it is clear there is community consensus on the entirety of the soft fork is putting the cart before the horse.

Taproot showed it was possible through the sustained efforts of many individuals and many organizations to achieve overwhelming community consensus for a soft fork. It is obviously impossible to get 100 percent consensus but Taproot appeared to get close to that. I did not identify any resistance whatsoever to merging Taproot PRs or the objective of getting Taproot activated although there was one long term contributor who effectively NACKed Taproot based on quantum resistance concerns.

Activation method and activation parameters ended up being more challenging to obtain overwhelming community consensus. Although I and a number of others participated in multiple open IRC meetings and spent months on IRC trying to find a way to get Taproot activated with at least rough consensus a number of disagreements remain. I don?t think these are necessarily showstoppers for future soft forks and assuming Taproot activates safely next month they ended up not being showstoppers for Taproot. However, it is clear the bar that was achieved regarding community consensus for the Taproot soft fork wasn?t met for the activation method and activation parameters. In a world where there isn?t overwhelming community consensus on the activation method, the activation parameters and what to do if the first activation attempt fails we have to accept that soft fork activations contain downside risk on top of the already acknowledged risks of bugs, consensus divergences and botched implementations of soft fork features. To layer on top a level of uncertainty over whether there is community consensus for the actual soft fork seems unacceptable to me.

This is an important additional argument for infrequent soft forks with batches of features rather than frequent soft forks with a single feature. If there is a chain split risk every time you attempt a soft fork you should not casually attempt a soft fork frequently or with abandon. There has to be community consensus that the upsides of the soft fork are sufficient to take on these downside risks of disruption or worse chain splits. I was of the strong personal view that the upsides outweighed the downside risks for Taproot activation in 2021 but this is a judgment we as a community will have to make for each and every future proposed soft fork. It is easy to get excited about shiny new features. It is harder to ensure harmful or suboptimal changes don?t creep into the consensus rules and harder yet to minimize the risk of chain splits if soft forks are attempted frequently.

--

Michael Folkson
Email: michaelfolkson at protonmail.com
Keybase: michaelfolkson
PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211011/500ca9d1/attachment.html>

From jlrubin at mit.edu  Mon Oct 11 19:12:58 2021
From: jlrubin at mit.edu (Jeremy)
Date: Mon, 11 Oct 2021 12:12:58 -0700
Subject: [bitcoin-dev] On the regularity of soft forks
In-Reply-To: <LmX3Gnfkf1T0Eb_wUXxPe8c0Tf2DNipfIqufkRS6oOPhttr4iZIOWtjUL_7QkcWEHr8eFvehHooaM140ZBKLwi98F5NwyQKSyEhAPZDK1YQ=@protonmail.com>
References: <LmX3Gnfkf1T0Eb_wUXxPe8c0Tf2DNipfIqufkRS6oOPhttr4iZIOWtjUL_7QkcWEHr8eFvehHooaM140ZBKLwi98F5NwyQKSyEhAPZDK1YQ=@protonmail.com>
Message-ID: <CAD5xwhj3JCxH1=5Tj+hgiSxLWchLgT584X0YutKVeuibnpwmtA@mail.gmail.com>

*> ... in this post I will argue against frequent soft forks with a single
or minimal*
*> set of features and instead argue for infrequent soft forks with batches*
*> of features.*

I think this type of development has been discussed in the past and has
been rejected.


from: Matt Corallo's post:
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-January/017547.html










*Matt: Follow the will of the community, irrespective of individuals
orunreasoned objection, but without ever overruling any
reasonableobjection. Recent history also includes "objection" to soft forks
in theform of "this is bad because it doesn't fix a different problem I
wantfixed ASAP". I don't think anyone would argue this qualifies as
areasonable objection to a change, and we should be in a place, as
acommunity (never as developers or purely one group), to ignore
suchobjections and make forward progress in spite of them. We don't
makegood engineering decisions by "bundling" unrelated features together to*
*enable political football and compromise.*

*AJ: - improvements: changes might not make everyone better off, but we*




*   don't want changes to screw anyone over either -- pareto   improvements
in economics, "first, do no harm", etc. (if we get this   right, there's no
need to make compromises and bundle multiple   flawed proposals so that
everyone's an equal mix of happy and*
*   miserable)*


I think Matt and AJ's PoV is widely reflected in the community that
bundling changes leads to the inclusion of suboptimal features.

This also has strong precedent in other important technical bodies, e.g.
from https://datatracker.ietf.org/doc/html/rfc7282 On Consensus and Humming
in the IETF.

      Even worse is the "horse-trading" sort of compromise: "I object to
   your proposal for such-and-so reasons.  You object to my proposal for
   this-and-that reason.  Neither of us agree.  If you stop objecting to
   my proposal, I'll stop objecting to your proposal and we'll put them
   both in."  That again results in an "agreement" of sorts, but instead
   of just one outstanding unaddressed issue, this sort of compromise
     results in two, again ignoring them for the sake of expedience.

   These sorts of "capitulation" or "horse-trading" compromises have no
   place in consensus decision making.  In each case, a chair who looks
   for "agreement" might find it in these examples because it appears
   that people have "agreed".  But answering technical disagreements is
   what is needed to achieve consensus, sometimes even when the people

      who stated the disagreements no longer wish to discuss them.


If you would like to advocate bitcoin development run counter to that,
you should provide a much stronger refutation of these engineering
norms.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211011/7c82c14f/attachment-0001.html>

From ZmnSCPxj at protonmail.com  Mon Oct 11 19:53:39 2021
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Mon, 11 Oct 2021 19:53:39 +0000
Subject: [bitcoin-dev] On the regularity of soft forks
In-Reply-To: <CAD5xwhj3JCxH1=5Tj+hgiSxLWchLgT584X0YutKVeuibnpwmtA@mail.gmail.com>
References: <LmX3Gnfkf1T0Eb_wUXxPe8c0Tf2DNipfIqufkRS6oOPhttr4iZIOWtjUL_7QkcWEHr8eFvehHooaM140ZBKLwi98F5NwyQKSyEhAPZDK1YQ=@protonmail.com>
 <CAD5xwhj3JCxH1=5Tj+hgiSxLWchLgT584X0YutKVeuibnpwmtA@mail.gmail.com>
Message-ID: <uX7nCIoeTnpXkL-x8qJiSS1OYy-J-YhvTP7hdsljoUUDpa6c8pC34NapUWhCxYSYMY4ciwLzABVUVQwiihggvCqQhH5sfwImDIKzXl61M80=@protonmail.com>

Good morning Jeremy,

> This also has strong precedent in other important technical bodies, e.g. from?https://datatracker.ietf.org/doc/html/rfc7282?On Consensus and Humming in the IETF.
>
>   Even worse is the "horse-trading" sort of compromise: "I object to
> ? ?your proposal for such-and-so reasons.? You object to my proposal for
> ? ?this-and-that reason.? Neither of us agree.? If you stop objecting to
> ? ?my proposal, I'll stop objecting to your proposal and we'll put them
> ? ?both in." ?That again results in an "agreement" of sorts, but instead
> ? ?of just one outstanding unaddressed issue, this sort of compromise
>   results in two, again ignoring them for the sake of expedience.
>
> ? ?These sorts of "capitulation" or "horse-trading" compromises have no
> ? ?place in consensus decision making.? In each case, a chair who looks
> ? ?for "agreement" might find it in these examples because it appears
> ? ?that people have "agreed".? But answering technical disagreements is
> ? ?what is needed to achieve consensus, sometimes even when the people?
>
>   who stated the disagreements no longer wish to discuss them.
>
> If you would like to advocate bitcoin development run counter to that, you should provide a much stronger refutation of these engineering norms.

The Internet has the maxim "be strict in what you provide, lenient in what you accept", which allows for slight incompatibilities between software to generally be papered over (xref the mountains of Javascript code that shim in various new ECMAScript features fairly reliably in a wide variety of browsers).

Bitcoin, as a consensus system, requires being paranoiacally strict on what transactions and blocks you accept.
Thus, the general engineering norm of separating concerns, of great application to "lenient in what you accept" systems, may not apply quite as well to "hell no I am not accepting that block" Bitcoin.

Bitcoin as well, as a resistance against state moneys, is inherently political, and it possible that the only way out is through: we may need to resist this horse-trading by other means than separating concerns, including political will to reject capitulation despite bundling.

Regards,
ZmnSCPxj

From prayank at tutanota.de  Mon Oct 11 16:03:16 2021
From: prayank at tutanota.de (Prayank)
Date: Mon, 11 Oct 2021 18:03:16 +0200 (CEST)
Subject: [bitcoin-dev] On the regularity of soft forks
Message-ID: <Mlk5-NX--3-2@tutanota.de>

Hi Michael,

Agree with almost everything.

> Miner signaling is a tool for signaling readiness. It is not voting for the soft fork or expressing support for the soft fork. There should not be any attempt to facilitate miner signaling until there is sufficient community consensus (the mining community is a subset of the community) on the soft fork. 

This is really important which gets ignored. I wish there was a way to solve this problem in a way that it is not misinterpreted by users.

During signalling for taproot, there were lots of users in different communities that believed miners are voting for taproot and we need some percentage of miners to agree before making any changes in Bitcoin. It was not just non-technical users but few mining pools, exchanges etc. also considered miners signaling as some voting process.

Best I could do at that moment was share this link: https://bitcoin.stackexchange.com/questions/97043/is-there-an-active-list-of-bips-currently-open-for-voting/

However I am sure there are lot of people who still think miners vote during signaling. Opinions of few developers on MASF vs UASF also adds more confusion to this thing. I could not think of any solution to solve this problem.
-- 
Prayank

A3B1 E430 2298 178F
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211011/9d118eee/attachment.html>

From jtimon at jtimon.cc  Tue Oct 12 19:04:02 2021
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Tue, 12 Oct 2021 21:04:02 +0200
Subject: [bitcoin-dev] On the regularity of soft forks
In-Reply-To: <Mlk5-NX--3-2@tutanota.de>
References: <Mlk5-NX--3-2@tutanota.de>
Message-ID: <CABm2gDpq94uG6_6Fk2Y48R=iBDjijCTOLjcDTr_yQAnkG1-UFA@mail.gmail.com>

On Tue, Oct 12, 2021 at 5:34 PM Prayank via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
>
> Hi Michael,
>
> Agree with almost everything.
>
> > Miner signaling is a tool for signaling readiness. It is not voting for the soft fork or expressing support for the soft fork. There should not be any attempt to facilitate miner signaling until there is sufficient community consensus (the mining community is a subset of the community) on the soft fork.
>
> This is really important which gets ignored. I wish there was a way to solve this problem in a way that it is not misinterpreted by users.
>
> During signalling for taproot, there were lots of users in different communities that believed miners are voting for taproot and we need some percentage of miners to agree before making any changes in Bitcoin. It was not just non-technical users but few mining pools, exchanges etc. also considered miners signaling as some voting process.
>
> Best I could do at that moment was share this link: https://bitcoin.stackexchange.com/questions/97043/is-there-an-active-list-of-bips-currently-open-for-voting/
>
> However I am sure there are lot of people who still think miners vote during signaling. Opinions of few developers on MASF vs UASF also adds more confusion to this thing. I could not think of any solution to solve this problem.

Yes, given most of the arguments given against activation at the end
of the period regardless of mining signaling, it seems sadly it's not
just users but developers too. They seem to believe that miners must
chose for users with bip8(false) because (according to them) with
bip8(true) it is developers who decide for users, and they don't want
to decide for users: they want miners to decide for users.
They don't seem to believe users can actually chose for themselves, sadly.
In the next softfork, sadly, probably the same discussions will be
repeated, the same rational arguments will be ignored and activation
will be once again done, in my opinion, the wrong way and most users
(many more, as we grow in numbers) will remain confused in the same
way and confusing the newcomers they explain bitcoin to.



> --
> Prayank
>
> A3B1 E430 2298 178F
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From vjudeu at gazeta.pl  Wed Oct 13 19:16:05 2021
From: vjudeu at gazeta.pl (vjudeu at gazeta.pl)
Date: Wed, 13 Oct 2021 21:16:05 +0200
Subject: [bitcoin-dev] Year 2038 problem and year 2106 chain halting
Message-ID: <50769965-423dd279413d4dba11ba459cbd98387b@pmq6v.m5r2.onet>

It seems that Bitcoin Core will stop working in 2038 because of assertion checking if the current time is non-negative. Also, the whole chain will halt after reaching median time 0xffffffff in 2106. More information: https://bitcointalk.org/index.php?topic=5365359.0
I wonder if that kind of issues are possible to fix in a soft-fork way.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211013/de85a966/attachment.html>

From darosior at protonmail.com  Thu Oct 14 10:48:55 2021
From: darosior at protonmail.com (darosior)
Date: Thu, 14 Oct 2021 10:48:55 +0000
Subject: [bitcoin-dev] Proposal: Package Mempool Accept and Package RBF
In-Reply-To: <CAFXO6=Lvcr7Pwn_ZD1CZohYUFKY-cC5sGRxdTOiP2MgnTvFnYA@mail.gmail.com>
References: <CAFXO6=+cHyQKM2n9yn4EhwLZO+AUB0ZD81qWPxmpN27rjUoU3w@mail.gmail.com>
 <CALZpt+HpvmEHUEOgye34T6pVQ+wnKKn-_8cTJTQXYQb9t1jOTA@mail.gmail.com>
 <CAFXO6=JzsYgiXJE2geSKMpfgPo+GGNX_+Pw0JQx1QQxAfhCdBQ@mail.gmail.com>
 <CALZpt+HQpdrebhWGXv_yLqiSCB5Ur71L1K13bd7w5TZb9DwJEQ@mail.gmail.com>
 <CAFXO6=Lvcr7Pwn_ZD1CZohYUFKY-cC5sGRxdTOiP2MgnTvFnYA@mail.gmail.com>
Message-ID: <UuYb6dx_tR9juBtZyEGQC3uQLMkTC0pBAFDWN_kD65OBLUOdk-7C8kIRbQljb3TbuJvud2l084lr8GYtW23AzqYCyPVlHqt-fbI93XLo22A=@protonmail.com>

Hi Gloria,

> In summary, it seems that the decisions that might still need attention/input from devs on this mailing list are:
> 1. Whether we should start with multiple-parent-1-child or 1-parent-1-child.
> 2. Whether it's ok to require that the child not have conflicts with mempool transactions.

I would like to point out that package relay is not only useful in Lightning's adversarial scenarii but also for a better user experience of CPFP.
Take for instance a wallet managing coins it can only spend using pre-signed transactions. It may batch these coins into a single transaction, but only after broadcasting the pre-signed tx for each of these coins.
So for a 3 utxos it'd be:
coin1 -----> pres. tx1 ----- |
coin2 -----> pres. tx2 ----- | - - - spending transaction
coin3 -----> pres. tx3 ----- |

Now all these pre-signed transactions are pre-signed with a fixed feerate, which might be below the mempool minimum fee at the time of broadcast.
This is a usecase for multiple-parents-1-child packages. This is also something we do for Revault: you have pre-signed Unvault transactions, each have a CPFP output [0]. Since their confirmation is not security critical, you'd really want to batch the child-fee-paying tx.

Regarding 2. i did not come up with a reason for dropping this rule (yet?) since if you need to replace the child you can use individual submission, and if you need to replace the parent the child itself does not conflict anymore.

Thanks for the effort put into requesting feedback,
Antoine

[0] https://github.com/revault/practical-revault/blob/master/transactions.md#unvault_tx
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211014/9782650f/attachment.html>

From aj at erisian.com.au  Thu Oct 14 23:52:07 2021
From: aj at erisian.com.au (Anthony Towns)
Date: Fri, 15 Oct 2021 09:52:07 +1000
Subject: [bitcoin-dev] On the regularity of soft forks
In-Reply-To: <CAD5xwhj3JCxH1=5Tj+hgiSxLWchLgT584X0YutKVeuibnpwmtA@mail.gmail.com>
References: <LmX3Gnfkf1T0Eb_wUXxPe8c0Tf2DNipfIqufkRS6oOPhttr4iZIOWtjUL_7QkcWEHr8eFvehHooaM140ZBKLwi98F5NwyQKSyEhAPZDK1YQ=@protonmail.com>
 <CAD5xwhj3JCxH1=5Tj+hgiSxLWchLgT584X0YutKVeuibnpwmtA@mail.gmail.com>
Message-ID: <20211014235207.GB6451@erisian.com.au>

On Mon, Oct 11, 2021 at 12:12:58PM -0700, Jeremy via bitcoin-dev wrote:
> > ...?in this post I will argue against frequent soft forks with a single or
> minimal
> > set of features and instead argue for infrequent soft forks with batches
> > of features.
> I think this type of development has been discussed in the past and has been
> rejected.

> AJ:?- improvements: changes might not make everyone better off, but we
> ? ?don't want changes to screw anyone over either -- pareto
> ? ?improvements in economics, "first, do no harm", etc. (if we get this
> ? ?right, there's no need to make compromises and bundle multiple
> ? ?flawed proposals so that everyone's an equal mix of happy and
> ? ?miserable)

I don't think your conclusion above matches my opinion, for what it's
worth.

If you've got two features, A and B, where the game theory is:

 If A happens, I'm +100, You're -50
 If B happens, I'm -50, You're +100

then even though A+B is +50, +50, then I do think the answer should
generally be "think harder and come up with better proposals" rather than
"implement A+B as a bundle that makes us both +50".

_But_ if the two features are more like:

  If C happens, I'm +100, You're +/- 0
  If D happens, I'm +/- 0, You're +100

then I don't have a problem with bundling them together as a single
simultaneous activation of both C and D.

Also, you can have situations where things are better together,
that is:

  If E happens, we're both at +100
  If F happens, we're both at +50
  If E+F both happen, we're both at +9000

In general, I think combining proposals when the combination is better
than the individual proposals were is obviously good; and combining
related proposals into a single activation can be good if it is easier
to think about the ideas as a set. 

It's only when you'd be rejecting the proposal on its own merits that
I think combining it with others is a bad idea in principle.

For specific examples, we bundled schnorr, Taproot, MAST, OP_SUCCESSx
and CHECKSIGADD together because they do have synergies like that; we
didn't bundle ANYPREVOUT and graftroot despite the potential synergies
because those features needed substantially more study.

The nulldummy soft-fork (bip 147) was deployed concurrently with
the segwit soft-fork (bip 141, 143), but I don't think there was any
particular synergy or need for those things to be combined, it just
reduced the overhead of two sets of activation signalling to one.

Note that the implementation code for nulldummy had already been merged
and were applied as relay policy well before activation parameters were
defined (May 2014 via PR#3843 vs Sep 2016 for PR#8636) let alone becoming
an active soft fork.

Cheers,
aj


From aj at erisian.com.au  Fri Oct 15 01:05:12 2021
From: aj at erisian.com.au (Anthony Towns)
Date: Fri, 15 Oct 2021 11:05:12 +1000
Subject: [bitcoin-dev] Taproot testnet wallet
In-Reply-To: <jNk5M87QACL8ySLGzR5sQVaJkQcIV8RrHnbWv6-lkjRhjTL6vEmaMJI7hENUO803wSZM0hOaCyeQI3jjPBbJKiMy8gBXtS_HLmKzPivk2M0=@wuille.net>
References: <cf1aa346-4f0d-8192-c8e8-1861e7840371@schildbach.de>
 <jNk5M87QACL8ySLGzR5sQVaJkQcIV8RrHnbWv6-lkjRhjTL6vEmaMJI7hENUO803wSZM0hOaCyeQI3jjPBbJKiMy8gBXtS_HLmKzPivk2M0=@wuille.net>
Message-ID: <20211015010512.GC6451@erisian.com.au>

On Sat, Oct 09, 2021 at 04:49:42PM +0000, Pieter Wuille via bitcoin-dev wrote:
> You can construct a taproot-capable wallet in Bitcoin Core as follows:
> * Have or create a descriptor wallet (createwallet RPC, with descriptors=true).
> * Import a taproot descriptor (of the form "tr(KEY)"), as active descriptor
> (with active=true), where KEY can be a tprv.../* or any other supported key
> expression.
> * Get a new address with addresstype=bech32m

Running master (which has PR#21500 merged), then the above can be
done with:

1. create a descriptor wallet

  bitcoin-cli -signet -named createwallet wallet_name=descwallet descriptors=true load_on_startup=true

2. get the associated bip32 tprv private key

  TPRV=$(bitcoin-cli -rpcwallet=descwallet -signet listdescriptors true | jq '.descriptors | .[].desc' | sed 's/^.*(//;s/[)/].*//' | uniq | head -n1)

(This step requires PR#21500 to extract the wallet's tprv; you'll need to
be running an updated version of bitcoin-cli here as well as bitcoind. You
could also generate the tprv some other way.)

3. construct the taproot descriptor per BIP 86

  DESC="tr($TPRV/86'/1'/0'/0/*)"
  CHK="$(bitcoin-cli -rpcwallet=descwallet -signet getdescriptorinfo "$DESC" | jq -r .checksum)"

4. import the descriptor

  bitcoin-cli -rpcwallet=descwallet -signet importdescriptors "[{\"desc\": \"$DESC#$CHK\", \"active\": true, \"timestamp\": \"now\", \"range\": [0,1000], \"next_index\": 1}]"

5. get an address

  bitcoin-cli -rpcwallet=descwallet -signet getnewaddress '' bech32m

You can then use the signet faucet to send a few million ssats to that
address directly.

Same stuff works with testnet, though I'm not sure if any testnet faucets
will accept bech32m addresses directly.

This is all a bit deliberately cumbersome prior to taproot activating on
mainnet; once that happens and PR#22364 is merged, you'll only need to
do steps (1) and (5).

Cheers,
aj


From aj at erisian.com.au  Fri Oct 15 04:41:50 2021
From: aj at erisian.com.au (Anthony Towns)
Date: Fri, 15 Oct 2021 14:41:50 +1000
Subject: [bitcoin-dev] Reorgs on SigNet - Looking for feedback on
 approach and parameters
In-Reply-To: <90AD5816-4B44-4BBB-A2FC-39CD381D6395@mattcorallo.com>
References: <20210914045610.GA25475@erisian.com.au>
 <90AD5816-4B44-4BBB-A2FC-39CD381D6395@mattcorallo.com>
Message-ID: <20211015044150.GD6451@erisian.com.au>

On Wed, Sep 15, 2021 at 08:24:43AM -0700, Matt Corallo via bitcoin-dev wrote:
> > On Sep 13, 2021, at 21:56, Anthony Towns <aj at erisian.com.au> wrote:
> > I'm not sure that's really the question you want answered?
> Of course it is? I?d like to understand the initial thinking and design analysis that went into this decision. That seems like an important question to ask when seeking changes in an existing system :).

Well, "are there any drawbacks to doing X instead, because that would make
it easier for me to do Y" just seems like a more interesting question?
Because:

> > Mostly
> > it's just "this is how mainnet works" plus "these are the smallest
> > changes to have blocks be chosen by a signature, rather than entirely
> > by PoW competition".

doesn't seem like that interesting an answer...

To be a bit more specific, it's not at all clear to me what you would
be happy with? (I mean, beyond "something magic that works exactly how
I want it, when I want it, even if I don't know what that is yet or
change my mind later" which is obviously the desired behaviour for all
software everywhere) 

You say you're happier with both mainnet and testnet3 than signet,
but mainnet isn't any faster than signet, while (if you've got an ASIC)
testnet3 will give you a block per second, especially if you don't mind
your blocks getting reorged out. There's a lot of ground between those
two extremes.

> > For integration testing across many services, I think a ten-minute-average
> > between blocks still makes sense -- protocols relying on CSV/CLTV to
> > ensure there's a delay they can use to recover funds, if they specify
> > that in blocks (as lightning's to_self_delay does), then significant
> > surges of blocks will cause uninteresting bugs. 
> Hmm, why would blocks coming quicker lead to a bug? I certainly hope no one has a bug if their block time is faster than per ten minutes. I presume here, you mean something like ?if the node can?t keep up with the block rate?, but I certainly hope the benchmark for may isn?t 10 minutes, or really even one.

The lightning to_self_delay is specified in blocks, but is meant to allow
you to be offline for some real time period; if you specify 1000 blocks
and are sure you'll be online every two days, that's fine on mainnet
and signet as it stands, but broken on testnet.

> > It would be easy enough to change things to target an average of 2 or
> > 5 minutes, I suppose, but then you'd probably need to propogate that
> > logic back into your apps that would otherwise think 144 blocks is around
> > about a day.
> Why? One useful thing for testing is compressing real time.

Sure, but if you're compressing _real_ time you need to manipulate the
nTime not just the number of blocks -- and that might be relevant for
nLocktime or nSequence checks by mtp rather than height. But that's
not something signet's appropriate for: you should be using regtest for
that scenario.

> > We could switch back to doing blocks exactly every 10 minutes, rather
> > than a poisson-ish distribution in the range of 1min to 60min, but that
> > doesn't seem like that huge a win, and makes it hard to test that things
> > behave properly when blocks arrive in bursts.
> Hmm, I suppose? If you want to test that the upper bound doesn?t
> need to be 100 minutes, though, it could be 10.

Mathematically, you can't have an average of 10 minutes and a max of 10
minutes without the minimum also being 10 minutes...

> > Best of luck to you then? Nobody's trying to sell you on a subscription
> > plan to using signet.
> lol, yes, I?m aware of that, nor did I mean to imply that anything has to be targeted at a specific person?s requirements. Rather, my point here is that I?m really confused as to who  the target user *is*, because we should be building products with target users in mind, even if those targets are often ?me? for open source projects.

I don't really think there's a definitive answer to that yet?

My guess is "integration testing" is close to right; whether it be
different services validating they interoperate, or users seeing if a
service works the way they expect in a nearly-live environment.

For private signets, the advantage over regtest is you don't risk some
random participant causing major reorgs, and can reasonably use it over
the internet without having to worry too much about securing things.

For the default public signet, the advantage over regtest is probably that
you've got additional infrastructure already setup (eg explorer.bc-2.jp
and mempool.space/signet, perhaps eventually a decent lightning test
network? there's signet-lightning.wakiyamap.dev)

The advantage of a private signet vs the default public one is probably
only that you can control the consensus rules to introduce and test a
new soft fork if you want. The advantage of the default public signet
over your own private one is probably mostly that it already has
miners/explorers/faucets setup and you don't have to do that yourself.

I think the default public signet makes sense as a "demo mainnet" --
somewhere you can do all the things you can on mainnet, with pretty
similar constraints, but some more interesting behaviours -- like bigger
reorgs, or earlier availability of new features.

So to me that adds up as:

 * are you a single developer making software? do it on regtest so you've
   got complete control

 * are you trying out some software? so it on the default public signet,
   because that's the least effort and will give you a pretty good idea
   how it will behave on mainnet

 * are you checking your software interoperates with some random other
   project in the way you expect? again, default public signet, since
   hopefully the other project already supports it

 * are you doing a collaboration/partnership with another development
   team? do you want a stable environment for your team to collaborate
   on? setup your own private signet for the test environment (but do
   local tests and CI tests via regtest too)

YMMV, that's just my opinion, etc.

FWIW, I was frustrated the other day when trying to mine [0] and seeing
the next block wasn't due for ~40 minutes (and the spend script had a
CSV constraint, so I had to wait for it to be mined before I could get
the followup txs in the mempool). OTOH, it wasn't frustrating enough to
either mine a block early or switch to regtest, so... I guess I see the
point, but not really any particular action to take for it?

[0] https://explorer.bc-2.jp/tx/ba58d99dfaad83e105a0de1a9becfcf8eaf897aaaada54bd7b08134ff579997c?input:0&expand

Cheers,
aj


From micaroni at gmail.com  Fri Oct 15 00:43:40 2021
From: micaroni at gmail.com (micaroni at gmail.com)
Date: Thu, 14 Oct 2021 21:43:40 -0300
Subject: [bitcoin-dev] On the regularity of soft forks
In-Reply-To: <20211014235207.GB6451@erisian.com.au>
References: <LmX3Gnfkf1T0Eb_wUXxPe8c0Tf2DNipfIqufkRS6oOPhttr4iZIOWtjUL_7QkcWEHr8eFvehHooaM140ZBKLwi98F5NwyQKSyEhAPZDK1YQ=@protonmail.com>
 <CAD5xwhj3JCxH1=5Tj+hgiSxLWchLgT584X0YutKVeuibnpwmtA@mail.gmail.com>
 <20211014235207.GB6451@erisian.com.au>
Message-ID: <CAHvMVPQ8jtfdbLg8NJv7bNM3a_nhF_aUfD2gwSdxpfgXQomn3A@mail.gmail.com>

Interesting discussion. Correct me if I'm wrong: but putting too many
features together in one shot just can't make things harder to debug in
production if something very unexpected happens. It's a basic principle of
software engineering.

Change. Deploy. Nothing bad happened? Change it a little more. Deployment.
Or: Change, change, change. Deploy. Did something bad happen? What change
caused the problem?

On Thu, Oct 14, 2021 at 8:53 PM Anthony Towns via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> On Mon, Oct 11, 2021 at 12:12:58PM -0700, Jeremy via bitcoin-dev wrote:
> > > ... in this post I will argue against frequent soft forks with a
> single or
> > minimal
> > > set of features and instead argue for infrequent soft forks with
> batches
> > > of features.
> > I think this type of development has been discussed in the past and has
> been
> > rejected.
>
> > AJ: - improvements: changes might not make everyone better off, but we
> >    don't want changes to screw anyone over either -- pareto
> >    improvements in economics, "first, do no harm", etc. (if we get this
> >    right, there's no need to make compromises and bundle multiple
> >    flawed proposals so that everyone's an equal mix of happy and
> >    miserable)
>
> I don't think your conclusion above matches my opinion, for what it's
> worth.
>
> If you've got two features, A and B, where the game theory is:
>
>  If A happens, I'm +100, You're -50
>  If B happens, I'm -50, You're +100
>
> then even though A+B is +50, +50, then I do think the answer should
> generally be "think harder and come up with better proposals" rather than
> "implement A+B as a bundle that makes us both +50".
>
> _But_ if the two features are more like:
>
>   If C happens, I'm +100, You're +/- 0
>   If D happens, I'm +/- 0, You're +100
>
> then I don't have a problem with bundling them together as a single
> simultaneous activation of both C and D.
>
> Also, you can have situations where things are better together,
> that is:
>
>   If E happens, we're both at +100
>   If F happens, we're both at +50
>   If E+F both happen, we're both at +9000
>
> In general, I think combining proposals when the combination is better
> than the individual proposals were is obviously good; and combining
> related proposals into a single activation can be good if it is easier
> to think about the ideas as a set.
>
> It's only when you'd be rejecting the proposal on its own merits that
> I think combining it with others is a bad idea in principle.
>
> For specific examples, we bundled schnorr, Taproot, MAST, OP_SUCCESSx
> and CHECKSIGADD together because they do have synergies like that; we
> didn't bundle ANYPREVOUT and graftroot despite the potential synergies
> because those features needed substantially more study.
>
> The nulldummy soft-fork (bip 147) was deployed concurrently with
> the segwit soft-fork (bip 141, 143), but I don't think there was any
> particular synergy or need for those things to be combined, it just
> reduced the overhead of two sets of activation signalling to one.
>
> Note that the implementation code for nulldummy had already been merged
> and were applied as relay policy well before activation parameters were
> defined (May 2014 via PR#3843 vs Sep 2016 for PR#8636) let alone becoming
> an active soft fork.
>
> Cheers,
> aj
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211014/5852b42f/attachment-0001.html>

From vjudeu at gazeta.pl  Fri Oct 15 11:51:45 2021
From: vjudeu at gazeta.pl (vjudeu at gazeta.pl)
Date: Fri, 15 Oct 2021 13:51:45 +0200
Subject: [bitcoin-dev] Taproot testnet wallet
In-Reply-To: <20211015010512.GC6451@erisian.com.au>
Message-ID: <143779041-18cce0a32a943cfd46965c00b6634d22@pmq4v.m5r2.onet>

On 2021-10-15 03:05:36 user Anthony Towns via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:

> Same stuff works with testnet, though I'm not sure if any testnet faucets
will accept bech32m addresses directly.

There are faucets that accept such addresses, for example https://bitcoinfaucet.uo1.net/, but you have to use bech32 checksum instead of bech32m, for example sending to tb1p84x2ryuyfevgnlpnxt9f39gm7r68gwtvllxqe5w2n5ru00s9aqus27cytz works fine, but sending to tb1p84x2ryuyfevgnlpnxt9f39gm7r68gwtvllxqe5w2n5ru00s9aquslzggwq does not work. The same with pages like mempool.space, you have to use bech32 instead of bech32m, then you can send to taproot from some old faucets or browse taproot addresses from some old block explorers.

From prayank at tutanota.de  Fri Oct 15 14:12:23 2021
From: prayank at tutanota.de (Prayank)
Date: Fri, 15 Oct 2021 16:12:23 +0200 (CEST)
Subject: [bitcoin-dev] Taproot testnet wallet
Message-ID: <Mm3HyxT--3-2@tutanota.de>

Hi Andreas,

> I'm trying to finish off bitcoinj's implementation for sending to 
taproot addresses. For this, I'd like to test against a wallet that can
receive to P2TR and spend back.

I did this transaction few days back which creates a P2TR output while answering a question on Bitcoin Stackexchange: https://blockstream.info/testnet/tx/2035ead4a9d0c8e2da1184924abc9034d26f2a7093371183ef12891623b235d1

Pieter Wuille and Anthony Towns already shared things that would be helpful. Still wanted to share the steps I did for above transaction:

https://bitcoin.stackexchange.com/a/108013/

TL;DR - 

1.Create a blank descriptor wallet with private keys disabled

2.Import TPUB descriptor in wallet

3.Create new bech32m address

4.Send some bitcoin from another wallet

The answer could be improved if test vectors are added in BIP 86 for TPRV for follow things suggested by Anthony Towns in last email.

-- 
Prayank

A3B1 E430 2298 178F
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211015/6fc09173/attachment-0001.html>

From jamtlu at gmail.com  Fri Oct 15 15:27:42 2021
From: jamtlu at gmail.com (James Lu)
Date: Fri, 15 Oct 2021 11:27:42 -0400
Subject: [bitcoin-dev] Year 2038 problem and year 2106 chain halting
In-Reply-To: <50769965-423dd279413d4dba11ba459cbd98387b@pmq6v.m5r2.onet>
References: <50769965-423dd279413d4dba11ba459cbd98387b@pmq6v.m5r2.onet>
Message-ID: <CANQHGB2Vm31Y8aGsrx3qNitRWh4y950LxxjzHqrD=qzwN=FmVw@mail.gmail.com>

Making Bitcoin function after 2038 is by definition a hard fork

I feel if we do HF, we should bundle other HF changes with it...

On Wed, Oct 13, 2021 at 5:19 PM vjudeu via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> It seems that Bitcoin Core will stop working in 2038 because of assertion
> checking if the current time is non-negative. Also, the whole chain will
> halt after reaching median time 0xffffffff in 2106. More information:
> https://bitcointalk.org/index.php?topic=5365359.0
>
> I wonder if that kind of issues are possible to fix in a soft-fork way.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211015/c898de49/attachment-0001.html>

From yanmaani at cock.li  Fri Oct 15 15:44:59 2021
From: yanmaani at cock.li (yanmaani at cock.li)
Date: Fri, 15 Oct 2021 15:44:59 +0000
Subject: [bitcoin-dev] Year 2038 problem and year 2106 chain halting
In-Reply-To: <50769965-423dd279413d4dba11ba459cbd98387b@pmq6v.m5r2.onet>
References: <50769965-423dd279413d4dba11ba459cbd98387b@pmq6v.m5r2.onet>
Message-ID: <5978620b3db064897840b6170eed25d2@cock.li>

It's well-known. Nobody really cares, because it's so far off. Not 
possible to do by softfork, no. It is possible to do by something that 
becomes a hardfork in 80 years, though, which is probably good enough.

I proposed a solution, but nobody was really interested. Let's see if 
anyone bites now.

---

Subject: Suggestion: Solve year 2106 problem by taking timestamps mod 
2^32
To 	Bitcoin Protocol Discussion
Date 	2020-09-19 12:36
Message Body
Currently, Bitcoin's timestamp rules are as follows:

1. The block timestamp may not be lower than the median of the last 11 
blocks'
2. The block timestamp may not be greater than the current time plus two 
hours
3. The block timestamp may not be greater than 2^32 (Sun, 07 Feb 2106 
06:28:16 +0000)

Thus, Bitcoin will "die" on or about 2106-02-07, when there is no 
timestamp below 2^32 that exceeds the median of the last 11 blocks.

If the rules were changed to the following, this problem would be 
solved:

1. The block timestamp plus k*2^32 may not be lower than the median of 
the last 11 blocks'
2. The block timestamp plus k*2^32 may not be greater than the current 
time plus two hours
3. k is an integer, whose value must be the same for the calculations of 
Rule 1 and Rule 2

This would cause a hardfork in the year 2106, which is approximately 
85.5 years from now, by which time 95% of nodes would hopefully have 
updated.

Another proposed solution is 64-bit timestamps. They would break 
compatibility with other software that has specific expectations of 
header fields, like ASICs' firmware. They would also cause a hardfork 
before the date of timestamp overflow. I thus believe them to be a less 
appropriate solution.

What do you think of this idea? Is it worth a BIP?

On 2021-10-13 19:16, vjudeu via bitcoin-dev wrote:
> It seems that Bitcoin Core will stop working in 2038 because of
> assertion checking if the current time is non-negative. Also, the
> whole chain will halt after reaching median time 0xffffffff in 2106.
> More information: https://bitcointalk.org/index.php?topic=5365359.0
> 
> I wonder if that kind of issues are possible to fix in a soft-fork
> way.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From vjudeu at gazeta.pl  Fri Oct 15 22:22:00 2021
From: vjudeu at gazeta.pl (vjudeu at gazeta.pl)
Date: Sat, 16 Oct 2021 00:22:00 +0200
Subject: [bitcoin-dev] Year 2038 problem and year 2106 chain halting
In-Reply-To: <5978620b3db064897840b6170eed25d2@cock.li>
Message-ID: <143289360-eb35e705fded3eb4175a6f8d7669b3a0@pmq5v.m5r2.onet>

Your solution seems to solve the problem of chain halting, but there are more issues. For example: if you have some time modulo 2^32, then you no longer know if timestamp zero is related to 1970 or 2106 or some higher year. Your "k" value representing in fact the most significant 32 bits of 64-bit timestamp has to be stored in all cases where time is used. If there is no "k", then zero should be used for backward compatibility. Skipping "k" could cause problems related to OP_CHECKLOCKTIMEVERIFY or nLockTime, because if some transaction was timestamped to 0xbadc0ded, then that transaction will be valid in 0x00000000badc0ded, invalid in 0x0000000100000000, and valid again in 0x00000001badc0ded, the same for timelocked outputs.

So, I think your "k" value should be added to the coinbase transaction, then you can combine two 32-bit values, the lower bits from the block header and the higher bits from the coinbase transaction. Also, adding your "k" value transaction nLockTime field is needed (maybe in a similar way as transaction witness was added in Segwit), because in other case after reaching 0x0000000100000000 all off-chain transactions with timelocks around 0x00000000ffffffff will be additionally timelocked for the next N years. The same is needed for each OP_CHECKLOCKTIMEVERIFY, maybe pushing high 32 bits before the currently used value will solve that (and assuming zero if there is only some 32-bit value).

On 2021-10-15 23:48:59 user yanmaani at cock.li wrote:
> It's well-known. Nobody really cares, because it's so far off. Not 
possible to do by softfork, no. It is possible to do by something that 
becomes a hardfork in 80 years, though, which is probably good enough.

I proposed a solution, but nobody was really interested. Let's see if 
anyone bites now.

---

Subject: Suggestion: Solve year 2106 problem by taking timestamps mod 
2^32
To 	Bitcoin Protocol Discussion
Date 	2020-09-19 12:36
Message Body
Currently, Bitcoin's timestamp rules are as follows:

1. The block timestamp may not be lower than the median of the last 11 
blocks'
2. The block timestamp may not be greater than the current time plus two 
hours
3. The block timestamp may not be greater than 2^32 (Sun, 07 Feb 2106 
06:28:16 +0000)

Thus, Bitcoin will "die" on or about 2106-02-07, when there is no 
timestamp below 2^32 that exceeds the median of the last 11 blocks.

If the rules were changed to the following, this problem would be 
solved:

1. The block timestamp plus k*2^32 may not be lower than the median of 
the last 11 blocks'
2. The block timestamp plus k*2^32 may not be greater than the current 
time plus two hours
3. k is an integer, whose value must be the same for the calculations of 
Rule 1 and Rule 2

This would cause a hardfork in the year 2106, which is approximately 
85.5 years from now, by which time 95% of nodes would hopefully have 
updated.

Another proposed solution is 64-bit timestamps. They would break 
compatibility with other software that has specific expectations of 
header fields, like ASICs' firmware. They would also cause a hardfork 
before the date of timestamp overflow. I thus believe them to be a less 
appropriate solution.

What do you think of this idea? Is it worth a BIP?

On 2021-10-13 19:16, vjudeu via bitcoin-dev wrote:
> It seems that Bitcoin Core will stop working in 2038 because of
> assertion checking if the current time is non-negative. Also, the
> whole chain will halt after reaching median time 0xffffffff in 2106.
> More information: https://bitcointalk.org/index.php?topic=5365359.0
> 
> I wonder if that kind of issues are possible to fix in a soft-fork
> way.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


From ZmnSCPxj at protonmail.com  Fri Oct 15 23:01:46 2021
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Fri, 15 Oct 2021 23:01:46 +0000
Subject: [bitcoin-dev] Year 2038 problem and year 2106 chain halting
In-Reply-To: <5978620b3db064897840b6170eed25d2@cock.li>
References: <50769965-423dd279413d4dba11ba459cbd98387b@pmq6v.m5r2.onet>
 <5978620b3db064897840b6170eed25d2@cock.li>
Message-ID: <nSiUl71p9JyISxvRJ3Jq71zNahe-rpanbFFv1MSHSk7rUKjq36yD7vmrJQ5Pnh5oUdDAFflgSzbCE5KK7RacFRepvjqFc9xp9qT7hU-twXA=@protonmail.com>

Good morning yanmaani,


> It's well-known. Nobody really cares, because it's so far off. Not
> possible to do by softfork, no.

I think it is possible by softfork if we try hard enough?


> 1.  The block timestamp may not be lower than the median of the last 11
>     blocks'
>
> 2.  The block timestamp may not be greater than the current time plus two
>     hours
>
> 3.  The block timestamp may not be greater than 2^32 (Sun, 07 Feb 2106
>     06:28:16 +0000)

What happens if a series of blocks has a timestamp of 0xFFFFFFFF at the appropriate time?

In that case:

1.  Is not violated, since "not lower than" means "greater than or equal to", and after a while the median becomes 0xFFFFFFFF and 0xFFFFFFFF == 0xFFFFFFFF
2.  Is not violated, since it would be a past actual real time.
3.  Is not violated since 0xFFFFFFFF < 0x100000000.

In that case, we could then add an additional rule, which is that a 64-bit (or 128-bit, or 256-bit) timestamp has to be present in the coinbase transaction, with similar rules except translated to 64-bit/128-bit/256-bit.

Possibly a similar scheme could be used for `nLockTime`; we could put a 64-bit `nLockTime64` in that additional signed block in Taproot SegWit v1 if the legacy v`nLockTime` is at the maximum seconds-timelock possible.

Regards,
ZmnSCPxj


From michaelfolkson at protonmail.com  Sat Oct 16 11:02:08 2021
From: michaelfolkson at protonmail.com (Michael Folkson)
Date: Sat, 16 Oct 2021 11:02:08 +0000
Subject: [bitcoin-dev] On the regularity of soft forks
In-Reply-To: <CAHvMVPQ8jtfdbLg8NJv7bNM3a_nhF_aUfD2gwSdxpfgXQomn3A@mail.gmail.com>
References: <LmX3Gnfkf1T0Eb_wUXxPe8c0Tf2DNipfIqufkRS6oOPhttr4iZIOWtjUL_7QkcWEHr8eFvehHooaM140ZBKLwi98F5NwyQKSyEhAPZDK1YQ=@protonmail.com>
 <CAD5xwhj3JCxH1=5Tj+hgiSxLWchLgT584X0YutKVeuibnpwmtA@mail.gmail.com>
 <20211014235207.GB6451@erisian.com.au>
 <CAHvMVPQ8jtfdbLg8NJv7bNM3a_nhF_aUfD2gwSdxpfgXQomn3A@mail.gmail.com>
Message-ID: <1HjQQw-RXvEW5i73Hjx_QqDms44sQMnNWWl9oQ_SwIoYGpog6LzGK4M_omAEMXxgXIID37V7sdyG_AW8WkaNByppB2EJ7wlzOZgrDloMv2c=@protonmail.com>

> Interesting discussion.Correct me if I'm wrong: but putting too many features together in one shot just can't make things harder to debug in production if something very unexpected happens.It's a basic principle of software engineering.

Soft fork features can (and should) obviously be tested thoroughly on testnet, signet, custom signets, sidechains etc on a standalone basis and a bundled basis. But whether or not it is a basic principle of general software engineering kind of misses the point. Security critical software clearly isn't engineered in the same way as a new social media app. Bugs are easily reverted in a new social media app. A consensus change is extremely hard to revert and probably requires a hard fork, a level of central coordination we generally attempt to avoid and a speed of deployment that we also attempt to avoid. On top of that we aren't just dealing with security critical software. One of the most important objectives is to keep all the nodes on the network in consensus. Introducing a consensus change before we are comfortable there is community consensus for it is a massive effective bug in itself. The network can split in multiple ways e.g. part of the network disagrees on whether to activate the consensus change, part of the network disagrees on how to resist that consensus change, part of the network disagrees on how to activate that consensus change etc

In addition, a social media app can experiment in production whether Feature A works, whether Feature B works or whether Feature A and B work best together. In Bitcoin if we activate consensus Feature A, later decide we want consensus Feature B but find out that by previously activating Feature A we can't have Feature B (it is now unsafe to activate it) or its design now has to be suboptimal because we have to ensure it can safely work in the presence of Feature A we have made a mistake by activating Feature A in the first place. Decentralized security critical consensus changes are an emerging field in itself and really can't be treated like any other software project. This will become universally understood I'm sure over time.

--

Michael Folkson
Email: michaelfolkson at protonmail.com
Keybase: michaelfolkson
PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3

??????? Original Message ???????
On Friday, October 15th, 2021 at 1:43 AM, Felipe Micaroni Lalli via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:

> Interesting discussion.Correct me if I'm wrong: but putting too many features together in one shot just can't make things harder to debug in production if something very unexpected happens. It's a basic principle of software engineering.
>
> Change. Deploy. Nothing bad happened? Change it a little more. Deployment.
>
> Or:Change, change, change. Deploy. Did something bad happen? What change caused the problem?
>
> On Thu, Oct 14, 2021 at 8:53 PM Anthony Towns via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> On Mon, Oct 11, 2021 at 12:12:58PM -0700, Jeremy via bitcoin-dev wrote:
>>> > ... in this post I will argue against frequent soft forks with a single or
>>> minimal
>>> > set of features and instead argue for infrequent soft forks with batches
>>> > of features.
>>> I think this type of development has been discussed in the past and has been
>>> rejected.
>>
>>> AJ: - improvements: changes might not make everyone better off, but we
>>> don't want changes to screw anyone over either -- pareto
>>> improvements in economics, "first, do no harm", etc. (if we get this
>>> right, there's no need to make compromises and bundle multiple
>>> flawed proposals so that everyone's an equal mix of happy and
>>> miserable)
>>
>> I don't think your conclusion above matches my opinion, for what it's
>> worth.
>>
>> If you've got two features, A and B, where the game theory is:
>>
>> If A happens, I'm +100, You're -50
>> If B happens, I'm -50, You're +100
>>
>> then even though A+B is +50, +50, then I do think the answer should
>> generally be "think harder and come up with better proposals" rather than
>> "implement A+B as a bundle that makes us both +50".
>>
>> _But_ if the two features are more like:
>>
>> If C happens, I'm +100, You're +/- 0
>> If D happens, I'm +/- 0, You're +100
>>
>> then I don't have a problem with bundling them together as a single
>> simultaneous activation of both C and D.
>>
>> Also, you can have situations where things are better together,
>> that is:
>>
>> If E happens, we're both at +100
>> If F happens, we're both at +50
>> If E+F both happen, we're both at +9000
>>
>> In general, I think combining proposals when the combination is better
>> than the individual proposals were is obviously good; and combining
>> related proposals into a single activation can be good if it is easier
>> to think about the ideas as a set.
>>
>> It's only when you'd be rejecting the proposal on its own merits that
>> I think combining it with others is a bad idea in principle.
>>
>> For specific examples, we bundled schnorr, Taproot, MAST, OP_SUCCESSx
>> and CHECKSIGADD together because they do have synergies like that; we
>> didn't bundle ANYPREVOUT and graftroot despite the potential synergies
>> because those features needed substantially more study.
>>
>> The nulldummy soft-fork (bip 147) was deployed concurrently with
>> the segwit soft-fork (bip 141, 143), but I don't think there was any
>> particular synergy or need for those things to be combined, it just
>> reduced the overhead of two sets of activation signalling to one.
>>
>> Note that the implementation code for nulldummy had already been merged
>> and were applied as relay policy well before activation parameters were
>> defined (May 2014 via PR#3843 vs Sep 2016 for PR#8636) let alone becoming
>> an active soft fork.
>>
>> Cheers,
>> aj
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211016/7c3b157b/attachment-0001.html>

From vjudeu at gazeta.pl  Sat Oct 16 09:06:17 2021
From: vjudeu at gazeta.pl (vjudeu at gazeta.pl)
Date: Sat, 16 Oct 2021 11:06:17 +0200
Subject: [bitcoin-dev] Year 2038 problem and year 2106 chain halting
In-Reply-To: <nSiUl71p9JyISxvRJ3Jq71zNahe-rpanbFFv1MSHSk7rUKjq36yD7vmrJQ5Pnh5oUdDAFflgSzbCE5KK7RacFRepvjqFc9xp9qT7hU-twXA=@protonmail.com>
Message-ID: <143903239-0c7634127ba6ddee7e69b14740b993cd@pmq3v.m5r2.onet>

> What happens if a series of blocks has a timestamp of 0xFFFFFFFF at the appropriate time?

The chain will halt for all old clients, because there is no 32-bit value greater than 0xffffffff.

> 1. Is not violated, since "not lower than" means "greater than or equal to"

No, because it has to be strictly "greater than" in the Bitcoin Core source code, it is rejected when it is "lower or equal to", see: https://github.com/bitcoin/bitcoin/blob/6f0cbc75be7644c276650fd98bfdb6358b827399/src/validation.cpp#L3089-L3094

> 2. Is not violated, since it would be a past actual real time.

If the current time is 0x0000000100000000, then the lowest 32 bits will point to some time around 1970, so for old clients two rules are violated at the same time.

> 3. Is not violated since 0xFFFFFFFF < 0x100000000.

This is hard to change, because 32-bit timestamps are included in block headers, so using any wider data type here will make it hardware-incompatible and will cause a hard-fork. That's why I think new timestamps should be placed in the coinbase transaction. But that still does not solve chain halting problem.

To test chain halting, all that is needed is starting regtest and producing one block with 0xffffffff timestamp, just after the Genesis Block. Then, median time is equal to 0xffffffff and adding any new blocks is no longer possible. The only soft-fork solution I can see require overwriting that block.

Example from https://bitcointalk.org/index.php?topic=5365359.0

submitblock 0100000006226e46111a0b59caaf126043eb5bbf28c34f3a5e332a1fc7b2b73cf188910f3663c0de115e2239e05df4df9c4bfa01b8e843aaf5dae590cac1d9bac0d44c0fffffffffffff7f200100000001020000000001010000000000000000000000000000000000000000000000000000000000000000ffffffff03510101ffffffff0200f2052a010000001976a91462e907b15cbf27d5425399ebf6f0fb50ebb88f1888ac0000000000000000266a24aa21a9ede2f61c3f71d1defd3fa999dfa36953755c690689799962b48bebd836974e8cf90120000000000000000000000000000000000000000000000000000000000000000000000000
null
generatetoaddress 1 mpXwg4jMtRhuSpVq4xS3HFHmCmWp9NyGKt
CreateNewBlock: TestBlockValidity failed: time-too-old, block's timestamp is too early (code -1)

I don't know any timestamp that can be used in any next block and accepted by old nodes.

On 2021-10-16 01:01:54 user ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:
> Good morning yanmaani,


> It's well-known. Nobody really cares, because it's so far off. Not
> possible to do by softfork, no.

I think it is possible by softfork if we try hard enough?


> 1.  The block timestamp may not be lower than the median of the last 11
>     blocks'
>
> 2.  The block timestamp may not be greater than the current time plus two
>     hours
>
> 3.  The block timestamp may not be greater than 2^32 (Sun, 07 Feb 2106
>     06:28:16 +0000)

What happens if a series of blocks has a timestamp of 0xFFFFFFFF at the appropriate time?

In that case:

1.  Is not violated, since "not lower than" means "greater than or equal to", and after a while the median becomes 0xFFFFFFFF and 0xFFFFFFFF == 0xFFFFFFFF
2.  Is not violated, since it would be a past actual real time.
3.  Is not violated since 0xFFFFFFFF < 0x100000000.

In that case, we could then add an additional rule, which is that a 64-bit (or 128-bit, or 256-bit) timestamp has to be present in the coinbase transaction, with similar rules except translated to 64-bit/128-bit/256-bit.

Possibly a similar scheme could be used for `nLockTime`; we could put a 64-bit `nLockTime64` in that additional signed block in Taproot SegWit v1 if the legacy v`nLockTime` is at the maximum seconds-timelock possible.

Regards,
ZmnSCPxj



From david at bakins-bits.com  Sat Oct 16 20:37:27 2021
From: david at bakins-bits.com (David Bakin)
Date: Sat, 16 Oct 2021 13:37:27 -0700
Subject: [bitcoin-dev] Year 2038 problem and year 2106 chain halting
In-Reply-To: <143903239-0c7634127ba6ddee7e69b14740b993cd@pmq3v.m5r2.onet>
References: <nSiUl71p9JyISxvRJ3Jq71zNahe-rpanbFFv1MSHSk7rUKjq36yD7vmrJQ5Pnh5oUdDAFflgSzbCE5KK7RacFRepvjqFc9xp9qT7hU-twXA=@protonmail.com>
 <143903239-0c7634127ba6ddee7e69b14740b993cd@pmq3v.m5r2.onet>
Message-ID: <CAF-gu8h0BJOnHUjdjLPh4_pQ0j1Xxuwb5axQnNetk_ctTNo-4Q@mail.gmail.com>

yes but ... just for the sake of argument ... if a change such as this
wraparound interpretation is made anytime in the next 5 years it'll be over
a *decade after that *before any wrapped-around timestamp is legitimately
mined ... and by then nobody will be running incompatible (decade old) node
software (especially since it would mean that a decade had gone by without
a *single* consensus change ... seems very unlikely).

On Sat, Oct 16, 2021 at 11:57 AM vjudeu via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> > What happens if a series of blocks has a timestamp of 0xFFFFFFFF at the
> appropriate time?
>
> The chain will halt for all old clients, because there is no 32-bit value
> greater than 0xffffffff.
>
> > 1. Is not violated, since "not lower than" means "greater than or equal
> to"
>
> No, because it has to be strictly "greater than" in the Bitcoin Core
> source code, it is rejected when it is "lower or equal to", see:
> https://github.com/bitcoin/bitcoin/blob/6f0cbc75be7644c276650fd98bfdb6358b827399/src/validation.cpp#L3089-L3094
>
> > 2. Is not violated, since it would be a past actual real time.
>
> If the current time is 0x0000000100000000, then the lowest 32 bits will
> point to some time around 1970, so for old clients two rules are violated
> at the same time.
>
> > 3. Is not violated since 0xFFFFFFFF < 0x100000000.
>
> This is hard to change, because 32-bit timestamps are included in block
> headers, so using any wider data type here will make it
> hardware-incompatible and will cause a hard-fork. That's why I think new
> timestamps should be placed in the coinbase transaction. But that still
> does not solve chain halting problem.
>
> To test chain halting, all that is needed is starting regtest and
> producing one block with 0xffffffff timestamp, just after the Genesis
> Block. Then, median time is equal to 0xffffffff and adding any new blocks
> is no longer possible. The only soft-fork solution I can see require
> overwriting that block.
>
> Example from https://bitcointalk.org/index.php?topic=5365359.0
>
> submitblock
> 0100000006226e46111a0b59caaf126043eb5bbf28c34f3a5e332a1fc7b2b73cf188910f3663c0de115e2239e05df4df9c4bfa01b8e843aaf5dae590cac1d9bac0d44c0fffffffffffff7f200100000001020000000001010000000000000000000000000000000000000000000000000000000000000000ffffffff03510101ffffffff0200f2052a010000001976a91462e907b15cbf27d5425399ebf6f0fb50ebb88f1888ac0000000000000000266a24aa21a9ede2f61c3f71d1defd3fa999dfa36953755c690689799962b48bebd836974e8cf90120000000000000000000000000000000000000000000000000000000000000000000000000
> null
> generatetoaddress 1 mpXwg4jMtRhuSpVq4xS3HFHmCmWp9NyGKt
> CreateNewBlock: TestBlockValidity failed: time-too-old, block's timestamp
> is too early (code -1)
>
> I don't know any timestamp that can be used in any next block and accepted
> by old nodes.
>
> On 2021-10-16 01:01:54 user ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:
> > Good morning yanmaani,
>
>
> > It's well-known. Nobody really cares, because it's so far off. Not
> > possible to do by softfork, no.
>
> I think it is possible by softfork if we try hard enough?
>
>
> > 1.  The block timestamp may not be lower than the median of the last 11
> >     blocks'
> >
> > 2.  The block timestamp may not be greater than the current time plus two
> >     hours
> >
> > 3.  The block timestamp may not be greater than 2^32 (Sun, 07 Feb 2106
> >     06:28:16 +0000)
>
> What happens if a series of blocks has a timestamp of 0xFFFFFFFF at the
> appropriate time?
>
> In that case:
>
> 1.  Is not violated, since "not lower than" means "greater than or equal
> to", and after a while the median becomes 0xFFFFFFFF and 0xFFFFFFFF ==
> 0xFFFFFFFF
> 2.  Is not violated, since it would be a past actual real time.
> 3.  Is not violated since 0xFFFFFFFF < 0x100000000.
>
> In that case, we could then add an additional rule, which is that a 64-bit
> (or 128-bit, or 256-bit) timestamp has to be present in the coinbase
> transaction, with similar rules except translated to 64-bit/128-bit/256-bit.
>
> Possibly a similar scheme could be used for `nLockTime`; we could put a
> 64-bit `nLockTime64` in that additional signed block in Taproot SegWit v1
> if the legacy v`nLockTime` is at the maximum seconds-timelock possible.
>
> Regards,
> ZmnSCPxj
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211016/c540960f/attachment.html>

From mercedes.catherine.salazar at gmail.com  Sat Oct 16 21:34:32 2021
From: mercedes.catherine.salazar at gmail.com (Kate Salazar)
Date: Sat, 16 Oct 2021 23:34:32 +0200
Subject: [bitcoin-dev] Year 2038 problem and year 2106 chain halting
In-Reply-To: <CAF-gu8h0BJOnHUjdjLPh4_pQ0j1Xxuwb5axQnNetk_ctTNo-4Q@mail.gmail.com>
References: <nSiUl71p9JyISxvRJ3Jq71zNahe-rpanbFFv1MSHSk7rUKjq36yD7vmrJQ5Pnh5oUdDAFflgSzbCE5KK7RacFRepvjqFc9xp9qT7hU-twXA=@protonmail.com>
 <143903239-0c7634127ba6ddee7e69b14740b993cd@pmq3v.m5r2.onet>
 <CAF-gu8h0BJOnHUjdjLPh4_pQ0j1Xxuwb5axQnNetk_ctTNo-4Q@mail.gmail.com>
Message-ID: <CAHiDt8CnLWkze8HTdPvh97n-U+A+WZyET1jNHxCGcoLR3K8opg@mail.gmail.com>

Hi, BIP 42 is a code base consensus soft fork that at the time of
activation does not really manifest as a fork because nobody is running any
code not already applying it. Can a similar thing be done in 17 years? (I
haven't really made sense of this year 2038 problem, I don't know or
understand what is required if there's something to be done).
Cheers!

On Sat, Oct 16, 2021 at 11:00 PM David Bakin via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> yes but ... just for the sake of argument ... if a change such as this
> wraparound interpretation is made anytime in the next 5 years it'll be over
> a *decade after that *before any wrapped-around timestamp is legitimately
> mined ... and by then nobody will be running incompatible (decade old) node
> software (especially since it would mean that a decade had gone by without
> a *single* consensus change ... seems very unlikely).
>
> On Sat, Oct 16, 2021 at 11:57 AM vjudeu via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> > What happens if a series of blocks has a timestamp of 0xFFFFFFFF at the
>> appropriate time?
>>
>> The chain will halt for all old clients, because there is no 32-bit value
>> greater than 0xffffffff.
>>
>> > 1. Is not violated, since "not lower than" means "greater than or equal
>> to"
>>
>> No, because it has to be strictly "greater than" in the Bitcoin Core
>> source code, it is rejected when it is "lower or equal to", see:
>> https://github.com/bitcoin/bitcoin/blob/6f0cbc75be7644c276650fd98bfdb6358b827399/src/validation.cpp#L3089-L3094
>>
>> > 2. Is not violated, since it would be a past actual real time.
>>
>> If the current time is 0x0000000100000000, then the lowest 32 bits will
>> point to some time around 1970, so for old clients two rules are violated
>> at the same time.
>>
>> > 3. Is not violated since 0xFFFFFFFF < 0x100000000.
>>
>> This is hard to change, because 32-bit timestamps are included in block
>> headers, so using any wider data type here will make it
>> hardware-incompatible and will cause a hard-fork. That's why I think new
>> timestamps should be placed in the coinbase transaction. But that still
>> does not solve chain halting problem.
>>
>> To test chain halting, all that is needed is starting regtest and
>> producing one block with 0xffffffff timestamp, just after the Genesis
>> Block. Then, median time is equal to 0xffffffff and adding any new blocks
>> is no longer possible. The only soft-fork solution I can see require
>> overwriting that block.
>>
>> Example from https://bitcointalk.org/index.php?topic=5365359.0
>>
>> submitblock
>> 0100000006226e46111a0b59caaf126043eb5bbf28c34f3a5e332a1fc7b2b73cf188910f3663c0de115e2239e05df4df9c4bfa01b8e843aaf5dae590cac1d9bac0d44c0fffffffffffff7f200100000001020000000001010000000000000000000000000000000000000000000000000000000000000000ffffffff03510101ffffffff0200f2052a010000001976a91462e907b15cbf27d5425399ebf6f0fb50ebb88f1888ac0000000000000000266a24aa21a9ede2f61c3f71d1defd3fa999dfa36953755c690689799962b48bebd836974e8cf90120000000000000000000000000000000000000000000000000000000000000000000000000
>> null
>> generatetoaddress 1 mpXwg4jMtRhuSpVq4xS3HFHmCmWp9NyGKt
>> CreateNewBlock: TestBlockValidity failed: time-too-old, block's timestamp
>> is too early (code -1)
>>
>> I don't know any timestamp that can be used in any next block and
>> accepted by old nodes.
>>
>> On 2021-10-16 01:01:54 user ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:
>> > Good morning yanmaani,
>>
>>
>> > It's well-known. Nobody really cares, because it's so far off. Not
>> > possible to do by softfork, no.
>>
>> I think it is possible by softfork if we try hard enough?
>>
>>
>> > 1.  The block timestamp may not be lower than the median of the last 11
>> >     blocks'
>> >
>> > 2.  The block timestamp may not be greater than the current time plus
>> two
>> >     hours
>> >
>> > 3.  The block timestamp may not be greater than 2^32 (Sun, 07 Feb 2106
>> >     06:28:16 +0000)
>>
>> What happens if a series of blocks has a timestamp of 0xFFFFFFFF at the
>> appropriate time?
>>
>> In that case:
>>
>> 1.  Is not violated, since "not lower than" means "greater than or equal
>> to", and after a while the median becomes 0xFFFFFFFF and 0xFFFFFFFF ==
>> 0xFFFFFFFF
>> 2.  Is not violated, since it would be a past actual real time.
>> 3.  Is not violated since 0xFFFFFFFF < 0x100000000.
>>
>> In that case, we could then add an additional rule, which is that a
>> 64-bit (or 128-bit, or 256-bit) timestamp has to be present in the coinbase
>> transaction, with similar rules except translated to 64-bit/128-bit/256-bit.
>>
>> Possibly a similar scheme could be used for `nLockTime`; we could put a
>> 64-bit `nLockTime64` in that additional signed block in Taproot SegWit v1
>> if the legacy v`nLockTime` is at the maximum seconds-timelock possible.
>>
>> Regards,
>> ZmnSCPxj
>>
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211016/25370194/attachment-0001.html>

From ZmnSCPxj at protonmail.com  Sat Oct 16 23:23:15 2021
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Sat, 16 Oct 2021 23:23:15 +0000
Subject: [bitcoin-dev] Year 2038 problem and year 2106 chain halting
In-Reply-To: <143903239-0c7634127ba6ddee7e69b14740b993cd@pmq3v.m5r2.onet>
References: <143903239-0c7634127ba6ddee7e69b14740b993cd@pmq3v.m5r2.onet>
Message-ID: <eAo_By_Oe44ra6anVBlZg2UbfKfzhZ1b1vtaF0NuIjdJcB_niagHBS-SoU2qcLzjDj8Kuo67O_FnBSuIgskAi2_fCsLE6_d4SwWq9skHuQI=@protonmail.com>

Good morning vjudeu,

> > What happens if a series of blocks has a timestamp of 0xFFFFFFFF at the appropriate time?
>
> The chain will halt for all old clients, because there is no 32-bit value greater than 0xffffffff.
>
> > 1.  Is not violated, since "not lower than" means "greater than or equal to"
>
> No, because it has to be strictly "greater than" in the Bitcoin Core source code, it is rejected when it is "lower or equal to", see:https://github.com/bitcoin/bitcoin/blob/6f0cbc75be7644c276650fd98bfdb6358b827399/src/validation.cpp#L3089-L3094

Then starting at Unix Epoch 0x80000000, post-softfork nodes just increment the timestamp by 1 on each new block.
This just kicks the can since that then imposes a limit on the maximum number of blocks, but at least the unit is now ~10 minutes instead of 1 second, a massive x600 increase in the amount of time we are forced to hardfork.

On the other hand, this does imply that the difficulty calculation will become astronomically and ludicrously high, since pre-softfork nodes will think that blocks are arriving at the rate of 1 per second, so ...

Regards,
ZmnSCPxj

From vjudeu at gazeta.pl  Sun Oct 17 07:24:43 2021
From: vjudeu at gazeta.pl (vjudeu at gazeta.pl)
Date: Sun, 17 Oct 2021 09:24:43 +0200
Subject: [bitcoin-dev] Year 2038 problem and year 2106 chain halting
Message-ID: <118811322-c002f9f41f003a55807feca44f9160a2@pmq8v.m5r2.onet>

> Then starting at Unix Epoch 0x80000000, post-softfork nodes just increment the timestamp by 1 on each new block.

It is possible to go even faster. The fastest rate is something like that, if you assume the time in the Genesis Block is zero:

0 1 2 2 3 3 3 3 4 4 4 4 4 4 5 5 5 5 5 5 6 ...

Then you can increment timestamps once per 6 blocks, that means x3600 increase, but then the difficulty is always multiplied by four, so you have to increase time once per difficulty change to keep it on real level, then it will wave between being multiplied by 4 and by 0.25.

On 2021-10-17 01:23:24 user ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:
> Good morning vjudeu,

> > What happens if a series of blocks has a timestamp of 0xFFFFFFFF at the appropriate time?
>
> The chain will halt for all old clients, because there is no 32-bit value greater than 0xffffffff.
>
> > 1.  Is not violated, since "not lower than" means "greater than or equal to"
>
> No, because it has to be strictly "greater than" in the Bitcoin Core source code, it is rejected when it is "lower or equal to", see:https://github.com/bitcoin/bitcoin/blob/6f0cbc75be7644c276650fd98bfdb6358b827399/src/validation.cpp#L3089-L3094

Then starting at Unix Epoch 0x80000000, post-softfork nodes just increment the timestamp by 1 on each new block.
This just kicks the can since that then imposes a limit on the maximum number of blocks, but at least the unit is now ~10 minutes instead of 1 second, a massive x600 increase in the amount of time we are forced to hardfork.

On the other hand, this does imply that the difficulty calculation will become astronomically and ludicrously high, since pre-softfork nodes will think that blocks are arriving at the rate of 1 per second, so ...

Regards,
ZmnSCPxj



From mercedes.catherine.salazar at gmail.com  Sun Oct 17 08:19:07 2021
From: mercedes.catherine.salazar at gmail.com (Kate Salazar)
Date: Sun, 17 Oct 2021 10:19:07 +0200
Subject: [bitcoin-dev] Year 2038 problem and year 2106 chain halting
In-Reply-To: <CANQHGB2Vm31Y8aGsrx3qNitRWh4y950LxxjzHqrD=qzwN=FmVw@mail.gmail.com>
References: <50769965-423dd279413d4dba11ba459cbd98387b@pmq6v.m5r2.onet>
 <CANQHGB2Vm31Y8aGsrx3qNitRWh4y950LxxjzHqrD=qzwN=FmVw@mail.gmail.com>
Message-ID: <CAHiDt8Arc6yBobY82KrqfF5-e+wCdGpCN_7okUVgf=9s58kcvQ@mail.gmail.com>

There is a hard fork wishlist:

https://en.bitcoin.it/wiki/Hardfork_Wishlist

Note last update is somewhat old.

Cheers.

On Sat, Oct 16, 2021 at 12:49 AM James Lu via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Making Bitcoin function after 2038 is by definition a hard fork
>
> I feel if we do HF, we should bundle other HF changes with it...
>
> On Wed, Oct 13, 2021 at 5:19 PM vjudeu via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> It seems that Bitcoin Core will stop working in 2038 because of assertion
>> checking if the current time is non-negative. Also, the whole chain will
>> halt after reaching median time 0xffffffff in 2106. More information:
>> https://bitcointalk.org/index.php?topic=5365359.0
>>
>> I wonder if that kind of issues are possible to fix in a soft-fork way.
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211017/db2469d9/attachment.html>

From yanmaani at cock.li  Sun Oct 17 15:14:41 2021
From: yanmaani at cock.li (yanmaani at cock.li)
Date: Sun, 17 Oct 2021 15:14:41 +0000
Subject: [bitcoin-dev] Year 2038 problem and year 2106 chain halting
In-Reply-To: <143289360-eb35e705fded3eb4175a6f8d7669b3a0@pmq5v.m5r2.onet>
References: <143289360-eb35e705fded3eb4175a6f8d7669b3a0@pmq5v.m5r2.onet>
Message-ID: <0d0b22a297d112939e11c86aa1f6d736@cock.li>

What, no. The `k` value is calculated implicitly, because there's only 
one value of it that could ever be valid - if `k` is 1 too small, we're 
70 years too far back, and then the block will violate median of last 
11. If `k` is 1 too large, we're 70 years too far in the future, then 
the block will violate 2 hour rule. Nothing is added to coinbase or 
anywhere else.

It's possible that you'd need some extra logic for locktime, yes, but it 
would only be a problem in very special cases. Worst-case, you'll have 
to use block time locking in the years around the switch, or softfork in 
64-bit locking.

But unless I'm missing something, 32-bit would be enough, you just 
wouldn't be able to locktime something past the timestamp for the 
switch. After the switchover, everything would be back to normal.

This is a hardfork, yes, but it's a hardfork that kicks in way into the 
future. And because it's a hardfork, you might as well do anything, as 
long as it doesn't change anything now.

On 2021-10-15 22:22, vjudeu at gazeta.pl wrote:
> Your solution seems to solve the problem of chain halting, but there
> are more issues. For example: if you have some time modulo 2^32, then
> you no longer know if timestamp zero is related to 1970 or 2106 or
> some higher year. Your "k" value representing in fact the most
> significant 32 bits of 64-bit timestamp has to be stored in all cases
> where time is used. If there is no "k", then zero should be used for
> backward compatibility. Skipping "k" could cause problems related to
> OP_CHECKLOCKTIMEVERIFY or nLockTime, because if some transaction was
> timestamped to 0xbadc0ded, then that transaction will be valid in
> 0x00000000badc0ded, invalid in 0x0000000100000000, and valid again in
> 0x00000001badc0ded, the same for timelocked outputs.
> 
> So, I think your "k" value should be added to the coinbase
> transaction, then you can combine two 32-bit values, the lower bits
> from the block header and the higher bits from the coinbase
> transaction. Also, adding your "k" value transaction nLockTime field
> is needed (maybe in a similar way as transaction witness was added in
> Segwit), because in other case after reaching 0x0000000100000000 all
> off-chain transactions with timelocks around 0x00000000ffffffff will
> be additionally timelocked for the next N years. The same is needed
> for each OP_CHECKLOCKTIMEVERIFY, maybe pushing high 32 bits before the
> currently used value will solve that (and assuming zero if there is
> only some 32-bit value).

From mercedes.catherine.salazar at gmail.com  Sun Oct 17 15:46:46 2021
From: mercedes.catherine.salazar at gmail.com (Kate Salazar)
Date: Sun, 17 Oct 2021 17:46:46 +0200
Subject: [bitcoin-dev] Year 2038 problem and year 2106 chain halting
In-Reply-To: <0d0b22a297d112939e11c86aa1f6d736@cock.li>
References: <143289360-eb35e705fded3eb4175a6f8d7669b3a0@pmq5v.m5r2.onet>
 <0d0b22a297d112939e11c86aa1f6d736@cock.li>
Message-ID: <CAHiDt8BY1dT=PhjudHbJS01eqm=So7Q1tvo8ft9sFLT=D33Kfg@mail.gmail.com>

Hi yanmaani

On Sun, Oct 17, 2021 at 5:28 PM yanmaani--- via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> What, no. The `k` value is calculated implicitly, because there's only
> one value of it that could ever be valid - if `k` is 1 too small, we're
> 70 years too far back, and then the block will violate median of last
> 11. If `k` is 1 too large, we're 70 years too far in the future, then
> the block will violate 2 hour rule. Nothing is added to coinbase or
> anywhere else.
>
> It's possible that you'd need some extra logic for locktime, yes, but it
> would only be a problem in very special cases. Worst-case, you'll have
> to use block time locking in the years around the switch, or softfork in
> 64-bit locking.
>
> But unless I'm missing something, 32-bit would be enough, you just
> wouldn't be able to locktime something past the timestamp for the
> switch. After the switchover, everything would be back to normal.
>
> This is a hardfork, yes, but it's a hardfork that kicks in way into the
> future. And because it's a hardfork, you might as well do anything, as
> long as it doesn't change anything now.
>

"Anything" is quite a word.
Ideally, hard fork requires upgrading every node that can be upgraded,
or at least have the node operator's consent to lose the node (for every
node that can't be upgraded).


>
> On 2021-10-15 22:22, vjudeu at gazeta.pl wrote:
> > Your solution seems to solve the problem of chain halting, but there
> > are more issues. For example: if you have some time modulo 2^32, then
> > you no longer know if timestamp zero is related to 1970 or 2106 or
> > some higher year. Your "k" value representing in fact the most
> > significant 32 bits of 64-bit timestamp has to be stored in all cases
> > where time is used. If there is no "k", then zero should be used for
> > backward compatibility. Skipping "k" could cause problems related to
> > OP_CHECKLOCKTIMEVERIFY or nLockTime, because if some transaction was
> > timestamped to 0xbadc0ded, then that transaction will be valid in
> > 0x00000000badc0ded, invalid in 0x0000000100000000, and valid again in
> > 0x00000001badc0ded, the same for timelocked outputs.
> >
> > So, I think your "k" value should be added to the coinbase
> > transaction, then you can combine two 32-bit values, the lower bits
> > from the block header and the higher bits from the coinbase
> > transaction. Also, adding your "k" value transaction nLockTime field
> > is needed (maybe in a similar way as transaction witness was added in
> > Segwit), because in other case after reaching 0x0000000100000000 all
> > off-chain transactions with timelocks around 0x00000000ffffffff will
> > be additionally timelocked for the next N years. The same is needed
> > for each OP_CHECKLOCKTIMEVERIFY, maybe pushing high 32 bits before the
> > currently used value will solve that (and assuming zero if there is
> > only some 32-bit value).
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211017/a7bd292d/attachment.html>

From damian at willtech.com.au  Sun Oct 17 22:38:16 2021
From: damian at willtech.com.au (damian at willtech.com.au)
Date: Sun, 17 Oct 2021 15:38:16 -0700
Subject: [bitcoin-dev] Year 2038 problem and year 2106 chain halting
In-Reply-To: <CANQHGB2Vm31Y8aGsrx3qNitRWh4y950LxxjzHqrD=qzwN=FmVw@mail.gmail.com>
References: <50769965-423dd279413d4dba11ba459cbd98387b@pmq6v.m5r2.onet>
 <CANQHGB2Vm31Y8aGsrx3qNitRWh4y950LxxjzHqrD=qzwN=FmVw@mail.gmail.com>
Message-ID: <429a737551829fe6dcd4e427dc508fa5@willtech.com.au>

Good Afternoon,

I am certain that as soon as we identify solutions they should be 
implemented. Basic life skills assert that procrastination is always a 
form of failure, where we could have realised and accomplished further 
yet we waited and in our present state could not ascertain what was in 
our benefit.

KING JAMES HRMH
Great British Empire

Regards,
The Australian
LORD HIS EXCELLENCY JAMES HRMH (& HMRH)
of Hougun Manor & Glencoe & British Empire
MR. Damian A. James Williamson
Wills

et al.


Willtech
www.willtech.com.au
www.go-overt.com
duigco.org DUIGCO API
and other projects


m. 0487135719
f. +61261470192


This email does not constitute a general advice. Please disregard this 
email if misdelivered.
On 2021-10-15 08:27, James Lu via bitcoin-dev wrote:
> Making Bitcoin function after 2038 is by definition a hard fork
> 
> I feel if we do HF, we should bundle other HF changes with it...
> 
> On Wed, Oct 13, 2021 at 5:19 PM vjudeu via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org> wrote:
> 
>> It seems that Bitcoin Core will stop working in 2038 because of
>> assertion checking if the current time is non-negative. Also, the
>> whole chain will halt after reaching median time 0xffffffff in 2106.
>> More information: https://bitcointalk.org/index.php?topic=5365359.0
>> 
>> I wonder if that kind of issues are possible to fix in a soft-fork
>> way. _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From yanmaani at cock.li  Mon Oct 18 02:55:32 2021
From: yanmaani at cock.li (yanmaani at cock.li)
Date: Mon, 18 Oct 2021 02:55:32 +0000
Subject: [bitcoin-dev] Year 2038 problem and year 2106 chain halting
In-Reply-To: <CAHiDt8BY1dT=PhjudHbJS01eqm=So7Q1tvo8ft9sFLT=D33Kfg@mail.gmail.com>
References: <143289360-eb35e705fded3eb4175a6f8d7669b3a0@pmq5v.m5r2.onet>
 <0d0b22a297d112939e11c86aa1f6d736@cock.li>
 <CAHiDt8BY1dT=PhjudHbJS01eqm=So7Q1tvo8ft9sFLT=D33Kfg@mail.gmail.com>
Message-ID: <01b421b8839b396e0ccc4f3a1c5aa627@cock.li>

Well, it's the right word. If you're going to do a hardfork by changing 
the timestamp definition, you're already doing a hardfork. At that 
point, you've already crossed the Rubicon and might as well put in any 
other necessary changes (e.g. to transaction locking), because it will 
be as much of a hardfork either way.

The important bit here is "as long as it doesn't change anything now" - 
this is indeed a hardfork, but it's a timestamp-activated hardfork that 
triggers in 2106. Until that point, it has absolutely no bearing on 
consensus rules (as opposed to the other proposals, which are at least a 
soft-fork today).

I understand that there's some problems in getting consensus for forks, 
but surely we can agree that everyone will update their Bitcoin at least 
once in the next 85 years? (If they don't, they're doomed anyway.)

On 2021-10-17 15:46, Kate Salazar wrote:
> Hi yanmaani
> 
...
>> This is a hardfork, yes, but it's a hardfork that kicks in way into
>> the
>> future. And because it's a hardfork, you might as well do anything,
>> as
>> long as it doesn't change anything now.
> 
> "Anything" is quite a word.
> Ideally, hard fork requires upgrading every node that can be upgraded,
> 
> or at least have the node operator's consent to lose the node (for
> every
> node that can't be upgraded).
> 
...
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From chris at suredbits.com  Mon Oct 18 19:09:14 2021
From: chris at suredbits.com (Chris Stewart)
Date: Mon, 18 Oct 2021 14:09:14 -0500
Subject: [bitcoin-dev] Bitcoin-s 1.8 released with DLCs negotiation via Tor
Message-ID: <CAFQwNuzXYSDsGihP8KD5+jB0dYM3KLnDSAry--gy+hvNVDeWpw@mail.gmail.com>

Hi all,

We released 1.8 today of bitcoin-s. This includes support for opening DLCs
over tor. This makes the UX much simpler to enter into a DLC with your
counterparty.

As part of this release, we wrote two detailed examples of entering into

1. A wallet election example
<https://bitcoin-s.org/docs/next/wallet/wallet-election-example>
2. A wallet price example
<https://bitcoin-s.org/docs/next/wallet/wallet-price-example>

These are meant to complement the existing oracle examples we have

1. An election oracle example
<https://bitcoin-s.org/docs/next/oracle/oracle-election-example>
2. A price oracle example
<https://bitcoin-s.org/docs/next/oracle/oracle-price-example>

*This is alpha software, please do not use it for large amounts of money.*

You need a github account to access the downloads, the artifacts are at the
very bottom of the release notes.

https://github.com/bitcoin-s/bitcoin-s/releases/tag/1.8.0

You can find a demonstration video for using the wallet to enter into a DLC
over tor here:

https://www.youtube.com/watch?v=oR0I0aHxNMM

You can find the dlc spec here:

https://github.com/discreetlogcontracts/dlcspecs

-Chris
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211018/4a380e44/attachment.html>

From ogunden at phauna.org  Wed Oct 20 12:54:36 2021
From: ogunden at phauna.org (Owen Gunden)
Date: Wed, 20 Oct 2021 08:54:36 -0400
Subject: [bitcoin-dev] bitcoin.org missing bitcoin core version 22.0
Message-ID: <144e8798-4e16-8939-51b6-fbf5abb87401@phauna.org>

When I search for "download bitcoin core" my top result is bitcoin.org, 
which is out of date and doesn't have 22.0.

It seems confusing to have two sites that seemingly both represent 
bitcoin core.

Maybe the download links could be removed from bitcoin.org and instead 
it could just link to bitcoincore.org?


From prayank at tutanota.de  Wed Oct 20 14:47:17 2021
From: prayank at tutanota.de (Prayank)
Date: Wed, 20 Oct 2021 16:47:17 +0200 (CEST)
Subject: [bitcoin-dev] bitcoin.org missing bitcoin core version 22.0
Message-ID: <MmT9umZ--3-2@tutanota.de>

Hi Owen,

>?When I search for "download bitcoin core" my top result is bitcoin.org, which is out of date and doesn't have 22.0

This is an issue related to SEO which only website owners can fix or maybe others can help who know better.

>?It seems confusing to have two sites that seemingly both represent bitcoin core.

There is only one website which represents Bitcoin Core full node implementation. You can download Bitcoin Core from https://bitcoincore.org
Ensure that you are using the correct domain as some people have registered domains which use punycode, looks similar and spreading malware:?https://bitcoin.stackexchange.com/a/107738/

>?Maybe the download links could be removed from bitcoin.org and instead it could just link to bitcoincore.org?

You can open an issue in website repository:?https://github.com/bitcoin-dot-org/bitcoin.org?and tag Cobra who owns the website and domain.

Alternately you could also try a derivative of Bitcoin Core:?https://bitcoinknots.org/?maintained by Luke Dashjr.

-- 
Prayank

A3B1 E430 2298 178F
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211020/54d849db/attachment.html>

From ogunden at phauna.org  Wed Oct 20 19:20:54 2021
From: ogunden at phauna.org (Owen Gunden)
Date: Wed, 20 Oct 2021 19:20:54 +0000
Subject: [bitcoin-dev] bitcoin.org missing bitcoin core version 22.0
In-Reply-To: <MmT9umZ--3-2@tutanota.de>
References: <MmT9umZ--3-2@tutanota.de>
Message-ID: <20211020192054.GA117785@jauntyelephant.191.37.198.vultr.com>

On Wed, Oct 20, 2021 at 04:47:17PM +0200, Prayank wrote:
> > It seems confusing to have two sites that seemingly both represent
> > bitcoin core.
>
> There is only one website which represents Bitcoin Core full node
> implementation. You can download Bitcoin Core from
> https://bitcoincore.org

I also notice that, as of 22.0, Wladimir is no longer signing the
releases, and I have no trust in my gpg network of the people who seem
to have replaced him.

Given the level of security at stake here, my eyebrows are raised at
this combination of items changing (new website + new gpg signers at the
same time).


From achow101-lists at achow101.com  Wed Oct 20 21:50:13 2021
From: achow101-lists at achow101.com (Andrew Chow)
Date: Wed, 20 Oct 2021 21:50:13 +0000
Subject: [bitcoin-dev] bitcoin.org missing bitcoin core version 22.0
Message-ID: <31c1a187-dcc7-900c-7298-0dc4b537c7bf@achow101.com>

On 10/20/2021 03:20 PM, Owen Gunden via bitcoin-dev wrote:
> I also notice that, as of 22.0, Wladimir is no longer signing the
> releases, and I have no trust in my gpg network of the people who seem
> to have replaced him.
It is signed with his personal key, as well as the personal keys of
several other developers.

> Given the level of security at stake here, my eyebrows are raised at
> this combination of items changing (new website + new gpg signers at the
> same time).
bitcoincore.org has been Bitcoin Core's official website for several
years. Binaries have not been posted to bitcoin.org by the release
maintainer for several major releases. The only reason they are still
available there is because bitcoin.org's maintainer mirrors them.


From bitcoin-dev at wuille.net  Wed Oct 20 19:37:48 2021
From: bitcoin-dev at wuille.net (Pieter Wuille)
Date: Wed, 20 Oct 2021 19:37:48 +0000
Subject: [bitcoin-dev] bitcoin.org missing bitcoin core version 22.0
In-Reply-To: <20211020192054.GA117785@jauntyelephant.191.37.198.vultr.com>
References: <MmT9umZ--3-2@tutanota.de>
 <20211020192054.GA117785@jauntyelephant.191.37.198.vultr.com>
Message-ID: <mP71HoRgYl2eMSvdc0w6btXHIXGN6snpz5CYemWm507aSGhR5ewAx3NuZYtNSRBQr02D9Wm_UU2IFmtHqkqiinsgjQdwGerwOEak1_Fh3hc=@wuille.net>

On Wednesday, October 20th, 2021 at 3:20 PM, Owen Gunden via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:

> I also notice that, as of 22.0, Wladimir is no longer signing the
> releases, and I have no trust in my gpg network of the people who seem
> to have replaced him.

This is not correct. Here are Wladimir's attestations on the 22.0 release: https://github.com/bitcoin-core/guix.sigs/tree/main/22.0/laanwj

There is no separate special release key anymore though. Instead, the build attestations (by anyone) can be used as your trust basis.

> Given the level of security at stake here, my eyebrows are raised at
> this combination of items changing (new website + new gpg signers at the
> same time).

There is no new website. The Bitcoin Core project website has been https://bitcoincore.org for years. I don't know why https://bitcoin.org hasn't updated to list the 22.0 release, though; that's up to them.

Cheers,

--
Pieter


From chill at degreesofzero.com  Wed Oct 20 19:43:27 2021
From: chill at degreesofzero.com (Charles Hill)
Date: Wed, 20 Oct 2021 20:43:27 +0100
Subject: [bitcoin-dev] bitcoin.org missing bitcoin core version 22.0
In-Reply-To: <20211020192054.GA117785@jauntyelephant.191.37.198.vultr.com>
References: <MmT9umZ--3-2@tutanota.de>
 <20211020192054.GA117785@jauntyelephant.191.37.198.vultr.com>
Message-ID: <7c2d608c-bd59-b04e-a9a5-a55098782700@degreesofzero.com>

Hello, Owen,

The GPG signature verification has changed for bitcoin core version 22 
and later. There were two main changes:

1) The sha256 checksums are now in a separate file from the GPG 
signatures. So download a new file named "SHA256SUMS" (contains the 
checksums) and also the "SHA256SUMS.asc" which contains the signatures.

2) The signature file now contains multiple signatures. These signatures 
are generated by multiple "builders" who have provided their own public 
keys to verify against. Not all builders will provide a signature for 
each release.

You can find more information at bitcoincore.org/en/download/ [1] under 
the "Linux verification instructions" section - click to expand.

Instructions about where to find and how to import the full list of 
"builder" public keys can be found in the bitcoin core github repo [2].

 > I also notice that, as of 22.0, Wladimir is no longer signing the 
releases, and I have no trust in my gpg network of the people who seem 
to have replaced him.

The list of "builder" public keys includes many long-time bitcoin core 
contributors as well as Wladimir's. Caution is always warranted but 
please do not spread unnecessary FUD.

- chill

[1] https://bitcoincore.org/en/download/
[2] https://github.com/bitcoin/bitcoin/tree/master/contrib/builder-keys


On 10/20/21 8:20 PM, Owen Gunden via bitcoin-dev wrote:
> On Wed, Oct 20, 2021 at 04:47:17PM +0200, Prayank wrote:
>>> It seems confusing to have two sites that seemingly both represent
>>> bitcoin core.
>> There is only one website which represents Bitcoin Core full node
>> implementation. You can download Bitcoin Core from
>> https://bitcoincore.org
> I also notice that, as of 22.0, Wladimir is no longer signing the
> releases, and I have no trust in my gpg network of the people who seem
> to have replaced him.
>
> Given the level of security at stake here, my eyebrows are raised at
> this combination of items changing (new website + new gpg signers at the
> same time).
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From ogunden at phauna.org  Wed Oct 20 19:49:19 2021
From: ogunden at phauna.org (Owen Gunden)
Date: Wed, 20 Oct 2021 19:49:19 +0000
Subject: [bitcoin-dev] bitcoin.org missing bitcoin core version 22.0
In-Reply-To: <mP71HoRgYl2eMSvdc0w6btXHIXGN6snpz5CYemWm507aSGhR5ewAx3NuZYtNSRBQr02D9Wm_UU2IFmtHqkqiinsgjQdwGerwOEak1_Fh3hc=@wuille.net>
References: <MmT9umZ--3-2@tutanota.de>
 <20211020192054.GA117785@jauntyelephant.191.37.198.vultr.com>
 <mP71HoRgYl2eMSvdc0w6btXHIXGN6snpz5CYemWm507aSGhR5ewAx3NuZYtNSRBQr02D9Wm_UU2IFmtHqkqiinsgjQdwGerwOEak1_Fh3hc=@wuille.net>
Message-ID: <20211020194919.GA118497@jauntyelephant.191.37.198.vultr.com>

On Wed, Oct 20, 2021 at 07:37:48PM +0000, Pieter Wuille wrote:
> On Wednesday, October 20th, 2021 at 3:20 PM, Owen Gunden via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
> > I also notice that, as of 22.0, Wladimir is no longer signing the
> > releases, and I have no trust in my gpg network of the people who seem
> > to have replaced him.
>
> This is not correct. Here are Wladimir's attestations on the 22.0
> release:
> https://github.com/bitcoin-core/guix.sigs/tree/main/22.0/laanwj
>
> There is no separate special release key anymore though. Instead, the
> build attestations (by anyone) can be used as your trust basis.

Ah, that explains it, thanks Pieter. I was looking for a signature from
the release key 01EA 5486 DE18 A882 D4C2  6845 90C8 019E 36C2 E964,
which was still being supplied as recently as 0.21.1.

> There is no new website. The Bitcoin Core project website has been
> https://bitcoincore.org for years.

Some of us are very old :).

From mercedes.catherine.salazar at gmail.com  Wed Oct 20 20:18:03 2021
From: mercedes.catherine.salazar at gmail.com (Kate Salazar)
Date: Wed, 20 Oct 2021 22:18:03 +0200
Subject: [bitcoin-dev] bitcoin.org missing bitcoin core version 22.0
In-Reply-To: <20211020192054.GA117785@jauntyelephant.191.37.198.vultr.com>
References: <MmT9umZ--3-2@tutanota.de>
 <20211020192054.GA117785@jauntyelephant.191.37.198.vultr.com>
Message-ID: <CAHiDt8A30DZtvsPnDDdtyVpho-NKKQhbP_4g8d0MGATawWvg_w@mail.gmail.com>

Hi Owen,

On Wed, Oct 20, 2021 at 9:25 PM Owen Gunden via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> On Wed, Oct 20, 2021 at 04:47:17PM +0200, Prayank wrote:
> > > It seems confusing to have two sites that seemingly both represent
> > > bitcoin core.
> >
> > There is only one website which represents Bitcoin Core full node
> > implementation. You can download Bitcoin Core from
> > https://bitcoincore.org
>
> I also notice that, as of 22.0, Wladimir is no longer signing the
> releases, and I have no trust in my gpg network of the people who seem
> to have replaced him.
>

He is taking the most sensible way forward, decreasing bus factor.

Read: https://laanwj.github.io/2021/01/21/decentralize.html


>
> Given the level of security at stake here, my eyebrows are raised at
> this combination of items changing (new website + new gpg signers at the
> same time).
>

Don't worry and build your own release;
but if you do, always verify the tree hash.
Trust signed annotated tags.
Cheers!


>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211020/ab18e285/attachment.html>

From niftynei at gmail.com  Tue Oct 26 02:56:21 2021
From: niftynei at gmail.com (lisa neigut)
Date: Mon, 25 Oct 2021 21:56:21 -0500
Subject: [bitcoin-dev] death to the mempool, long live the mempool
Message-ID: <CAM1a7P04apCwwmqNp4VLRam5_uk59tVRWv74UVD_g0-DUGNghw@mail.gmail.com>

Hi all,

In a recent conversation with @glozow, I had the realization that the
mempool is obsolete and should be eliminated.

Instead, users should submit their transactions directly to mining pools,
preferably over an anonymous communication network such as tor. This can
easily be achieved by mining pools running a tor onion node for this
express purpose (or via a lightning network extension etc)

Mempools make sense in a world where mining is done by a large number of
participating nodes, eg where the block template is constructed by a
majority of the participants on the network. In this case, it is necessary
to socialize pending transaction data to all participants, as you don?t
know which participant will be constructing the winning block template.

In reality however, mempool relay is unnecessary where the majority of
hashpower and thus block template creation is concentrated in a
semi-restricted set.

Removing the mempool would greatly reduce the bandwidth requirement for
running a node, keep intentionality of transactions private until
confirmed/irrevocable, and naturally resolve all current issues inherent in
package relay and rbf rules. It also resolves the recent minimum relay
questions, as relay is no longer a concern for unmined transactions.

Provided the number of block template producing actors remains beneath, say
1000, it?d be quite feasible to publish a list of tor endpoints that nodes
can independently  + directly submit their transactions to. In fact, merely
allowing users to select their own list of endpoints to use alternatively
to the mempool would be a low effort starting point for the eventual
replacement.

On the other hand, removing the mempool would greatly complicate solo
mining and would also make BetterHash proposals, which move the block
template construction away from a centralized mining pool back to the
individual miner, much more difficult. It also makes explicit the target
for DoS attacks.

A direct communication channel between block template construction venues
and transaction proposers also provides a venue for direct feedback wrt
acceptable feerates at the time, which both makes transaction confirmation
timelines less variable as well as provides block producers a mechanism for
(independently) enforcing their own minimum security budget. In other
words, expressing a minimum acceptable feerate for continued operation.

Initial feerate estimation would need to be based on published blocks, not
pending transactions (as this information would no longer be available), or
from direct interactions with block producers.


~niftynei
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211025/a5c7aebf/attachment.html>

From ZmnSCPxj at protonmail.com  Tue Oct 26 08:02:24 2021
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Tue, 26 Oct 2021 08:02:24 +0000
Subject: [bitcoin-dev] death to the mempool, long live the mempool
In-Reply-To: <CAM1a7P04apCwwmqNp4VLRam5_uk59tVRWv74UVD_g0-DUGNghw@mail.gmail.com>
References: <CAM1a7P04apCwwmqNp4VLRam5_uk59tVRWv74UVD_g0-DUGNghw@mail.gmail.com>
Message-ID: <wIXL_bs_B1FfWrIOMjdog--VwQWD_okme8nPjerWyszHuKhFcPTi3yetwPKYYYR79PEeQcbA3lqZTL107k_KED8-RMs4HPyvhLh5b1miSr4=@protonmail.com>


Good morning lisa,

> Hi all,
>
> In a recent conversation with @glozow, I had the realization that the mempool is obsolete and should be eliminated.
>
> Instead, users should submit their transactions directly to mining pools, preferably over an anonymous communication network such as tor. This can easily be achieved by mining pools running a tor onion node for this express purpose (or via a lightning network extension etc)
>
> Mempools make sense in a world where mining is done by a large number of participating nodes, eg where the block template is constructed by a majority of the participants on the network. In this case, it is necessary to socialize pending transaction data to all participants, as you don?t know which participant will be constructing the winning block template.
>
> In reality however, mempool relay is unnecessary where the majority of hashpower and thus block template creation is concentrated in a semi-restricted set.?
>
> Removing the mempool would greatly reduce the bandwidth requirement for running a node, keep intentionality of transactions private until confirmed/irrevocable, and naturally resolve all current issues inherent in package relay and rbf rules. It also resolves the recent minimum relay questions, as relay is no longer a concern for unmined transactions.
>
> Provided the number of block template producing actors remains beneath, say 1000, it?d be quite feasible to publish a list of tor endpoints that nodes can independently ?+ directly submit their transactions to. In fact, merely allowing users to select their own list of endpoints to use alternatively to the mempool would be a low effort starting point for the eventual replacement.
>
> On the other hand, removing the mempool would greatly complicate solo mining and would also make BetterHash proposals, which move the block template construction away from a centralized mining pool back to the individual miner, much more difficult. It also makes explicit the target for DoS attacks.

Unfortunately, this requires that miners have a persistent identity by which they can be contacted.
While pseudonymity is possible, we all know that in practice, it can be easily pierced.
For instance, consider that the injunction against address reuse is a recognition that a persistent pseudonym is a privacy leak.

Ideally, the mining set should be as anonymous as possible, as some attacks are possible with sufficient hashpower, and making the miners keep a persistent identity by which they can be found may enable easier state co-option of mines.
The strongest man on Earth cannot destroy his enemy if he does not know who and where his enemy is; so with enemies of Bitcoin and the miners of Bitcoin.
(granted, near every darned mining pool self-identifies, sigh, wtf)

Ideally, the set of relaying nodes hides the miners.
Of course, in practice we can have a good guess of which relaying nodes are miners and which are not -- those who get blocks earlier are probably miners.
Against this, we should note that this method of identification is probabilistic and not absolute (whereas miners advertising their services so they can be contacted and given unconfirmed transactions are a *definite* flag "I am a miner").
And there is always the chance, however slim, that some node that has not been getting blocks "early" suddenly decides to buy a mining rig and start mining.

In short: what you propose is to switch to side fee markets (as I understand it).
Non-side fees are simply an anonymity layer, by which neither the miner nor the transactor need to know the identity of each other, they simply broadcast to the wider world.
This anonymity layer remains important, however, as they help maintain the fee market: https://github.com/libbitcoin/libbitcoin-system/wiki/Side-Fee-Fallacy


Ultimately, my objection here is simply that this requires miners to identify themselves.
In practice, miners already identify themselves (even though they really, really should not), so this objection may be moot at this point.

(Not to mention: something like P2Pool, as-is, would not work well in that model; you would need to implement a mempool for those as well)

Regards,
ZmnSCPxj

From eric at voskuil.org  Tue Oct 26 08:31:18 2021
From: eric at voskuil.org (eric at voskuil.org)
Date: Tue, 26 Oct 2021 01:31:18 -0700
Subject: [bitcoin-dev] death to the mempool, long live the mempool
In-Reply-To: <wIXL_bs_B1FfWrIOMjdog--VwQWD_okme8nPjerWyszHuKhFcPTi3yetwPKYYYR79PEeQcbA3lqZTL107k_KED8-RMs4HPyvhLh5b1miSr4=@protonmail.com>
References: <CAM1a7P04apCwwmqNp4VLRam5_uk59tVRWv74UVD_g0-DUGNghw@mail.gmail.com>
 <wIXL_bs_B1FfWrIOMjdog--VwQWD_okme8nPjerWyszHuKhFcPTi3yetwPKYYYR79PEeQcbA3lqZTL107k_KED8-RMs4HPyvhLh5b1miSr4=@protonmail.com>
Message-ID: <009901d7ca43$d9dd2f50$8d978df0$@voskuil.org>

Agree ZmnSCPxj

Hi lisa,

I'm all for removing it from memory. :) Did that a while ago. We just call it the transaction pool.

There will always be unconfirmed transactions floating around (even just from reorgs). Best to store them somewhere. Disk is cheap, block distribution (e.g. compact) works better if you have them already prevalidated, even if you aren't going to mine on them.

How you get them technically is not so important. There will always be a set of unconfirmed transactions, it's conceptual. But above all, anonymity is very important - on both ends. This is why transactions have integral fees. Anyone can get paid to mine, just need the txs.

Mining may be semi-restricted set is today, it may not be tomorrow. Imagine China everywhere, just like financial controls already are. That's when you see what Bitcoin can do from a security standpoint.

Treating miners as someone else is a poor security architecture. Everyone should look like a potential miner on the network, and a potential spender.

I think you are thinking of it a bit backwards. A node is a big pool of connected transactions. Block headers come along occasionally, and impose order on a subset of them.

e

> -----Original Message-----
> From: bitcoin-dev <bitcoin-dev-bounces at lists.linuxfoundation.org> On Behalf
> Of ZmnSCPxj via bitcoin-dev
> Sent: Tuesday, October 26, 2021 1:02 AM
> To: lisa neigut <niftynei at gmail.com>; Bitcoin Protocol Discussion <bitcoin-
> dev at lists.linuxfoundation.org>
> Subject: Re: [bitcoin-dev] death to the mempool, long live the mempool
> 
> 
> Good morning lisa,
> 
> > Hi all,
> >
> > In a recent conversation with @glozow, I had the realization that the
> mempool is obsolete and should be eliminated.
> >
> > Instead, users should submit their transactions directly to mining pools,
> preferably over an anonymous communication network such as tor. This can
> easily be achieved by mining pools running a tor onion node for this express
> purpose (or via a lightning network extension etc)
> >
> > Mempools make sense in a world where mining is done by a large number
> of participating nodes, eg where the block template is constructed by a
> majority of the participants on the network. In this case, it is necessary to
> socialize pending transaction data to all participants, as you don?t know which
> participant will be constructing the winning block template.
> >
> > In reality however, mempool relay is unnecessary where the majority of
> hashpower and thus block template creation is concentrated in a semi-
> restricted set.
> >
> > Removing the mempool would greatly reduce the bandwidth requirement
> for running a node, keep intentionality of transactions private until
> confirmed/irrevocable, and naturally resolve all current issues inherent in
> package relay and rbf rules. It also resolves the recent minimum relay
> questions, as relay is no longer a concern for unmined transactions.
> >
> > Provided the number of block template producing actors remains beneath,
> say 1000, it?d be quite feasible to publish a list of tor endpoints that nodes can
> independently  + directly submit their transactions to. In fact, merely allowing
> users to select their own list of endpoints to use alternatively to the mempool
> would be a low effort starting point for the eventual replacement.
> >
> > On the other hand, removing the mempool would greatly complicate solo
> mining and would also make BetterHash proposals, which move the block
> template construction away from a centralized mining pool back to the
> individual miner, much more difficult. It also makes explicit the target for DoS
> attacks.
> 
> Unfortunately, this requires that miners have a persistent identity by which
> they can be contacted.
> While pseudonymity is possible, we all know that in practice, it can be easily
> pierced.
> For instance, consider that the injunction against address reuse is a
> recognition that a persistent pseudonym is a privacy leak.
> 
> Ideally, the mining set should be as anonymous as possible, as some attacks
> are possible with sufficient hashpower, and making the miners keep a
> persistent identity by which they can be found may enable easier state co-
> option of mines.
> The strongest man on Earth cannot destroy his enemy if he does not know
> who and where his enemy is; so with enemies of Bitcoin and the miners of
> Bitcoin.
> (granted, near every darned mining pool self-identifies, sigh, wtf)
> 
> Ideally, the set of relaying nodes hides the miners.
> Of course, in practice we can have a good guess of which relaying nodes are
> miners and which are not -- those who get blocks earlier are probably miners.
> Against this, we should note that this method of identification is probabilistic
> and not absolute (whereas miners advertising their services so they can be
> contacted and given unconfirmed transactions are a *definite* flag "I am a
> miner").
> And there is always the chance, however slim, that some node that has not
> been getting blocks "early" suddenly decides to buy a mining rig and start
> mining.
> 
> In short: what you propose is to switch to side fee markets (as I understand it).
> Non-side fees are simply an anonymity layer, by which neither the miner nor
> the transactor need to know the identity of each other, they simply broadcast
> to the wider world.
> This anonymity layer remains important, however, as they help maintain the
> fee market: https://github.com/libbitcoin/libbitcoin-system/wiki/Side-Fee-
> Fallacy
> 
> 
> Ultimately, my objection here is simply that this requires miners to identify
> themselves.
> In practice, miners already identify themselves (even though they really,
> really should not), so this objection may be moot at this point.
> 
> (Not to mention: something like P2Pool, as-is, would not work well in that
> model; you would need to implement a mempool for those as well)
> 
> Regards,
> ZmnSCPxj
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


From ZmnSCPxj at protonmail.com  Tue Oct 26 08:56:10 2021
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Tue, 26 Oct 2021 08:56:10 +0000
Subject: [bitcoin-dev] death to the mempool, long live the mempool
In-Reply-To: <009901d7ca43$d9dd2f50$8d978df0$@voskuil.org>
References: <CAM1a7P04apCwwmqNp4VLRam5_uk59tVRWv74UVD_g0-DUGNghw@mail.gmail.com>
 <wIXL_bs_B1FfWrIOMjdog--VwQWD_okme8nPjerWyszHuKhFcPTi3yetwPKYYYR79PEeQcbA3lqZTL107k_KED8-RMs4HPyvhLh5b1miSr4=@protonmail.com>
 <009901d7ca43$d9dd2f50$8d978df0$@voskuil.org>
Message-ID: <a33WOOHRYsOk5DCaJQAQUh-fwyudYXQYggn54KmIKx_CVA4qCnjaAxG3-Drl639NTGC8smZUNeIulUcAxoOp44kRZhf8BQS0JqqW1BkJpCU=@protonmail.com>

Good morning e, and lisa,

> Agree ZmnSCPxj
>
> Hi lisa,
>
> I'm all for removing it from memory. :) Did that a while ago. We just call it the transaction pool.
>
> There will always be unconfirmed transactions floating around (even just from reorgs). Best to store them somewhere. Disk is cheap, block distribution (e.g. compact) works better if you have them already prevalidated, even if you aren't going to mine on them.
>
> How you get them technically is not so important. There will always be a set of unconfirmed transactions, it's conceptual. But above all, anonymity is very important - on both ends. This is why transactions have integral fees. Anyone can get paid to mine, just need the txs.
>
> Mining may be semi-restricted set is today, it may not be tomorrow. Imagine China everywhere, just like financial controls already are. That's when you see what Bitcoin can do from a security standpoint.
>
> Treating miners as someone else is a poor security architecture. Everyone should look like a potential miner on the network, and a potential spender.
>
> I think you are thinking of it a bit backwards. A node is a big pool of connected transactions. Block headers come along occasionally, and impose order on a subset of them.


On the subject of thinking backwards....

The current design gossips txes.

I believe much of what lisa wants would be doable by gossiping mining endpoints instead of txes.
Then transactors can connect to mining endpoints.

Tx gossip is limited by fees (which is why the RBF rules even exist in the first place).
Thus, mining endpoint gossip must be limited by something as well, such as by requiring some trivial stake of BTC.
(BTC exchanges are commonplace enough, I believe, that requiring completely new miners (i.e. those who currently own 0 BTC) to acquire some trivial stake of BTC would be feasible; for most people it would be easier to buy BTC than to acquire a mining rig and the supporting infrastructure needed for a mine.)
We could have the endpoint encoded in some sign-to-contract or pay-to-contract construction.

Miners can change their identity by spending their stake (which makes nodes drop their endpoint record).
Then, they can use now-common anonymity techniques --- mostly CoinJoin, but also the upcoming CoinSwap implementation --- to acquire a new stake whose identity is not easily traceable to the previous stake.

(This is not proof-of-stake, BTW --- the stake only attests the mining endpoint (in much the same way published Lightning channels are attested by their funding tx outpoints), and has no effect on block validity, only on gossiping of mining endpoints.)

The advantage here is that we expect the set of miner identities to change less often than the set of txes, thus reducing global bandwidth usage,

Against the above, we must notice that the anonymity-preserving regular changing of staked identity is more expensive than having a persistent identity.
WE should really design systems where anonymity-preservation should be as cheap as possible, but onchain activity is no longer cheap at all, given the growing importance of Bitcoin.

--

Also:

> A direct communication channel between block template construction venues and transaction proposers also provides a venue for direct feedback wrt acceptable feerates at the time, which both makes transaction confirmation timelines less variable

Unless you contact ***all*** miners globally, there is always some non-zero probability that one of the miners you did *not* contact (and thus does not have your tx, and thus will not be able to confirm your tx) gets the next block.
Since miners can enter and leave the network at any time, it is entirely possible that this mechanism *increases* variability rather than decreases it.

Regards,
ZmnSCPxj


From darosior at protonmail.com  Tue Oct 26 14:09:28 2021
From: darosior at protonmail.com (darosior)
Date: Tue, 26 Oct 2021 14:09:28 +0000
Subject: [bitcoin-dev] death to the mempool, long live the mempool
In-Reply-To: <CAM1a7P04apCwwmqNp4VLRam5_uk59tVRWv74UVD_g0-DUGNghw@mail.gmail.com>
References: <CAM1a7P04apCwwmqNp4VLRam5_uk59tVRWv74UVD_g0-DUGNghw@mail.gmail.com>
Message-ID: <Opd1OVGiyYCh2nGyCF1WbAozszMGHZSXiiC4cxN80cuIGS8TLzfzDjzcGulIOZDrq1bffF_UV6DO4QPFq1jmMY9UI0g5baUZMjWoeFqQvtM=@protonmail.com>

Hi Niftynei,

I share the concerns raised about direct connections to mining pools being a centralization pressure: de-anonymization and an inevitable higher barrier to entry. Making it more difficult to reach smaller miners is another one.
Regarding fee estimation you state:
> Initial feerate estimation would need to be based on published blocks, not pending transactions (as this information would no longer be available)
The current fee estimation algorithm uses both, not only the pending transactions (game-able). Although i agree that past-block(s) based fee estimation isn't that bad, it's worth mentioning that not tracking the confirmation time of relayed transactions drops the ability to have a target in the estimation. That is it's good enough for time-sensitive transactions where you always target the next block but not for other usages which usually target a few dozen of blocks in the future.

However, as we discussed recently, i do believe their is a failure mode here. On one hand, directly connecting to pools is already possible today and pretty effective given the current mining centralization. On the other hand, it's not possible for most pre-signed txs protocols to reliably (securely) use the Bitcoin tx relay network today to propagate time-sensitive transactions. Furthermore, even if these are fixed (eg via package-relay for (today's) Lightning Network) it seems like there is a stark contrast between what "L2 [0] protocols" need and what regular node operators can reasonably offer. A node operator is incentivized to relay transactions to:
- have more privacy *if* they use their node to broadcast transactions (make it less distinguishable which relayed transaction comes from the wallet)
- provide fee estimates *if* they need them
- avoid bandwidth spikes on block connection (compact block relay)

L2s would ideally need the tx relaying nodes to do more computation and lift their DOS mitigations for all miner-incentive-compatible transactions to eventually be relayed to most of the miners. One obvious instance of such a dilemma is the RBF rules.
Such protocols getting increasingly developed and used create a strong incentive for their users/stakeholders to directly connect to mining pools [1], with all the consequences for the network mentioned in the replies to your mail and elsewhere.
Before we get to this, i think there is a strong case for an opt-in and publicly accessible "overlay" network to relay miner-incentive compatible transactions with higher DOS limits. This way the cost is not imposed to L1 node runners, and L2s can operate with more safety assumptions without (entirely) falling for centralization.

Thanks for publicly starting this discussion,
Antoine

[0] Using "L2s" for the sake of brevety, whatever it means i mean "protocols using pre-signed Bitcoin transactions which timely confirmation might be a security requirement".
[1] Wen block space insurance contracts

??????? Original Message ???????
Le mardi 26 octobre 2021 ? 4:56 AM, lisa neigut via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> a ?crit :

> Hi all,
>
> In a recent conversation with @glozow, I had the realization that the mempool is obsolete and should be eliminated.
>
> Instead, users should submit their transactions directly to mining pools, preferably over an anonymous communication network such as tor. This can easily be achieved by mining pools running a tor onion node for this express purpose (or via a lightning network extension etc)
>
> Mempools make sense in a world where mining is done by a large number of participating nodes, eg where the block template is constructed by a majority of the participants on the network. In this case, it is necessary to socialize pending transaction data to all participants, as you don?t know which participant will be constructing the winning block template.
>
> In reality however, mempool relay is unnecessary where the majority of hashpower and thus block template creation is concentrated in a semi-restricted set.
>
> Removing the mempool would greatly reduce the bandwidth requirement for running a node, keep intentionality of transactions private until confirmed/irrevocable, and naturally resolve all current issues inherent in package relay and rbf rules. It also resolves the recent minimum relay questions, as relay is no longer a concern for unmined transactions.
>
> Provided the number of block template producing actors remains beneath, say 1000, it?d be quite feasible to publish a list of tor endpoints that nodes can independently + directly submit their transactions to. In fact, merely allowing users to select their own list of endpoints to use alternatively to the mempool would be a low effort starting point for the eventual replacement.
>
> On the other hand, removing the mempool would greatly complicate solo mining and would also make BetterHash proposals, which move the block template construction away from a centralized mining pool back to the individual miner, much more difficult. It also makes explicit the target for DoS attacks.
>
> A direct communication channel between block template construction venues and transaction proposers also provides a venue for direct feedback wrt acceptable feerates at the time, which both makes transaction confirmation timelines less variable as well as provides block producers a mechanism for (independently) enforcing their own minimum security budget. In other words, expressing a minimum acceptable feerate for continued operation.
>
> Initial feerate estimation would need to be based on published blocks, not pending transactions (as this information would no longer be available), or from direct interactions with block producers.
>
> ~niftynei
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211026/d151987f/attachment.html>

From ZmnSCPxj at protonmail.com  Tue Oct 26 16:38:27 2021
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Tue, 26 Oct 2021 16:38:27 +0000
Subject: [bitcoin-dev] death to the mempool, long live the mempool
In-Reply-To: <Opd1OVGiyYCh2nGyCF1WbAozszMGHZSXiiC4cxN80cuIGS8TLzfzDjzcGulIOZDrq1bffF_UV6DO4QPFq1jmMY9UI0g5baUZMjWoeFqQvtM=@protonmail.com>
References: <CAM1a7P04apCwwmqNp4VLRam5_uk59tVRWv74UVD_g0-DUGNghw@mail.gmail.com>
 <Opd1OVGiyYCh2nGyCF1WbAozszMGHZSXiiC4cxN80cuIGS8TLzfzDjzcGulIOZDrq1bffF_UV6DO4QPFq1jmMY9UI0g5baUZMjWoeFqQvtM=@protonmail.com>
Message-ID: <9_2myR-nlP12e_UV4y5HTY8vLzvFirBNZSnNh4kiYP9BTvvHaByALKV1D5kl2BqTyexYYtmVbBZ1xLPYgtbTp-nNsfXEzc1ZlNN8wMGeiVg=@protonmail.com>


Good morning Antoine,

> However, as we discussed recently, i do believe their is a failure mode here. On one hand, directly connecting to pools is already possible today and pretty effective given the current mining centralization. On the other hand, it's not possible for most pre-signed txs protocols to reliably (securely) use the Bitcoin tx relay network today to propagate time-sensitive transactions. Furthermore, even if these are fixed (eg via package-relay for (today's) Lightning Network) it seems like there is a stark contrast between what "L2 [0] protocols" need and what regular node operators can reasonably offer. A node operator is incentivized to relay transactions to:
> - have more privacy *if* they use their node to broadcast transactions (make it less distinguishable which relayed transaction comes from the wallet)
> - provide fee estimates *if* they need them
> - avoid bandwidth spikes on block connection (compact block relay)

To be clear: it is possible to design L2 protocols such that various counterparties (whose incentives may not align perfectly) can bid to put their views of the L2 state on the blockchain.
For instance, in Lightning, you may wish to close a channel at a high feerate in order to use your onchain funds quickly, yet your channel counterparty has no similar time pressure to get their onchain funds in a usable state.
Solutions such as anchor outputs have been devised to allow each counterparty to pay fees as they see fit, however, for instance for anchor outputs the commitment transaction has to be at the minimum relay feerate.
At times of high blockchain congestion, node operators may raise the minimum feerate they are willing to relay (in order to alleviate bandwidth use), which may prevent commitment transactions from being relayed; even if a CPFP were relayed later, since the parent transaction is not propagated in the first place, the CPFP transaction spending the anchor outputs cannot propagate too.

I believe that is what you are referring to here as an example of how an L2 protocol cannot rely on the current L1 network for timely confirmation?

> L2s would ideally need the tx relaying nodes to do more computation and lift their DOS mitigations for all miner-incentive-compatible transactions to eventually be relayed to most of the miners. One obvious instance of such a dilemma is the RBF rules.
> Such protocols getting increasingly developed and used create a strong incentive for their users/stakeholders to directly connect to mining pools [1], with all the consequences for the network mentioned in the replies to your mail and elsewhere.
> Before we get to this, i think there is a strong case for an opt-in and publicly accessible "overlay" network to relay miner-incentive compatible transactions with higher DOS limits. This way the cost is not imposed to L1 node runners, and L2s can operate with more safety assumptions without (entirely) falling for centralization.


Let us imagine how such a network would operate.

It seems to me that an issue here is that *relay is free*.

And: "you get what you pay for".
Since mere relay is free (i.e. nodes do not charge any fee to merely *relay* transactions) the quality of that relay is low.
Thus, L1 node operators may insist on policies that do not support well miner-incentive transaction packages.


So the question is: who should pay for relay?
Ultimately of course the transactor pays, but it seems to me that the direct payer should be miners; transactors would offer higher fees, and miners would then pay part of those fees to relayers to get those transactions.

So, perhaps, let us consider a sketch (which is probably impossible, but may trigger other ideas):


Ideally, a relayer system (whether a single node, or some cooperating/competing overlay network of nodes) would gather a bunch of transactions in a proposed package of high-paying transactions.
Miners then offer to pay the relayer contingent on learning such a package.

The miner wants:

* To be given proof, before paying, that the package:
  * Pays some certain value in fees.
  * Consist of valid transactions.
* To atomically get the actual package once they pay.

The relayer wants:

* To be paid when it reveals such a package of transaction.

Transactors want:

* Fees to the relayer to be included as part of the mining fees in their transaction.


The flow would be something like this:

* A transactor has a group of transactions it wants confirmed.
* It goes to some relayer and feeds the transactions to them.
* The relayer figures out the most lucrative package (a task that may be NP-hard?  Knapsack problem equivalent?).
* Miners look for relayers who have figured out the most lucrative next package.
* Miners pick the best package and pay for the package.
* Miners compete on mining.

The issues are:

* We need some way to prove that a bunch of transactions are valid, without showing the actual transactions.
  * Maybe part of Utreexo or similar concepts can be used?
* We need some way to prove that a bunch of transactions pays some fee.
  * Again, Utreexo?  Maybe?  Probably not?
  * Fees = inputs - outputs, so the delta between the input UTXO set and the output UTXO set should be the fee, how do we prove that?
* We need some way to actually get the actual transaction data contingent on some PTLC or HTLC.


Hmm.
Thoughts?

Regards,
ZmnSCPxj

From bitcoin-dev at wuille.net  Tue Oct 26 16:26:43 2021
From: bitcoin-dev at wuille.net (Pieter Wuille)
Date: Tue, 26 Oct 2021 16:26:43 +0000
Subject: [bitcoin-dev] death to the mempool, long live the mempool
In-Reply-To: <CAM1a7P04apCwwmqNp4VLRam5_uk59tVRWv74UVD_g0-DUGNghw@mail.gmail.com>
References: <CAM1a7P04apCwwmqNp4VLRam5_uk59tVRWv74UVD_g0-DUGNghw@mail.gmail.com>
Message-ID: <yHOT3ZeXzyc6k8iXDhHfeZuZeYIND51V_-EcadZ9womZdjRLIVOynuOr8lD5FZlSgAGTrgLIzzTKrKxAPz390NoOykkA6wM5CjYgC3yPzT4=@wuille.net>

On Monday, October 25th, 2021 at 10:56 PM, lisa neigut via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:

> Hi all,
>
> In a recent conversation with @glozow, I had the realization that the mempool is obsolete and should be eliminated.

Hi Lisa,

I see where this idea is coming from, especially as it relates to reducing complexities around transaction relays, but I strongly believe this is throwing out the baby with the bathwater. Comments inline below.

> In reality however, mempool relay is unnecessary where the majority of hashpower and thus block template creation is concentrated in a semi-restricted set.

The *entire* reason mining and PoW exist, as opposed to having a fixed, centralized (set of) actors who decide transaction ordering, is to make the "censorship rights" of the network permissionless. It is essential that anyone can become a miner if they dislike what existing miners are doing, with income close to proportional to their investment. The existing reality isn't perfect, but it's fairly close to that. Sure, at any given point in time, a nontrivial fraction of mining power is in the hands of a few, but over time, those can, and have, changed a lot. Furthermore, if miners were to actually exercise censorship, it could quite reasonably incentivize other ecosystem players to start mining, perhaps close at cost or even at a small loss.

Your proposal, as far as I can tell, makes it *far* harder to become a miner. Ideas to provide a mechanism for miners to publish their "tx submit" URL/IP/onion on chain don't help; that's dependent on other miners to not censor the publishing. Furthermore, it gives a tremendous centralizing incentive: it's just far easier for most wallets to just submit to the largest few pools, because the cost/complexity of an additional submission is independent of the pool's hashrate, but the benefit is directly proportional to it. There would be very little incentive to submit to a sub-1% pool for anyone.

> Removing the mempool would greatly reduce the bandwidth requirement for running a node,

That's not true due to compact blocks (most transactions are relayed exactly once to every node, and not repeated in blocks), and with Erlay it will be even less the case.

> keep intentionality of transactions private until confirmed/irrevocable,

Except to miners; it's replacing socialized transparency with a few who get to see the actual details. Not the same scale obviously, but there is some similarity to banks in the existing financial system. Our privacy goals shouldn't be relying on a few trusted gatekeepers.

> and naturally resolve all current issues inherent in package relay and rbf rules. It also resolves the recent minimum relay questions, as relay is no longer a concern for unmined transactions.

There are other solutions to this, like weak blocks (miners get to relay partial PoW solutuon of say 10% of the difficulty to the network; and nodes which receive such a weak block can "forcibly" insert its transaction to their mempool, as there is evidence it's actually being worked on, while still being DoS resistant because partial PoW is still PoW).

> Provided the number of block template producing actors remains beneath, say 1000, it?d be quite feasible to publish a list of tor endpoints that nodes can independently + directly submit their transactions to. In fact, merely allowing users to select their own list of endpoints to use alternatively to the mempool would be a low effort starting point for the eventual replacement.

In this scenario, there is no incentive for miners to relay to each other. The fewer other miners know about a high fee-paying transaction, the better you as a miner.

More conceptually: it is a responsibility of the full node network to relay blocks between miners quickly, to limit how much advantage well-connected miners over less-well-connected ones have. If the network doesn't have the transactions being included in those blocks, this is *far* harder (additional roundtrips, as nodes can't reconstruct from mempools).

> A direct communication channel between block template construction venues and transaction proposers also provides a venue for direct feedback wrt acceptable feerates at the time, which both makes transaction confirmation timelines less variable as well as provides block producers a mechanism for (independently) enforcing their own minimum security budget. In other words, expressing a minimum acceptable feerate for continued operation.

Yes, it's definitely easier. That doesn't make it right.

Cheers,

--
Pieter
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211026/f0c847d7/attachment-0001.html>

From gloriajzhao at gmail.com  Tue Oct 26 18:16:51 2021
From: gloriajzhao at gmail.com (Gloria Zhao)
Date: Tue, 26 Oct 2021 19:16:51 +0100
Subject: [bitcoin-dev] death to the mempool, long live the mempool
In-Reply-To: <CAM1a7P04apCwwmqNp4VLRam5_uk59tVRWv74UVD_g0-DUGNghw@mail.gmail.com>
References: <CAM1a7P04apCwwmqNp4VLRam5_uk59tVRWv74UVD_g0-DUGNghw@mail.gmail.com>
Message-ID: <CAFXO6=Jk0MAqQ6u5JCrpC3eMv=bF3DT6wH6Y60zb_b-beU4mcg@mail.gmail.com>

Hi Lisa,

Some background for people who are not familiar with mempool:

The mempool is a cache of unconfirmed transactions, designed in a way
to help miners efficiently pick the highest feerate packages to
include in new blocks. It stores more than a block's worth of
transactions because transaction volume fluctuates and any rational
miner would try to maximize the fees in their blocks; in a reorg, we
also don't want to completely forget what transactions were in the
now-stale tip.

In Bitcoin Core, full nodes keep a mempool by default. The additional
requirements for keeping a mempool are minimal (300MB storage, can be
configured to be lower) because anyone, anywhere in the world, should
be able to run a node and broadcast a Bitcoin payment without special
connectivity to some specific set of people or expensive/inaccessible
hardware. Perhaps connecting directly to miners can be a solution for
some people, but I don't think it's healthy for the network.

Some benefits of keeping a mempool as a non-mining node include:
- Fee estimation based on your node's knowledge of unconfirmed
transactions + historical data.
- Dramatically increased block validation (and thus propagation)
speed, since you cache signature and script results of transactions
before they are confirmed.
- Reduced block relay bandwidth usage (Bitcoin Core nodes use BIP152
compact block relay), as you don't need to re-request the block
transactions you already have in your mempool.
- Wallet ability to send/receive transactions that spend unconfirmed outputs.

> I had the realization that the mempool is obsolete and should be eliminated.

I assume you mean that the mempool should still exist but be turned
off for non-mining nodes. A block template producer needs to keep
unconfirmed transactions somewhere.
On Bitcoin Core today, you can use the -blocksonly config option to
ignore incoming transactions (effectively switching off your mempool),
but there are strong reasons not to do this:
- It is trivial for your peers to detect that all transactions
broadcasted by your node = from your wallet. Linking your node to your
transactions is a very bad privacy leak.
- You must query someone else for what feerate to put on your transaction.
- You can't use BIP152 compact block relay, so your network bandwidth
usage spikes at every block. You also can't cache validation results,
so your block validation speed slows down.

> Removing the mempool would greatly reduce the bandwidth requirement for running a node...

If you're having problems with your individual node's bandwidth usage,
you can also decrease the number of connections you make or turn off
incoming connections. There are efforts to reduce transaction relay
bandwidth usage network-wide [1].

> Removing the mempool would... keep intentionality of transactions private until confirmed/irrevocable...

I'm confused - what is the purpose of keeping a transaction private
until it confirms? Don't miners still see your transaction? A
confirmed transaction is not irrevocable; reorgs happen.

> Removing the mempool would... naturally resolve all current issues inherent in package relay and rbf rules.

Removing the mempool does not help with this. How does a miner decide
whether a conflicting transaction is an economically-advantageous
replacement or a DoS attack? How do you submit your CPFP if the parent
is below the mempool minimum feerate? Do they already have a different
relay/mempool implementation handling all of these problems but don't
aren't sharing it with the rest of the community?

> Initial feerate estimation would need to be based on published blocks, not pending transactions (as this information would no longer be available), or from direct interactions with block producers.

There are many reasons why using only published blocks for fee
estimates is a flawed design, including:

- The miner of a block can artificially inflate the feerate of the
transactions in their mempool simply by including a few of their own
transactions that pay extremely high feerates. This costs them
nothing, as they collect the fees.
- A miner constructs a block based on the transactions in their
mempool. Your transaction's feerate may have been enough to be
included 2 blocks ago or a week ago, but it will be compared to the
other unconfirmed transactions available to the miner now. They can
tell you what's in their mempool or what the next-block feerate is,
but you would be a fool to believe them.

See also [2],[3].

> Provided the number of block template producing actors remains beneath, say 1000, it?d be quite feasible to publish a list of tor endpoints that nodes can independently  + directly submit their transactions to. In fact, merely allowing users to select their own list of endpoints to use alternatively to the mempool would be a low effort starting point for the eventual replacement.

As a thought experiment, let's imagine we have some public registry of
mining nodes' tor endpoints and we use it for this secondary
direct-to-miner transaction relay network. If the registry is
maintained (by who?) and accurate (based on whose word?), it is a
point of failure for transaction censorship and deanonymization, as
well as an additional barrier to becoming a miner, encouraging
centralization.
The other possibility is that the registry is not accurate. In fact,
unless the registry requires miners to identify themselves (which
others on this thread have already pointed out is ill-advised), this
should be treated similarly to regular addr gossip. We would never
automatically trust that the entity behind the endpoint provides the
service it advertises, is an honest node that won't simply blackhole
our transaction, or even belongs to a Bitcoin node at all.

Best,
Gloria

[1]: https://arxiv.org/pdf/1905.10518.pdf
[2]: https://bitcointechtalk.com/an-introduction-to-bitcoin-core-fee-estimation-27920880ad0
[3]: https://gist.github.com/morcos/d3637f015bc4e607e1fd10d8351e9f41


On Tue, Oct 26, 2021 at 8:38 AM lisa neigut via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Hi all,
>
> In a recent conversation with @glozow, I had the realization that the
> mempool is obsolete and should be eliminated.
>
> Instead, users should submit their transactions directly to mining pools,
> preferably over an anonymous communication network such as tor. This can
> easily be achieved by mining pools running a tor onion node for this
> express purpose (or via a lightning network extension etc)
>
> Mempools make sense in a world where mining is done by a large number of
> participating nodes, eg where the block template is constructed by a
> majority of the participants on the network. In this case, it is necessary
> to socialize pending transaction data to all participants, as you don?t
> know which participant will be constructing the winning block template.
>
> In reality however, mempool relay is unnecessary where the majority of
> hashpower and thus block template creation is concentrated in a
> semi-restricted set.
>
> Removing the mempool would greatly reduce the bandwidth requirement for
> running a node, keep intentionality of transactions private until
> confirmed/irrevocable, and naturally resolve all current issues inherent in
> package relay and rbf rules. It also resolves the recent minimum relay
> questions, as relay is no longer a concern for unmined transactions.
>
> Provided the number of block template producing actors remains beneath,
> say 1000, it?d be quite feasible to publish a list of tor endpoints that
> nodes can independently  + directly submit their transactions to. In fact,
> merely allowing users to select their own list of endpoints to use
> alternatively to the mempool would be a low effort starting point for the
> eventual replacement.
>
> On the other hand, removing the mempool would greatly complicate solo
> mining and would also make BetterHash proposals, which move the block
> template construction away from a centralized mining pool back to the
> individual miner, much more difficult. It also makes explicit the target
> for DoS attacks.
>
> A direct communication channel between block template construction venues
> and transaction proposers also provides a venue for direct feedback wrt
> acceptable feerates at the time, which both makes transaction confirmation
> timelines less variable as well as provides block producers a mechanism for
> (independently) enforcing their own minimum security budget. In other
> words, expressing a minimum acceptable feerate for continued operation.
>
> Initial feerate estimation would need to be based on published blocks, not
> pending transactions (as this information would no longer be available), or
> from direct interactions with block producers.
>
>
> ~niftynei
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211026/9477b0b5/attachment.html>

From antoine.riard at gmail.com  Tue Oct 26 23:44:45 2021
From: antoine.riard at gmail.com (Antoine Riard)
Date: Tue, 26 Oct 2021 19:44:45 -0400
Subject: [bitcoin-dev] death to the mempool, long live the mempool
In-Reply-To: <CAM1a7P04apCwwmqNp4VLRam5_uk59tVRWv74UVD_g0-DUGNghw@mail.gmail.com>
References: <CAM1a7P04apCwwmqNp4VLRam5_uk59tVRWv74UVD_g0-DUGNghw@mail.gmail.com>
Message-ID: <CALZpt+E_47wmDg3Z2N5=z5x3cca5vSxP6fgYzMK_pybwp1161g@mail.gmail.com>

Hi Lisa,

Network mempools constitute a blockspace marketplace where block demand
meets the offer in real-time. Block producers are acting to discover the
best feerate bids compensating for their operational costs and transaction
proposers are acting to offer the best feerate in function of their
confirmation preferences.

Of course in a distributed system like bitcoin, we can't guarantee perfect
information from the market participants. But moving away from this model
by decreasing the ability of the non-mining nodes to observe the current
demand is softening the requirements for potential attackers.

As transaction proposers are competing with each other to publish, they
have an interest to "front-run" each other by querying the pending
transactions to the block producers instead of observing only the published
blocks. Therefore good connections to
the block producers are now critical and censorship-resistance of the
mining endpoints must be guaranteed.

Such a list of endpoints couldn't be static otherwise it's an artificial
barrier to enter in the mining competition, and as such a centralization
vector. Dynamic, trust-minimized discovery of the mining endpoints assumes
an address-relay network, of which the robustness must be high enough
against sophisticated sybil attacks. One current defense mechanism in core
to achieve that is selecting outbound peers based in different /16 subnets
as it's harder for an attacker to obtain IP addresses. Replicating this
mechanism for the mining endpoints binds the mining topology to the
Internet one, which is downgrading the mining competition.

Relying on tor to guarantee the confidentiality of the transaction
announcement is raising its own issues. Flowing by default all the bitcoin
traffic over tor will change the incentive structure of tor attackers,
potentially attracting a new class of attackers able to do deanonymization
attacks, not that expensive in practice [0]. Tor bridges are another
censorship vector as the fingerprint of the bitcoin traffic (a block every
10 min, etc) make it possible to drop or delay the tor channel, in the lack
of high-bandwidth consuming "synthetic" traffic.

Further, identified mining endpoints make it easier to launch partition
attacks, where mining mempools are sent low-feerate clusters of
transactions, to prevent the replacement by a better feerate offer. This is
especially concerning for L2 nodes with time-sensitive requirements [1]

Lastly, removing the mempool won't solve the current issues inherent with
pre-signed transactions under the mempool min fee as ultimately miner's
mempools are also finite in memory and a dynamic lower bound must exist to
prevent spam. These lower bounds potentially increase after the signature
exchange of the time-sensitive transactions.

Antoine

[0] https://www.usenix.org/system/files/sec19-jansen.pdf
[1] See "The Ugly"
https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-June/002758.html

Le mar. 26 oct. 2021 ? 03:37, lisa neigut via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> a ?crit :

> Hi all,
>
> In a recent conversation with @glozow, I had the realization that the
> mempool is obsolete and should be eliminated.
>
> Instead, users should submit their transactions directly to mining pools,
> preferably over an anonymous communication network such as tor. This can
> easily be achieved by mining pools running a tor onion node for this
> express purpose (or via a lightning network extension etc)
>
> Mempools make sense in a world where mining is done by a large number of
> participating nodes, eg where the block template is constructed by a
> majority of the participants on the network. In this case, it is necessary
> to socialize pending transaction data to all participants, as you don?t
> know which participant will be constructing the winning block template.
>
> In reality however, mempool relay is unnecessary where the majority of
> hashpower and thus block template creation is concentrated in a
> semi-restricted set.
>
> Removing the mempool would greatly reduce the bandwidth requirement for
> running a node, keep intentionality of transactions private until
> confirmed/irrevocable, and naturally resolve all current issues inherent in
> package relay and rbf rules. It also resolves the recent minimum relay
> questions, as relay is no longer a concern for unmined transactions.
>
> Provided the number of block template producing actors remains beneath,
> say 1000, it?d be quite feasible to publish a list of tor endpoints that
> nodes can independently  + directly submit their transactions to. In fact,
> merely allowing users to select their own list of endpoints to use
> alternatively to the mempool would be a low effort starting point for the
> eventual replacement.
>
> On the other hand, removing the mempool would greatly complicate solo
> mining and would also make BetterHash proposals, which move the block
> template construction away from a centralized mining pool back to the
> individual miner, much more difficult. It also makes explicit the target
> for DoS attacks.
>
> A direct communication channel between block template construction venues
> and transaction proposers also provides a venue for direct feedback wrt
> acceptable feerates at the time, which both makes transaction confirmation
> timelines less variable as well as provides block producers a mechanism for
> (independently) enforcing their own minimum security budget. In other
> words, expressing a minimum acceptable feerate for continued operation.
>
> Initial feerate estimation would need to be based on published blocks, not
> pending transactions (as this information would no longer be available), or
> from direct interactions with block producers.
>
>
> ~niftynei
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211026/bd8c767a/attachment-0001.html>

From willtech at live.com.au  Wed Oct 27 08:44:42 2021
From: willtech at live.com.au (LORD HIS EXCELLENCY JAMES HRMH)
Date: Wed, 27 Oct 2021 08:44:42 +0000
Subject: [bitcoin-dev] death to the mempool, long live the mempool
In-Reply-To: <CAM1a7P04apCwwmqNp4VLRam5_uk59tVRWv74UVD_g0-DUGNghw@mail.gmail.com>
References: <CAM1a7P04apCwwmqNp4VLRam5_uk59tVRWv74UVD_g0-DUGNghw@mail.gmail.com>
Message-ID: <PS2P216MB1089038AABE45CE8FF36E32F9D859@PS2P216MB1089.KORP216.PROD.OUTLOOK.COM>

Good Afternoon,

No. This has been discussed previously and eliminated as there is no proof that the transaction can exist without population through the mempool. As a method of payment not hearing about a transaction until it is possibly mined three months later as I have experienced is non-functional, there were discussions in this mailing list. The purpose of the mempool is not gossip it is gossip and any node technically can mine if they do.

KING JAMES HRMH
Great British Empire

Regards,
The Australian
LORD HIS EXCELLENCY JAMES HRMH (& HMRH)
of Hougun Manor & Glencoe & British Empire
MR. Damian A. James Williamson
Wills

et al.


Willtech
www.willtech.com.au
www.go-overt.com
duigco.org DUIGCO API
and other projects


m. 0487135719
f. +61261470192


This email does not constitute a general advice. Please disregard this email if misdelivered.
________________________________
From: bitcoin-dev <bitcoin-dev-bounces at lists.linuxfoundation.org> on behalf of lisa neigut via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>
Sent: Tuesday, 26 October 2021 1:56 PM
To: bitcoin-dev at lists.linuxfoundation.org <bitcoin-dev at lists.linuxfoundation.org>
Subject: [bitcoin-dev] death to the mempool, long live the mempool

Hi all,

In a recent conversation with @glozow, I had the realization that the mempool is obsolete and should be eliminated.

Instead, users should submit their transactions directly to mining pools, preferably over an anonymous communication network such as tor. This can easily be achieved by mining pools running a tor onion node for this express purpose (or via a lightning network extension etc)

Mempools make sense in a world where mining is done by a large number of participating nodes, eg where the block template is constructed by a majority of the participants on the network. In this case, it is necessary to socialize pending transaction data to all participants, as you don?t know which participant will be constructing the winning block template.

In reality however, mempool relay is unnecessary where the majority of hashpower and thus block template creation is concentrated in a semi-restricted set.

Removing the mempool would greatly reduce the bandwidth requirement for running a node, keep intentionality of transactions private until confirmed/irrevocable, and naturally resolve all current issues inherent in package relay and rbf rules. It also resolves the recent minimum relay questions, as relay is no longer a concern for unmined transactions.

Provided the number of block template producing actors remains beneath, say 1000, it?d be quite feasible to publish a list of tor endpoints that nodes can independently  + directly submit their transactions to. In fact, merely allowing users to select their own list of endpoints to use alternatively to the mempool would be a low effort starting point for the eventual replacement.

On the other hand, removing the mempool would greatly complicate solo mining and would also make BetterHash proposals, which move the block template construction away from a centralized mining pool back to the individual miner, much more difficult. It also makes explicit the target for DoS attacks.

A direct communication channel between block template construction venues and transaction proposers also provides a venue for direct feedback wrt acceptable feerates at the time, which both makes transaction confirmation timelines less variable as well as provides block producers a mechanism for (independently) enforcing their own minimum security budget. In other words, expressing a minimum acceptable feerate for continued operation.

Initial feerate estimation would need to be based on published blocks, not pending transactions (as this information would no longer be available), or from direct interactions with block producers.


~niftynei
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211027/e9782e39/attachment.html>

From pete at petertodd.org  Wed Oct 27 20:01:51 2021
From: pete at petertodd.org (Peter Todd)
Date: Wed, 27 Oct 2021 16:01:51 -0400
Subject: [bitcoin-dev] death to the mempool, long live the mempool
In-Reply-To: <CALZpt+E_47wmDg3Z2N5=z5x3cca5vSxP6fgYzMK_pybwp1161g@mail.gmail.com>
References: <CAM1a7P04apCwwmqNp4VLRam5_uk59tVRWv74UVD_g0-DUGNghw@mail.gmail.com>
 <CALZpt+E_47wmDg3Z2N5=z5x3cca5vSxP6fgYzMK_pybwp1161g@mail.gmail.com>
Message-ID: <20211027200151.GC5674@petertodd.org>

On Tue, Oct 26, 2021 at 07:44:45PM -0400, Antoine Riard via bitcoin-dev wrote:
> Such a list of endpoints couldn't be static otherwise it's an artificial
> barrier to enter in the mining competition, and as such a centralization
> vector. Dynamic, trust-minimized discovery of the mining endpoints assumes
> an address-relay network, of which the robustness must be high enough
> against sophisticated sybil attacks. One current defense mechanism in core
> to achieve that is selecting outbound peers based in different /16 subnets
> as it's harder for an attacker to obtain IP addresses. Replicating this
> mechanism for the mining endpoints binds the mining topology to the
> Internet one, which is downgrading the mining competition.

I think a really simple way to put it is if we didn't have the mempool, it'd be
good to create a free service that got transactions to miners in an equal
opportunity, decentralized, way. A simple flood fill scheme would be a great
way to do that... at which point you've re-invented the mempool.

Nothing wrong with people running nodes that opt-out of transaction
broadcasting, and it may even make sense for such nodes to preferentially peer
with each other. But there's always going to be a need for a scheme like the
existing mempool, so might as well just keep it.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 488 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211027/237114b0/attachment.sig>

From ZmnSCPxj at protonmail.com  Thu Oct 28 01:04:10 2021
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Thu, 28 Oct 2021 01:04:10 +0000
Subject: [bitcoin-dev] death to the mempool, long live the mempool
In-Reply-To: <CAFXO6=Jk0MAqQ6u5JCrpC3eMv=bF3DT6wH6Y60zb_b-beU4mcg@mail.gmail.com>
References: <CAM1a7P04apCwwmqNp4VLRam5_uk59tVRWv74UVD_g0-DUGNghw@mail.gmail.com>
 <CAFXO6=Jk0MAqQ6u5JCrpC3eMv=bF3DT6wH6Y60zb_b-beU4mcg@mail.gmail.com>
Message-ID: <DS-9LyYKokVu_2m2j7ZxpVY3CmOLF_efYCGftH4pqHF1Wk2mFBQNl_ILazvKXJvTiVSQ3b_v5vg29DRFQs301wwNwfEgKUCPo3MOq_VIPm0=@protonmail.com>

Good morning Gloria, et al,


> > Removing the mempool would... naturally resolve all current issues inherent in package relay and rbf rules.
>
> Removing the mempool does not help with this. How does a miner decide whether a conflicting transaction is an economically-advantageous replacement or a DoS attack? How do you submit your CPFP if the parent is below the mempool minimum feerate? Do they already have a different relay/mempool implementation handling all of these problems but don't aren't sharing it with the rest of the community?

This seems an important key point: *even if* miners maintain some kind of "accept any transaction!!!" endpoint, the miner still wants to have *some* policy on how to evict transactions from its pool of transactions, for the simple reason that *everyone* has limited resources, even miners.

Perhaps the issue is that eviction is done *immediately*, i.e. if a node receives a transaction below some feerate treshhold, it immediately drops the transaction.

What if instead we did the eviction lazily?

Suppose we used something like a garbage collector.
Unconfirmed transactions are objects that point to other objects (i.e. the input of a transaction "points to" another object).
"Primitive" objects are UTXOs of *confirmed* transactions, i.e. the UTXO set at the block tip.
Then, a GC algorithm would start at some roots and then terminate when it reaches primitive objects.

I describe here an algorithm based on semispace GC, but the GC algorithm space is well-studied and other algorithms may also be devised (in particular, spam is likely to match quite well with "infant mortality" concept in GC, i.e. "most objects die young", so some kind of nursery / generational GC may work better against spam in practice).

A semispace GC has two "spaces" for memory.
One is the "from-space" and the other is the "to-space".
During normal operation, the "from-space" is used and the "to-space" is empty.
(Note that we can implement a "space" as a table (`std::map`) from txid to transaction, and avoid having to *actually* copy the transaction data; the important thing is having two spaces)
There is a maximum size that from-space and to-space can be.

As we receive transactions, we allocate them on the from-space.
Once the from-space is filled, we stop operations and perform a GC cycle.

We select "roots" by ordering all transactions in the from-space, from highest-feerate to lowest-feerate (figure out some way to handle ties later, maybe put a timestamp or monotonic counter?).
Starting with the highest-feerate tx, we trace all the transactions they refer to, recursively, copying them from from-space to to-space.
We stop once the to-space is filled more than half.

At this point, we drop all transactions in the from-space that are not already in to-space, and then delete the from-space.
Then we promote the to-space as the from-space, and continue our merry way, allocating more transactions.

(Nothing prevents doing this on disk rather than in memory; xref Eric Voskuil)

Note that the algorithm operates on individual transactions, not packages of transactions.
The algorithm is vulnerable to spam where the spammer creates several large low-feerate transactions, then anchors them all using a tiny high-feerate transaction (a "tall" attack).
It is also vulnerable to spam where the spammer creates several high-feerate transactions spending the same UTXO (a "wide" attack), thus preventing anyone else from getting any transactions into the mempool.

Against these exploit, we can mitigate by *first* moving objects to a smaller "packagespace" instead of directly on the to-space.
When tracing a new root, we first move the transactions that are not already in to-space to the packagespace, then measure the aggregate fee divided by the aggregate memory usage.
If this is below, say, half the feerate of the root transaction, then we drop the packagespace (put it back into from-space) and move on to the next root.
This protects against "tall" attacks.
To protect against "wide" attacks, if the packagespace consumes a TXO that is already consumed in the to-space, we also drop the packagespace (i.e. only retain the highest-feerate version in a "wide" attack).
Once the above checks pass, we merge the packagespace into the to-space.

This algorithm means that we do not need package relay; instead, we just send transactions in the correct order (parents first, then children), and if the receiver does not need to do a GC in-between, then everything ends up in the mempool.
If the child transaction is high-fee enough to be a root transaction, and pays enough that its feerate dominates in the packagespace result, then the entire sequence will remain in the mempool.

The algorithm allows for conflicting transactions to be retained in the mempool temporarily, until the next GC triggers (at which point conflicting transactions are resolved by whoever is higher-feerate).
This is helpful since a conflicting transaction may be what ends up getting confirmed in a block from a miner whose mempool did not contain the "best" feerate transaction.


WDYT?
Regards,
ZmnSCPxj

From billy.tetrud at gmail.com  Fri Oct 29 15:47:12 2021
From: billy.tetrud at gmail.com (Billy Tetrud)
Date: Fri, 29 Oct 2021 10:47:12 -0500
Subject: [bitcoin-dev] TAPLEAF_UPDATE_VERIFY covenant opcode
In-Reply-To: <CAO3Pvs-OviFkDJOsOWOt3Ea9t0g6xXb79hYdEe_L1=nB5ZjSQw@mail.gmail.com>
References: <20210909064138.GA22496@erisian.com.au>
 <CAO3Pvs-OviFkDJOsOWOt3Ea9t0g6xXb79hYdEe_L1=nB5ZjSQw@mail.gmail.com>
Message-ID: <CAGpPWDYWB+TXMjLSTFw_ti62++w43ojAhPKTXOPghvN-=Qa4Kg@mail.gmail.com>

I very much like this idea. It seems pretty flexible and has a lot of
interesting properties and use cases without being very complex. I'll have
to read through this more deeply later. I'm curious to understand more how
it compares to OP_CTV. It seems that implementing OP_TLUV wouldn't make
OP_CTV obsolete/uncessary, is that right?

> And second, it doesn't provide a way for utxos to "interact",

This is talking about sending data to the output from an input or getting
data from a parent input, other than any added output tapscripts, right? I
think this can/should be done with a separate opcode, so I'm not sure I
would really call this a limitation here. I wrote a proposal for something
that does allow interaction like that (specifically sending data to an
output: OP_PUSHOUTPUTSTACK
<https://github.com/fresheneesz/bip-efficient-bitcoin-vaults/blob/main/bip-pushoutputstack.md>
).

On Wed, Sep 22, 2021 at 7:29 PM Olaoluwa Osuntokun via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Hi AJ,
>
> Happy to see that this proposal has finally seen the light of day! I've
> been
> hearing about it in hinted background convos over the past few months, so
> happy I can finally dig into the specifics of its operation.
>
> > So the idea is to do just that via a new opcode "TAPLEAF_UPDATE_VERIFY"
> > (TLUV) that takes three inputs: one that specifies how to update the
> > internal public key (X), one that specifies a new step for the merkle
> path
> > (F), and one that specifies whether to remove the current script and/or
> > how many merkle path steps to remove
>
> What if instead, it obtained the script from the _annex_? I think this
> small
> modification would make the op code even _more_ powerful. Consider that
> this
> allows a new script to be passed _dynamically_ after the output has been
> created, possibly by a threshold of parties that control the output, or
> them
> all (mu sig, etc, etc). This serves to create a generic "upgrade" mechanism
> for any tapscript output (covenant or not). Functionally, this is similar
> to
> the existence of "admin keys" or voted DAO upgrades that exists in chains
> that utilize an account based systems. This is really useful as it allows a
> script any given output to optional add in graftroot like behavior (leaf in
> tree that accepts script updates), and also allows contract developers to
> progressively upgrade or fix issues in prior versions of their deployed
> contracts.
>
> This little trick is secure since unlike the witness itself, the annex is
> actually _signed_ within the sighash like everything else. Incorporating
> this proposal would require the addition of an OP_PUSH_ANNEX op code, which
> by itself seems expertly useful. If one views the annex as a sort of
> authenticated associated data that can be passed into the script execution
> context, then this actually serves to absorb _some_ uses cases of a
> hypothetical OP_CHECKSIG_FROM_STACK opcode. A push annex op code also makes
> doing things like output delegation to a given key passed into the witness
> secure since the prior "owner" of the output commits to the key within the
> sighash.
>
> Even assuming a more powerful type of covenant that allows partial
> application of binding logic, something like this is still super useful
> since the action of re-creating a new tapscript tree based in dynamic input
> data would generate a rather large witness if only something like OP_CAT
> was
> available. The unique "update" nature of this appears to augment any other
> type of covenant, which is pretty cool. Consider that it would allow you
> (with the annex addition above), take something like a CTV congestion tree,
> and add in _new_ users at the tree is already being unrolled (just a toy
> example).
>
> It would also allow an individual to _join_ the payment pool construct
> described earlier which makes it 1000x more useful (vs just supporting
> unrolling). I haven't written it all down yet, but I think this along with
> something like CTV or CSFS makes it possible to implement a Plasma Cash [4]
> like Commit Chain [5], which is super exciting (assume a counter is
> embedded
> in the main script that tracks the next free leaf slot(s). With this model
> an "operator" is able to include a single transaction in the chain that
> stamps a batch of updates in the payment tree.  Users then get a
> contestation period where they can refute a modification to the tree in
> order to withdraw their funds.
>
> > And second, it doesn't provide a way for utxos to "interact",
>
> This is due to the fact that the op code doesn't allow any sort of late
> binding or pattern matching then constraining _where_ (or whence?) the
> coins
> can Be sent to. There's a group of developers that are attempting to make
> an
> AMM-like system on Liquid [1] using more generic stack based covenants [2]
> (see the `OP_INSPECTINPUT` op code, which seems very much inspired by
> jl2012's old proposal). However one challenge that still need to be tackled
> in the UTXO model is allowing multiple participants to easily interact w/
> the
> contract in a single block w/o a coordination layer to synchronize the
> access.
>
> One solution to this concurrency issue, that I believe is already employed
> by Chia is to allow "contracts" to be identified via a fixed ID (as long as
> their active in the chain) [3]. This lets transactions spend/interact with
> a
> contract, without always needing to know the set of active UTXOs where that
> contract lives. Transactions then specify their contract and "regular"
> inputs, with the requirement that every transaction spends at least a
> single
> regular input.
>
> The trade-off here is that nodes need to maintain this extra index into the
> UTXO set. However, this can be alleviated by applying a utreexo like
> solution: nodes maintain some merklized data structure over the index and
> require that spending transactions provide an _inclusion_ proof of the
> active contract. Nodes then only need to maintain root hashes of the UTXO
> and contract set.
>
> I'm super happy w.r.t how the covenant space has been processing over the
> past few years. IMO its the single most important (along with the utreexo
> type stateless stuff mentioned above) missing component to allow the
> creation of more decentralized self-custodial applications built on top of
> Bitcoin.
>
> -- Laolu
>
> [1]: https://medium.com/bit-matrix
> [2]:
> https://github.com/sanket1729/elements/blob/84339ba5e5dc65328d98afe2b1b33dcb69ba4311/doc/tapscript_opcodes.md
> [3]:
> https://forum.celestia.org/t/accounts-strict-access-lists-and-utxos/37
> [4]: https://www.learnplasma.org/en/learn/cash.html
> [5]: https://eprint.iacr.org/2018/642
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211029/e1788cdc/attachment.html>

From yanmaani at cock.li  Wed Oct 27 23:05:59 2021
From: yanmaani at cock.li (yanmaani at cock.li)
Date: Wed, 27 Oct 2021 23:05:59 +0000
Subject: [bitcoin-dev] death to the mempool, long live the mempool
In-Reply-To: <CAM1a7P04apCwwmqNp4VLRam5_uk59tVRWv74UVD_g0-DUGNghw@mail.gmail.com>
References: <CAM1a7P04apCwwmqNp4VLRam5_uk59tVRWv74UVD_g0-DUGNghw@mail.gmail.com>
Message-ID: <d85b843702f2f04a90158ccc75b06226@cock.li>

[I removed a comment regarding the moderation of this list here because 
it caused for my message to be rejected]

On 2021-10-26 02:56, lisa neigut via bitcoin-dev wrote:
> [...] the mempool is obsolete and should be eliminated.
> 
> Instead, users should submit their transactions directly to mining
> pools, [...]
> Mempools make sense in a world where mining is done by a large number
> of participating nodes, [...] as you don?t know which participant will
> be constructing the winning block template.
> 
> In reality however, mempool relay is unnecessary where the majority of
> hashpower and thus block template creation is concentrated in a
> semi-restricted set.

It's true that there is some centralization, but this is hardly a 
desirable goal that should be formally enshrined.

By that point, you might as well block people from keeping their coins 
in their own wallet, on the basis that in practice mostly everyone keeps 
them on the exchange.

And as the others have pointed out: even if you did hold this to be 
desirable, why would removing the mempool be a good idea? The pools 
would still need some way to get transactions, and a mempool seems like 
an excellent way to do this.

I think most of the people here have laid out all of the other obvious 
issues with the proposal.

> Removing the mempool would greatly reduce the bandwidth requirement
> for running a node

You can disable it already if you're strapped for cash. Is there a 
reason why this is not adequate?

> keep intentionality of transactions private until confirmed/irrevocable

What is the "intentionality" of a transaction and why do I want to keep 
it private? My transactions are 100% intentional because I am trying to 
send money, and I wouldn't make them otherwise - what is a 
non-intentional transaction supposed to be?

> Provided the number of block template producing actors remains
> beneath, say 1000, it?d be quite feasible to publish a list of tor
> endpoints that nodes can independently  + directly submit their
> transactions to.

If nothing else, this would be a significant departure from the security 
model of Bitcoin:

> The network is robust in its unstructured simplicity.
> Nodes work all at once with little coordination.
> They do not need to be identified, since messages are not routed to any 
> particular place and only need to be delivered on a best effort basis.
> Nodes can leave and rejoin the network at will, accepting the 
> proof-of-work chain as proof of what happened while they were gone.

If you posit that the security model should be changed, that is one 
thing, but you should lay out your reasoning for this claim.

> On the other hand, removing the mempool would greatly complicate solo
> mining and would also make BetterHash proposals, which move the block
> template construction away from a centralized mining pool back to the
> individual miner, much more difficult.

I am amazed that you are intelligent enough to realize these trade-offs, 
yet still made this post. Are you suggesting that you find them to be 
acceptable?

> It also makes explicit the target for DoS attacks.

Perhaps the only good aspect of this proposal. Under such conditions, 
denial of service attacks would be both just and desirable.

> A direct communication channel between block template construction
> venues and transaction proposers also provides a venue for direct
> feedback wrt acceptable feerates at the time, which both makes
> transaction confirmation timelines less variable as well as provides
> block producers a mechanism for (independently) enforcing their own
> minimum security budget. In other words, expressing a minimum
> acceptable feerate for continued operation.

Why couldn't they just run a website about this for anyone who cares? 
Communicating two numbers can easily be done over HTTP. This technology 
exists already.

> ~niftynei
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

