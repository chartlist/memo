From pete at petertodd.org  Thu Mar  1 15:11:29 2018
From: pete at petertodd.org (Peter Todd)
Date: Thu, 1 Mar 2018 10:11:29 -0500
Subject: [bitcoin-dev] Revisiting BIP 125 RBF policy.
In-Reply-To: <CAMZUoK=Htds5fu5vnqAhEoZDrwHALKe6uwqXnmJb17pa_peFFw@mail.gmail.com>
References: <CAMZUoKnGx3p7=Kg96E3EEyJ8aFC7ezsvec_pAnN7oJz7-VbyLA@mail.gmail.com>
	<20180212225828.GB8551@fedora-23-dvm>
	<CAMZUoKnFBVFhaq61wKu_CcZgRKc5aoeTa-wq9h2CXH0WWHd3NQ@mail.gmail.com>
	<20180212234225.GA9131@fedora-23-dvm>
	<CAMZUoK=Htds5fu5vnqAhEoZDrwHALKe6uwqXnmJb17pa_peFFw@mail.gmail.com>
Message-ID: <20180301151129.GA9373@fedora-23-dvm>

On Tue, Feb 27, 2018 at 11:25:59AM -0500, Russell O'Connor wrote:
> On Mon, Feb 12, 2018 at 6:42 PM, Peter Todd <pete at petertodd.org> wrote:
> 
> >
> > Ah ok, I misunderstood and didn't realise you were talking about the case
> > where
> > Alice re-spends her unconfirmed payment. Unfortunately I don't think that
> > case
> > is possible to solve without putting some kind of restriction on spending
> > unconfirmed outputs; with a restriction it's fairly simple to solve.
> 
> 
> When you say that you don't think it is possible to solve, do you mean that
> there is a specific problem with this proposal of replacing transactions
> when offered a new transaction whose fee rate exceeds the package fee rate
> of the original transaction (and ensuring that the fee increase covers the
> size of the transactions being ejected)?  Is your concern only about the
> ability to computing and track the package fee rate for transactions within
> the mempool or is there some other issue you foresee?

I mean, I think in general solving this problem is probably not possible.
Basically, the fundamental problem is someone else has consumed network
bandwidth that should be paid for with fees. What you're trying to do is
replace a transaction without paying those fees, which is identical to what an
attacker is trying to do, and thus any such scheme will be as vulnerable to
attack as not having that protection in the first place.

...which does give you an out: maybe the attack isn't important enough to
matter. :)

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 488 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180301/3ef65ba8/attachment-0001.sig>

From karl at dglab.com  Thu Mar  1 05:11:54 2018
From: karl at dglab.com (=?UTF-8?B?44Ki44Or44Og44CA44Kr44O844Or44Oo44OP44Oz?=)
Date: Thu, 1 Mar 2018 05:11:54 +0000
Subject: [bitcoin-dev] Simple lock/unlock mechanism
In-Reply-To: <20180228223044.GA31415@erisian.com.au>
References: <CALJw2w4hKCAJY5U7Li82FbHHnXZKjcZ0Cw67V+=WxvknkY=Zxg@mail.gmail.com>
	<CALJw2w7BQcMEHDa=mx6Gf_JQP603D_hpPq1YN5Em1cfsr4BDAw@mail.gmail.com>
	<20180228223044.GA31415@erisian.com.au>
Message-ID: <CALJw2w5TqjYVAEUVMa9BVaqscoqXAs3xD65vP3kMfxmPTqD9Eg@mail.gmail.com>

On Wed, Feb 28, 2018 at 10:30 PM, Anthony Towns <aj at erisian.com.au> wrote:
> On Wed, Feb 28, 2018 at 04:34:18AM +0000, ??? ?????? via bitcoin-dev wrote:
>> 1. Graftroot probably breaks this (someone could just sign the
>> time-locked output with a script that has no time-lock).
>
> Making the graftroot key be a 2-of-2 muSig with an independent third party
> that commits to only signing CLTV scripts could avoid this. Making it
> 3-of-3 or 5-of-5 could be even better if you can find multiple independent
> services that will do it.

That kind of defeats the purpose. If you go through the trouble of
doing that, you can just do multisig and skip the freezing part
entirely. A robber would have to get you and the cosigner to sign in
both cases, and the CLTV could be overridden with graftroot.

On Wed, Feb 28, 2018 at 11:36 PM, Adam Back <adam.back at gmail.com> wrote:
> Coincidentally I had thought of something similar to what Kalle posted
> about a kind of software only time-lock vault, and described the idea
> to a few people off-list.  Re. Root incompatibility, if the key is
> deleted (as it must be) then a delegated signature can not be made
> that bypasses the CSV timeout restriction, so Root should not be
> incompatible with this.  I think it would be disadvantageous to mark
> keys as Rootable vs not in a sighash sense, because then that is
> another privacy/fungibility loss eroding  the uniformity advantage of
> Root when the delegate is not used.

1. Create TX1=(tx, sig) from UTXO A to p2sh B which has a CSV
timelock. Discard privkey A.
2. After broadcasting TX1, you need privkey B to spend it.
3. Use graftroot and privkey B with a script without timelock to spend B.

The robber can simply force you to execute step 3, since you have the
privkey to B.

> One drawback is deleting keys may itself be a bit difficult to assure
> with HD wallet seeds setup-time backup model.

That's a good point. Even more of a reason to include as part of
'freezing' a send to a new ephemeral key as 'initialization'. Sucks to
pay triple fees though (freeze ephemeral + unfreeze + actual use).

> As Anthony described I think, a simpler though less robust model would
> be to have a third party refuse to co-sign until a pre-arranged time,
> and this would have the advantage of not requiring two on-chain
> transactions.

I was hoping there was a way for a person to simply lock-up the major
portion of their coins easily.

As a sidenote: a security firm (e.g. one that comes to your house when
the alarm goes off) could have a service where seeing an unfreeze
transaction which you have told them about without you giving a heads
up beforehand is equal to alarm going off.

-Kalle.

From karl at dglab.com  Mon Mar  5 14:53:16 2018
From: karl at dglab.com (=?UTF-8?B?44Ki44Or44Og44CA44Kr44O844Or44Oo44OP44Oz?=)
Date: Mon, 5 Mar 2018 14:53:16 +0000
Subject: [bitcoin-dev] Simple lock/unlock mechanism
In-Reply-To: <CALJw2w5TqjYVAEUVMa9BVaqscoqXAs3xD65vP3kMfxmPTqD9Eg@mail.gmail.com>
References: <CALJw2w4hKCAJY5U7Li82FbHHnXZKjcZ0Cw67V+=WxvknkY=Zxg@mail.gmail.com>
	<CALJw2w7BQcMEHDa=mx6Gf_JQP603D_hpPq1YN5Em1cfsr4BDAw@mail.gmail.com>
	<20180228223044.GA31415@erisian.com.au>
	<CALJw2w5TqjYVAEUVMa9BVaqscoqXAs3xD65vP3kMfxmPTqD9Eg@mail.gmail.com>
Message-ID: <CALJw2w4Rcqw_G1eaMiiSkYyCLqRnx8wQvKpaHyPJKvOyRR3LWg@mail.gmail.com>

On Thu, Mar 1, 2018 at 5:11 AM, ?????????? <karl at dglab.com> wrote:
> That kind of defeats the purpose. If you go through the trouble of
> doing that, you can just do multisig and skip the freezing part
> entirely. A robber would have to get you and the cosigner to sign in
> both cases, and the CLTV could be overridden with graftroot.

I think I'm confused on this. To use graftroot it has to be a pubkey,
not a p2sh thing.

-Kalle.

From jl2012 at xbt.hk  Mon Mar  5 15:28:20 2018
From: jl2012 at xbt.hk (Johnson Lau)
Date: Mon, 5 Mar 2018 10:28:20 -0500
Subject: [bitcoin-dev] BIP 117 Feedback
In-Reply-To: <87608btgyd.fsf@rustcorp.com.au>
References: <87608btgyd.fsf@rustcorp.com.au>
Message-ID: <9A9BCC75-F116-4382-B4DE-2E69E79C1DDB@xbt.hk>

Altstack in v0 P2WSH should be left untouched. If anyone is already using altstack, BIP117 would very likely confiscate those UTXOs because the altstack would unlikely be executable.

Even in v1 witness, I think altstack should remain be a temporary data storage.

The ?(many scripts) concatinated together in reverse order to form a serialized script? in BIP117 is exactly the same security hole of Satoshi?s scriptSig + OP_CODESAPARATOR + scriptPubKey . That means it is possible to skip execution of scriptPubKey by using a scriptSig with an invalid push operation, so the whole concatenated script becomes a simple push.

For SigOp limit, I think it?d become more and more difficult to maintain the current statical analyzability model as we try to introduce more functions. I think we should just migrate to a model of limiting sigop per weight, and count the actual number of sigop during execution.  ( https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-February/015764.html ) Actually, this approach is cheaper to analyse, as you only need to look at the witness size, and don?t need to look at the script at all.



> On 9 Jan 2018, at 6:22 AM, Rusty Russell via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
> 
> I've just re-read BIP 117, and I'm concerned about its flexibility.  It
> seems to be doing too much.
> 
> The use of altstack is awkward, and makes me query this entire approach.
> I understand that CLEANSTACK painted us into a corner here :(
> 
> The simplest implementation of tail recursion would be a single blob: if
> a single element is left on the altstack, pop and execute it.  That
> seems trivial to specify.  The treatment of concatenation seems like
> trying to run before we can walk.
> 
> Note that if we restrict this for a specific tx version, we can gain
> experience first and get fancier later.
> 
> BIP 117 also drops SIGOP and opcode limits.  This requires more
> justification, in particular, measurements and bounds on execution
> times.  If this analysis has been done, I'm not aware of it.
> 
> We could restore statically analyzability by rules like so:
> 1.  Only applied for tx version 3 segwit txs.
> 2.  For version 3, top element of stack is counted for limits (perhaps
>    with discount).
> 3.  The blob popped off for tail recursion must be identical to that top
>    element of the stack (ie. the one counted above).
> 
> Again, future tx versions could drop such restrictions.
> 
> Cheers,
> Rusty.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev



From btcdrak at gmail.com  Wed Mar  7 08:19:57 2018
From: btcdrak at gmail.com (Btc Drak)
Date: Wed, 7 Mar 2018 08:19:57 +0000
Subject: [bitcoin-dev] BIP proposal: Reserved nversion bits in blockheader
Message-ID: <CADJgMzv85-So7F1+nyDP_xA2GH5erodA21PM-uAJw8P6_ix6hA@mail.gmail.com>

Hi,

The following proposal reduces the number of version-bits that can be used
for parallel soft-fork signalling, reserving 16 bits for non-specific use.
This would reduce the number of parallel soft-fork activations using
versionbits to from 29 to 13 and prevent node software from emitting false
warnings about unknown signalling bits under the versionbits signalling
system (BIP8/9). I chose the upper bits of the nVersion, because looking at
the versionbits implementation in the most widely deployed node software,
it is easier to implement than say annexing the lower 2 bytes of the field.

The scope of the BIP is deliberately limited to reserving bits for general
use without specifying specific uses for each bit, although there have
previously been various discussions of some use-cases of nVersion bits
including version-rolling AsicBoost[1], and nonce rolling to reduce CPU
load on mining controllers because ntime-rolling can only be done for short
periods otherwise it could have negative side effects distorting time.
However, specific use cases are not important for this BIP.

I am reviving discussion on this topic now, specifically, because the new
DragonMint miner uses version-rolling AsicBoost on mainnet[2]. It is
important to bring up so node software can adapt the versionbits warning
system to prevent false positives. This BIP has the added advantage that
when a new use for bits is found, mining manufacturers can play in the
designated area without causing disruption or inconvenience (as
unfortuntely, the use of version-rolling will cause until BIP8/9 warning
systems are adapted). I appologise for the inconvenience in advance, but
this is the unfortunate result of restraints while negotiating to get the
patent opened[3] and licensed defensively[4] in the first place.

I believe there was a similar proposal[5] made some years ago, before the
advent of BIP9. This proposal differs in that it's primary purpose is to
remove bits from the versionbits soft-fork activation system and earmark 16
bits for general use without allocating fixed uses for each bit. The BIP
cites a couple of usecases for good measure, but they are just
informational examples, not part of a specification laid down. For this
reason, there no is mention of the version-rolling Stratum extension[6]
specifics within the BIP text other than a reference to the specification
itself.

Refs:

[1] https://arxiv.org/pdf/1604.00575.pdf
[2]
https://halongmining.com/blog/2018/03/07/dragonmint-btc-miner-uses-version-rolling-asicboost/
[3]
https://www.asicboost.com/single-post/2018/03/01/opening-asicboost-for-defensive-use/
[4] https://blockchaindpl.org/
[5] https://github.com/BlockheaderNonce2/bitcoin/wiki
[6] http://stratumprotocol.org/stratum-extensions

<pre>
  BIP: ?
  Title: Reserved nversion bits in blockheader
  Author: BtcDrak <btcdrak at gmail.com>
  Comments-Summary: No comments yet.
  Comments-URI: https://github.com/bitcoin/bips/wiki/Comments:BIP-????
  Status: Draft
  Type: Informational
  Created: 2018-03-01
  License: BSD-3-Clause
           CC0-1.0
</pre>

==Abstract==

This BIP reserves 16 bits of the block header nVersion field for
general purpose use and removes their meaning for the purpose of
version bits soft-fork signalling.

==Motivation==

There are a variety of things that miners may desire to use some of
the nVersion field bits for. However, due to their use to coordinate
miner activated soft-forks, full node software will generate false
warnings about unknown soft forks if those bits are used for non soft
fork signalling purposes. By reserving bits from the nVersion field
for general use, node software can be updated to ignore those bits and
therefore will not emit false warnings. Reserving 16 bits for general
use leaves enough for 13 parallel soft-forks using version bits.

==Example Uses==

The following are example cases that would benefit from using some of
the bits from the nVersion field. This list is not exhaustive.

Bitcoin mining hardware currently can exhaust the 32 bit nonce field
in less than 200ms requiring the controller to distribute new jobs
very frequently to each mining chip consuming a lot of bandwidth and
CPU time. This can be greatly reduced by rolling more bits. Rolling
too many bits from nTime is not ideal because it may distort the
timestamps over a longer period.

Version-rolling AsicBoost requires two bits from the nVersion field to
calculate 4-way collisions. Any two bits can be used and mining
equipment can negotiate which bits are to be used with mining pools
via the Stratum "version-rolling" extension.

==Specification==

Sixteen bits from the block header nVersion field, starting from 13
and ending at 28 inclusive (0x1fffe000), are reserved for general use
and removed from BIP8 and BIP9 specifications. A mask of 0xe0001fff
should be applied to nVersion bits so bits 13-28 inclusive will be
ignored for soft-fork signalling and unknown soft-fork warnings.

This specification does not reserve specific bits for specific purposes.

==Backwards Compatibility==

This proposal is backwards compatible, and does not require a soft
fork to implement.

==References==

[[bip-0008.mediawiki|BIP8]]
[[bip-0009.mediawiki|BIP9]]
[https://arxiv.org/pdf/1604.00575.pdf AsicBoost white paper]
[https://github.com/BlockheaderNonce2/bitcoin/wiki nNonce2 proposal]
[http://stratumprotocol.org/ Stratum protocol extension for version-rolling]

==Copyright==

This document is dual licensed as BSD 3-clause, and Creative Commons
CC0 1.0 Universal.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180307/363336e6/attachment-0001.html>

From luke at dashjr.org  Wed Mar  7 14:43:11 2018
From: luke at dashjr.org (Luke Dashjr)
Date: Wed, 7 Mar 2018 14:43:11 +0000
Subject: [bitcoin-dev] BIP proposal: Reserved nversion bits in
	blockheader
In-Reply-To: <CADJgMzv85-So7F1+nyDP_xA2GH5erodA21PM-uAJw8P6_ix6hA@mail.gmail.com>
References: <CADJgMzv85-So7F1+nyDP_xA2GH5erodA21PM-uAJw8P6_ix6hA@mail.gmail.com>
Message-ID: <201803071443.13417.luke@dashjr.org>

Why are you posting this obsolete draft? You've already received review in 
private, and been given useful suggestions. There's even a shared Google Doc 
with the current draft:

    https://docs.google.com/document/d/1GedKia78NUAtylCzeRD3lMlLrpPVBFg9TV9LRqvStak/edit?usp=sharing

Again:

* This is no different from what Timo and Sergio proposed years ago, and as 
such should be based on their work instead of outright not-invented-here 
respecification. The current draft integrates their work while not trying to 
steal credit for it (they are included as primary authors).

* The specification should be complete, including updates for GBT and the 
Stratum mining protocol. These are included in the current draft.

Additionally, it is not appropriate to begin using a draft BIP on mainnet 
before any discussion or consensus has been reached. Doing so seems quite 
malicious, in fact. I hope DragonMint miners can still operate using the 
*current* Bitcoin protocol.

Luke


On Wednesday 07 March 2018 8:19:57 AM Btc Drak via bitcoin-dev wrote:
> Hi,
> 
> The following proposal reduces the number of version-bits that can be used
> for parallel soft-fork signalling, reserving 16 bits for non-specific use.
> This would reduce the number of parallel soft-fork activations using
> versionbits to from 29 to 13 and prevent node software from emitting false
> warnings about unknown signalling bits under the versionbits signalling
> system (BIP8/9). I chose the upper bits of the nVersion, because looking at
> the versionbits implementation in the most widely deployed node software,
> it is easier to implement than say annexing the lower 2 bytes of the field.
> 
> The scope of the BIP is deliberately limited to reserving bits for general
> use without specifying specific uses for each bit, although there have
> previously been various discussions of some use-cases of nVersion bits
> including version-rolling AsicBoost[1], and nonce rolling to reduce CPU
> load on mining controllers because ntime-rolling can only be done for short
> periods otherwise it could have negative side effects distorting time.
> However, specific use cases are not important for this BIP.
> 
> I am reviving discussion on this topic now, specifically, because the new
> DragonMint miner uses version-rolling AsicBoost on mainnet[2]. It is
> important to bring up so node software can adapt the versionbits warning
> system to prevent false positives. This BIP has the added advantage that
> when a new use for bits is found, mining manufacturers can play in the
> designated area without causing disruption or inconvenience (as
> unfortuntely, the use of version-rolling will cause until BIP8/9 warning
> systems are adapted). I appologise for the inconvenience in advance, but
> this is the unfortunate result of restraints while negotiating to get the
> patent opened[3] and licensed defensively[4] in the first place.
> 
> I believe there was a similar proposal[5] made some years ago, before the
> advent of BIP9. This proposal differs in that it's primary purpose is to
> remove bits from the versionbits soft-fork activation system and earmark 16
> bits for general use without allocating fixed uses for each bit. The BIP
> cites a couple of usecases for good measure, but they are just
> informational examples, not part of a specification laid down. For this
> reason, there no is mention of the version-rolling Stratum extension[6]
> specifics within the BIP text other than a reference to the specification
> itself.
> 
> Refs:
> 
> [1] https://arxiv.org/pdf/1604.00575.pdf
> [2]
> https://halongmining.com/blog/2018/03/07/dragonmint-btc-miner-uses-version-> rolling-asicboost/ [3]
> https://www.asicboost.com/single-post/2018/03/01/opening-asicboost-for-defe
> nsive-use/ [4] https://blockchaindpl.org/
> [5] https://github.com/BlockheaderNonce2/bitcoin/wiki
> [6] http://stratumprotocol.org/stratum-extensions
> 
> <pre>
>   BIP: ?
>   Title: Reserved nversion bits in blockheader
>   Author: BtcDrak <btcdrak at gmail.com>
>   Comments-Summary: No comments yet.
>   Comments-URI: https://github.com/bitcoin/bips/wiki/Comments:BIP-????
>   Status: Draft
>   Type: Informational
>   Created: 2018-03-01
>   License: BSD-3-Clause
>            CC0-1.0
> </pre>
> 
> ==Abstract==
> 
> This BIP reserves 16 bits of the block header nVersion field for
> general purpose use and removes their meaning for the purpose of
> version bits soft-fork signalling.
> 
> ==Motivation==
> 
> There are a variety of things that miners may desire to use some of
> the nVersion field bits for. However, due to their use to coordinate
> miner activated soft-forks, full node software will generate false
> warnings about unknown soft forks if those bits are used for non soft
> fork signalling purposes. By reserving bits from the nVersion field
> for general use, node software can be updated to ignore those bits and
> therefore will not emit false warnings. Reserving 16 bits for general
> use leaves enough for 13 parallel soft-forks using version bits.
> 
> ==Example Uses==
> 
> The following are example cases that would benefit from using some of
> the bits from the nVersion field. This list is not exhaustive.
> 
> Bitcoin mining hardware currently can exhaust the 32 bit nonce field
> in less than 200ms requiring the controller to distribute new jobs
> very frequently to each mining chip consuming a lot of bandwidth and
> CPU time. This can be greatly reduced by rolling more bits. Rolling
> too many bits from nTime is not ideal because it may distort the
> timestamps over a longer period.
> 
> Version-rolling AsicBoost requires two bits from the nVersion field to
> calculate 4-way collisions. Any two bits can be used and mining
> equipment can negotiate which bits are to be used with mining pools
> via the Stratum "version-rolling" extension.
> 
> ==Specification==
> 
> Sixteen bits from the block header nVersion field, starting from 13
> and ending at 28 inclusive (0x1fffe000), are reserved for general use
> and removed from BIP8 and BIP9 specifications. A mask of 0xe0001fff
> should be applied to nVersion bits so bits 13-28 inclusive will be
> ignored for soft-fork signalling and unknown soft-fork warnings.
> 
> This specification does not reserve specific bits for specific purposes.
> 
> ==Backwards Compatibility==
> 
> This proposal is backwards compatible, and does not require a soft
> fork to implement.
> 
> ==References==
> 
> [[bip-0008.mediawiki|BIP8]]
> [[bip-0009.mediawiki|BIP9]]
> [https://arxiv.org/pdf/1604.00575.pdf AsicBoost white paper]
> [https://github.com/BlockheaderNonce2/bitcoin/wiki nNonce2 proposal]
> [http://stratumprotocol.org/ Stratum protocol extension for
> version-rolling]
> 
> ==Copyright==
> 
> This document is dual licensed as BSD 3-clause, and Creative Commons
> CC0 1.0 Universal.

From luke at dashjr.org  Wed Mar  7 15:48:00 2018
From: luke at dashjr.org (Luke Dashjr)
Date: Wed, 7 Mar 2018 15:48:00 +0000
Subject: [bitcoin-dev] BIP proposal: Reserved nversion bits in
	blockheader - stratum mining.configure
In-Reply-To: <20180307164349.1cfa51b3@glum>
References: <CADJgMzv85-So7F1+nyDP_xA2GH5erodA21PM-uAJw8P6_ix6hA@mail.gmail.com>
	<201803071443.13417.luke@dashjr.org> <20180307164349.1cfa51b3@glum>
Message-ID: <201803071548.01405.luke@dashjr.org>

On Wednesday 07 March 2018 3:43:49 PM Jan ?apek wrote:
> Our reasoning for coming up with a new method for miner configuration
> was stated here: https://github.com/slushpool/stratumprotocol/issues/1

This reasoning is not sound.

> It is primarily the determinism of expecting the response. That is
> the reason why we chose a new method mining.configure instead of an
> existing mining.capabilities that was not being very well documented or
> used.

It was as well documented as the original stratum protocol, and in use since 
2014.

While the response type is admittedly undefined, simply defining that would 
have been a better solution than to reinvent it incompatibly for no reason. 
(Although version rolling does not actually require a response at all.)

> 
> 
> On Wed, 7 Mar 2018 14:43:11 +0000 Luke Dashjr via bitcoin-dev
> 
> <bitcoin-dev at lists.linuxfoundation.org> wrote:
> > Why are you posting this obsolete draft? You've already received
> > review in private, and been given useful suggestions. There's even a
> > 
> > shared Google Doc with the current draft:
> >     https://docs.google.com/document/d/1GedKia78NUAtylCzeRD3lMlLrpPVBFg9T
> >     V9LRqvStak/edit?usp=sharing
> > 
> > Again:
> > 
> > * This is no different from what Timo and Sergio proposed years ago,
> > and as such should be based on their work instead of outright
> > not-invented-here respecification. The current draft integrates their
> > work while not trying to steal credit for it (they are included as
> > primary authors).
> > 
> > * The specification should be complete, including updates for GBT and
> > the Stratum mining protocol. These are included in the current draft.
> > 
> > Additionally, it is not appropriate to begin using a draft BIP on
> > mainnet before any discussion or consensus has been reached. Doing so
> > seems quite malicious, in fact. I hope DragonMint miners can still
> > operate using the *current* Bitcoin protocol.
> > 
> > Luke
> > 
> > On Wednesday 07 March 2018 8:19:57 AM Btc Drak via bitcoin-dev wrote:
> > > Hi,
> > > 
> > > The following proposal reduces the number of version-bits that can
> > > be used for parallel soft-fork signalling, reserving 16 bits for
> > > non-specific use. This would reduce the number of parallel
> > > soft-fork activations using versionbits to from 29 to 13 and
> > > prevent node software from emitting false warnings about unknown
> > > signalling bits under the versionbits signalling system (BIP8/9). I
> > > chose the upper bits of the nVersion, because looking at the
> > > versionbits implementation in the most widely deployed node
> > > software, it is easier to implement than say annexing the lower 2
> > > bytes of the field.
> > > 
> > > The scope of the BIP is deliberately limited to reserving bits for
> > > general use without specifying specific uses for each bit, although
> > > there have previously been various discussions of some use-cases of
> > > nVersion bits including version-rolling AsicBoost[1], and nonce
> > > rolling to reduce CPU load on mining controllers because
> > > ntime-rolling can only be done for short periods otherwise it could
> > > have negative side effects distorting time. However, specific use
> > > cases are not important for this BIP.
> > > 
> > > I am reviving discussion on this topic now, specifically, because
> > > the new DragonMint miner uses version-rolling AsicBoost on
> > > mainnet[2]. It is important to bring up so node software can adapt
> > > the versionbits warning system to prevent false positives. This BIP
> > > has the added advantage that when a new use for bits is found,
> > > mining manufacturers can play in the designated area without
> > > causing disruption or inconvenience (as unfortuntely, the use of
> > > version-rolling will cause until BIP8/9 warning systems are
> > > adapted). I appologise for the inconvenience in advance, but this
> > > is the unfortunate result of restraints while negotiating to get
> > > the patent opened[3] and licensed defensively[4] in the first place.
> > > 
> > > I believe there was a similar proposal[5] made some years ago,
> > > before the advent of BIP9. This proposal differs in that it's
> > > primary purpose is to remove bits from the versionbits soft-fork
> > > activation system and earmark 16 bits for general use without
> > > allocating fixed uses for each bit. The BIP cites a couple of
> > > usecases for good measure, but they are just informational
> > > examples, not part of a specification laid down. For this reason,
> > > there no is mention of the version-rolling Stratum extension[6]
> > > specifics within the BIP text other than a reference to the
> > > specification itself.
> > > 
> > > Refs:
> > > 
> > > [1] https://arxiv.org/pdf/1604.00575.pdf
> > > [2]
> > > https://halongmining.com/blog/2018/03/07/dragonmint-btc-miner-uses-vers
> > > ion-> rolling-asicboost/ [3]
> > > https://www.asicboost.com/single-post/2018/03/01/opening-asicboost-for-> > > defe nsive-use/ [4] https://blockchaindpl.org/ [5]
> > > https://github.com/BlockheaderNonce2/bitcoin/wiki [6]
> > > http://stratumprotocol.org/stratum-extensions
> > > 
> > > <pre>
> > > 
> > >   BIP: ?
> > >   Title: Reserved nversion bits in blockheader
> > >   Author: BtcDrak <btcdrak at gmail.com>
> > >   Comments-Summary: No comments yet.
> > > 
> > >   Comments-URI:
> > > https://github.com/bitcoin/bips/wiki/Comments:BIP-???? Status: Draft
> > > 
> > >   Type: Informational
> > >   Created: 2018-03-01
> > >   License: BSD-3-Clause
> > >   
> > >            CC0-1.0
> > > 
> > > </pre>
> > > 
> > > ==Abstract==
> > > 
> > > This BIP reserves 16 bits of the block header nVersion field for
> > > general purpose use and removes their meaning for the purpose of
> > > version bits soft-fork signalling.
> > > 
> > > ==Motivation==
> > > 
> > > There are a variety of things that miners may desire to use some of
> > > the nVersion field bits for. However, due to their use to coordinate
> > > miner activated soft-forks, full node software will generate false
> > > warnings about unknown soft forks if those bits are used for non
> > > soft fork signalling purposes. By reserving bits from the nVersion
> > > field for general use, node software can be updated to ignore those
> > > bits and therefore will not emit false warnings. Reserving 16 bits
> > > for general use leaves enough for 13 parallel soft-forks using
> > > version bits.
> > > 
> > > ==Example Uses==
> > > 
> > > The following are example cases that would benefit from using some
> > > of the bits from the nVersion field. This list is not exhaustive.
> > > 
> > > Bitcoin mining hardware currently can exhaust the 32 bit nonce field
> > > in less than 200ms requiring the controller to distribute new jobs
> > > very frequently to each mining chip consuming a lot of bandwidth and
> > > CPU time. This can be greatly reduced by rolling more bits. Rolling
> > > too many bits from nTime is not ideal because it may distort the
> > > timestamps over a longer period.
> > > 
> > > Version-rolling AsicBoost requires two bits from the nVersion field
> > > to calculate 4-way collisions. Any two bits can be used and mining
> > > equipment can negotiate which bits are to be used with mining pools
> > > via the Stratum "version-rolling" extension.
> > > 
> > > ==Specification==
> > > 
> > > Sixteen bits from the block header nVersion field, starting from 13
> > > and ending at 28 inclusive (0x1fffe000), are reserved for general
> > > use and removed from BIP8 and BIP9 specifications. A mask of
> > > 0xe0001fff should be applied to nVersion bits so bits 13-28
> > > inclusive will be ignored for soft-fork signalling and unknown
> > > soft-fork warnings.
> > > 
> > > This specification does not reserve specific bits for specific
> > > purposes.
> > > 
> > > ==Backwards Compatibility==
> > > 
> > > This proposal is backwards compatible, and does not require a soft
> > > fork to implement.
> > > 
> > > ==References==
> > > 
> > > [[bip-0008.mediawiki|BIP8]]
> > > [[bip-0009.mediawiki|BIP9]]
> > > [https://arxiv.org/pdf/1604.00575.pdf AsicBoost white paper]
> > > [https://github.com/BlockheaderNonce2/bitcoin/wiki nNonce2 proposal]
> > > [http://stratumprotocol.org/ Stratum protocol extension for
> > > version-rolling]
> > > 
> > > ==Copyright==
> > > 
> > > This document is dual licensed as BSD 3-clause, and Creative Commons
> > > CC0 1.0 Universal.
> > 
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev at lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From jan.capek at braiins.cz  Wed Mar  7 15:43:49 2018
From: jan.capek at braiins.cz (Jan =?UTF-8?B?xIxhcGVr?=)
Date: Wed, 7 Mar 2018 16:43:49 +0100
Subject: [bitcoin-dev] BIP proposal: Reserved nversion bits in
 blockheader - stratum mining.configure
In-Reply-To: <201803071443.13417.luke@dashjr.org>
References: <CADJgMzv85-So7F1+nyDP_xA2GH5erodA21PM-uAJw8P6_ix6hA@mail.gmail.com>
	<201803071443.13417.luke@dashjr.org>
Message-ID: <20180307164349.1cfa51b3@glum>

Hello,

Our reasoning for coming up with a new method for miner configuration
was stated here: https://github.com/slushpool/stratumprotocol/issues/1

It is primarily the determinism of expecting the response. That is
the reason why we chose a new method mining.configure instead of an
existing mining.capabilities that was not being very well documented or
used.


On Wed, 7 Mar 2018 14:43:11 +0000 Luke Dashjr via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:

> Why are you posting this obsolete draft? You've already received
> review in private, and been given useful suggestions. There's even a
> shared Google Doc with the current draft:
> 
>     https://docs.google.com/document/d/1GedKia78NUAtylCzeRD3lMlLrpPVBFg9TV9LRqvStak/edit?usp=sharing
> 
> Again:
> 
> * This is no different from what Timo and Sergio proposed years ago,
> and as such should be based on their work instead of outright
> not-invented-here respecification. The current draft integrates their
> work while not trying to steal credit for it (they are included as
> primary authors).
> 
> * The specification should be complete, including updates for GBT and
> the Stratum mining protocol. These are included in the current draft.
> 
> Additionally, it is not appropriate to begin using a draft BIP on
> mainnet before any discussion or consensus has been reached. Doing so
> seems quite malicious, in fact. I hope DragonMint miners can still
> operate using the *current* Bitcoin protocol.
> 
> Luke
> 
> 
> On Wednesday 07 March 2018 8:19:57 AM Btc Drak via bitcoin-dev wrote:
> > Hi,
> > 
> > The following proposal reduces the number of version-bits that can
> > be used for parallel soft-fork signalling, reserving 16 bits for
> > non-specific use. This would reduce the number of parallel
> > soft-fork activations using versionbits to from 29 to 13 and
> > prevent node software from emitting false warnings about unknown
> > signalling bits under the versionbits signalling system (BIP8/9). I
> > chose the upper bits of the nVersion, because looking at the
> > versionbits implementation in the most widely deployed node
> > software, it is easier to implement than say annexing the lower 2
> > bytes of the field.
> > 
> > The scope of the BIP is deliberately limited to reserving bits for
> > general use without specifying specific uses for each bit, although
> > there have previously been various discussions of some use-cases of
> > nVersion bits including version-rolling AsicBoost[1], and nonce
> > rolling to reduce CPU load on mining controllers because
> > ntime-rolling can only be done for short periods otherwise it could
> > have negative side effects distorting time. However, specific use
> > cases are not important for this BIP.
> > 
> > I am reviving discussion on this topic now, specifically, because
> > the new DragonMint miner uses version-rolling AsicBoost on
> > mainnet[2]. It is important to bring up so node software can adapt
> > the versionbits warning system to prevent false positives. This BIP
> > has the added advantage that when a new use for bits is found,
> > mining manufacturers can play in the designated area without
> > causing disruption or inconvenience (as unfortuntely, the use of
> > version-rolling will cause until BIP8/9 warning systems are
> > adapted). I appologise for the inconvenience in advance, but this
> > is the unfortunate result of restraints while negotiating to get
> > the patent opened[3] and licensed defensively[4] in the first place.
> > 
> > I believe there was a similar proposal[5] made some years ago,
> > before the advent of BIP9. This proposal differs in that it's
> > primary purpose is to remove bits from the versionbits soft-fork
> > activation system and earmark 16 bits for general use without
> > allocating fixed uses for each bit. The BIP cites a couple of
> > usecases for good measure, but they are just informational
> > examples, not part of a specification laid down. For this reason,
> > there no is mention of the version-rolling Stratum extension[6]
> > specifics within the BIP text other than a reference to the
> > specification itself.
> > 
> > Refs:
> > 
> > [1] https://arxiv.org/pdf/1604.00575.pdf
> > [2]
> > https://halongmining.com/blog/2018/03/07/dragonmint-btc-miner-uses-version->
> > rolling-asicboost/ [3]
> > https://www.asicboost.com/single-post/2018/03/01/opening-asicboost-for-defe
> > nsive-use/ [4] https://blockchaindpl.org/ [5]
> > https://github.com/BlockheaderNonce2/bitcoin/wiki [6]
> > http://stratumprotocol.org/stratum-extensions
> > 
> > <pre>
> >   BIP: ?
> >   Title: Reserved nversion bits in blockheader
> >   Author: BtcDrak <btcdrak at gmail.com>
> >   Comments-Summary: No comments yet.
> >   Comments-URI:
> > https://github.com/bitcoin/bips/wiki/Comments:BIP-???? Status: Draft
> >   Type: Informational
> >   Created: 2018-03-01
> >   License: BSD-3-Clause
> >            CC0-1.0
> > </pre>
> > 
> > ==Abstract==
> > 
> > This BIP reserves 16 bits of the block header nVersion field for
> > general purpose use and removes their meaning for the purpose of
> > version bits soft-fork signalling.
> > 
> > ==Motivation==
> > 
> > There are a variety of things that miners may desire to use some of
> > the nVersion field bits for. However, due to their use to coordinate
> > miner activated soft-forks, full node software will generate false
> > warnings about unknown soft forks if those bits are used for non
> > soft fork signalling purposes. By reserving bits from the nVersion
> > field for general use, node software can be updated to ignore those
> > bits and therefore will not emit false warnings. Reserving 16 bits
> > for general use leaves enough for 13 parallel soft-forks using
> > version bits.
> > 
> > ==Example Uses==
> > 
> > The following are example cases that would benefit from using some
> > of the bits from the nVersion field. This list is not exhaustive.
> > 
> > Bitcoin mining hardware currently can exhaust the 32 bit nonce field
> > in less than 200ms requiring the controller to distribute new jobs
> > very frequently to each mining chip consuming a lot of bandwidth and
> > CPU time. This can be greatly reduced by rolling more bits. Rolling
> > too many bits from nTime is not ideal because it may distort the
> > timestamps over a longer period.
> > 
> > Version-rolling AsicBoost requires two bits from the nVersion field
> > to calculate 4-way collisions. Any two bits can be used and mining
> > equipment can negotiate which bits are to be used with mining pools
> > via the Stratum "version-rolling" extension.
> > 
> > ==Specification==
> > 
> > Sixteen bits from the block header nVersion field, starting from 13
> > and ending at 28 inclusive (0x1fffe000), are reserved for general
> > use and removed from BIP8 and BIP9 specifications. A mask of
> > 0xe0001fff should be applied to nVersion bits so bits 13-28
> > inclusive will be ignored for soft-fork signalling and unknown
> > soft-fork warnings.
> > 
> > This specification does not reserve specific bits for specific
> > purposes.
> > 
> > ==Backwards Compatibility==
> > 
> > This proposal is backwards compatible, and does not require a soft
> > fork to implement.
> > 
> > ==References==
> > 
> > [[bip-0008.mediawiki|BIP8]]
> > [[bip-0009.mediawiki|BIP9]]
> > [https://arxiv.org/pdf/1604.00575.pdf AsicBoost white paper]
> > [https://github.com/BlockheaderNonce2/bitcoin/wiki nNonce2 proposal]
> > [http://stratumprotocol.org/ Stratum protocol extension for
> > version-rolling]
> > 
> > ==Copyright==
> > 
> > This document is dual licensed as BSD 3-clause, and Creative Commons
> > CC0 1.0 Universal.  
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev



-- 
CEO Braiins Systems | Slushpool.com
tel: +420 604 566 382
email: jan.capek at braiins.cz
http://braiins.cz
http://slushpool.com
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180307/87b5929e/attachment-0001.sig>

From roconnor at blockstream.io  Thu Mar  8 15:39:46 2018
From: roconnor at blockstream.io (Russell O'Connor)
Date: Thu, 8 Mar 2018 10:39:46 -0500
Subject: [bitcoin-dev] Revisiting BIP 125 RBF policy.
In-Reply-To: <20180301151129.GA9373@fedora-23-dvm>
References: <CAMZUoKnGx3p7=Kg96E3EEyJ8aFC7ezsvec_pAnN7oJz7-VbyLA@mail.gmail.com>
	<20180212225828.GB8551@fedora-23-dvm>
	<CAMZUoKnFBVFhaq61wKu_CcZgRKc5aoeTa-wq9h2CXH0WWHd3NQ@mail.gmail.com>
	<20180212234225.GA9131@fedora-23-dvm>
	<CAMZUoK=Htds5fu5vnqAhEoZDrwHALKe6uwqXnmJb17pa_peFFw@mail.gmail.com>
	<20180301151129.GA9373@fedora-23-dvm>
Message-ID: <CAMZUoKkG8tbdb+6tGmpvgXb-=3Tu4JsTWXz77o3EC+4Bcbd17A@mail.gmail.com>

On Thu, Mar 1, 2018 at 10:11 AM, Peter Todd <pete at petertodd.org> wrote:

> On Tue, Feb 27, 2018 at 11:25:59AM -0500, Russell O'Connor wrote:
> > When you say that you don't think it is possible to solve, do you mean
> that
> > there is a specific problem with this proposal of replacing transactions
> > when offered a new transaction whose fee rate exceeds the package fee
> rate
> > of the original transaction (and ensuring that the fee increase covers
> the
> > size of the transactions being ejected)?  Is your concern only about the
> > ability to computing and track the package fee rate for transactions
> within
> > the mempool or is there some other issue you foresee?
>
> I mean, I think in general solving this problem is probably not possible.
> Basically, the fundamental problem is someone else has consumed network
> bandwidth that should be paid for with fees. What you're trying to do is
> replace a transaction without paying those fees, which is identical to
> what an
> attacker is trying to do, and thus any such scheme will be as vulnerable to
> attack as not having that protection in the first place.
>
> ...which does give you an out: maybe the attack isn't important enough to
> matter. :)
>

Thanks, that makes sense.

I still think it is worthwhile pursuing this proposed change in RBF policy
as it would seem that the current policy is problematic in practice today
where participants are just performing normal transactions and are not
trying to attack each other.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180308/dfce6649/attachment.html>

From pete at petertodd.org  Thu Mar  8 18:34:26 2018
From: pete at petertodd.org (Peter Todd)
Date: Thu, 8 Mar 2018 13:34:26 -0500
Subject: [bitcoin-dev] Revisiting BIP 125 RBF policy.
In-Reply-To: <CAMZUoKkG8tbdb+6tGmpvgXb-=3Tu4JsTWXz77o3EC+4Bcbd17A@mail.gmail.com>
References: <CAMZUoKnGx3p7=Kg96E3EEyJ8aFC7ezsvec_pAnN7oJz7-VbyLA@mail.gmail.com>
	<20180212225828.GB8551@fedora-23-dvm>
	<CAMZUoKnFBVFhaq61wKu_CcZgRKc5aoeTa-wq9h2CXH0WWHd3NQ@mail.gmail.com>
	<20180212234225.GA9131@fedora-23-dvm>
	<CAMZUoK=Htds5fu5vnqAhEoZDrwHALKe6uwqXnmJb17pa_peFFw@mail.gmail.com>
	<20180301151129.GA9373@fedora-23-dvm>
	<CAMZUoKkG8tbdb+6tGmpvgXb-=3Tu4JsTWXz77o3EC+4Bcbd17A@mail.gmail.com>
Message-ID: <20180308183426.GA1093@fedora-23-dvm>

On Thu, Mar 08, 2018 at 10:39:46AM -0500, Russell O'Connor wrote:
> On Thu, Mar 1, 2018 at 10:11 AM, Peter Todd <pete at petertodd.org> wrote:
> > I mean, I think in general solving this problem is probably not possible.
> > Basically, the fundamental problem is someone else has consumed network
> > bandwidth that should be paid for with fees. What you're trying to do is
> > replace a transaction without paying those fees, which is identical to
> > what an
> > attacker is trying to do, and thus any such scheme will be as vulnerable to
> > attack as not having that protection in the first place.
> >
> > ...which does give you an out: maybe the attack isn't important enough to
> > matter. :)
> >
> 
> Thanks, that makes sense.
> 
> I still think it is worthwhile pursuing this proposed change in RBF policy
> as it would seem that the current policy is problematic in practice today
> where participants are just performing normal transactions and are not
> trying to attack each other.

But that's not a good argument: whether or not normal users are trying to
attack each other has nothing to do with whether or not you're opening up an
attack by relaxing anti-DoS protections.

Equally, how often are normal users who aren't attacking each other creating
issues anyway? You can always have your wallet code just skip use of RBF
replacements in the event that someone does spend an unconfirmed output that
you sent them; how often does this actually happen in practice? Not many
wallets let you spend unconfirmed outputs that you didn't create.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 614 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180308/dccea518/attachment.sig>

From roconnor at blockstream.io  Thu Mar  8 20:07:43 2018
From: roconnor at blockstream.io (Russell O'Connor)
Date: Thu, 8 Mar 2018 15:07:43 -0500
Subject: [bitcoin-dev] Revisiting BIP 125 RBF policy.
In-Reply-To: <20180308183426.GA1093@fedora-23-dvm>
References: <CAMZUoKnGx3p7=Kg96E3EEyJ8aFC7ezsvec_pAnN7oJz7-VbyLA@mail.gmail.com>
	<20180212225828.GB8551@fedora-23-dvm>
	<CAMZUoKnFBVFhaq61wKu_CcZgRKc5aoeTa-wq9h2CXH0WWHd3NQ@mail.gmail.com>
	<20180212234225.GA9131@fedora-23-dvm>
	<CAMZUoK=Htds5fu5vnqAhEoZDrwHALKe6uwqXnmJb17pa_peFFw@mail.gmail.com>
	<20180301151129.GA9373@fedora-23-dvm>
	<CAMZUoKkG8tbdb+6tGmpvgXb-=3Tu4JsTWXz77o3EC+4Bcbd17A@mail.gmail.com>
	<20180308183426.GA1093@fedora-23-dvm>
Message-ID: <CAMZUoKkDnJv33H-DveHtwpnyALS5LoX-OAnabJyvPo4c1DBJRQ@mail.gmail.com>

On Thu, Mar 8, 2018 at 1:34 PM, Peter Todd <pete at petertodd.org> wrote:

> On Thu, Mar 08, 2018 at 10:39:46AM -0500, Russell O'Connor wrote:
> > On Thu, Mar 1, 2018 at 10:11 AM, Peter Todd <pete at petertodd.org> wrote:
> > > I mean, I think in general solving this problem is probably not
> possible.
> > > Basically, the fundamental problem is someone else has consumed network
> > > bandwidth that should be paid for with fees. What you're trying to do
> is
> > > replace a transaction without paying those fees, which is identical to
> > > what an
> > > attacker is trying to do, and thus any such scheme will be as
> vulnerable to
> > > attack as not having that protection in the first place.
> > >
> > > ...which does give you an out: maybe the attack isn't important enough
> to
> > > matter. :)
> > >
> >
> > Thanks, that makes sense.
> >
> > I still think it is worthwhile pursuing this proposed change in RBF
> policy
> > as it would seem that the current policy is problematic in practice today
> > where participants are just performing normal transactions and are not
> > trying to attack each other.
>
> But that's not a good argument: whether or not normal users are trying to
> attack each other has nothing to do with whether or not you're opening up
> an
> attack by relaxing anti-DoS protections.
>

I'm not suggesting removing the anti-DoS protections.  I'm suggesting that
replaced transaction require a fee increase of at least the min-fee-rate
times the size of all the transactions being ejected (in addition to the
other proposed requirements).


> Equally, how often are normal users who aren't attacking each other
> creating
> issues anyway? You can always have your wallet code just skip use of RBF
>
replacements in the event that someone does spend an unconfirmed output that
> you sent them; how often does this actually happen in practice?


Just ask rhavar.  It happens regularly.

Not many wallets let you spend unconfirmed outputs that you didn't create.
>

The problem is with institutional wallets sweeping incoming payments.  It
seems that in practice they are happy to sweep unconfirmed outputs.

Setting all of the above aside for a moment.  We need to understand that
rational miners are going to prefer to transactions with higher package fee
rates regardless of whatever your personal preferred RBF policy is.  If we
do not bring the RBF policy to alignment with what is economically
rational, then miners are going to change their own policies anyways,
probably all in slightly different ways.  It behooves everyone to develop a
reasonable standard RBF policy, that is still robust against possible DoS
vectors, and aligns with miner incentives, so that all participants know
what behaviour they can reasonably expect.  It is simply a bonus that this
change in RBF policy also partially mitigates the problem of pinned
transactions.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180308/9193b079/attachment.html>

From eric at voskuil.org  Fri Mar  9 07:50:02 2018
From: eric at voskuil.org (Eric Voskuil)
Date: Thu, 8 Mar 2018 23:50:02 -0800
Subject: [bitcoin-dev] version.relay behavior change
Message-ID: <e2fd3226-91ff-d0ca-67c7-2c4a98c6628f@voskuil.org>

/Satoshi:0.15.0/ and later nodes appear to be no longer honoring the
version.relay=false flag (BIP37). Could someone familiar with the change
please explain the rational?

Thanks,

e

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 490 bytes
Desc: OpenPGP digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180308/ae314972/attachment-0001.sig>

From achow101-lists at achow101.com  Fri Mar  9 15:33:32 2018
From: achow101-lists at achow101.com (Andrew Chow)
Date: Fri, 9 Mar 2018 10:33:32 -0500
Subject: [bitcoin-dev] version.relay behavior change
In-Reply-To: <e2fd3226-91ff-d0ca-67c7-2c4a98c6628f@voskuil.org>
References: <e2fd3226-91ff-d0ca-67c7-2c4a98c6628f@voskuil.org>
Message-ID: <620d4b5e-61c4-4501-9787-c73109908418@achow101.com>

Looking through the code, I don't think that this behavior has changed.
Are you sure that you are actually connected to Satoshi:0.15.0 nodes and
not a node that has simply set their user-agent to that (i.e. not a real
Satoshi:0.15.0 node)?

If what you are seeing is true, it is likely a bug and not an
intentional change. In that case, can you provide specific details on
how to reproduce?

Andrew


On 03/09/2018 02:50 AM, Eric Voskuil via bitcoin-dev wrote:
> /Satoshi:0.15.0/ and later nodes appear to be no longer honoring the
> version.relay=false flag (BIP37). Could someone familiar with the change
> please explain the rational?
>
> Thanks,
>
> e
>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180309/5add8c1b/attachment.html>

From pete at petertodd.org  Fri Mar  9 18:28:03 2018
From: pete at petertodd.org (Peter Todd)
Date: Fri, 9 Mar 2018 13:28:03 -0500
Subject: [bitcoin-dev] Revisiting BIP 125 RBF policy.
In-Reply-To: <CAMZUoKkDnJv33H-DveHtwpnyALS5LoX-OAnabJyvPo4c1DBJRQ@mail.gmail.com>
References: <CAMZUoKnGx3p7=Kg96E3EEyJ8aFC7ezsvec_pAnN7oJz7-VbyLA@mail.gmail.com>
	<20180212225828.GB8551@fedora-23-dvm>
	<CAMZUoKnFBVFhaq61wKu_CcZgRKc5aoeTa-wq9h2CXH0WWHd3NQ@mail.gmail.com>
	<20180212234225.GA9131@fedora-23-dvm>
	<CAMZUoK=Htds5fu5vnqAhEoZDrwHALKe6uwqXnmJb17pa_peFFw@mail.gmail.com>
	<20180301151129.GA9373@fedora-23-dvm>
	<CAMZUoKkG8tbdb+6tGmpvgXb-=3Tu4JsTWXz77o3EC+4Bcbd17A@mail.gmail.com>
	<20180308183426.GA1093@fedora-23-dvm>
	<CAMZUoKkDnJv33H-DveHtwpnyALS5LoX-OAnabJyvPo4c1DBJRQ@mail.gmail.com>
Message-ID: <20180309182803.GE2786@fedora-23-dvm>

On Thu, Mar 08, 2018 at 03:07:43PM -0500, Russell O'Connor wrote:
> On Thu, Mar 8, 2018 at 1:34 PM, Peter Todd <pete at petertodd.org> wrote:
> > But that's not a good argument: whether or not normal users are trying to
> > attack each other has nothing to do with whether or not you're opening up
> > an
> > attack by relaxing anti-DoS protections.
> >
> 
> I'm not suggesting removing the anti-DoS protections.  I'm suggesting that
> replaced transaction require a fee increase of at least the min-fee-rate
> times the size of all the transactions being ejected (in addition to the
> other proposed requirements).

Fair: you're not removing them entirely, but you are weakening them compared to
the status quo.

> > Equally, how often are normal users who aren't attacking each other
> > creating
> > issues anyway? You can always have your wallet code just skip use of RBF
> >
> replacements in the event that someone does spend an unconfirmed output that
> > you sent them; how often does this actually happen in practice?
> 
> 
> Just ask rhavar.  It happens regularly.
> 
> Not many wallets let you spend unconfirmed outputs that you didn't create.
> >
> 
> The problem is with institutional wallets sweeping incoming payments.  It
> seems that in practice they are happy to sweep unconfirmed outputs.

Pity, that does sound like a problem. :(

> Setting all of the above aside for a moment.  We need to understand that
> rational miners are going to prefer to transactions with higher package fee
> rates regardless of whatever your personal preferred RBF policy is.  If we
> do not bring the RBF policy to alignment with what is economically
> rational, then miners are going to change their own policies anyways,
> probably all in slightly different ways.  It behooves everyone to develop a
> reasonable standard RBF policy, that is still robust against possible DoS
> vectors, and aligns with miner incentives, so that all participants know
> what behaviour they can reasonably expect.  It is simply a bonus that this
> change in RBF policy also partially mitigates the problem of pinned
> transactions.

Miners and full nodes have slightly different priorities here; it's not clear
to me why it matters that they implement slightly different policies.


Still, re-reading your initital post, I'm convinced that the weakening of the
DoS protections is probably not a huge problem, so maybe lets try this in a
release and see what happens.

Notably, if people actually use this new replacement behavior, the institutions
doing these sweeps of unconfirmed outputs might stop doing that! That's
probably a good thing, as respends of potentially conflicted unconfirmed
outputs can be dangerous in reorgs; we're better off if outputs are buried
deeply before being spent again.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 614 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180309/5a952ae2/attachment.sig>

From rhavar at protonmail.com  Fri Mar  9 18:40:34 2018
From: rhavar at protonmail.com (rhavar at protonmail.com)
Date: Fri, 09 Mar 2018 13:40:34 -0500
Subject: [bitcoin-dev] Revisiting BIP 125 RBF policy.
In-Reply-To: <20180309182803.GE2786@fedora-23-dvm>
References: <CAMZUoKnGx3p7=Kg96E3EEyJ8aFC7ezsvec_pAnN7oJz7-VbyLA@mail.gmail.com>
	<20180212225828.GB8551@fedora-23-dvm>
	<CAMZUoKnFBVFhaq61wKu_CcZgRKc5aoeTa-wq9h2CXH0WWHd3NQ@mail.gmail.com>
	<20180212234225.GA9131@fedora-23-dvm>
	<CAMZUoK=Htds5fu5vnqAhEoZDrwHALKe6uwqXnmJb17pa_peFFw@mail.gmail.com>
	<20180301151129.GA9373@fedora-23-dvm>
	<CAMZUoKkG8tbdb+6tGmpvgXb-=3Tu4JsTWXz77o3EC+4Bcbd17A@mail.gmail.com>
	<20180308183426.GA1093@fedora-23-dvm>
	<CAMZUoKkDnJv33H-DveHtwpnyALS5LoX-OAnabJyvPo4c1DBJRQ@mail.gmail.com>
	<20180309182803.GE2786@fedora-23-dvm>
Message-ID: <ZmiZUf6iUcddY1CKMADBa8FryCgrZ1235R4bHParR8NpwibjA-EY38D_GElA9jv4Z-zPZE9juQKgJjpd4MFfjg9ySFvO51dOHNoObHdaLjo=@protonmail.com>

> Still, re-reading your initital post, I'm convinced that the weakening of the
> DoS protections is probably not a huge problem, so maybe lets try this in a
> release and see what happens.

Awesome! I very much agree. The relaxation of some of these DoS prevention rules I think will really open up a lot of use cases and adoption 

> Notably, if people actually use this new replacement behavior, the institutions
> doing these sweeps of unconfirmed outputs might stop doing that! 

Agree, I'm pretty sure it's unintentional. I know a lot of services struggle with coin selection, so what they do is conceptually have a receive wallet from which they can sweep to their hot wallet (or cold storage) to keep their utxo manageable.

Currently some of them are sweeping unconfirmed inputs with it, but I don't think it's a conscious design choice, just something that happens to be working well now.

(FWIW I observed this behavior like 6+ months ago, I haven't kept track of if it's still happening or how often. But at the time I had to write off the idea of low-fee rbf batch transactions as it was happening too often to be feasible)


?-Ryan?

??????? Original Message ???????

On March 9, 2018 1:28 PM, Peter Todd via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:

> ??
> 
> On Thu, Mar 08, 2018 at 03:07:43PM -0500, Russell O'Connor wrote:
> 
> > On Thu, Mar 8, 2018 at 1:34 PM, Peter Todd pete at petertodd.org wrote:
> > 
> > > But that's not a good argument: whether or not normal users are trying to
> > > 
> > > attack each other has nothing to do with whether or not you're opening up
> > > 
> > > an
> > > 
> > > attack by relaxing anti-DoS protections.
> > 
> > I'm not suggesting removing the anti-DoS protections. I'm suggesting that
> > 
> > replaced transaction require a fee increase of at least the min-fee-rate
> > 
> > times the size of all the transactions being ejected (in addition to the
> > 
> > other proposed requirements).
> 
> Fair: you're not removing them entirely, but you are weakening them compared to
> 
> the status quo.
> 
> > > Equally, how often are normal users who aren't attacking each other
> > > 
> > > creating
> > > 
> > > issues anyway? You can always have your wallet code just skip use of RBF
> > 
> > replacements in the event that someone does spend an unconfirmed output that
> > 
> > > you sent them; how often does this actually happen in practice?
> > 
> > Just ask rhavar. It happens regularly.
> > 
> > Not many wallets let you spend unconfirmed outputs that you didn't create.
> > 
> > > 
> > 
> > The problem is with institutional wallets sweeping incoming payments. It
> > 
> > seems that in practice they are happy to sweep unconfirmed outputs.
> 
> Pity, that does sound like a problem. :(
> 
> > Setting all of the above aside for a moment. We need to understand that
> > 
> > rational miners are going to prefer to transactions with higher package fee
> > 
> > rates regardless of whatever your personal preferred RBF policy is. If we
> > 
> > do not bring the RBF policy to alignment with what is economically
> > 
> > rational, then miners are going to change their own policies anyways,
> > 
> > probably all in slightly different ways. It behooves everyone to develop a
> > 
> > reasonable standard RBF policy, that is still robust against possible DoS
> > 
> > vectors, and aligns with miner incentives, so that all participants know
> > 
> > what behaviour they can reasonably expect. It is simply a bonus that this
> > 
> > change in RBF policy also partially mitigates the problem of pinned
> > 
> > transactions.
> 
> Miners and full nodes have slightly different priorities here; it's not clear
> 
> to me why it matters that they implement slightly different policies.
> 
> Still, re-reading your initital post, I'm convinced that the weakening of the
> 
> DoS protections is probably not a huge problem, so maybe lets try this in a
> 
> release and see what happens.
> 
> Notably, if people actually use this new replacement behavior, the institutions
> 
> doing these sweeps of unconfirmed outputs might stop doing that! That's
> 
> probably a good thing, as respends of potentially conflicted unconfirmed
> 
> outputs can be dangerous in reorgs; we're better off if outputs are buried
> 
> deeply before being spent again.
> 
> 
> -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> 
> https://petertodd.org 'peter'\[:-1\]@petertodd.org
> 
> bitcoin-dev mailing list
> 
> bitcoin-dev at lists.linuxfoundation.org
> 
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev



From jose.femenias at gmail.com  Sun Mar 11 12:44:47 2018
From: jose.femenias at gmail.com (=?utf-8?Q?JOSE_FEMENIAS_CA=C3=91UELO?=)
Date: Sun, 11 Mar 2018 13:44:47 +0100
Subject: [bitcoin-dev] Bulletproof CT as basis for election voting?
Message-ID: <90096274-9576-4A08-A86A-E1C4F3E3B5DE@gmail.com>

If I understand Bulletproof Confidential Transactions properly, their main virtue is being able to hide not the senders/receivers of a coin but the amount transferred.
That sounds to me like a perfect use case for an election.
For instance, in my country, every citizen is issued a National ID Card with a digital certificate. 
So, a naive implementation could simply be that the Voting Authority, sends a coin (1 coin = 1 vote) to each citizen above 18. This would be an open transaction, so it is easily auditable.
Later on, each voter sends her coin to her preferred party, as part of a Bulletproof CT, along with 0 coins to other parties to disguise her vote.
In the end, each party will accrue as may votes as coins received.

Is there any gotcha I?m missing here? Are there any missing features required in Bulletproof to support this use case?

From tim.ruffing at mmci.uni-saarland.de  Mon Mar 12 09:32:55 2018
From: tim.ruffing at mmci.uni-saarland.de (Tim Ruffing)
Date: Mon, 12 Mar 2018 10:32:55 +0100
Subject: [bitcoin-dev] Bulletproof CT as basis for election voting?
In-Reply-To: <90096274-9576-4A08-A86A-E1C4F3E3B5DE@gmail.com>
References: <90096274-9576-4A08-A86A-E1C4F3E3B5DE@gmail.com>
Message-ID: <1520847175.2339.14.camel@mmci.uni-saarland.de>

You're right that this is a simple electronic voting scheme. The thing
is that cryptographers are working on e-voting for decades and the idea
to use homomorphic commitments (or encryption) and zero-knowledge
proofs is not new in this area. It's rather the case that e-voting
inspired a lot of work on homomorphic crypto and related zero-knowledge 
proofs. For example, range proofs are overkill in e-voting. You just
need to ensure that the sum of all my votes (over all candidates) is 1.
 
E-voting protocols typically require some "bulletin board", where
ballots are stored. A blockchain could indeed be helpful in specific
cases (but not in all cases)...

If you're interested in that stuff, I'd suggest you to read some
literature about e-voting. (For example, 
https://arxiv.org/pdf/1801.08064 looks interesting for the connection
to blockchains -- I haven't read it though). There are pretty
sophisticated protocols in the literature. And I think that this
mailing list may not be the best place to discuss these.

Best,
Tim 



On Sun, 2018-03-11 at 13:44 +0100, JOSE FEMENIAS CA?UELO via bitcoin-
dev wrote:
> If I understand Bulletproof Confidential Transactions properly, their
> main virtue is being able to hide not the senders/receivers of a coin
> but the amount transferred.
> That sounds to me like a perfect use case for an election.
> For instance, in my country, every citizen is issued a National ID
> Card with a digital certificate. 
> So, a naive implementation could simply be that the Voting Authority,
> sends a coin (1 coin = 1 vote) to each citizen above 18. This would
> be an open transaction, so it is easily auditable.
> Later on, each voter sends her coin to her preferred party, as part
> of a Bulletproof CT, along with 0 coins to other parties to disguise
> her vote.
> In the end, each party will accrue as may votes as coins received.
> 
> Is there any gotcha I?m missing here? Are there any missing features
> required in Bulletproof to support this use case?
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From ZmnSCPxj at protonmail.com  Mon Mar 12 04:14:42 2018
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Mon, 12 Mar 2018 00:14:42 -0400
Subject: [bitcoin-dev] Bulletproof CT as basis for election voting?
In-Reply-To: <90096274-9576-4A08-A86A-E1C4F3E3B5DE@gmail.com>
References: <90096274-9576-4A08-A86A-E1C4F3E3B5DE@gmail.com>
Message-ID: <Us9RkES7hdpIyleB-Di6Cb3p-ydB293c8hKvumN3uJI9v_5b33YIMkSK6zGSgtWm4bclRklNeCAIcqBk-9MK7TFUjWbyZsNGgftfVW4KPHY=@protonmail.com>

Good morning Jose,

By my understanding, the sender needs to reveal some secrets to the receiver, and the receiver will then know if it received 0 or 1 coin from that sender.  (At least from my understanding of MimbleWimble; it might not be the case for CT, but MW is an extension of CT so...)

If voters send vote-coins directly to The Party, then The Party knows the votes of particular voters, and may then dispatch subcontractors to dispatch those voters.  It may be possible to have aggregators/mixers, but then you would have to trust the aggregators/mixers operate correctly and send to the correct destination party, and that the mixers are not recording voters.

Maybe in combination with something like CoinSwap or CoinJoin protocol would work to obscure the source of coins: a voter would have to swap several times with many, many other voters to ensure increased anonymity set (and then maybe some voters may report their transactions to The Party).

In any case sending directly from the tx of the Voting Authority to another tx to your selected The Party would let The Party members who secretly control the Voting Authority records to figure out, which voters got which txouts of the Voting Authority (presumably the Voting Authority has strict public records of which txout went to which voter, in order to prevent the Voting Authority secretly giving multiple vote-coins to a single One Man, All Votes).

Regards,
ZmnSCPxj


?Sent with ProtonMail Secure Email.?

??????? Original Message ???????

On March 11, 2018 8:44 PM, JOSE FEMENIAS CA?UELO via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:

> If I understand Bulletproof Confidential Transactions properly, their main virtue is being able to hide not the senders/receivers of a coin but the amount transferred.
> 
> That sounds to me like a perfect use case for an election.
> 
> For instance, in my country, every citizen is issued a National ID Card with a digital certificate.
> 
> So, a naive implementation could simply be that the Voting Authority, sends a coin (1 coin = 1 vote) to each citizen above 18. This would be an open transaction, so it is easily auditable.
> 
> Later on, each voter sends her coin to her preferred party, as part of a Bulletproof CT, along with 0 coins to other parties to disguise her vote.
> 
> In the end, each party will accrue as may votes as coins received.
> 
> Is there any gotcha I?m missing here? Are there any missing features required in Bulletproof to support this use case?
> 
> bitcoin-dev mailing list
> 
> bitcoin-dev at lists.linuxfoundation.org
> 
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev



From ZmnSCPxj at protonmail.com  Mon Mar 12 06:46:39 2018
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Mon, 12 Mar 2018 02:46:39 -0400
Subject: [bitcoin-dev] Bulletproof CT as basis for election voting?
In-Reply-To: <Us9RkES7hdpIyleB-Di6Cb3p-ydB293c8hKvumN3uJI9v_5b33YIMkSK6zGSgtWm4bclRklNeCAIcqBk-9MK7TFUjWbyZsNGgftfVW4KPHY=@protonmail.com>
References: <90096274-9576-4A08-A86A-E1C4F3E3B5DE@gmail.com>
	<Us9RkES7hdpIyleB-Di6Cb3p-ydB293c8hKvumN3uJI9v_5b33YIMkSK6zGSgtWm4bclRklNeCAIcqBk-9MK7TFUjWbyZsNGgftfVW4KPHY=@protonmail.com>
Message-ID: <K9SaeVjj0_bUr7O-78hlSUwsQqa0SYI-UUOj6L_hnDSF4zhk4dFLg4XWBOQ9xBHD2vu5H-y7nrjLjKlXue6Fi5sR5TGbGIY2f0KwnYINgBU=@protonmail.com>

Good morning again Jose,

Another idea is that with sufficiently high stakes (i.e. control of the government of an entire country) it would be possible for a miner-strong The Party to censor transactions that do not give it non-zero amounts of coins.  If The Party has a strong enough power over miners (or is composed of miners) then it would be possible for The Party to censor transactions using some simple heuristics: (1) At least one output goes to The Party (2) the number of inputs equals the number of vote-coins that go to The Party output.  Since The Party must know how many vote-coins it received, it can know #2, and it assumes that each input has 1 coin, since that is what is issued by the Voting Authority.  This prevents mixing, too, since transactions that do not involve The Party cannot be confirmed.

Presumably other parties may exist that have some miners, but if everyone starts censoring transactions then parties end up voting by their controlled hashpower rather than anything else (simply censor all transactions that fail the above heuristics and build the longest chain: as long as you get even 1 vote and all others get 0 votes on the longest chain, you win. since presumably you are also a valid voter, you can just give that single vote-coin issued to you-as-voter to you-as-party, then censor all other transactions in the blockchain so that other voters cannot give their coins to their preferred parties).  One could try using proof-of-stake if one has managed to create a solution to nothing-at-stake and stake-grinding that itself does not require proof-of-work (hint, there are none).

This can be mitigated by using a multi-asset international blockchain with confidential assets, such that no single The Party can control enough hashpower to censor, but that makes small blocks even more important to help fight against centralization (and control of cheap energy becomes even more important such that some international entity may very well bend elections in individual countries to its favor to get more energy with which to control more energy, and so on).

You can only trust the miners of the blockchain to the extent that you pay fees to those miners, effectively buying a portion of hashrate in a (mostly) fair auction.  You can expect that miners will attempt to charge as much as they can for the hashrate, and therefore that vote transfers (if they can be detected by miners) are likely to be charged at whatever is the going rate for that vote.  If what is being voted on is important enough, you can assure yourself, that miners will ally with politicians and use the fact that CT is confidential only between receiver and sender to discern preferred vote transfers.

Uncensorability may be possible though; I think Peter Todd was working on those.  A simple one is a two-step commitment, where an earlier miner only knows of a sealed commitment (a hash of a transaction), publishes it, then a future commitment shows the entire transaction and the earlier miner gets paid only if the second commitment pushes through (the fee gets split somehow between the earlier and later miner).  But once you reveal a transaction and it is not one of those desired by the later miner, if the vote is valuable enough then the miner might very well forgo its fee in favor of never confirming the second commitment.

It may be better to focus more on libertarian solutions (e.g. assurance contracts) on top of blockchains than attempting to shoehorn democractic ideals on top of blockchains.

Regards,
ZmnSCPxj

From willtech at live.com.au  Tue Mar 13 13:26:17 2018
From: willtech at live.com.au (Damian Williamson)
Date: Tue, 13 Mar 2018 13:26:17 +0000
Subject: [bitcoin-dev] Sign / Verify message against SegWit P2SH and Bech32
 addresses
Message-ID: <PS2P216MB017926BA48E2A8E1B4E17BBE9DD20@PS2P216MB0179.KORP216.PROD.OUTLOOK.COM>

Current implementation of sign/verify is broken for SegWit and Bech32 addresses.


Please add the following reference to the use cases:

---

# Does blockchain.info show balances for addresses that are in cold storage?

Yes.

>... is there any way for me in another country to confirm that what my colleague views is actually accurate and correct?

Since they use Bitcoin Core, yes, there is a way to verify that they hold the addresses that they claim. Have them sign a message with each address that they claim to have the holdings on, using Bitcoin Core you can verify that they indeed have those addresses and check them on blockchain.info to find the current balance.

Only works in Bitcoin Core currently for addresses starting with a '1' (not Segwit addresses starting with a '3' and not Bech32 addresses starting with 'bc1' - the developers are aware of this and I will remind them shortly.)

In Bitcoin Core, your transaction opposite goes to File -> Sign Message and signs any message with one of the holding addresses. Copy the message, address and signature and send to you via probably plain text format email is the easiest. Repeat for each additional address holding the balance of BTC that they are offering to sell.

In Bitcoin Core, you go to File -> Verify Message and key the details provided EXACTLY - spaces, new lines and all characters must be an EXACT match. Click on verify and voil?.

I prefer the form of signed message as follows (don't key the top and bottom bar rows for the message, just the contents and you can check this yourself, the bottom row is the signature). I like to key the address used for verifying as a part of the message but that is not strictly necessary:

    ------------------------------
    Something that I want to sign.

    bitcoin:1PMUf9aaQ41M4bgVbCAPVwAeuKvj8CwxJg
    ------------------------------
    Signture:
    IGaXlQNRHHM6ferJ+Ocr3cN9dRJhIWxo+n9PGwgg1uPdOLVYIeCuaccEzDygVgYPJMXqmQeSaLaZVoG6FMHPJkg=

This contains all of the compact information necessary to verify the message.

Example of verified message:
![verified message][1]

[1]: https://i.stack.imgur.com/zv1xq.png

---

https://bitcoin.stackexchange.com/a/72281/75001



Solution seems to be straight-forward, as noted in Issue# [10542](https://github.com/bitcoin/bitcoin/issues/10542#issuecomment-306584383)


>And it would in theory be possible to make signmessage work for a P2SH-P2WPKH address, in cases where the verifier knows the embedded pubkeyhash already. But in that case you don't need "sign with a witness address" functionality - *you could just sign with the embedded key (see validateaddress), and have the verifier check that*.


>The point is to not further the misunderstanding that signmessage signs with an address - it never did. It signs with a keyhash, and verify with a keyhash.


This is an important feature, there are few other ways to verify that an address is held. Note that the linked issue is not currently labeld GUI and probably could be - unless a new issue should also be opened?


Regards,

Damian Williamson

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180313/7dbec559/attachment.html>

From pete at petertodd.org  Wed Mar 14 00:37:52 2018
From: pete at petertodd.org (Peter Todd)
Date: Tue, 13 Mar 2018 20:37:52 -0400
Subject: [bitcoin-dev] Data structure for efficient proofs of
	non-inclusion
In-Reply-To: <CAD438Ht8ewJDyGbcqjkZdRWn6Yy2sRH9H_3jg2gWSgoJkrAHVA@mail.gmail.com>
References: <CAD438Ht8ewJDyGbcqjkZdRWn6Yy2sRH9H_3jg2gWSgoJkrAHVA@mail.gmail.com>
Message-ID: <20180314003752.GA3853@fedora-23-dvm>

On Wed, Feb 14, 2018 at 09:09:18PM +0000, Daniel Robinson wrote:
> Hi Peter,
> 
> Thanks for the mind-expanding presentation on client-side validation at
> Chaincode last week.
> 

CCing bitcoin-dev because this is of general interest...

For background, Daniel is asking about my client-side verified single-use-seals
via proof-of-publication model, previously published here?, which creates an
anti-double-spend primitive via a proof-of-publication, and many
proofs-of-non-publication.

tl;dr: A seal is closed by publishing a valid signature for the seal to a
ledger. The first signature is the valid one, so if Alice want to convince Bob
you she closed a seal correctly (e.g. to pay Bob), she has to supply Bob with a
proof that the signature _was_ published - a proof-of-publication - as well as
proof that prior to being published, no valid signature was previously
published - a proof-of-non-publication.

It's the proofs-of-non-publication that take up the most space.

> I'm trying to sketch out a system that would use some of those ideas, and
> am looking at what Merkle-tree-like structure for the "transaction root"
> would best allow efficient proofs of non-inclusion in a block. The simplest
> solution seems to just be a Merkle tree with the transactions sorted by
> public key, but it seems like having intermediate information about ranges
> higher up in the branches might help make non-inclusion proofs more
> compact. Other solutions I've seen like Patricia trees and sparse Merkle
> trees seem to be optimizing for easy recomputation on update, which doesn't
> seem to be necessary for at least this version of the idea.
>
> Are there any verifiable data structures you've found that improve on
> sorted Merkle trees with respect to proofs of non-inclusion?

So remember that the system I proposed? used sorted merkle trees only within a
block; for blocks themselves you mostly can't do any better than a linear list.
Though I think there may be some exceptions which deserves another email. :)

As you know, asymptotically merkle trees have excellent log2(n) proof size
growth. But as you correctly suggest, their high overhead in the small-n case
suggests that we can do better. In fact, Bram Cohen previously proposed? a "TXO
Bitfield" for the somewhat similar use-case of committing to the spentness of
outputs efficiently.


# Naive Analysis

So suppose at an intermediate node you commit to a simple bitfield where each
possible value under that node is represented by a single bit. Thus for m
values under that point in the tree, the marginal size of the non-inclusion
proof is m bits. By comparison a naive merkle tree built from a hash function
with k bits takes approximately k*log2(m) bits to prove non-inclusion. For an
rough, unsophisticated, analysis just solve:

    m = k * log2(m)

Apparently you can do this analytically, but as this analysis is only
approximate a much better idea is to just plot it on a graph: for k=256bits the
crossover point is roughly m=3000.


# Merkle Tree Structure

But what is the bitfield competing against exactly? Just saying "merkle tree"
isn't very precise. Most designs along these lines use something like a
merkelized patricia tree, where each bit of the key is compared individually
and each node is a two-way (left vs right) branch. Better yet is the radix
tree, where each inner node that has only one child is merged with its parent.

Regardless, the logic of these kinds of trees can be thought of a recursive
query, where each type of node has a `get(key)` operation that returns either a
value or None.

So let's define a new type of inner node that commits to a
merkle-mountain-range (MMR) tip and a 2^j element bitfield. `get(key)` is then
this pseudo-rust:

    fn get(self, prefix) -> Option<Value> {
        let idx = Int::from(prefix[0 .. j]);
        if self.bitfield[idx] {
            let mmr_idx = node.bitfield[0 .. idx].count_ones() - 1;
            Some(node.mmr[mmr_idx].get(prefix)
        } else {
            None
        }
    }

The hard part with this is choosing when to use a bitfield-based inner node
instead of a standard one. Assuming keys are randomly distributed, it makes
sense to do so when the bitfield table is partially empty, but how empty? It's
a trade-off between multiple parameters, including the size of
proofs-of-publication - although the latter may be OK to ignore as the number
of proof-of-non-publication needed should usually greatly outnumber
proofs-of-publication.

Question: is it OK for this choice to not be part of the deterministic
consensus? Is that even possible to enforce?


# Security

For a proof-of-publication to be secure, it must ensure that any attempt to
construct a false proof-of-non-publication will fail. In the pruning model,
that means that a proof-of-publication is simply the data necessary for the
proof-of-non-publication verifier to return false. Concretely:

    fn verify_pop(tree, key) -> bool {
        !verify_non_pop(tree, key)
    }

However note the subtle difference in trust model with respect to censorship
between the following two possible non-pop verifiers:

    fn verify_non_pop(tree, key) -> bool {
        !tree.exists(key)
    }

    fn verify_non_pop(tree, key) -> bool {
        match tree.get(key) {
            Some(value) => !verify_signature(value),
            None => true,
        }
    }


## False Positives

Note how if we use the second `verify_non_pop()` function shown above we can
also use probabilistic data structures such as bloom filters in place of a
bitfield. This works because a false-positive is acceptable, as it will still
fail signature verification (or sooner if the key is committed in the leaf
node).

For example, it's plausible that a compressed bloom filter would be more space
efficient than a bitfield, as the multiple hashing steps might use the bits in
the filter more efficiently. Investigating this further would be a good
research topic.


# References

1) "[bitcoin-dev] Scalable Semi-Trustless Asset Transfer via Single-Use-Seals and Proof-of-Publication",
   Peter Todd, Dec 5th 2017, https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-December/015350.html

2) "[bitcoin-dev] The TXO bitfield",
   Bram Cohen, Mar 31st 2017, https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-March/013928.html

3) "Bloom filters",
    Wikipedia, Jan 27th 2018, https://en.wikipedia.org/w/index.php?title=Bloom_filter&oldid=822632093

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 614 bytes
Desc: Digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180313/7f08d01f/attachment-0001.sig>

From karljohan-alm at garage.co.jp  Wed Mar 14 08:09:20 2018
From: karljohan-alm at garage.co.jp (Karl Johan Alm)
Date: Wed, 14 Mar 2018 04:09:20 -0400
Subject: [bitcoin-dev] {sign|verify}message replacement
Message-ID: <CALJw2w5=g-FL+MZ08DEoLxVzOKbSXeKu50drE1b4P0JZJpdTyA@mail.gmail.com>

Hello,

I am considering writing a replacement for the message signing tools
that are currently broken for all but the legacy 1xx addresses. The
approach (suggested by Pieter Wuille) is to do a script based
approach. This does not seem to require a lot of effort for
implementing in Bitcoin Core*. Below is my proposal for this system:

A new structure SignatureProof is added, which is a simple scriptSig &
witnessProgram container that can be serialized. This is passed out
from/into the signer/verifier.

RPC commands:

sign <address> <message> [<prehashed>=false]

Generates a signature proof for <message> using the same method that
would be used to spend coins sent to <address>.**

verify <address> <message> <proof> [<prehashed>=false]

Deserializes and executes the proof using a custom signature checker
whose sighash is derived from <message>. Returns true if the check
succeeds, and false otherwise. The scriptPubKey is derived directly
from <address>.**

Feedback welcome.

-Kalle.

(*) Looks like you can simply use VerifyScript with a new signature
checker class. (h/t Nicolas Dorier)
(**) If <prehashed> is true, <message> is the sighash, otherwise
sighash=sha256d(message).

From luke at dashjr.org  Wed Mar 14 12:36:47 2018
From: luke at dashjr.org (Luke Dashjr)
Date: Wed, 14 Mar 2018 12:36:47 +0000
Subject: [bitcoin-dev] {sign|verify}message replacement
In-Reply-To: <CALJw2w5=g-FL+MZ08DEoLxVzOKbSXeKu50drE1b4P0JZJpdTyA@mail.gmail.com>
References: <CALJw2w5=g-FL+MZ08DEoLxVzOKbSXeKu50drE1b4P0JZJpdTyA@mail.gmail.com>
Message-ID: <201803141236.48869.luke@dashjr.org>

I don't see a need for a new RPC interface, just a new signature format.

Ideally, it should support not only just "proof I receive at this address", 
but also "proof of funds" (as a separate feature) since this is a popular 
misuse of the current message signing (which doesn't actually prove funds at 
all). To do this, it needs to be capable of signing for multiple inputs.

Preferably, it should also avoid disclosing the public key for existing or 
future UTXOs. But I don't think it's possible to avoid this without something 
MAST-like first. Perhaps it can be a MAST upgrade later on, but the new 
signature scheme should probably be designed with it in mind.

Luke


On Wednesday 14 March 2018 8:09:20 AM Karl Johan Alm via bitcoin-dev wrote:
> Hello,
> 
> I am considering writing a replacement for the message signing tools
> that are currently broken for all but the legacy 1xx addresses. The
> approach (suggested by Pieter Wuille) is to do a script based
> approach. This does not seem to require a lot of effort for
> implementing in Bitcoin Core*. Below is my proposal for this system:
> 
> A new structure SignatureProof is added, which is a simple scriptSig &
> witnessProgram container that can be serialized. This is passed out
> from/into the signer/verifier.
> 
> RPC commands:
> 
> sign <address> <message> [<prehashed>=false]
> 
> Generates a signature proof for <message> using the same method that
> would be used to spend coins sent to <address>.**
> 
> verify <address> <message> <proof> [<prehashed>=false]
> 
> Deserializes and executes the proof using a custom signature checker
> whose sighash is derived from <message>. Returns true if the check
> succeeds, and false otherwise. The scriptPubKey is derived directly
> from <address>.**
> 
> Feedback welcome.
> 
> -Kalle.
> 
> (*) Looks like you can simply use VerifyScript with a new signature
> checker class. (h/t Nicolas Dorier)
> (**) If <prehashed> is true, <message> is the sighash, otherwise
> sighash=sha256d(message).
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From kalle at rosenbaum.se  Wed Mar 14 09:46:55 2018
From: kalle at rosenbaum.se (Kalle Rosenbaum)
Date: Wed, 14 Mar 2018 10:46:55 +0100
Subject: [bitcoin-dev] {sign|verify}message replacement
In-Reply-To: <CALJw2w5=g-FL+MZ08DEoLxVzOKbSXeKu50drE1b4P0JZJpdTyA@mail.gmail.com>
References: <CALJw2w5=g-FL+MZ08DEoLxVzOKbSXeKu50drE1b4P0JZJpdTyA@mail.gmail.com>
Message-ID: <CAPswA9xuVT74L87QO9TXGc6=O6Gd2kbQMBdmn=7zUm5OHXcfOA@mail.gmail.com>

Thank you.

I can't really see from your proposal if you had thought of this: A soft
fork can make old nodes accept invalid message signatures as valid. For
example, a "signer" can use a witness version unknown to the verifier to
fool the verifier. Witness version is detectable (just reject unknown
witness versions)  but there may be more subtle changes. Segwit was not
"detectable" in that way, for example.

This is the reason why I withdrew BIP120. If you have thought about the
above, I'd be very interested.

/Kalle

Sent from my Sinclair ZX81

Den 14 mars 2018 16:10 skrev "Karl Johan Alm via bitcoin-dev" <
bitcoin-dev at lists.linuxfoundation.org>:

Hello,

I am considering writing a replacement for the message signing tools
that are currently broken for all but the legacy 1xx addresses. The
approach (suggested by Pieter Wuille) is to do a script based
approach. This does not seem to require a lot of effort for
implementing in Bitcoin Core*. Below is my proposal for this system:

A new structure SignatureProof is added, which is a simple scriptSig &
witnessProgram container that can be serialized. This is passed out
from/into the signer/verifier.

RPC commands:

sign <address> <message> [<prehashed>=false]

Generates a signature proof for <message> using the same method that
would be used to spend coins sent to <address>.**

verify <address> <message> <proof> [<prehashed>=false]

Deserializes and executes the proof using a custom signature checker
whose sighash is derived from <message>. Returns true if the check
succeeds, and false otherwise. The scriptPubKey is derived directly
from <address>.**

Feedback welcome.

-Kalle.

(*) Looks like you can simply use VerifyScript with a new signature
checker class. (h/t Nicolas Dorier)
(**) If <prehashed> is true, <message> is the sighash, otherwise
sighash=sha256d(message).
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180314/a857358a/attachment.html>

From aj at erisian.com.au  Wed Mar 14 16:12:11 2018
From: aj at erisian.com.au (Anthony Towns)
Date: Wed, 14 Mar 2018 12:12:11 -0400
Subject: [bitcoin-dev] {sign|verify}message replacement
In-Reply-To: <CAPswA9xuVT74L87QO9TXGc6=O6Gd2kbQMBdmn=7zUm5OHXcfOA@mail.gmail.com>
References: <CALJw2w5=g-FL+MZ08DEoLxVzOKbSXeKu50drE1b4P0JZJpdTyA@mail.gmail.com>
	<CAPswA9xuVT74L87QO9TXGc6=O6Gd2kbQMBdmn=7zUm5OHXcfOA@mail.gmail.com>
Message-ID: <C0F50D50-C514-47BE-BC93-A5BD01E5826E@erisian.com.au>

On 14 March 2018 5:46:55 AM GMT-04:00, Kalle Rosenbaum via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
>Thank you.
>
>I can't really see from your proposal if you had thought of this: A
>soft
>fork can make old nodes accept invalid message signatures as valid. For
>example, a "signer" can use a witness version unknown to the verifier
>to
>fool the verifier. Witness version is detectable (just reject unknown
>witness versions)  but there may be more subtle changes. Segwit was not
>"detectable" in that way, for example.
>
>This is the reason why I withdrew BIP120. If you have thought about the
>above, I'd be very interested.
>
>/Kalle
>
>Sent from my Sinclair ZX81
>
>Den 14 mars 2018 16:10 skrev "Karl Johan Alm via bitcoin-dev" <
>bitcoin-dev at lists.linuxfoundation.org>:
>
>Hello,
>
>I am considering writing a replacement for the message signing tools
>that are currently broken for all but the legacy 1xx addresses. The
>approach (suggested by Pieter Wuille) is to do a script based
>approach. This does not seem to require a lot of effort for
>implementing in Bitcoin Core*. Below is my proposal for this system:
>
>A new structure SignatureProof is added, which is a simple scriptSig &
>witnessProgram container that can be serialized. This is passed out
>from/into the signer/verifier.
>
>RPC commands:
>
>sign <address> <message> [<prehashed>=false]
>
>Generates a signature proof for <message> using the same method that
>would be used to spend coins sent to <address>.**
>
>verify <address> <message> <proof> [<prehashed>=false]
>
>Deserializes and executes the proof using a custom signature checker
>whose sighash is derived from <message>. Returns true if the check
>succeeds, and false otherwise. The scriptPubKey is derived directly
>from <address>.**
>
>Feedback welcome.
>
>-Kalle.
>
>(*) Looks like you can simply use VerifyScript with a new signature
>checker class. (h/t Nicolas Dorier)
>(**) If <prehashed> is true, <message> is the sighash, otherwise
>sighash=sha256d(message).
>_______________________________________________
>bitcoin-dev mailing list
>bitcoin-dev at lists.linuxfoundation.org
>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

Wouldn't it be sufficient for old nodes to check for standardness of the spending script and report non-standard scripts as either invalid outright, or at least highly questionable? That should prevent confusion as long as soft forks are only making nonstandard behaviours invalid.

Cheers,
aj

-- 
Sent from my phone.

From karljohan-alm at garage.co.jp  Thu Mar 15 03:01:03 2018
From: karljohan-alm at garage.co.jp (Karl Johan Alm)
Date: Wed, 14 Mar 2018 23:01:03 -0400
Subject: [bitcoin-dev] {sign|verify}message replacement
In-Reply-To: <CAPswA9xuVT74L87QO9TXGc6=O6Gd2kbQMBdmn=7zUm5OHXcfOA@mail.gmail.com>
References: <CALJw2w5=g-FL+MZ08DEoLxVzOKbSXeKu50drE1b4P0JZJpdTyA@mail.gmail.com>
	<CAPswA9xuVT74L87QO9TXGc6=O6Gd2kbQMBdmn=7zUm5OHXcfOA@mail.gmail.com>
Message-ID: <CALJw2w719qQnyvaJbe1wc39+4ERDST+zXCOjt0DiJpktD74QCA@mail.gmail.com>

On Wed, Mar 14, 2018 at 5:46 AM, Kalle Rosenbaum <kalle at rosenbaum.se> wrote:
> I can't really see from your proposal if you had thought of this: A soft
> fork can make old nodes accept invalid message signatures as valid. For
> example, a "signer" can use a witness version unknown to the verifier to
> fool the verifier. Witness version is detectable (just reject unknown
> witness versions)  but there may be more subtle changes. Segwit was not
> "detectable" in that way, for example.
>
> This is the reason why I withdrew BIP120. If you have thought about the
> above, I'd be very interested.

I'm not sure I see the problem. The scriptPubKey is derived directly
from the address in all cases, which means the unknown witness version
would have to be committed to in the address itself.

So yeah, I can make a P2SH address with a witness version > 0 and a to
me unknown pubkey and then fool you into thinking I own it, but I
don't really see why you'd ultimately care. In other words, if I can
SPEND funds sent to that address today, I can prove that I can spend
today, which is the purpose of the tool, I think.

For the case where the witness version HAS been upgraded, the above
still applies, but I'm not sure it's a big issue. And it doesn't
really require an old node. I just need to set witness version >
current witness version and the problem applies to all nodes.

On Wed, Mar 14, 2018 at 8:36 AM, Luke Dashjr <luke at dashjr.org> wrote:
> I don't see a need for a new RPC interface, just a new signature format.

All right.

> Ideally, it should support not only just "proof I receive at this address",
> but also "proof of funds" (as a separate feature) since this is a popular
> misuse of the current message signing (which doesn't actually prove funds at
> all). To do this, it needs to be capable of signing for multiple inputs.

I assume by inputs you mean addresses/keys. The address field could
optionally be an array. That'd be enough?

> Preferably, it should also avoid disclosing the public key for existing or
> future UTXOs. But I don't think it's possible to avoid this without something
> MAST-like first. Perhaps it can be a MAST upgrade later on, but the new
> signature scheme should probably be designed with it in mind.

I'd love to not have to reveal the public key, but I'm not sure how it
would be done, even with MAST.

On Wed, Mar 14, 2018 at 12:12 PM, Anthony Towns <aj at erisian.com.au> wrote:
> Wouldn't it be sufficient for old nodes to check for standardness of the spending script and report non-standard scripts as either invalid outright, or at least highly questionable? That should prevent confusion as long as soft forks are only making nonstandard behaviours invalid.

That seems sensible to me. A warning would probably be useful, in case
the verifier is running old software.

-Kalle.

From karljohan-alm at garage.co.jp  Thu Mar 15 07:25:21 2018
From: karljohan-alm at garage.co.jp (Karl Johan Alm)
Date: Thu, 15 Mar 2018 07:25:21 +0000
Subject: [bitcoin-dev] {sign|verify}message replacement
In-Reply-To: <CADZtCSiSCDb1dzLCgr24jCNzzmcfsXuPyNVh+YJJ6rNqdsQh2Q@mail.gmail.com>
References: <CALJw2w5=g-FL+MZ08DEoLxVzOKbSXeKu50drE1b4P0JZJpdTyA@mail.gmail.com>
	<CAPswA9xuVT74L87QO9TXGc6=O6Gd2kbQMBdmn=7zUm5OHXcfOA@mail.gmail.com>
	<CALJw2w719qQnyvaJbe1wc39+4ERDST+zXCOjt0DiJpktD74QCA@mail.gmail.com>
	<CADZtCSiSCDb1dzLCgr24jCNzzmcfsXuPyNVh+YJJ6rNqdsQh2Q@mail.gmail.com>
Message-ID: <CALJw2w7LAUB+7FpLSGz=+pL7y_t5sy0yGuhYML7yJjuKc8Uw_w@mail.gmail.com>

On Thu, Mar 15, 2018 at 6:43 AM, Jim Posen <jim.posen at gmail.com> wrote:
> How are scripts with OP_CLTV and OP_CSV handled by verifiers? Do they always
> succeed? Or should an nLockTime and nSequence also be included in the proof
> in a way that can be parsed out and displayed to verifiers?

Good question.. Since you don't really have the input(s), I think it's
fine to always assume sufficient time/height on CLTV/CSV checks.

> I assume any signatures in the scriptSig/witness data would have no sighash
> type?

I think it would just use the default (SIGHASH_ALL?) for simplicity.
Is there a good reason to tweak it?

-Kalle.

From karljohan-alm at garage.co.jp  Thu Mar 15 07:36:48 2018
From: karljohan-alm at garage.co.jp (Karl Johan Alm)
Date: Thu, 15 Mar 2018 07:36:48 +0000
Subject: [bitcoin-dev] {sign|verify}message replacement
In-Reply-To: <201803141236.48869.luke@dashjr.org>
References: <CALJw2w5=g-FL+MZ08DEoLxVzOKbSXeKu50drE1b4P0JZJpdTyA@mail.gmail.com>
	<201803141236.48869.luke@dashjr.org>
Message-ID: <CALJw2w5NQ=WX1Cm4aUXMN=uxn8NLHAfYDLEtqUptce5DCWZXWA@mail.gmail.com>

On Wed, Mar 14, 2018 at 12:36 PM, Luke Dashjr <luke at dashjr.org> wrote:
> Ideally, it should support not only just "proof I receive at this address",
> but also "proof of funds" (as a separate feature) since this is a popular
> misuse of the current message signing (which doesn't actually prove funds at
> all). To do this, it needs to be capable of signing for multiple inputs.

Re-reading this, I think what you mean is it should be possible to
create a proof for (a) specific UTXO(s), hence "inputs". That sounds
pretty useful, yeah!

So you could provide a mix of addresses and inputs (as txid:vout) and
it would generate a proof that signs the message for each input
(taking scriptPubKey from address or from the UTXO data directly on
the blockchain).

-Kalle.

From jim.posen at gmail.com  Thu Mar 15 06:43:21 2018
From: jim.posen at gmail.com (Jim Posen)
Date: Wed, 14 Mar 2018 23:43:21 -0700
Subject: [bitcoin-dev] {sign|verify}message replacement
In-Reply-To: <CALJw2w719qQnyvaJbe1wc39+4ERDST+zXCOjt0DiJpktD74QCA@mail.gmail.com>
References: <CALJw2w5=g-FL+MZ08DEoLxVzOKbSXeKu50drE1b4P0JZJpdTyA@mail.gmail.com>
	<CAPswA9xuVT74L87QO9TXGc6=O6Gd2kbQMBdmn=7zUm5OHXcfOA@mail.gmail.com>
	<CALJw2w719qQnyvaJbe1wc39+4ERDST+zXCOjt0DiJpktD74QCA@mail.gmail.com>
Message-ID: <CADZtCSiSCDb1dzLCgr24jCNzzmcfsXuPyNVh+YJJ6rNqdsQh2Q@mail.gmail.com>

I like this proposal, it seems sufficiently general.

How are scripts with OP_CLTV and OP_CSV handled by verifiers? Do they
always succeed? Or should an nLockTime and nSequence also be included in
the proof in a way that can be parsed out and displayed to verifiers?

I assume any signatures in the scriptSig/witness data would have no sighash
type?

On Wed, Mar 14, 2018 at 8:01 PM, Karl Johan Alm via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> On Wed, Mar 14, 2018 at 5:46 AM, Kalle Rosenbaum <kalle at rosenbaum.se>
> wrote:
> > I can't really see from your proposal if you had thought of this: A soft
> > fork can make old nodes accept invalid message signatures as valid. For
> > example, a "signer" can use a witness version unknown to the verifier to
> > fool the verifier. Witness version is detectable (just reject unknown
> > witness versions)  but there may be more subtle changes. Segwit was not
> > "detectable" in that way, for example.
> >
> > This is the reason why I withdrew BIP120. If you have thought about the
> > above, I'd be very interested.
>
> I'm not sure I see the problem. The scriptPubKey is derived directly
> from the address in all cases, which means the unknown witness version
> would have to be committed to in the address itself.
>
> So yeah, I can make a P2SH address with a witness version > 0 and a to
> me unknown pubkey and then fool you into thinking I own it, but I
> don't really see why you'd ultimately care. In other words, if I can
> SPEND funds sent to that address today, I can prove that I can spend
> today, which is the purpose of the tool, I think.
>
> For the case where the witness version HAS been upgraded, the above
> still applies, but I'm not sure it's a big issue. And it doesn't
> really require an old node. I just need to set witness version >
> current witness version and the problem applies to all nodes.
>
> On Wed, Mar 14, 2018 at 8:36 AM, Luke Dashjr <luke at dashjr.org> wrote:
> > I don't see a need for a new RPC interface, just a new signature format.
>
> All right.
>
> > Ideally, it should support not only just "proof I receive at this
> address",
> > but also "proof of funds" (as a separate feature) since this is a popular
> > misuse of the current message signing (which doesn't actually prove
> funds at
> > all). To do this, it needs to be capable of signing for multiple inputs.
>
> I assume by inputs you mean addresses/keys. The address field could
> optionally be an array. That'd be enough?
>
> > Preferably, it should also avoid disclosing the public key for existing
> or
> > future UTXOs. But I don't think it's possible to avoid this without
> something
> > MAST-like first. Perhaps it can be a MAST upgrade later on, but the new
> > signature scheme should probably be designed with it in mind.
>
> I'd love to not have to reveal the public key, but I'm not sure how it
> would be done, even with MAST.
>
> On Wed, Mar 14, 2018 at 12:12 PM, Anthony Towns <aj at erisian.com.au> wrote:
> > Wouldn't it be sufficient for old nodes to check for standardness of the
> spending script and report non-standard scripts as either invalid outright,
> or at least highly questionable? That should prevent confusion as long as
> soft forks are only making nonstandard behaviours invalid.
>
> That seems sensible to me. A warning would probably be useful, in case
> the verifier is running old software.
>
> -Kalle.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180314/0281bc39/attachment-0001.html>

From eric at voskuil.org  Thu Mar 15 09:17:22 2018
From: eric at voskuil.org (Eric Voskuil)
Date: Thu, 15 Mar 2018 10:17:22 +0100
Subject: [bitcoin-dev] version.relay behavior change
In-Reply-To: <620d4b5e-61c4-4501-9787-c73109908418@achow101.com>
References: <e2fd3226-91ff-d0ca-67c7-2c4a98c6628f@voskuil.org>
	<620d4b5e-61c4-4501-9787-c73109908418@achow101.com>
Message-ID: <8C660724-A76D-44C1-9140-AD3215768CE1@voskuil.org>

Thanks for the reply Andrew. I?ve reviewed the relevant Core sources and I do not see any problem. We have also synced against a Core node locally and not seen the problem.

The reason I suspected it was Core is that it is very common and all of the User Agents are consistent (with an occasional exception for forked nodes). So there?s no easy way to determine what sort of nodes we are seeing. 

We tend to cycle through many more connections during sync than a Core node, so may just be seeing it more frequently, but I assume Core would log this behavior as well. Even so, seeing that wouldn?t help much. I?m as certain as I can be at this point that we are setting the flag and version correctly (and that we do not set bip37 filters).

This behavior started infrequently with 0.14.0 peers and has become more common over time. Just wondering at this point what fork would report as Core and be that common? We used to drop peers that did this (for protocol noncompliance), and I?m considering reinstating that behavior.

e

> On Mar 9, 2018, at 16:33, Andrew Chow via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
> 
> Looking through the code, I don't think that this behavior has changed. Are you sure that you are actually connected to Satoshi:0.15.0 nodes and not a node that has simply set their user-agent to that (i.e. not a real Satoshi:0.15.0 node)?
> 
> If what you are seeing is true, it is likely a bug and not an intentional change. In that case, can you provide specific details on how to reproduce?
> 
> Andrew
> 
>> On 03/09/2018 02:50 AM, Eric Voskuil via bitcoin-dev wrote:
>> /Satoshi:0.15.0/ and later nodes appear to be no longer honoring the
>> version.relay=false flag (BIP37). Could someone familiar with the change
>> please explain the rational?
>> 
>> Thanks,
>> 
>> e
>> 
>> 
>> 
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180315/40bc2617/attachment-0001.html>

From willtech at live.com.au  Thu Mar 15 10:15:17 2018
From: willtech at live.com.au (Damian Williamson)
Date: Thu, 15 Mar 2018 10:15:17 +0000
Subject: [bitcoin-dev] {sign|verify}message replacement
In-Reply-To: <201803141236.48869.luke@dashjr.org>
References: <CALJw2w5=g-FL+MZ08DEoLxVzOKbSXeKu50drE1b4P0JZJpdTyA@mail.gmail.com>,
	<201803141236.48869.luke@dashjr.org>
Message-ID: <PS2P216MB0179B77615F7FCD64EDFDDB09DD00@PS2P216MB0179.KORP216.PROD.OUTLOOK.COM>

That is very helpful Luke. I would not have been concerned if it was necessary to sign multiple times for multiple utxo's on different addresses but, since it is a feature it may as well be best usable. Signing for multiple inputs verifying that you have the priv key for each in your wallet is certainly usable for this popular misuse.


>Ideally, it should support not only just "proof I receive at this address",
but also "proof of funds" (as a separate feature) since this is a popular
misuse of the current message signing (which doesn't actually prove funds at
all). To do this, it needs to be capable of signing for multiple inputs.

________________________________
From: bitcoin-dev-bounces at lists.linuxfoundation.org <bitcoin-dev-bounces at lists.linuxfoundation.org> on behalf of Luke Dashjr via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>
Sent: Wednesday, 14 March 2018 11:36:47 PM
To: Karl Johan Alm; Bitcoin Protocol Discussion
Subject: Re: [bitcoin-dev] {sign|verify}message replacement

I don't see a need for a new RPC interface, just a new signature format.

Ideally, it should support not only just "proof I receive at this address",
but also "proof of funds" (as a separate feature) since this is a popular
misuse of the current message signing (which doesn't actually prove funds at
all). To do this, it needs to be capable of signing for multiple inputs.

Preferably, it should also avoid disclosing the public key for existing or
future UTXOs. But I don't think it's possible to avoid this without something
MAST-like first. Perhaps it can be a MAST upgrade later on, but the new
signature scheme should probably be designed with it in mind.

Luke


On Wednesday 14 March 2018 8:09:20 AM Karl Johan Alm via bitcoin-dev wrote:
> Hello,
>
> I am considering writing a replacement for the message signing tools
> that are currently broken for all but the legacy 1xx addresses. The
> approach (suggested by Pieter Wuille) is to do a script based
> approach. This does not seem to require a lot of effort for
> implementing in Bitcoin Core*. Below is my proposal for this system:
>
> A new structure SignatureProof is added, which is a simple scriptSig &
> witnessProgram container that can be serialized. This is passed out
> from/into the signer/verifier.
>
> RPC commands:
>
> sign <address> <message> [<prehashed>=false]
>
> Generates a signature proof for <message> using the same method that
> would be used to spend coins sent to <address>.**
>
> verify <address> <message> <proof> [<prehashed>=false]
>
> Deserializes and executes the proof using a custom signature checker
> whose sighash is derived from <message>. Returns true if the check
> succeeds, and false otherwise. The scriptPubKey is derived directly
> from <address>.**
>
> Feedback welcome.
>
> -Kalle.
>
> (*) Looks like you can simply use VerifyScript with a new signature
> checker class. (h/t Nicolas Dorier)
> (**) If <prehashed> is true, <message> is the sighash, otherwise
> sighash=sha256d(message).
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
_______________________________________________
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180315/8fdff9f2/attachment.html>

From luke at dashjr.org  Thu Mar 15 14:14:04 2018
From: luke at dashjr.org (Luke Dashjr)
Date: Thu, 15 Mar 2018 14:14:04 +0000
Subject: [bitcoin-dev] {sign|verify}message replacement
In-Reply-To: <CALJw2w5NQ=WX1Cm4aUXMN=uxn8NLHAfYDLEtqUptce5DCWZXWA@mail.gmail.com>
References: <CALJw2w5=g-FL+MZ08DEoLxVzOKbSXeKu50drE1b4P0JZJpdTyA@mail.gmail.com>
	<201803141236.48869.luke@dashjr.org>
	<CALJw2w5NQ=WX1Cm4aUXMN=uxn8NLHAfYDLEtqUptce5DCWZXWA@mail.gmail.com>
Message-ID: <201803151414.06301.luke@dashjr.org>

On Thursday 15 March 2018 7:36:48 AM Karl Johan Alm wrote:
> On Wed, Mar 14, 2018 at 12:36 PM, Luke Dashjr <luke at dashjr.org> wrote:
> > Ideally, it should support not only just "proof I receive at this
> > address", but also "proof of funds" (as a separate feature) since this
> > is a popular misuse of the current message signing (which doesn't
> > actually prove funds at all). To do this, it needs to be capable of
> > signing for multiple inputs.
> 
> Re-reading this, I think what you mean is it should be possible to
> create a proof for (a) specific UTXO(s), hence "inputs". That sounds
> pretty useful, yeah!

Not necessarily specific UTXOs (that would contradict fungibility, as well as 
be impossible for hot/cold wallet separation), but just to prove funds are 
available. The current sign message cannot be used to prove present possession 
of funds, only that you receive funds.

From achow101-lists at achow101.com  Thu Mar 15 15:44:43 2018
From: achow101-lists at achow101.com (Andrew Chow)
Date: Thu, 15 Mar 2018 11:44:43 -0400
Subject: [bitcoin-dev] version.relay behavior change
In-Reply-To: <8C660724-A76D-44C1-9140-AD3215768CE1@voskuil.org>
References: <e2fd3226-91ff-d0ca-67c7-2c4a98c6628f@voskuil.org>
	<620d4b5e-61c4-4501-9787-c73109908418@achow101.com>
	<8C660724-A76D-44C1-9140-AD3215768CE1@voskuil.org>
Message-ID: <1659f63f-5003-40d5-85a9-11e7a8f34edb@achow101.com>

I don't think the nodes that you are connecting to that have this
behavior are actually forked from Bitcoin Core. It seems more like fake
nodes - nodes that don't actually do any verification or follow the
protocol. Such fake nodes can set whatever user agent they want, common
ones being Bitcoin Core's user agents.

IMO your best solution would be to drop peers for protocol noncompliance.

Andrew


On 03/15/2018 05:17 AM, Eric Voskuil wrote:
> Thanks for the reply Andrew. I?ve reviewed the relevant Core sources
> and I do not see any problem. We have also synced against a Core node
> locally and not seen the problem.
>
> The reason I suspected it was Core is that it is very common and all
> of the User Agents are consistent (with an occasional exception for
> forked nodes). So there?s no easy way to determine what sort of nodes
> we are seeing.?
>
> We tend to cycle through many more connections during sync than a Core
> node, so may just be seeing it more frequently, but I assume Core
> would log this behavior as well. Even so, seeing that wouldn?t help
> much. I?m as certain as I can be at this point that we are setting the
> flag and version correctly (and that we do not set bip37 filters).
>
> This behavior started infrequently with 0.14.0 peers and has become
> more common over time. Just wondering at this point what fork would
> report as Core and be that common? We used to drop peers that did this
> (for protocol noncompliance), and I?m considering reinstating that
> behavior.
>
> e
>
> On Mar 9, 2018, at 16:33, Andrew Chow via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org
> <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:
>
>> Looking through the code, I don't think that this behavior has
>> changed. Are you sure that you are actually connected to
>> Satoshi:0.15.0 nodes and not a node that has simply set their
>> user-agent to that (i.e. not a real Satoshi:0.15.0 node)?
>>
>> If what you are seeing is true, it is likely a bug and not an
>> intentional change. In that case, can you provide specific details on
>> how to reproduce?
>>
>> Andrew
>>
>>
>> On 03/09/2018 02:50 AM, Eric Voskuil via bitcoin-dev wrote:
>>> /Satoshi:0.15.0/ and later nodes appear to be no longer honoring the
>>> version.relay=false flag (BIP37). Could someone familiar with the change
>>> please explain the rational?
>>>
>>> Thanks,
>>>
>>> e
>>>
>>>
>>>
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev at lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> <mailto:bitcoin-dev at lists.linuxfoundation.org>
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180315/831fdf48/attachment-0001.html>

From jim.posen at gmail.com  Thu Mar 15 20:53:34 2018
From: jim.posen at gmail.com (Jim Posen)
Date: Thu, 15 Mar 2018 13:53:34 -0700
Subject: [bitcoin-dev] {sign|verify}message replacement
In-Reply-To: <CALJw2w7LAUB+7FpLSGz=+pL7y_t5sy0yGuhYML7yJjuKc8Uw_w@mail.gmail.com>
References: <CALJw2w5=g-FL+MZ08DEoLxVzOKbSXeKu50drE1b4P0JZJpdTyA@mail.gmail.com>
	<CAPswA9xuVT74L87QO9TXGc6=O6Gd2kbQMBdmn=7zUm5OHXcfOA@mail.gmail.com>
	<CALJw2w719qQnyvaJbe1wc39+4ERDST+zXCOjt0DiJpktD74QCA@mail.gmail.com>
	<CADZtCSiSCDb1dzLCgr24jCNzzmcfsXuPyNVh+YJJ6rNqdsQh2Q@mail.gmail.com>
	<CALJw2w7LAUB+7FpLSGz=+pL7y_t5sy0yGuhYML7yJjuKc8Uw_w@mail.gmail.com>
Message-ID: <CADZtCSj8HJm+7DswrPXPibSGGSAb4xE4S=AT9ur_v0HDmCp1Nw@mail.gmail.com>

>
> Good question.. Since you don't really have the input(s), I think it's
> fine to always assume sufficient time/height on CLTV/CSV checks.
>

In this general signing-a-script context, I think a verifier might want to
see the time conditions under which it may be spent. The proof container
could include an optional nLockTime which defaults to 0 and nSequence which
defaults to 0xFFFF...


> I think it would just use the default (SIGHASH_ALL?) for simplicity.
> Is there a good reason to tweak it?
>

I took another look and there should definitely be a byte appended to the
end of the sig so that the encoding checks pass, but I think it might as
well be a 0x00 byte since it's not actually a sighash flag.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180315/6046c7a8/attachment.html>

From karljohan-alm at garage.co.jp  Fri Mar 16 00:38:06 2018
From: karljohan-alm at garage.co.jp (Karl Johan Alm)
Date: Fri, 16 Mar 2018 00:38:06 +0000
Subject: [bitcoin-dev] {sign|verify}message replacement
In-Reply-To: <201803151414.06301.luke@dashjr.org>
References: <CALJw2w5=g-FL+MZ08DEoLxVzOKbSXeKu50drE1b4P0JZJpdTyA@mail.gmail.com>
	<201803141236.48869.luke@dashjr.org>
	<CALJw2w5NQ=WX1Cm4aUXMN=uxn8NLHAfYDLEtqUptce5DCWZXWA@mail.gmail.com>
	<201803151414.06301.luke@dashjr.org>
Message-ID: <CALJw2w49bsBNFBj3MvdzNL+Hu3zuq-dmrTv_6wgmO_ZaCXt1nA@mail.gmail.com>

On Thu, Mar 15, 2018 at 2:14 PM, Luke Dashjr <luke at dashjr.org> wrote:
> Not necessarily specific UTXOs (that would contradict fungibility, as well as
> be impossible for hot/cold wallet separation), but just to prove funds are
> available. The current sign message cannot be used to prove present possession
> of funds, only that you receive funds.

By saying "not necessarily specific UTXOs", are you saying it may be
spent outputs? I'm a little confused I think.

On Thu, Mar 15, 2018 at 8:53 PM, Jim Posen <jim.posen at gmail.com> wrote:
> In this general signing-a-script context, I think a verifier might want to
> see the time conditions under which it may be spent. The proof container
> could include an optional nLockTime which defaults to 0 and nSequence which
> defaults to 0xFFFF...

Good point!

>> I think it would just use the default (SIGHASH_ALL?) for simplicity.
>> Is there a good reason to tweak it?
>
> I took another look and there should definitely be a byte appended to the
> end of the sig so that the encoding checks pass, but I think it might as
> well be a 0x00 byte since it's not actually a sighash flag.

I think the sighash flag affects the outcome of the actual
verification, but I could be mistaken.

-Kalle.

From gsanders87 at gmail.com  Fri Mar 16 01:59:45 2018
From: gsanders87 at gmail.com (Greg Sanders)
Date: Thu, 15 Mar 2018 21:59:45 -0400
Subject: [bitcoin-dev] {sign|verify}message replacement
In-Reply-To: <CALJw2w49bsBNFBj3MvdzNL+Hu3zuq-dmrTv_6wgmO_ZaCXt1nA@mail.gmail.com>
References: <CALJw2w5=g-FL+MZ08DEoLxVzOKbSXeKu50drE1b4P0JZJpdTyA@mail.gmail.com>
	<201803141236.48869.luke@dashjr.org>
	<CALJw2w5NQ=WX1Cm4aUXMN=uxn8NLHAfYDLEtqUptce5DCWZXWA@mail.gmail.com>
	<201803151414.06301.luke@dashjr.org>
	<CALJw2w49bsBNFBj3MvdzNL+Hu3zuq-dmrTv_6wgmO_ZaCXt1nA@mail.gmail.com>
Message-ID: <CAB3F3Dv=xk68sx5c=9vKL9ynAVJoEY_hnbuZQ0tSe=7cBdWSVA@mail.gmail.com>

Sorry if I missed the rationale earlier, but why not just do a transaction,
with a FORKID specifically for this? Then a node can have a mempool
acceptance test that returns true even if the signature is not valid as per
Bitcoin consensus, but only due to the FORKID?

This way basically any wallet can support this provided generic FORKID
support.

On Thu, Mar 15, 2018 at 8:38 PM, Karl Johan Alm via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> On Thu, Mar 15, 2018 at 2:14 PM, Luke Dashjr <luke at dashjr.org> wrote:
> > Not necessarily specific UTXOs (that would contradict fungibility, as
> well as
> > be impossible for hot/cold wallet separation), but just to prove funds
> are
> > available. The current sign message cannot be used to prove present
> possession
> > of funds, only that you receive funds.
>
> By saying "not necessarily specific UTXOs", are you saying it may be
> spent outputs? I'm a little confused I think.
>
> On Thu, Mar 15, 2018 at 8:53 PM, Jim Posen <jim.posen at gmail.com> wrote:
> > In this general signing-a-script context, I think a verifier might want
> to
> > see the time conditions under which it may be spent. The proof container
> > could include an optional nLockTime which defaults to 0 and nSequence
> which
> > defaults to 0xFFFF...
>
> Good point!
>
> >> I think it would just use the default (SIGHASH_ALL?) for simplicity.
> >> Is there a good reason to tweak it?
> >
> > I took another look and there should definitely be a byte appended to the
> > end of the sig so that the encoding checks pass, but I think it might as
> > well be a 0x00 byte since it's not actually a sighash flag.
>
> I think the sighash flag affects the outcome of the actual
> verification, but I could be mistaken.
>
> -Kalle.
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180315/e55106e6/attachment.html>

From karljohan-alm at garage.co.jp  Fri Mar 16 02:04:51 2018
From: karljohan-alm at garage.co.jp (Karl Johan Alm)
Date: Fri, 16 Mar 2018 02:04:51 +0000
Subject: [bitcoin-dev] {sign|verify}message replacement
In-Reply-To: <CAB3F3Dv=xk68sx5c=9vKL9ynAVJoEY_hnbuZQ0tSe=7cBdWSVA@mail.gmail.com>
References: <CALJw2w5=g-FL+MZ08DEoLxVzOKbSXeKu50drE1b4P0JZJpdTyA@mail.gmail.com>
	<201803141236.48869.luke@dashjr.org>
	<CALJw2w5NQ=WX1Cm4aUXMN=uxn8NLHAfYDLEtqUptce5DCWZXWA@mail.gmail.com>
	<201803151414.06301.luke@dashjr.org>
	<CALJw2w49bsBNFBj3MvdzNL+Hu3zuq-dmrTv_6wgmO_ZaCXt1nA@mail.gmail.com>
	<CAB3F3Dv=xk68sx5c=9vKL9ynAVJoEY_hnbuZQ0tSe=7cBdWSVA@mail.gmail.com>
Message-ID: <CALJw2w5o5ykPA-iR==6zavYGH_MLWeqMzB2gH-bEho0+JUQvAQ@mail.gmail.com>

On Fri, Mar 16, 2018 at 1:59 AM, Greg Sanders <gsanders87 at gmail.com> wrote:
> Sorry if I missed the rationale earlier, but why not just do a transaction,
> with a FORKID specifically for this? Then a node can have a mempool
> acceptance test that returns true even if the signature is not valid as per
> Bitcoin consensus, but only due to the FORKID?
>
> This way basically any wallet can support this provided generic FORKID
> support.

You'd basically have to provide an entire transaction rather than just
the signature, so there's some overhead. (Copy-pasting may become
unwieldy quicker.)

From eric at voskuil.org  Fri Mar 16 08:27:47 2018
From: eric at voskuil.org (Eric Voskuil)
Date: Fri, 16 Mar 2018 09:27:47 +0100
Subject: [bitcoin-dev] version.relay behavior change
In-Reply-To: <1659f63f-5003-40d5-85a9-11e7a8f34edb@achow101.com>
References: <e2fd3226-91ff-d0ca-67c7-2c4a98c6628f@voskuil.org>
	<620d4b5e-61c4-4501-9787-c73109908418@achow101.com>
	<8C660724-A76D-44C1-9140-AD3215768CE1@voskuil.org>
	<1659f63f-5003-40d5-85a9-11e7a8f34edb@achow101.com>
Message-ID: <E756D0FC-F6E1-4DD8-8F96-E144E3D88E1D@voskuil.org>

Agree, thanks for the input Andrew.

e

> On Mar 15, 2018, at 16:44, Andrew Chow <achow101-lists at achow101.com> wrote:
> 
> I don't think the nodes that you are connecting to that have this behavior are actually forked from Bitcoin Core. It seems more like fake nodes - nodes that don't actually do any verification or follow the protocol. Such fake nodes can set whatever user agent they want, common ones being Bitcoin Core's user agents.
> 
> IMO your best solution would be to drop peers for protocol noncompliance.
> 
> Andrew
> 
>> On 03/15/2018 05:17 AM, Eric Voskuil wrote:
>> Thanks for the reply Andrew. I?ve reviewed the relevant Core sources and I do not see any problem. We have also synced against a Core node locally and not seen the problem.
>> 
>> The reason I suspected it was Core is that it is very common and all of the User Agents are consistent (with an occasional exception for forked nodes). So there?s no easy way to determine what sort of nodes we are seeing. 
>> 
>> We tend to cycle through many more connections during sync than a Core node, so may just be seeing it more frequently, but I assume Core would log this behavior as well. Even so, seeing that wouldn?t help much. I?m as certain as I can be at this point that we are setting the flag and version correctly (and that we do not set bip37 filters).
>> 
>> This behavior started infrequently with 0.14.0 peers and has become more common over time. Just wondering at this point what fork would report as Core and be that common? We used to drop peers that did this (for protocol noncompliance), and I?m considering reinstating that behavior.
>> 
>> e
>> 
>> On Mar 9, 2018, at 16:33, Andrew Chow via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
>> 
>>> Looking through the code, I don't think that this behavior has changed. Are you sure that you are actually connected to Satoshi:0.15.0 nodes and not a node that has simply set their user-agent to that (i.e. not a real Satoshi:0.15.0 node)?
>>> 
>>> If what you are seeing is true, it is likely a bug and not an intentional change. In that case, can you provide specific details on how to reproduce?
>>> 
>>> Andrew
>>> 
>>>> On 03/09/2018 02:50 AM, Eric Voskuil via bitcoin-dev wrote:
>>>> /Satoshi:0.15.0/ and later nodes appear to be no longer honoring the
>>>> version.relay=false flag (BIP37). Could someone familiar with the change
>>>> please explain the rational?
>>>> 
>>>> Thanks,
>>>> 
>>>> e
>>>> 
>>>> 
>>>> 
>>>> _______________________________________________
>>>> bitcoin-dev mailing list
>>>> bitcoin-dev at lists.linuxfoundation.org
>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>> 
>>> _______________________________________________
>>> bitcoin-dev mailing list
>>> bitcoin-dev at lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180316/9d42def7/attachment-0001.html>

From willtech at live.com.au  Sun Mar 18 01:34:20 2018
From: willtech at live.com.au (Damian Williamson)
Date: Sun, 18 Mar 2018 01:34:20 +0000
Subject: [bitcoin-dev] feature: Enhance privacy by change obfuscation
Message-ID: <PS2P216MB0179FCA8077AAE946BFA2D069DD50@PS2P216MB0179.KORP216.PROD.OUTLOOK.COM>

Application: Bitcoin Core

Feature: Enhanced privacy by change obfuscation

Operation: Provide a user selectable 'Enhanced privacy' option for transaction creation, when true the transaction randomly distributes change across up to twenty output addresses (minimum five?), provided each output is not dust.

Suggestions: Perhaps limit the total random number of addresses to distribute to by change amount. Optionally: If necessary, additional inputs can be selected if available to increase change although consider if this may eventually result in a decrease in obfuscation in some cases when the outputs are spent.

Issues: Transaction cost will be higher for the initial spend with the change due to increased outputs and, possibly for later spending the change depending on the future spend amount(s) and the number of inputs required.

Argument: If transaction linkage is possible, it is still possible with the obfuscated change but, it is far more difficult to guess what was retained by the owner of the originating utxo's unless the new change outputs are spent together in the same transaction.


Regards,

Damian Williamson
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180318/f91f0fd9/attachment.html>

From evan at eklitzke.org  Sun Mar 18 05:50:34 2018
From: evan at eklitzke.org (Evan Klitzke)
Date: Sat, 17 Mar 2018 22:50:34 -0700
Subject: [bitcoin-dev] feature: Enhance privacy by change obfuscation
In-Reply-To: <PS2P216MB0179FCA8077AAE946BFA2D069DD50@PS2P216MB0179.KORP216.PROD.OUTLOOK.COM>
References: <PS2P216MB0179FCA8077AAE946BFA2D069DD50@PS2P216MB0179.KORP216.PROD.OUTLOOK.COM>
Message-ID: <874lleexud.fsf@eklitzke.org>


Damian Williamson via bitcoin-dev 
<bitcoin-dev at lists.linuxfoundation.org> writes:
> Operation: Provide a user selectable 'Enhanced privacy' option for
> transaction creation, when true the transaction randomly distributes
> change across up to twenty output addresses (minimum five?), provided
> each output is not dust.

This would be really expensive for the network due to the bloat in UTXO
size, a cost everyone has to pay for. Not to mention the fact that it
doesn't really seem that private, as the wallet is likely going to have
to rejoin those inputs in future transactions (and the user will have to
pay a high transaction fee as a result).

--
Evan Klitzke
https://eklitzke.org/

From willtech at live.com.au  Sun Mar 18 07:07:58 2018
From: willtech at live.com.au (Damian Williamson)
Date: Sun, 18 Mar 2018 07:07:58 +0000
Subject: [bitcoin-dev] feature: Enhance privacy by change obfuscation
In-Reply-To: <874lleexud.fsf@eklitzke.org>
References: <PS2P216MB0179FCA8077AAE946BFA2D069DD50@PS2P216MB0179.KORP216.PROD.OUTLOOK.COM>,
	<874lleexud.fsf@eklitzke.org>
Message-ID: <PS2P216MB017905271ED76F964F4615829DD50@PS2P216MB0179.KORP216.PROD.OUTLOOK.COM>

Alright, but even if two (or more) of the change outputs were linked in a future transaction, no-one can tell if they are still linked to your wallet or not unless there is also an additional re-used address on the new transaction input side that has also been previously linked to one of the inputs on the transaction creating the change.


Yes, I understand the additional cost but still thought it worthy of consideration.


Regards,

Damian Williamson

________________________________
From: Evan Klitzke <evan at eklitzke.org>
Sent: Sunday, 18 March 2018 4:50:34 PM
To: Damian Williamson; Bitcoin Protocol Discussion
Subject: Re: [bitcoin-dev] feature: Enhance privacy by change obfuscation


Damian Williamson via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> writes:
> Operation: Provide a user selectable 'Enhanced privacy' option for
> transaction creation, when true the transaction randomly distributes
> change across up to twenty output addresses (minimum five?), provided
> each output is not dust.

This would be really expensive for the network due to the bloat in UTXO
size, a cost everyone has to pay for. Not to mention the fact that it
doesn't really seem that private, as the wallet is likely going to have
to rejoin those inputs in future transactions (and the user will have to
pay a high transaction fee as a result).

--
Evan Klitzke
https://eklitzke.org/
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180318/adae0170/attachment.html>

From aj at erisian.com.au  Wed Mar 21 04:06:18 2018
From: aj at erisian.com.au (Anthony Towns)
Date: Wed, 21 Mar 2018 14:06:18 +1000
Subject: [bitcoin-dev] Soft-forks and schnorr signature aggregation
Message-ID: <20180321040618.GA4494@erisian.com.au>

Hello world,

There was a lot of discussion on Schnorr sigs and key and signature
aggregation at the recent core-dev-tech meeting (one relevant conversation
is transcribed at [0]).

Quick summary, with more background detail in the corresponding footnotes:
signature aggregation is awesome [1], and the possibility of soft-forking
in new opcodes via OP_RETURN_VALID opcodes (instead of OP_NOP) is also
awesome [2].

Unfortunately doing both of these together may turn out to be awful.

RETURN_VALID and Signature Aggregation
--------------------------------------

Bumping segwit script versions and redefining OP_NOP opcodes are
fairly straightforward to deal with even with signature aggregation,
the straightforward implementation of both combined is still a soft-fork.

RETURN_VALID, unfortunately, has a serious potential pitfall: any
aggregatable signature operations that occur after it have to go into
separate buckets.

As an example of why this is the case, imagine introducing a covenant
opcode that pulls a potentially complicated condition from the stack
(perhaps, "an output pays at least 50000 satoshi to address xyzzy"),
checks the condition against the transaction, and then pushes 1 (or 0)
back onto the stack indicating compliance with the covenant (or not).
You might then write a script allowing a single person to spend the coins
if they comply with the covenant, and allow breaking the covenant with
someone else's sign-off in addition. You could write this as:

  pubkey1 CHECKSIGVERIFY
  cond CHECKCOVENANT IFDUP NOTIF pubkey2 CHECKSIG ENDIF

If you pass the covenant, you supply "SIGHASHALL|BUCKET_1" and aggregate
the signature for pubkey1 into bucket1 and you're set; otherwise you supply
"SIGHASHALL|BUCKET_1 SIGHASHALL|BUCKET_1" and aggregate signatures for both
pubkey1 and pubkey2 into bucket1 and you're set. Great!

But this isn't a soft-fork: old nodes would see this script as:

  pubkey1 CHECKSIGVERIFY
  cond RETURN_VALID IFDUP NOTIF pubkey2 CHECKSIG ENDIF

which it would just interpret as:

  pubkey1 CHECKSIGVERIFY cond RETURN_VALID

which is fine if the covenant was passing; but no good if the covenant
didn't pass -- they'd be expecting the aggregted sig to just be for
pubkey1 when it's actually pubkey1+pubkey2, so old nodes would fail the
tx and new nodes would accept it, making it a hard fork.

Solution 0a / 0b
----------------

There are two obvious solutions here:

 0a) Just be very careful to ensure any aggregated signatures that
     are conditional on an redefined RETURN_VALID opcode go into later
     buckets, but be careful about having separate sets of buckets every
     time a soft-fork introduces a new redefined opcode. Probably very
     complicated to implement correctly, and essentially doubles the
     number of buckets you have to potentially deal with every time you
     soft fork in a new opcode.

 0b) Alternatively, forget about the hope that RETURN_VALID
     opcodes could be converted to anything, and just reserve OP_NOP
     opcodes and convert them to CHECK_foo_VERIFY opcodes just as we
     have been doing, and when we can't do that bump the segwit witness
     version for a whole new version of script. Or in twitter speak:
     "non-verify upgrades should be done with new script versions" [3]

I think with a little care we can actually salvage RETURN_VALID though!

Solution 1
----------

You don't actually have to write your scripts in ways that can cause
this problem, as long as you're careful. In particular, the problem only
occurs if you do aggregatable CHECKSIG operations after "RETURN_VALID"
-- if you do all the CHECKSIGs first, then all nodes will be checking
for the same signatures, and there's no problem. So you could rewrite
the script above as:

  pubkey1 CHECKSIGVERIFY
  IF pubkey2 CHECKSIG ENDIF
  cond CHECKCOVENANT OR

which is redeemable either by:

  sig1 0        [and covenant is met]
  sig1 1 sig2   [covenant is not checked]

The witness in this case is essentially committing to the execution path
that would have been taken in the first script by a fully validating node,
then the new script checks all the signatures, and then validates that the
committed execution path was in fact the one that was meant to be taken.

If people are clever enough to write scripts this way, I believe you
can make RETURN_VALID soft-fork safe simply by having every soft-forked
RETURN_VALID operation set a state flag that makes every subsequent
CHECKSIG operation require a non-aggregated sig.

The drawback of this approach is that if the script is complicated
(eg it has multiple IF conditions, some of which are nested), it may be
difficult to write the script to ensure the signatures are checked in the
same combination as the later logic actually requires -- you might have
to store the flag indicating whether you checked particular signatures
on the altstack, or use DUP and PICK/ROLL to organise it on the stack.

Solution 2
----------

We could make that simpler for script authors by making dedicated opcodes
to help with "do all the signatures first" and "check the committed
execution path against reality" steps. I think a reasonable approach
would be something like:

  0b01 pubkey2 pubkey1 2 CHECK_AGGSIG_VERIFY
  cond CHECKCOVENANT 0b10 CHECK_AGG_SIGNERS OR

which is redeemed either by:

  sighash1 0   [and passing the covenant cond]
  sighash2 sighash1 0b10

(I'm using the notation 0b10110 to express numbers as binary bitfields;
0b10110 = 22 eg)

That is, two new opcodes, namely:

CHECK_AGGSIG_VERIFY which takes from the stack:
    - N: a count of pubkeys
    - pubkey1..pubkeyN: N pubkeys
    - REQ: a bitmask of which pubkeys are required to sign
    - OPT: a bitmask of which optional pubkeys have signed
    - sighashes: M sighashes for the pubkeys corresponding to the set
      bits of (REQ|OPT)

  CHECK_AGGSIG_VERIFY fails if:
    - the stack doesn't have enough elements
    - the aggregated signature doesn't pass
    - a redefined RETURN_VALID opcode has already been seen
    - a previous CHECK_AGGSIG_VERIFY has already been seen in this script

  REQ|OPT is stored as state

CHECK_AGG_SIGNERS takes from the stack:
    - B: a bitmask of which pubkeys are being queried
  and it pushes to the stack 1 or 0 based on:
    - (REQ|OPT) & B == B ? 1 : 0

A possible way to make sure the "no agg sigs after an upgraded
RETURN_VALID" behaviour works right might be to have "RETURN_VALID"
fail if CHECK_AGGSIG_VERIFY hasn't already been seen. That way once you
redefine RETURN_VALID in a soft-fork, if you have a CHECK_AGGSIG_VERIFY
after a RETURN_VALID you've either already failed (because the
RETURN_VALID wasn't after a CHECK_AGGSIG_VERIFY), or you automatically
fail (because you've already seen a CHECK_AGGSIG_VERIFY).

There would be no need to make CHECKSIG, CHECKSIGVERIFY, CHECKMULTISIG
and CHECKMULTISIGVERIFY do signature aggregation in this case. They could
be left around to allow script authors to force non-aggregate signatures
or could be dropped entirely, I think.

This construct would let you do M-of-N aggregated multisig in a fairly
straightforward manner without needing an explicit opcode, eg:

  0 pubkey5 pubkey4 pubkey3 pubkey2 pubkey1 5 CHECK_AGGSIG_VERIFY
  0b10000 CHECK_AGG_SIGNERS
  0b01000 CHECK_AGG_SIGNERS ADD
  0b00100 CHECK_AGG_SIGNERS ADD
  0b00010 CHECK_AGG_SIGNERS ADD
  0b00001 CHECK_AGG_SIGNERS ADD
  3 NUMEQUAL

redeemable by, eg:

  0b10110 sighash5 sighash3 sighash2

and a single aggregate signature by the private keys corresponding to
pubkey{2,3,5}.

Of course, another way of getting M-of-N aggregated multisig is via MAST,
which brings us to another approach...

Solution 3
----------

All we're doing above is committing to an execution path and validating
signatures for that path before checking the path was the right one. But
MAST is a great way of committing to an execution path, so another
approach would just be "don't have alternative execution paths, just have
MAST and CHECK/VERIFY codes". Taking the example I've been running with,
that would be:

  branch1: 2 pubkey2 pubkey1 2 CHECKMULTISIG
  branch2: pubkey1 CHECKSIGVERIFY cond CHECKCOVENANT

So long as MAST is already supported when signature aggregation becomes
possible, that works fine. The drawback is MAST can end up with lots of
branches, eg the 3-of-5 multisig check has 10 branches:

  branch1: 3 pubkey3 pubkey2 pubkey1 3 CHECKMULTISIG
  branch2: 3 pubkey4 pubkey2 pubkey1 3 CHECKMULTISIG
  branch3: 3 pubkey5 pubkey2 pubkey1 3 CHECKMULTISIG
  branch4: 3 pubkey4 pubkey3 pubkey1 3 CHECKMULTISIG
  branch5: 3 pubkey5 pubkey3 pubkey1 3 CHECKMULTISIG
  branch6: 3 pubkey5 pubkey4 pubkey1 3 CHECKMULTISIG
  branch7: 3 pubkey4 pubkey3 pubkey2 3 CHECKMULTISIG
  branch8: 3 pubkey5 pubkey3 pubkey2 3 CHECKMULTISIG
  branch9: 3 pubkey5 pubkey4 pubkey2 3 CHECKMULTISIG
  branch10: 3 pubkey5 pubkey4 pubkey3 3 CHECKMULTISIG

while if you want, say, 6-of-11 multisig you get 462 branches, versus
just:

  0 pubkey11 pubkey10 pubkey9 pubkey8 pubkey7 pubkey6
    pubkey5 pubkey4 pubkey3 pubkey2 pubkey1 11 CHECK_AGGSIG_VERIFY
  0b10000000000 CHECK_AGG_SIGNERS
  0b01000000000 CHECK_AGG_SIGNERS ADD
  0b00100000000 CHECK_AGG_SIGNERS ADD
  0b00010000000 CHECK_AGG_SIGNERS ADD
  0b00001000000 CHECK_AGG_SIGNERS ADD
  0b00000100000 CHECK_AGG_SIGNERS ADD
  0b00000010000 CHECK_AGG_SIGNERS ADD
  0b00000001000 CHECK_AGG_SIGNERS ADD
  0b00000000100 CHECK_AGG_SIGNERS ADD
  0b00000000010 CHECK_AGG_SIGNERS ADD
  0b00000000001 CHECK_AGG_SIGNERS ADD
  6 NUMEQUAL

Provided doing lots of hashes to calculate merkle paths is cheaper than
publishing to the blockchain, MAST will likely still be better though:
you'd be doing 6 pubkeys and 9 steps in the merkle path for about 15*32
bytes in MAST, versus showing off all 11 pubkeys above for 11*(32+4)
bytes, and the above is roughly the worst case for m-of-11 multisig
via MAST.

If everyone's happy to use MAST, then it could be the only solution:
drop OP_IF and friends, and require all the CHECKSIG ops to occur before
any RETURN_VALID ops: since there's no branching, that's just a matter of
reordering your script a bit and should be pretty easy for script authors.

I think there's a couple of drawbacks to this approach that it shouldn't
be the only solution:

 a) we don't have a lot of experience with using MAST
 b) MAST is a bit more complicated than just dealing with branches in
    a script (probably solvable once (a) is no longer the case) 
 c) some useful scripts might be a bit cheaper expressed with 
    of branches and be better expressed without MAST

If other approaches than MAST are still desirable, then MAST works fine
in combination with either of the earlier solutions as far as I can see.

Summary
-------

I think something along the lines of solution 2 makes the most sense,
so I think a good approach for aggregate signatures is:

 - introduce a new segwit witness version, which I'll call v2 (but which
   might actually be v1 or v3 etc, of course)

 - v2 must support Schnorr signature verification.

 - v2 should have a "pay to public key (hash?)" witness format. direct
   signatures of the transaction via the corresponding private key should
   be aggregatable.

 - v2 should have a "pay to script hash" witness format: probably via
   taproot+MAST, possibly via graftroot as well

 - v2 should support MAST scripts: again, probably via taproot+MAST

 - v2 taproot shouldn't have a separate script version (ie,
   the pubkey shouldn't be P+H(P,version,scriptroot)), as signatures
   for later-versioned scripts couldn't be aggregated, so there's no
   advantage over bumping the segwit witness version

 - v2 scripts should have a CHECK_AGG_SIG_VERIFY opcode roughly as
   described above for aggregating signatures, along with CHECK_AGG_SIGNERS

 - CHECK{MULTI,}SIG{VERIFY,} in v2 scripts shouldn't support aggregated
   signatures, and possibly shouldn't be present at all?

 - v2 signers should be able to specify an aggregation bucket for each
   signature, perhaps in the range 0-7 or so?

 - v2 scripts should have a bunch of RETURN_VALID opcodes for future
   soft-forks, constrained so that CHECK_AGG_SIG_VERIFY doesn't appear
   after them. the currently disabled opcodes should be redefined as
   RETURN_VALID eg.

For soft-fork upgrades from that point:

 - introducing new opcodes just means redefining an RETURN_VALID opcode

 - introducing new sighash versions requires bumping the segwit witness
   version (to v3, etc)

 - if non-interactive half-signature aggregation isn't ready to go, it
   would likewise need a bump in the segwit witness version when
   introduced

I think it's worth considering bundling a hard-fork upgrade something
like:

 - ~5 years after v2 scripts are activated, existing p2pk/p2pkh UTXOs
   (either matching the pre-segwit templates or v0 segwit p2wpkh) can
   be spent via a v2-aggregated-signature (but not via taproot)
   [4]

 - core will maintain a config setting that allows users to prevent
   that hard fork from activating via UASF up until the next release
   after activation (probably with UASF-enforced miner-signalling that
   the hard-fork will not go ahead)

This is already very complicated of course, but note that there's still
*more* things that need to be considered for signature aggregation:

 - whether to use Bellare-Neven or muSig in the consensus-critical
   aggregation algorithm

 - whether to assign the aggregate sigs to inputs and plunk them in the
   witness data somewhere, or to add a new structure and commitment and
   worry about p2p impact

 - whether there are new sighash options that should go in at the same time

 - whether non-interactive half-sig aggregation can go in at the same time

That leads me to think that interactive signature aggregation is going to
take a lot of time and work, and it would make sense to do a v1-upgrade
that's "just" Schnorr (and taproot and MAST and re-enabling opcodes and
...) in the meantime. YMMV.

Cheers,
aj

[0] http://diyhpl.us/wiki/transcripts/bitcoin-core-dev-tech/2018-03-06-taproot-graftroot-etc/

[1] Signature aggregation:

    Signature aggregation is cool because it lets you post a transaction
    spending many inputs, but only providing a single 64 byte signature
    that proves authorisation by the holders of all the private keys
    for all the inputs. So the witnesses for your inputs might be:

     p2wpkh: pubkey1 SIGHASH_ALL
     p2wpkh: pubkey2 SIGHASH_ALL
     p2wsh: "3 pubkey1 pubkey3 pubkey4 3 CHECKMULTISIG" SIGHASH_ALL SIGHASH_ALL SIGHASH_ALL

    where instead of including full 65-byte signature for each CHECKSIG
    operation in each input witness, you just include the ~1-byte sighash,
    and provide a single 64-byte signature elsewhere, calculated either
    according to the Bellare-Neven algorithm, or the muSig algorithm.

    In the above case, that means going from about 500 witness bytes
    for 5 public keys and 5 signatures, to about 240 witness bytes for
    5 public keys and just 1 signature.

    A complication here is that because the signatures are aggregated,
    in order to validate *any* signature you have to be able to validate
    *every* signature.

    It's possible to limit that a bit, and have aggregation
    "buckets". This might be something you just choose when signing, eg:

     p2wpkh: pubkey1 SIGHASH_ALL|BUCKET_1
     p2wpkh: pubkey2 SIGHASH_ALL|BUCKET_2
     p2wsh: "3 pubkey1 pubkey3 pubkey4 3 CHECKMULTISIG" SIGHASH_ALL|BUCKET_1 SIGHASH_ALL|BUCKET_2 SIGHASH_ALL|BUCKET_2

     bucket1: 64 byte sig for (pubkey1, pubkey1)
     bucket2: 64 byte sig for (pubkey2, pubkey3, pubkey4)

    That way you get the choice to verify both of the pubkey1 signatures
    or all of the pubkey{2,3,4} signatures or all the signatures (or
    none of the signatures).

    This might be useful if the private key for pubkey1 is essentially
    offline, and can't easily participate in an interactive protocol
    -- with separate buckets the separate signatures can be generated
    independently at different times, while with only one bucket,
    everyone has to coordinate to produce the signature)

    (For clarity: each bucket corresponds to many CHECKSIG operations,
    but only contains a single 64-byte signature)

    Different buckets will also be necessary when dealing with new
    segwit script versions: if there are any aggregated signatures for
    v1 addresses that go into bucket X, then aggregate signatures for
    v2 addresses cannot go into bucket X, as that would prevent nodes
    that support v1 addresses but not v2 addresses from validating
    bucket X, which would prevent them from validating the v1 addresses
    corresponding to that bucket, which would make the v2 upgrade a hard
    fork rather than a soft fork. So each segwit version will need to
    introduce a new set of aggregation buckets, which in turn reduces
    the benefit you get from signature aggregation.

    Note that it's obviously fine to use an aggregated signature in
    response to CHECKSIGVERIFY or n-of-n CHECKMULTISIGVERIFY -- when
    processing the script you just assume it succeeds, relying on the
    fact that the aggregated signature will fail the entire transaction
    if there was a problem. However it's also fine to use an aggregated
    signature in response to CHECKSIG for most plausible scripts, since:

        sig key CHECKSIG

    can be treated as equivalent to

        sig DUP IF key CHECKSIGVERIFY OP_1 FI

    provided invalid signatures are supplied as a "false" value. So
    for the purpose of this email, I'll mostly be treating CHECKSIG and
    n-of-n CHECKMULTISIG as if they support aggregation.

[2] Soft-forks and RETURN_VALID:

    There are two approaches for soft-forking in new opcodes that are
    reasonably well understood:

    1) We can bump the segwit script version, introducing a new class of
       bc1 bech32 addresses, which behave however we like, but can't be
       validated at all by existing nodes. This has the downside that it
       effectively serialises upgrades.

    2) We can redefine OP_NOP opcodes as OP_CHECK_foo_VERIFY
       opcodes, along the same lines as OP_CHECKLOCKTIMEVERIFY or
       OP_CHECKSEQUENCEVERIFY. This has the downside that it's pretty
       restrictive in what new opcodes you can introduce.

    A third approach seems possible as well though, which would combine
    the benefits of both approaches: allowing any new opcode to be
    introduced, and allowing different opcodes to be introduced in
    concurrent soft-forks. Namely:

    3) If we introduce some RETURN_VALID opcodes (in script for a new
       segwit witness version), we can then redefine those as having any
       behaviour we might want, including ones that manipulate the stack,
       and have the change simply be a soft-fork. RETURN_VALID would
       force the script to immediately succeed, in contrast to OP_RETURN
       which forces the script to immediately fail.

[3] https://twitter.com/bramcohen/status/972205820275388416

[4] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-January/015580.html


From aj at erisian.com.au  Wed Mar 21 11:21:19 2018
From: aj at erisian.com.au (Anthony Towns)
Date: Wed, 21 Mar 2018 21:21:19 +1000
Subject: [bitcoin-dev] Soft-forks and schnorr signature aggregation
In-Reply-To: <d_OOMciZ--WI6X8V1PWVCcPGyEFo7AWcNcXls8uUK8itK8pkoUJLRsekBYUdXTRYg_pOinoBQliMFKfzWW48kd3isE6DbkIVoI5frIxOBFo=@protonmail.com>
References: <20180321040618.GA4494@erisian.com.au>
	<d_OOMciZ--WI6X8V1PWVCcPGyEFo7AWcNcXls8uUK8itK8pkoUJLRsekBYUdXTRYg_pOinoBQliMFKfzWW48kd3isE6DbkIVoI5frIxOBFo=@protonmail.com>
Message-ID: <20180321112119.GA6588@erisian.com.au>

On Wed, Mar 21, 2018 at 03:53:59AM -0400, ZmnSCPxj wrote:
> Good morning aj,

Good evening Zeeman!

[pulled from the bottom of your mail]
> This way, rather than gathering signatures, we gather public keys for aggregate signature checking.  

Sorry, I probably didn't explain it well (or at all): during the script,
you're collecting public keys and messages (ie, BIP 143 style digests)
which then go into the signing/verification algorithm to produce/check
the signature.

You do need to gather signatures from each private key holder when
producing the aggregate signature, but that happens at the wallet/p2p
level, rather than the consensus level.

> I am probably wrong, but could solution 2 be simplified by using the below opcodes for aggregated signatures?
> 
> OP_ADD_AGG_PUBKEY - Adds a public key for verification of an aggregated signature.
> OP_CHECK_AGG_SIG[VERIFY] - Check that the gathered public keys matches the aggregated signature.

Checking the gathered public keys match the aggregated signature is
something that only happens for the entire transaction as a whole, so
you don't need an opcode for it in the scripts, since they're per-input.

Otherwise, I think that's pretty similar to what I was already saying;
having:

   SIGHASH_ALL|BUCKET_1 pubkey OP_CHECKSIG

would be adding "pubkey" and a message hash calculated via the SIGHASH_ALL
hashing rules to the list of things that the signature for bucket 1 verifies.

FWIW, the Bellare-Neven verification algorithm looks something like:

   s*G = R + K   (s,R is the signature)
   K = sum( H(R, L, i, m) * X_i )   for i corresponding to each pubkey X_i
   L = the concatenation of all the pubkeys, X_0..X_n
   m = the concatenation of all the message hashes, m_0..m_n

So the way I look at it is each input puts a public key and a message hash
(X_i, m_i) into the bucket via a CHECKSIG operation (or similar), and once
you're done, you look into the bucket and there's just a single signature
(s,R) left to verify. You can't start verifying any of it until you've
looked through all the scripts because you need to know L and m before
you can do anything, and both of those require info from every part of
the aggregation.

[0] [1]

> The effect is that in the OP_CHECKCOVENANT case, pre-softfork nodes will not actually do any checking.

Pre-softfork nodes not doing any checking doesn't work with cross-input
signature aggregation as far as I can see. If it did, all you would have
to do to steal people's funds is mine a non-standard transaction:

  inputs:
   my-millions:
     pay-to-pubkey pubkey1
     witness=SIGHASH_ALL|BUCKET_1
   your-two-cents:
     pay-to-script-hash script=[1 OP_RETURN_TRUE pubkey2 CHECKSIG]
     witness=SIGHASH_ALL|BUCKET_1

   bucket1: 64-random-bytes
  output:
   all-the-money: you

Because there's no actual soft-fork at this point every node is an "old"
node, so they all see the OP_RETURN_TRUE and stop validating signatures,
accepting the transaction as valid, and giving you all my money, despite
you being unable to actually produce my signature.

Make sense?

Cheers,
aj

[0] For completeness: constructing the signature for Bellare-Neven
    requires two communication phases amongst the signers, and looks
    roughly like:

     1. each party generates a random variable r_i, and sharing the
        corresponding curve point R_i=r_i*G and their sighash choice
        (ie, m_i) with the other signers.

     2. this allows each party to calculate R=sum(R_i) and m,
        and hence H(R,L,i,m), at which point each party calculates a
        partial signature using their respective private key, x_i:

          s_i = r_i + H(R,L,i,m)*x_i

        all these s_i values are then communicated to each signer.

     3. these combine to give the final signature (s,R),
        with s=sum(s_i), allowing each signer to verify that the signing
        protocol completed successfully, and any signer can broadcast
        the transaction to the blockchain

[1] muSig differs in the details, but is basically the same.


From apoelstra at wpsoftware.net  Wed Mar 21 12:45:21 2018
From: apoelstra at wpsoftware.net (Andrew Poelstra)
Date: Wed, 21 Mar 2018 12:45:21 +0000
Subject: [bitcoin-dev] Soft-forks and schnorr signature aggregation
In-Reply-To: <20180321040618.GA4494@erisian.com.au>
References: <20180321040618.GA4494@erisian.com.au>
Message-ID: <20180321124521.GI9082@boulet.lan>

On Wed, Mar 21, 2018 at 02:06:18PM +1000, Anthony Towns via bitcoin-dev wrote:
> 
> That leads me to think that interactive signature aggregation is going to
> take a lot of time and work, and it would make sense to do a v1-upgrade
> that's "just" Schnorr (and taproot and MAST and re-enabling opcodes and
> ...) in the meantime. YMMV.
>

Unfortunately I agree. Another complication with aggregate signatures is
that they complicate blind signature protocols such as [1]. In particular
they break the assumption "one signature can spend at most one UTXO"
meaning that a blind signer cannot tell how many coins they're authorizing
with a given signature, even if they've ensured that the key they're using
only controls UTXOs of a fixed value.

This seems solvable with creative use of ZKPs, but the fact that it's even
a problem caught me off guard, and makes me think that signature aggregation
is much harder to think about than e.g. Taproot which does not change
signature semantics at all.


Andrew



[1] https://github.com/jonasnick/scriptless-scripts/blob/blind-swaps/md/partially-blind-swap.md



-- 
Andrew Poelstra
Mathematics Department, Blockstream
Email: apoelstra at wpsoftware.net
Web:   https://www.wpsoftware.net/andrew

"A goose alone, I suppose, can know the loneliness of geese
 who can never find their peace,
 whether north or south or west or east"
       --Joanna Newsom

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 455 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180321/535daaca/attachment.sig>

From eric at voskuil.org  Sun Mar 18 18:59:28 2018
From: eric at voskuil.org (Eric Voskuil)
Date: Sun, 18 Mar 2018 19:59:28 +0100
Subject: [bitcoin-dev] feature: Enhance privacy by change obfuscation
In-Reply-To: <874lleexud.fsf@eklitzke.org>
References: <PS2P216MB0179FCA8077AAE946BFA2D069DD50@PS2P216MB0179.KORP216.PROD.OUTLOOK.COM>
	<874lleexud.fsf@eklitzke.org>
Message-ID: <BCD26605-01B0-437C-8D4A-68DA52055AFB@voskuil.org>

> This would be really expensive for the network due to the bloat in UTXO size, a cost everyone has to pay for.

Without commenting on the merits of this proposal, I?d just like to correct this common misperception. There is no necessary additional cost to the network from the count of unspent outputs. This perception arises from an implementation detail of particular node software. There is no requirement for redundant indexing of unspent outputs.

e

From ZmnSCPxj at protonmail.com  Wed Mar 21 07:53:59 2018
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Wed, 21 Mar 2018 03:53:59 -0400
Subject: [bitcoin-dev] Soft-forks and schnorr signature aggregation
In-Reply-To: <20180321040618.GA4494@erisian.com.au>
References: <20180321040618.GA4494@erisian.com.au>
Message-ID: <d_OOMciZ--WI6X8V1PWVCcPGyEFo7AWcNcXls8uUK8itK8pkoUJLRsekBYUdXTRYg_pOinoBQliMFKfzWW48kd3isE6DbkIVoI5frIxOBFo=@protonmail.com>

Good morning aj,

I am probably wrong, but could solution 2 be simplified by using the below opcodes for aggregated signatures?

OP_ADD_AGG_PUBKEY - Adds a public key for verification of an aggregated signature.

OP_CHECK_AGG_SIG[VERIFY] - Check that the gathered public keys matches the aggregated signature.

Then:

 pubkey1 OP_ADD_AGG_PUBKEY
 OP_IF
   pubkey2 OP_ADD_AGG_PUBKEY
 OP_ELSE
   cond OP_CHECKCOVENANT
 OP_ENDIF
 OP_CHECK_AGG_SIG

(omitting the existence of buckets)

I imagine that aggregated signatures, being linear, would allow pubkey to be aggregated also by adding the pubkey points (but note that I am not a mathematician, I only parrot what better mathematicians say) so OP_ADD_AGG_PUBKEY would not require storing all public keys, just adding them linearly.

The effect is that in the OP_CHECKCOVENANT case, pre-softfork nodes will not actually do any checking.

OP_CHECK_AGG_SIG might accept the signature on the stack (combined signature of pubkey1 and pubkey2 and from other inputs), or the bucket the signature is stored in.

We might even consider using the altstack: no more OP_ADD_AGG_PUBKEY (one less opcode to reserve!), just push pubkeys on the altstack, and OP_CHECK_AGG_SIG would take the entire altstack as all the public keys to be used in aggregated signature checking.

This way, rather than gathering signatures, we gather public keys for aggregate signature checking.  OP_RETURN_TRUE interacts with that by not performing aggregate signature checking at all if we encounter OP_RETURN_TRUE first (which makes sense: old nodes have no idea what OP_RETURN_TRUE is really doing, and would fail to understand all its details).


I am very probably wrong but am willing to learn how to break the above, though.  I am probably making a mistake somewhere.

Regards,
ZmnSCPxj

?Sent with ProtonMail Secure Email.?

??????? Original Message ???????

On March 21, 2018 12:06 PM, Anthony Towns via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:

> Hello world,
> 
> There was a lot of discussion on Schnorr sigs and key and signature
> 
> aggregation at the recent core-dev-tech meeting (one relevant conversation
> 
> is transcribed at \[0\]).
> 
> Quick summary, with more background detail in the corresponding footnotes:
> 
> signature aggregation is awesome \[1\], and the possibility of soft-forking
> 
> in new opcodes via OP\_RETURN\_VALID opcodes (instead of OP_NOP) is also
> 
> awesome \[2\].
> 
> Unfortunately doing both of these together may turn out to be awful.
> 
> RETURN_VALID and Signature Aggregation
> 
> 
> -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> 
> Bumping segwit script versions and redefining OP_NOP opcodes are
> 
> fairly straightforward to deal with even with signature aggregation,
> 
> the straightforward implementation of both combined is still a soft-fork.
> 
> RETURN_VALID, unfortunately, has a serious potential pitfall: any
> 
> aggregatable signature operations that occur after it have to go into
> 
> separate buckets.
> 
> As an example of why this is the case, imagine introducing a covenant
> 
> opcode that pulls a potentially complicated condition from the stack
> 
> (perhaps, "an output pays at least 50000 satoshi to address xyzzy"),
> 
> checks the condition against the transaction, and then pushes 1 (or 0)
> 
> back onto the stack indicating compliance with the covenant (or not).
> 
> You might then write a script allowing a single person to spend the coins
> 
> if they comply with the covenant, and allow breaking the covenant with
> 
> someone else's sign-off in addition. You could write this as:
> 
> pubkey1 CHECKSIGVERIFY
> 
> cond CHECKCOVENANT IFDUP NOTIF pubkey2 CHECKSIG ENDIF
> 
> If you pass the covenant, you supply "SIGHASHALL|BUCKET_1" and aggregate
> 
> the signature for pubkey1 into bucket1 and you're set; otherwise you supply
> 
> "SIGHASHALL|BUCKET\_1 SIGHASHALL|BUCKET\_1" and aggregate signatures for both
> 
> pubkey1 and pubkey2 into bucket1 and you're set. Great!
> 
> But this isn't a soft-fork: old nodes would see this script as:
> 
> pubkey1 CHECKSIGVERIFY
> 
> cond RETURN_VALID IFDUP NOTIF pubkey2 CHECKSIG ENDIF
> 
> which it would just interpret as:
> 
> pubkey1 CHECKSIGVERIFY cond RETURN_VALID
> 
> which is fine if the covenant was passing; but no good if the covenant
> 
> didn't pass -- they'd be expecting the aggregted sig to just be for
> 
> pubkey1 when it's actually pubkey1+pubkey2, so old nodes would fail the
> 
> tx and new nodes would accept it, making it a hard fork.
> 
> Solution 0a / 0b
> 
> 
> ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> 
> There are two obvious solutions here:
> 
> 0a) Just be very careful to ensure any aggregated signatures that
> 
> are conditional on an redefined RETURN_VALID opcode go into later
> 
> buckets, but be careful about having separate sets of buckets every
> 
> time a soft-fork introduces a new redefined opcode. Probably very
> 
> complicated to implement correctly, and essentially doubles the
> 
> number of buckets you have to potentially deal with every time you
> 
> soft fork in a new opcode.
> 
> 0b) Alternatively, forget about the hope that RETURN_VALID
> 
> opcodes could be converted to anything, and just reserve OP_NOP
> 
> opcodes and convert them to CHECK\_foo\_VERIFY opcodes just as we
> 
> have been doing, and when we can't do that bump the segwit witness
> 
> version for a whole new version of script. Or in twitter speak:
> 
> "non-verify upgrades should be done with new script versions" \[3\]
> 
> I think with a little care we can actually salvage RETURN_VALID though!
> 
> Solution 1
> 
> 
> ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> 
> You don't actually have to write your scripts in ways that can cause
> 
> this problem, as long as you're careful. In particular, the problem only
> 
> occurs if you do aggregatable CHECKSIG operations after "RETURN_VALID"
> 
> \-\- if you do all the CHECKSIGs first, then all nodes will be checking
> 
> for the same signatures, and there's no problem. So you could rewrite
> 
> the script above as:
> 
> pubkey1 CHECKSIGVERIFY
> 
> IF pubkey2 CHECKSIG ENDIF
> 
> cond CHECKCOVENANT OR
> 
> which is redeemable either by:
> 
> sig1 0 \[and covenant is met\]
> 
> sig1 1 sig2 \[covenant is not checked\]
> 
> The witness in this case is essentially committing to the execution path
> 
> that would have been taken in the first script by a fully validating node,
> 
> then the new script checks all the signatures, and then validates that the
> 
> committed execution path was in fact the one that was meant to be taken.
> 
> If people are clever enough to write scripts this way, I believe you
> 
> can make RETURN_VALID soft-fork safe simply by having every soft-forked
> 
> RETURN_VALID operation set a state flag that makes every subsequent
> 
> CHECKSIG operation require a non-aggregated sig.
> 
> The drawback of this approach is that if the script is complicated
> 
> (eg it has multiple IF conditions, some of which are nested), it may be
> 
> difficult to write the script to ensure the signatures are checked in the
> 
> same combination as the later logic actually requires -- you might have
> 
> to store the flag indicating whether you checked particular signatures
> 
> on the altstack, or use DUP and PICK/ROLL to organise it on the stack.
> 
> Solution 2
> 
> 
> ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> 
> We could make that simpler for script authors by making dedicated opcodes
> 
> to help with "do all the signatures first" and "check the committed
> 
> execution path against reality" steps. I think a reasonable approach
> 
> would be something like:
> 
> 0b01 pubkey2 pubkey1 2 CHECK\_AGGSIG\_VERIFY
> 
> cond CHECKCOVENANT 0b10 CHECK\_AGG\_SIGNERS OR
> 
> which is redeemed either by:
> 
> sighash1 0 \[and passing the covenant cond\]
> 
> sighash2 sighash1 0b10
> 
> (I'm using the notation 0b10110 to express numbers as binary bitfields;
> 
> 0b10110 = 22 eg)
> 
> That is, two new opcodes, namely:
> 
> CHECK\_AGGSIG\_VERIFY which takes from the stack:
> 
> \- N: a count of pubkeys
> 
> \- pubkey1..pubkeyN: N pubkeys
> 
> \- REQ: a bitmask of which pubkeys are required to sign
> 
> \- OPT: a bitmask of which optional pubkeys have signed
> 
> \- sighashes: M sighashes for the pubkeys corresponding to the set
> 
> bits of (REQ|OPT)
> 
> CHECK\_AGGSIG\_VERIFY fails if:
> 
> \- the stack doesn't have enough elements
> 
> \- the aggregated signature doesn't pass
> 
> \- a redefined RETURN_VALID opcode has already been seen
> 
> \- a previous CHECK\_AGGSIG\_VERIFY has already been seen in this script
> 
> REQ|OPT is stored as state
> 
> CHECK\_AGG\_SIGNERS takes from the stack:
> 
> \- B: a bitmask of which pubkeys are being queried
> 
> and it pushes to the stack 1 or 0 based on:
> 
> \- (REQ|OPT) & B == B ? 1 : 0
> 
> A possible way to make sure the "no agg sigs after an upgraded
> 
> RETURN\_VALID" behaviour works right might be to have "RETURN\_VALID"
> 
> fail if CHECK\_AGGSIG\_VERIFY hasn't already been seen. That way once you
> 
> redefine RETURN\_VALID in a soft-fork, if you have a CHECK\_AGGSIG_VERIFY
> 
> after a RETURN_VALID you've either already failed (because the
> 
> RETURN\_VALID wasn't after a CHECK\_AGGSIG_VERIFY), or you automatically
> 
> fail (because you've already seen a CHECK\_AGGSIG\_VERIFY).
> 
> There would be no need to make CHECKSIG, CHECKSIGVERIFY, CHECKMULTISIG
> 
> and CHECKMULTISIGVERIFY do signature aggregation in this case. They could
> 
> be left around to allow script authors to force non-aggregate signatures
> 
> or could be dropped entirely, I think.
> 
> This construct would let you do M-of-N aggregated multisig in a fairly
> 
> straightforward manner without needing an explicit opcode, eg:
> 
> 0 pubkey5 pubkey4 pubkey3 pubkey2 pubkey1 5 CHECK\_AGGSIG\_VERIFY
> 
> 0b10000 CHECK\_AGG\_SIGNERS
> 
> 0b01000 CHECK\_AGG\_SIGNERS ADD
> 
> 0b00100 CHECK\_AGG\_SIGNERS ADD
> 
> 0b00010 CHECK\_AGG\_SIGNERS ADD
> 
> 0b00001 CHECK\_AGG\_SIGNERS ADD
> 
> 3 NUMEQUAL
> 
> redeemable by, eg:
> 
> 0b10110 sighash5 sighash3 sighash2
> 
> and a single aggregate signature by the private keys corresponding to
> 
> pubkey{2,3,5}.
> 
> Of course, another way of getting M-of-N aggregated multisig is via MAST,
> 
> which brings us to another approach...
> 
> Solution 3
> 
> 
> ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> 
> All we're doing above is committing to an execution path and validating
> 
> signatures for that path before checking the path was the right one. But
> 
> MAST is a great way of committing to an execution path, so another
> 
> approach would just be "don't have alternative execution paths, just have
> 
> MAST and CHECK/VERIFY codes". Taking the example I've been running with,
> 
> that would be:
> 
> branch1: 2 pubkey2 pubkey1 2 CHECKMULTISIG
> 
> branch2: pubkey1 CHECKSIGVERIFY cond CHECKCOVENANT
> 
> So long as MAST is already supported when signature aggregation becomes
> 
> possible, that works fine. The drawback is MAST can end up with lots of
> 
> branches, eg the 3-of-5 multisig check has 10 branches:
> 
> branch1: 3 pubkey3 pubkey2 pubkey1 3 CHECKMULTISIG
> 
> branch2: 3 pubkey4 pubkey2 pubkey1 3 CHECKMULTISIG
> 
> branch3: 3 pubkey5 pubkey2 pubkey1 3 CHECKMULTISIG
> 
> branch4: 3 pubkey4 pubkey3 pubkey1 3 CHECKMULTISIG
> 
> branch5: 3 pubkey5 pubkey3 pubkey1 3 CHECKMULTISIG
> 
> branch6: 3 pubkey5 pubkey4 pubkey1 3 CHECKMULTISIG
> 
> branch7: 3 pubkey4 pubkey3 pubkey2 3 CHECKMULTISIG
> 
> branch8: 3 pubkey5 pubkey3 pubkey2 3 CHECKMULTISIG
> 
> branch9: 3 pubkey5 pubkey4 pubkey2 3 CHECKMULTISIG
> 
> branch10: 3 pubkey5 pubkey4 pubkey3 3 CHECKMULTISIG
> 
> while if you want, say, 6-of-11 multisig you get 462 branches, versus
> 
> just:
> 
> 0 pubkey11 pubkey10 pubkey9 pubkey8 pubkey7 pubkey6
> 
> pubkey5 pubkey4 pubkey3 pubkey2 pubkey1 11 CHECK\_AGGSIG\_VERIFY
> 
> 0b10000000000 CHECK\_AGG\_SIGNERS
> 
> 0b01000000000 CHECK\_AGG\_SIGNERS ADD
> 
> 0b00100000000 CHECK\_AGG\_SIGNERS ADD
> 
> 0b00010000000 CHECK\_AGG\_SIGNERS ADD
> 
> 0b00001000000 CHECK\_AGG\_SIGNERS ADD
> 
> 0b00000100000 CHECK\_AGG\_SIGNERS ADD
> 
> 0b00000010000 CHECK\_AGG\_SIGNERS ADD
> 
> 0b00000001000 CHECK\_AGG\_SIGNERS ADD
> 
> 0b00000000100 CHECK\_AGG\_SIGNERS ADD
> 
> 0b00000000010 CHECK\_AGG\_SIGNERS ADD
> 
> 0b00000000001 CHECK\_AGG\_SIGNERS ADD
> 
> 6 NUMEQUAL
> 
> Provided doing lots of hashes to calculate merkle paths is cheaper than
> 
> publishing to the blockchain, MAST will likely still be better though:
> 
> you'd be doing 6 pubkeys and 9 steps in the merkle path for about 1532bytes in MAST, versus showing off all 11 pubkeys above for 11(32+4)
> 
> bytes, and the above is roughly the worst case for m-of-11 multisig
> 
> via MAST.
> 
> If everyone's happy to use MAST, then it could be the only solution:
> 
> drop OP_IF and friends, and require all the CHECKSIG ops to occur before
> 
> any RETURN_VALID ops: since there's no branching, that's just a matter of
> 
> reordering your script a bit and should be pretty easy for script authors.
> 
> I think there's a couple of drawbacks to this approach that it shouldn't
> 
> be the only solution:
> 
> a) we don't have a lot of experience with using MAST
> 
> b) MAST is a bit more complicated than just dealing with branches in
> 
> a script (probably solvable once (a) is no longer the case)
> 
> c) some useful scripts might be a bit cheaper expressed with
> 
> of branches and be better expressed without MAST
> 
> If other approaches than MAST are still desirable, then MAST works fine
> 
> in combination with either of the earlier solutions as far as I can see.
> 
> Summary
> 
> 
> ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> 
> I think something along the lines of solution 2 makes the most sense,
> 
> so I think a good approach for aggregate signatures is:
> 
> -   introduce a new segwit witness version, which I'll call v2 (but which
>     
>     might actually be v1 or v3 etc, of course)
>     
> -   v2 must support Schnorr signature verification.
> -   v2 should have a "pay to public key (hash?)" witness format. direct
>     
>     signatures of the transaction via the corresponding private key should
>     
>     be aggregatable.
>     
> -   v2 should have a "pay to script hash" witness format: probably via
>     
>     taproot+MAST, possibly via graftroot as well
>     
> -   v2 should support MAST scripts: again, probably via taproot+MAST
> -   v2 taproot shouldn't have a separate script version (ie,
>     
>     the pubkey shouldn't be P+H(P,version,scriptroot)), as signatures
>     
>     for later-versioned scripts couldn't be aggregated, so there's no
>     
>     advantage over bumping the segwit witness version
>     
> -   v2 scripts should have a CHECK\_AGG\_SIG_VERIFY opcode roughly as
>     
>     described above for aggregating signatures, along with CHECK\_AGG\_SIGNERS
>     
> -   CHECK{MULTI,}SIG{VERIFY,} in v2 scripts shouldn't support aggregated
>     
>     signatures, and possibly shouldn't be present at all?
>     
> -   v2 signers should be able to specify an aggregation bucket for each
>     
>     signature, perhaps in the range 0-7 or so?
>     
> -   v2 scripts should have a bunch of RETURN_VALID opcodes for future
>     
>     soft-forks, constrained so that CHECK\_AGG\_SIG_VERIFY doesn't appear
>     
>     after them. the currently disabled opcodes should be redefined as
>     
>     RETURN_VALID eg.
>     
>     For soft-fork upgrades from that point:
>     
> -   introducing new opcodes just means redefining an RETURN_VALID opcode
> -   introducing new sighash versions requires bumping the segwit witness
>     
>     version (to v3, etc)
>     
> -   if non-interactive half-signature aggregation isn't ready to go, it
>     
>     would likewise need a bump in the segwit witness version when
>     
>     introduced
>     
>     I think it's worth considering bundling a hard-fork upgrade something
>     
>     like:
>     
> -   ~5 years after v2 scripts are activated, existing p2pk/p2pkh UTXOs
>     
>     (either matching the pre-segwit templates or v0 segwit p2wpkh) can
>     
>     be spent via a v2-aggregated-signature (but not via taproot)
>     
>     \[4\]
>     
> -   core will maintain a config setting that allows users to prevent
>     
>     that hard fork from activating via UASF up until the next release
>     
>     after activation (probably with UASF-enforced miner-signalling that
>     
>     the hard-fork will not go ahead)
>     
>     This is already very complicated of course, but note that there's still
>     
>     more things that need to be considered for signature aggregation:
>     
> -   whether to use Bellare-Neven or muSig in the consensus-critical
>     
>     aggregation algorithm
>     
> -   whether to assign the aggregate sigs to inputs and plunk them in the
>     
>     witness data somewhere, or to add a new structure and commitment and
>     
>     worry about p2p impact
>     
> -   whether there are new sighash options that should go in at the same time
> -   whether non-interactive half-sig aggregation can go in at the same time
>     
>     That leads me to think that interactive signature aggregation is going to
>     
>     take a lot of time and work, and it would make sense to do a v1-upgrade
>     
>     that's "just" Schnorr (and taproot and MAST and re-enabling opcodes and
>     
>     ...) in the meantime. YMMV.
>     
>     Cheers,
>     
>     aj
>     
>     \[0\] http://diyhpl.us/wiki/transcripts/bitcoin-core-dev-tech/2018-03-06-taproot-graftroot-etc/
>     
>     \[1\] Signature aggregation:
>     
>     Signature aggregation is cool because it lets you post a transaction
>     
>     spending many inputs, but only providing a single 64 byte signature
>     
>     that proves authorisation by the holders of all the private keys
>     
>     for all the inputs. So the witnesses for your inputs might be:
>     
>     p2wpkh: pubkey1 SIGHASH_ALL
>     
>     p2wpkh: pubkey2 SIGHASH_ALL
>     
>     p2wsh: "3 pubkey1 pubkey3 pubkey4 3 CHECKMULTISIG" SIGHASH\_ALL SIGHASH\_ALL SIGHASH_ALL
>     
>     where instead of including full 65-byte signature for each CHECKSIG
>     
>     operation in each input witness, you just include the ~1-byte sighash,
>     
>     and provide a single 64-byte signature elsewhere, calculated either
>     
>     according to the Bellare-Neven algorithm, or the muSig algorithm.
>     
>     In the above case, that means going from about 500 witness bytes
>     
>     for 5 public keys and 5 signatures, to about 240 witness bytes for
>     
>     5 public keys and just 1 signature.
>     
>     A complication here is that because the signatures are aggregated,
>     
>     in order to validate any signature you have to be able to validate
>     
>     every signature.
>     
>     It's possible to limit that a bit, and have aggregation
>     
>     "buckets". This might be something you just choose when signing, eg:
>     
>     p2wpkh: pubkey1 SIGHASH\_ALL|BUCKET\_1
>     
>     p2wpkh: pubkey2 SIGHASH\_ALL|BUCKET\_2
>     
>     p2wsh: "3 pubkey1 pubkey3 pubkey4 3 CHECKMULTISIG" SIGHASH\_ALL|BUCKET\_1 SIGHASH\_ALL|BUCKET\_2 SIGHASH\_ALL|BUCKET\_2
>     
>     bucket1: 64 byte sig for (pubkey1, pubkey1)
>     
>     bucket2: 64 byte sig for (pubkey2, pubkey3, pubkey4)
>     
>     That way you get the choice to verify both of the pubkey1 signatures
>     
>     or all of the pubkey{2,3,4} signatures or all the signatures (or
>     
>     none of the signatures).
>     
>     This might be useful if the private key for pubkey1 is essentially
>     
>     offline, and can't easily participate in an interactive protocol
>     
>     \-\- with separate buckets the separate signatures can be generated
>     
>     independently at different times, while with only one bucket,
>     
>     everyone has to coordinate to produce the signature)
>     
>     (For clarity: each bucket corresponds to many CHECKSIG operations,
>     
>     but only contains a single 64-byte signature)
>     
>     Different buckets will also be necessary when dealing with new
>     
>     segwit script versions: if there are any aggregated signatures for
>     
>     v1 addresses that go into bucket X, then aggregate signatures for
>     
>     v2 addresses cannot go into bucket X, as that would prevent nodes
>     
>     that support v1 addresses but not v2 addresses from validating
>     
>     bucket X, which would prevent them from validating the v1 addresses
>     
>     corresponding to that bucket, which would make the v2 upgrade a hard
>     
>     fork rather than a soft fork. So each segwit version will need to
>     
>     introduce a new set of aggregation buckets, which in turn reduces
>     
>     the benefit you get from signature aggregation.
>     
>     Note that it's obviously fine to use an aggregated signature in
>     
>     response to CHECKSIGVERIFY or n-of-n CHECKMULTISIGVERIFY -- when
>     
>     processing the script you just assume it succeeds, relying on the
>     
>     fact that the aggregated signature will fail the entire transaction
>     
>     if there was a problem. However it's also fine to use an aggregated
>     
>     signature in response to CHECKSIG for most plausible scripts, since:
>     
>     sig key CHECKSIG
>     
>     can be treated as equivalent to
>     
>     sig DUP IF key CHECKSIGVERIFY OP_1 FI
>     
>     provided invalid signatures are supplied as a "false" value. So
>     
>     for the purpose of this email, I'll mostly be treating CHECKSIG and
>     
>     n-of-n CHECKMULTISIG as if they support aggregation.
>     
>     \[2\] Soft-forks and RETURN_VALID:
>     
>     There are two approaches for soft-forking in new opcodes that are
>     
>     reasonably well understood:
>     
>     1.  We can bump the segwit script version, introducing a new class of
>         
>         bc1 bech32 addresses, which behave however we like, but can't be
>         
>         validated at all by existing nodes. This has the downside that it
>         
>         effectively serialises upgrades.
>         
>     2.  We can redefine OP\_NOP opcodes as OP\_CHECK\_foo\_VERIFY
>         
>         opcodes, along the same lines as OP_CHECKLOCKTIMEVERIFY or
>         
>         OP_CHECKSEQUENCEVERIFY. This has the downside that it's pretty
>         
>         restrictive in what new opcodes you can introduce.
>         
>         A third approach seems possible as well though, which would combine
>         
>         the benefits of both approaches: allowing any new opcode to be
>         
>         introduced, and allowing different opcodes to be introduced in
>         
>         concurrent soft-forks. Namely:
>         
>     3.  If we introduce some RETURN_VALID opcodes (in script for a new
>         
>         segwit witness version), we can then redefine those as having any
>         
>         behaviour we might want, including ones that manipulate the stack,
>         
>         and have the change simply be a soft-fork. RETURN_VALID would
>         
>         force the script to immediately succeed, in contrast to OP_RETURN
>         
>         which forces the script to immediately fail.
>         
>         \[3\] https://twitter.com/bramcohen/status/972205820275388416
>         
>         \[4\] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-January/015580.html
>         
> 
> bitcoin-dev mailing list
> 
> bitcoin-dev at lists.linuxfoundation.org
> 
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev



From ZmnSCPxj at protonmail.com  Wed Mar 21 23:28:00 2018
From: ZmnSCPxj at protonmail.com (ZmnSCPxj at protonmail.com)
Date: Wed, 21 Mar 2018 19:28:00 -0400
Subject: [bitcoin-dev] Soft-forks and schnorr signature aggregation
In-Reply-To: <20180321112119.GA6588@erisian.com.au>
References: <20180321040618.GA4494@erisian.com.au>
	<d_OOMciZ--WI6X8V1PWVCcPGyEFo7AWcNcXls8uUK8itK8pkoUJLRsekBYUdXTRYg_pOinoBQliMFKfzWW48kd3isE6DbkIVoI5frIxOBFo=@protonmail.com>
	<20180321112119.GA6588@erisian.com.au>
Message-ID: <OKn7sFgvccsvoIDlGE_fFQ8a2EnGkfiWLvzBWs1vI2LdrWdQaO4ufySwLj1GDrykFrT5GI6-gLHDwuiKQOnvUEBe9oHLT3gu0jJJx7gw_xM=@protonmail.com>

Good morning aj,




?Sent with ProtonMail Secure Email.?

??????? Original Message ???????

On March 21, 2018 7:21 PM, Anthony Towns <aj at erisian.com.au> wrote:

> On Wed, Mar 21, 2018 at 03:53:59AM -0400, ZmnSCPxj wrote:
> 
> > Good morning aj,
> 
> Good evening Zeeman!
> 
> [pulled from the bottom of your mail]
> 
> > This way, rather than gathering signatures, we gather public keys for aggregate signature checking.
> 
> Sorry, I probably didn't explain it well (or at all): during the script,
> 
> you're collecting public keys and messages (ie, BIP 143 style digests)
> 
> which then go into the signing/verification algorithm to produce/check
> 
> the signature.

Yes, I think this is indeed what OP_CHECK_AGG_SIG really does.

What I propose is that we have two places where we aggregate public keys: one at the script level, and one at the transaction level.  OP_ADD_AGG_PUBKEY adds to the script-level aggregate, then OP_CHECK_AGG_SIG adds the script-level aggregate to the transaction-level aggregate.

Unfortunately it will not work since transaction-level aggregate (which is actually what gets checked) is different between pre-fork and post-fork nodes.

It looks like signature aggregation is difficult to reconcile with script...

Regards,
ZmnSCPxj

From bram at chia.net  Thu Mar 22 00:47:01 2018
From: bram at chia.net (Bram Cohen)
Date: Wed, 21 Mar 2018 17:47:01 -0700
Subject: [bitcoin-dev] Soft-forks and schnorr signature aggregation
Message-ID: <CAHUJnBAQwLfmvLyApcw8dKz1u8xX1KmdjiUmzSHXyw7npWUXHQ@mail.gmail.com>

Regarding the proposed segwit v2 with reclaiming most things as
RETURN_VALID, the net result for what's being proposed in the near future
for supporting aggregated signatures in the not-so-near future is to punt.
A number of strategies are possible for how to deal with new opcodes being
added later on, and the general strategy of making unused opcodes be
RETURN_VALID for now and figuring out how to handle it later works for all
of them. I think this is the right approach, but wanted to clarify that it
is in fact the approach being proposed.

That said, there are some subtleties to getting it right which the last
message doesn't really cover. Most unused opcodes should be reclaimed as
RETURN_VALID, but there should still be one OP_NOP and there should be a
'real' RETURN_VALID, which (a) is guaranteed to not be soft forked into
something else in the future, and (b) doesn't have any parsing weirdness.
The parsing weirdness of all the unclaimed opcodes is interesting. Because
everything in an IF clause needs to be parsed in order to find where the
ELSE is, you have a few options for dealing with an unknown opcode getting
parsed in an unexecuted section of code. They are (a) avoid the problem
completely by exterminating IF and MASTing (b) avoid the problem completely
by getting rid of IF and adding IFJUMP, IFNJUMP, and JUMP which specify a
number of bytes (this also allows for script merkleization) (c) require all
new opcodes have fixed length 1, even after they're soft forked, (d) do
almost like (c) but require that on new soft forks people hack their old
scripts to still parse properly by avoiding the OP_ELSE in inopportune
places (yuck!) (e) make it so that the unknown opcodes case a RETURN_VALID
even when they're parsed, regardless of whether they're being executed.

By far the most expedient option is (e) cause a RETURN_VALID at parse time.
There's even precedent for this sort of behavior in the other direction
with disabled opcodes causing failure at parse time even if they aren't
being executed.

A lot can be said about all the options, but one thing I feel like snarking
about is that if you get rid of IFs using MAST, then it's highly unclear
whether OP_DEPTH should be nuked as well. My feeling is that it should and
that strict parsing should require that the bottom thing in the witness
gets referenced at some point.

Hacking in a multisig opcode isn't a horrible idea, but it is very stuck
specifically on m-of-n and doesn't support more complex formulas for how
signatures can be combined, which makes it feel hacky and weird.

Also it may make sense to seriously consider BLS signatures, which have a
lot of practical benefits starting with them being noninteractively
aggregatable so you can always assume that they're aggregated instead of
requiring complex semantics to specify what's aggregated with what. My team
is working on an implementation which has several advantages over what's
currently in the published literature but it isn't quite ready for public
consumption yet. This should probably go on the pile of reasons why it's
premature to finalize a plan for aggregation at this point.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180321/c0b652a2/attachment.html>

From samad.sajanlal at gmail.com  Wed Mar 21 22:04:35 2018
From: samad.sajanlal at gmail.com (Samad Sajanlal)
Date: Wed, 21 Mar 2018 17:04:35 -0500
Subject: [bitcoin-dev] Soft Fork Activation & Enforcement w/o Signaling?
Message-ID: <CAAQZUuDEJeMFTxxJcgUEmTUQbxM_ZWkBD1k+UOvafsqbqj++Jg@mail.gmail.com>

Is it possible to activate soft forks such as BIP65 and BIP66 without prior
signaling from miners? I noticed in chainparams.cpp that there are block
heights where the enforcement begins.

I understand this is already active on bitcoin. I'm working on a project
that is a clone of a clone of bitcoin, and we currently do not have BIP65
or BIP66 enforced - no signaling of these soft forks either (most of the
network is on a source code fork of bitcoin 0.9). This project does not and
never intends to attempt to replace bitcoin - we know that without bitcoin
our project could never exist, so we owe a great deal of gratitude to the
bitcoin developers.

If the entire network upgrades to the correct version of the software
(based on bitcoin 0.15), which includes the block height that has
enforcement, can we simply skip over the signaling and go straight into
activation/enforcement?

At this time we are lucky that our network is very small, so it is
reasonable to assume that the whole network will upgrade their clients
within a short window (~2 weeks). We would schedule the activation ~2
months out from when the client is released, just to ensure everyone has
time to upgrade.

We have been stuck on the 0.9 code branch and my goal is to bring it up to
0.15 at least, so that we can implement Segwit and other key features that
bitcoin has introduced. The 0.15 client currently works with regards to
sending and receiving transactions but the soft forks are not active. I
understand that activating them will segregate the 0.15 clients onto their
own fork, which is why I'd like to understand the repercussions of doing it
without any signaling beforehand. I also would prefer not to have to make
intermediate releases such as 0.10, 0.11.. etc to get the soft forks
activated.

Another related question - does the block version get bumped up
automatically at the time that a soft fork activates, or is there
additional stuff that I need to do within the code to ensure it bumps up at
the same time? From what I saw in the code it appears that it will bump up
automatically, but I would like some confirmation on that.

Regards,
Samad
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180321/bf373a90/attachment.html>

From daniel.g.repp at gmail.com  Sat Mar 24 13:52:56 2018
From: daniel.g.repp at gmail.com (Daniel R)
Date: Sat, 24 Mar 2018 14:52:56 +0100
Subject: [bitcoin-dev] Lookinf for issues to contribute to
Message-ID: <CAHtYR9E4GpwBqqmD0HDKutiXeg=cDtpyvhd_+23srv5_c1AFQQ@mail.gmail.com>

Hey guys,

I want to contribute to bitcoin core. I am an intermediate programmer and
want to get started contributing fast. I have already cloned the git
repository. Can you maybe direct me to sources where I can learn more about
the structure of bitcoin core and specifically to problems where I can get
experience working with the source code?

I know Python, C/C++ and a bit of Java. I have advanced knowledge of
cryptographic concepts and procedures and try to teach myself some of the
math, especially around elliptic curves. I am currently composing a
bachelor thesis around the question of Blockchain usage in the
car-industry, where I try to look at different aspects of protocol-design.


Best Regards
Daniel
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180324/648f2a0f/attachment-0001.html>

From rhavar at protonmail.com  Sat Mar 24 14:50:54 2018
From: rhavar at protonmail.com (rhavar at protonmail.com)
Date: Sat, 24 Mar 2018 10:50:54 -0400
Subject: [bitcoin-dev] Lookinf for issues to contribute to
In-Reply-To: <CAHtYR9E4GpwBqqmD0HDKutiXeg=cDtpyvhd_+23srv5_c1AFQQ@mail.gmail.com>
References: <CAHtYR9E4GpwBqqmD0HDKutiXeg=cDtpyvhd_+23srv5_c1AFQQ@mail.gmail.com>
Message-ID: <CUwmNgWvHi6PWaF4d3ZV7mWMzRuep-EQoerQuNivqtTirNzGbSi7bXmeQp2sZjh5wRTuTYASGwFmEIv6N7oE2fkmbW4rkFfnfL-WXQjle0E=@protonmail.com>

Maybe:
https://github.com/bitcoin/bitcoin/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22

Just pick something small (even if it's not interesting), struggle with it, struggle with it some more, do a git blame on the parts you need to modify and try contact the person if there's something you need help with.

I'd say start with simple and boring changes, and you'll organically get a better understanding.

But even better, go to:
https://github.com/bitcoin/bitcoin/pulls

And find some issues, reproduce the problem, test the fix -- and look at how the person did it. Post your results/feedback on the pull requests

I think you'll find in bitcoin (and cryptocurrencies in general) there's a lot more demand for elbow grease than advanced maths stuff =)

-Ryan

??????? Original Message ???????
On March 24, 2018 8:52 AM, Daniel R via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:

> Hey guys,
>
> I want to contribute to bitcoin core. I am an intermediate programmer and want to get started contributing fast. I have already cloned the git repository. Can you maybe direct me to sources where I can learn more about the structure of bitcoin core and specifically to problems where I can get experience working with the source code?
>
> I know Python, C/C++ and a bit of Java. I have advanced knowledge of cryptographic concepts and procedures and try to teach myself some of the math, especially around elliptic curves. I am currently composing a bachelor thesis around the question of Blockchain usage in the car-industry, where I try to look at different aspects of protocol-design.
>
> Best Regards
> Daniel
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180324/50b8c8b4/attachment.html>

From pieter.wuille at gmail.com  Mon Mar 26 08:53:23 2018
From: pieter.wuille at gmail.com (Pieter Wuille)
Date: Mon, 26 Mar 2018 01:53:23 -0700
Subject: [bitcoin-dev] {sign|verify}message replacement
In-Reply-To: <CALJw2w5=g-FL+MZ08DEoLxVzOKbSXeKu50drE1b4P0JZJpdTyA@mail.gmail.com>
References: <CALJw2w5=g-FL+MZ08DEoLxVzOKbSXeKu50drE1b4P0JZJpdTyA@mail.gmail.com>
Message-ID: <CAPg+sBhAaw7uqdz6ZoMCkut2GM=3=F22FiHw6J_aLwTMEcqt1g@mail.gmail.com>

Hello,

Thanks for starting a discussion about this idea.

A few comments inline:

On Wed, Mar 14, 2018 at 1:09 AM, Karl Johan Alm via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Hello,
>
> I am considering writing a replacement for the message signing tools
> that are currently broken for all but the legacy 1xx addresses. The
> approach (suggested by Pieter Wuille) is to do a script based
> approach. This does not seem to require a lot of effort for
> implementing in Bitcoin Core*. Below is my proposal for this system:
>
> A new structure SignatureProof is added, which is a simple scriptSig &
> witnessProgram container that can be serialized. This is passed out
> from/into the signer/verifier.
>

You need a bit more logic to deal with softforks and compatibility. The
question is which script validation flags you verify with:
* If you make them fixed, it means signatures can't evolve with new address
types being introduced that rely on new features.
* If you make it just consensus flags (following mainnet), it means that
people with old software will see future invalid signatures as always
valid; this is probably not acceptable.
* If you make it standardness flags, you will get future valid signatures
that fail to verify.

One solution is to include a version number in the signature, which
explicitly corresponds to a set of validation flags. When the version
number is something a verifier doesn't know, it can be reported as
inconclusive (it's relying on features you don't know about).

An solution is to verify twice; once with all consensus rules you know
about, and once with standardness rules. If they're both valid, the
signature is valid. If they're both invalid, the signature is invalid. If
they're different (consensus valid but standardness invalid), you report
the signature validation as inconclusive (it's relying on features you
don't know about). This approach works as long as new features only use
previous standardness-invalid scripts, but perhaps a version number is
still needed to indicate the standardness flags.

RPC commands:
>
> sign <address> <message> [<prehashed>=false]
>

Why not extend the existing signmessage/verifymessage RPC? For legacy
addresses it can fall back to the existing signature algorithm, while using
the script-based approach for all others.


>
> Generates a signature proof for <message> using the same method that
> would be used to spend coins sent to <address>.**
>
> verify <address> <message> <proof> [<prehashed>=false]
>
> Deserializes and executes the proof using a custom signature checker
> whose sighash is derived from <message>. Returns true if the check
> succeeds, and false otherwise. The scriptPubKey is derived directly
> from <address>.**
>
> Feedback welcome.
>
> -Kalle.
>
>
(**) If <prehashed> is true, <message> is the sighash, otherwise
> sighash=sha256d(message).
>

That's very dangerous I'm afraid. It could be used to trick someone into
signing off on an actual transaction, if you get them to sign a "random
looking" prehashed message. Even if you have a prehashed message, there is
no problem with treating it as hex input to a second hashing step, so I
think the prehashed option isn't needed. It's why the existing message
signing functionality always forcibly prefixes "Bitcoin signed message:",
to avoid signing something that unintentionally corresponds to a message
intended for another goal.

Cheers,

-- 
Pieter
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180326/8324b4f3/attachment.html>

From aj at erisian.com.au  Tue Mar 27 06:34:33 2018
From: aj at erisian.com.au (Anthony Towns)
Date: Tue, 27 Mar 2018 16:34:33 +1000
Subject: [bitcoin-dev] Soft-forks and schnorr signature aggregation
In-Reply-To: <CAHUJnBAQwLfmvLyApcw8dKz1u8xX1KmdjiUmzSHXyw7npWUXHQ@mail.gmail.com>
References: <CAHUJnBAQwLfmvLyApcw8dKz1u8xX1KmdjiUmzSHXyw7npWUXHQ@mail.gmail.com>
Message-ID: <20180327063433.GA24123@erisian.com.au>

On Wed, Mar 21, 2018 at 05:47:01PM -0700, Bram Cohen via bitcoin-dev wrote:
> [...] Most unused opcodes should be reclaimed as RETURN_VALID,
> but there should still be one OP_NOP and there should be a 'real' RETURN_VALID,
> which (a) is guaranteed to not be soft forked into something else in the
> future, and (b) doesn't have any parsing weirdness.

What's the reason for those? I could see an argument for RETURN_VALID, I guess:

  confA IF condB IF condC IF [pathA] RETURN_VALID ENDIF ENDIF ENDIF [pathB]

is probably simpler and saves 3 bytes compared to:

  1 condA IF condB IF condC IF [pathA] NOT ENDIF ENDIF ENDIF IF [pathB] ENDIF

but that doesn't seem crazy compelling? I don't see a reason to just keep
one OP_NOP though.

> The parsing weirdness of
> all the unclaimed opcodes is interesting. Because everything in an IF clause
> needs to be parsed in order to find where the ELSE is, you have a few options
> for dealing with an unknown opcode getting parsed in an unexecuted section of
> code. They are (a) avoid the problem completely by exterminating IF and MASTing
> (b) avoid the problem completely by getting rid of IF and adding IFJUMP,
> IFNJUMP, and JUMP which specify a number of bytes (this also allows for script
> merkleization) (c) require all new opcodes have fixed length 1, even after
> they're soft forked, (d) do almost like (c) but require that on new soft forks
> people hack their old scripts to still parse properly by avoiding the OP_ELSE
> in inopportune places (yuck!) (e) make it so that the unknown opcodes case a
> RETURN_VALID even when they're parsed, regardless of whether they're being
> executed.

I was figuring (c), fwiw, and assuming that opcodes will just be about
manipulating stack values and marking the script as invalid, rather than,
say, introducing new flow control ops.

> By far the most expedient option is (e) cause a RETURN_VALID at parse time.
> There's even precedent for this sort of behavior in the other direction with
> disabled opcodes causing failure at parse time even if they aren't being
> executed.

You're probably right. That still doesn't let you implement intercal's
COMEFROM statement as a new opcode, of course. :)

> A lot can be said about all the options, but one thing I feel like snarking
> about is that if you get rid of IFs using MAST, then it's highly unclear
> whether OP_DEPTH should be nuked as well. My feeling is that it should and that
> strict parsing should require that the bottom thing in the witness gets
> referenced at some point.

I guess when passing the script you could perhaps check if each witness
item could have been replaced with OP_FALSE or OP_1 and still get the
same result, and consider the transaction non-standard if so?

> Hacking in a multisig opcode isn't a horrible idea, but it is very stuck
> specifically on m-of-n and doesn't support more complex formulas for how
> signatures can be combined, which makes it feel hacky and weird.

Hmm? The opcode I suggested works just as easily with arbitrary formulas,
eg, "There must be at least 1 signer from pka{1,2,3}, and 3 signers all
up, except each of pkb{1,2,3,4,5,6} only counts for half":

  0 pkb6 pkb5 pkb4 pkb3 pkb2 pkb1 pka3 pka2 pka1 9 CHECK_AGGSIG_VERIFY
    (declare pubkeys)
  0b111 CHECK_AGG_SIGNERS VERIFY
    (one of pka{1,2,3} must sign)
  0b001 CHECK_AGG_SIGNERS
  0b010 CHECK_AGG_SIGNERS ADD
  0b100 CHECK_AGG_SIGNERS ADD
  DUP ADD
    (pka{1,2,3} count double)
  0b000001000 CHECK_AGG_SIGNERS ADD
  0b000010000 CHECK_AGG_SIGNERS ADD
  0b000100000 CHECK_AGG_SIGNERS ADD
  0b001000000 CHECK_AGG_SIGNERS ADD
  0b010000000 CHECK_AGG_SIGNERS ADD
  0b100000000 CHECK_AGG_SIGNERS ADD
    (pkb{1..6} count single)
  6 EQUAL
    (summing to a total of 3 doubled)

Not sure that saves it from being "hacky and weird" though...

(There are different ways you could do "CHECK_AGG_SIGNERS": for
instance, take a bitmask of keys and return the bitwise-and with the
keys that signed, or take a bitmask and just return the number of keys
matching that bitmask that signed, or take a pubkey index and return a
boolean whether that key signed)

Cheers,
aj


From karljohan-alm at garage.co.jp  Tue Mar 27 08:09:41 2018
From: karljohan-alm at garage.co.jp (Karl Johan Alm)
Date: Tue, 27 Mar 2018 17:09:41 +0900
Subject: [bitcoin-dev] {sign|verify}message replacement
In-Reply-To: <CAPg+sBhAaw7uqdz6ZoMCkut2GM=3=F22FiHw6J_aLwTMEcqt1g@mail.gmail.com>
References: <CALJw2w5=g-FL+MZ08DEoLxVzOKbSXeKu50drE1b4P0JZJpdTyA@mail.gmail.com>
	<CAPg+sBhAaw7uqdz6ZoMCkut2GM=3=F22FiHw6J_aLwTMEcqt1g@mail.gmail.com>
Message-ID: <CALJw2w7UJ6JT9jpuV9c9OtOMrqNNGR0OJauwnMo7DJfWG=L_SQ@mail.gmail.com>

Pieter,

Thanks for the feedback. Comments below:

On Mon, Mar 26, 2018 at 5:53 PM, Pieter Wuille <pieter.wuille at gmail.com> wrote:
> One solution is to include a version number in the signature, which
> explicitly corresponds to a set of validation flags. When the version number
> is something a verifier doesn't know, it can be reported as inconclusive
> (it's relying on features you don't know about).
>
> An solution is to verify twice; once with all consensus rules you know
> about, and once with standardness rules. If they're both valid, the
> signature is valid. If they're both invalid, the signature is invalid. If
> they're different (consensus valid but standardness invalid), you report the
> signature validation as inconclusive (it's relying on features you don't
> know about). This approach works as long as new features only use previous
> standardness-invalid scripts, but perhaps a version number is still needed
> to indicate the standardness flags.

I think the double verify approach seems promising. I assume old nodes
consider new consensus rule enforcing transactions as non-standard but
valid. If this is always the case, it may be an idea to simply fail
verification with a message indicating the node is unable to verify
due to unknown consensus rules.

>> RPC commands:
>>
>> sign <address> <message> [<prehashed>=false]
>
> Why not extend the existing signmessage/verifymessage RPC? For legacy
> addresses it can fall back to the existing signature algorithm, while using
> the script-based approach for all others.

Yes, I initially thought it would be better to not use it as the
legacy behavior could be depended on god knows where, but I think
adding a legacy mode or simply doing the old way for 1xx is
sufficient.

>> (**) If <prehashed> is true, <message> is the sighash, otherwise
>> sighash=sha256d(message).
>
>
> That's very dangerous I'm afraid. It could be used to trick someone into
> signing off on an actual transaction, if you get them to sign a "random
> looking" prehashed message. Even if you have a prehashed message, there is
> no problem with treating it as hex input to a second hashing step, so I
> think the prehashed option isn't needed. It's why the existing message
> signing functionality always forcibly prefixes "Bitcoin signed message:", to
> avoid signing something that unintentionally corresponds to a message
> intended for another goal.

Eek.. good point...

From jim.posen at gmail.com  Tue Mar 27 23:31:58 2018
From: jim.posen at gmail.com (Jim Posen)
Date: Tue, 27 Mar 2018 16:31:58 -0700
Subject: [bitcoin-dev] Optimized Header Sync
Message-ID: <CADZtCSg7+x-sg-ysgacXobRexOVwT+k9fr6a9S-6xU2w8f8m3A@mail.gmail.com>

Based on some ideas that were thrown around in this thread (
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-December/015385.html),
I have been working on a P2P extension that will allow faster header sync
mechanisms. The one-sentence summary is that by encoding headers more
efficiently (eg. omitting prev_hash) and downloading evenly spaced
checkpoints throughout history (say every 1,000th) from all peers first, we
could speed up header sync, which would be a huge improvement for light
clients. Here is a draft of the BIP:
https://github.com/jimpo/bips/blob/headers-sync/headersv2.mediawiki. The
full text is below as well.

I'd love to hear any feedback people have.

----------------------------------------------------------

== Abstract ==

This BIP describes a P2P network extension enabling faster, more
reliable methods for syncing the block header chain. New P2P messages
are proposed as more efficient replacements for
<code>getheaders</code> and <code>headers</code> during initial block
download. The proposed header download protocol reduces bandwidth
usage by ~40%-50% and supports downloading headers ranges from
multiple peers in parallel, which is not possible with the current
mechanism. This also enables sync strategies with better resistance to
denial-of-service attacks.

== Motivation ==

Since 2015, optimized Bitcoin clients fetch all block headers before
blocks themselves in order to avoid downloading ones that are not part
of the most work chain. The protocol currently in use for fetching
headers leaves room for further optimization, specifically by
compressing header data and downloading more headers
simulaneously<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-December/015385.html</ref>.
Any savings here should have a large impact given that both full nodes
and light clients must sync the header chain as a first step, and that
the time to validate and index the headers is negligible compared to
the time spent downloading them from the network. Furthermore, some
current implementations of headers syncing rely on preconfigured
checkpoints to discourage attackers attempting to fill up a victim's
disk space with low-work headers. The proposed messages enable sync
strategies that are resilient against these types of attacks. The P2P
messages are designed to be flexible, supporting multiple header sync
strategies and leaving room for future innovations, while also
compact.

== Definitions ==

''double-SHA256'' is a hash algorithm defined by two invocations of
SHA-256: <code>double-SHA256(x) = SHA256(SHA256(x))</code>.

== Specification ==

The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
"SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this
document are to be interpreted as described in RFC 2119.

=== New Structures ===

==== Compressed Headers ====

Bitcoin headers are serialized by default in 80 bytes as follows:

{| class="wikitable"
! Field Name
! Data Type
! Byte Size
! Description
|-
| version
| int32_t
| 4
| Block version information
|-
| prev_block
| uint256
| 32
| The hash of the previous block
|-
| merkle_root
| uint256
| 32
| The root hash of the transaction Merkle tree
|-
| timestamp
| uint32_t
| 4
| A Unix timestamp of the block creation time, as reported by the miner
|-
| bits
| uint32_t
| 4
| The calculated difficulty target for this block
|-
| nonce
| uint32_t
| 4
| A nonce that is set such that the header's hash matches the difficulty target
|}

When deserializing a correctly-formed sequence of block headers
encoded in this way, it can be noted that:

* The prev_block field should always match the double-SHA256 hash of
the previous header, making it redundant
* According to Bitcoin consensus rules, the bits field only changes
every 2016 blocks
* The version often matches that of a recent ancestor block
* The timestamp is often a small delta from the preceding header's timestamp

To take advantage of these possible savings, this document defines a
variable-sized ''compressed encoding'' of block headers that occur in
a range. Note that no savings are possible when serializing a single
header; it should only be used for vectors of sequential headers. The
full headers are reconstructed using data from previous headers in the
range. The serialization begins with an ''encoding indicator'', which
is a bitfield specifying how each field is serialized. The bits of the
indicator have the following semantics:

{| class="wikitable"
! Bit Index
! Reconstruction
! Description
|-
| 0
| <code>prev_block[i] = DSHA256(header[i-1])</code>
| The prev_block field is ommitted and assigned to the double-SHA256
hash of the previous uncompressed header.
|-
| 1
| <code>nbits[i] = nbits[i-1]</code>
| The nbits field is omitted and matches that of the previous header.
|-
| 2
| <code>timestamp[i] = timestamp[i-1] + value</code>
| The timestamp is replaced by a 2-byte signed short int, representing
an offset from the previous block's timestamp
|-
| 3
|
| Interpreted along with bits 4 & 5.
|-
| 4
|
| Interpreted along with bits 3 & 5.
|-
| 5
| <code>version[i] = version[i - ((bit[3] << 2) + (bit[4] << 1) +
bit[5])]</code>
| Bits 3, 4, and 5 are first interpreted as a 3-bit offset, with bit
index 3 as the most significant and bit index 5 as the least
significant. If the offset is non-zero, the version field is omitted
and assigned to the version of the block at the offset number of
blocks prior.
|-
| 6
|
| Reserved.
|-
| 7
|
| Reserved. May be used in a future encoding version to signal another
indicator byte.
|}

The compressed header format is versioned by a 256-bit unsigned
integer. This document defines version 0.

==== VarInt ====

''VarInt'' is a variable-length unsigned integer encoding that
supports a greater range of numbers than the standard ''CompactSize''.
This encoding was introduced at the database layer in Bitcoin
Core<ref>https://github.com/bitcoin/bitcoin/commit/4d6144f97faf9d2a6c89f41d7d2360f21f0b71e2</ref>
in 2012, but is new to the Bitcoin P2P layer.

This definition is per the code comments in Bitcoin Core written by
Pieter Wuille:

<pre>
Variable-length integers: bytes are a MSB base-128 encoding of the number.
The high bit in each byte signifies whether another digit follows. To make
the encoding is one-to-one, one is subtracted from all but the last digit.
Thus, the byte sequence a[] with length len, where all but the last byte
has bit 128 set, encodes the number:

  (a[len-1] & 0x7F) + sum(i=1..len-1, 128^i*((a[len-i-1] & 0x7F)+1))

Properties:
* Very small (0-127: 1 byte, 128-16511: 2 bytes, 16512-2113663: 3 bytes)
* Every integer has exactly one encoding
* Encoding does not depend on size of original integer type
* No redundancy: every (infinite) byte sequence corresponds to a list
  of encoded integers.

0:         [0x00]  256:        [0x81 0x00]
1:         [0x01]  16383:      [0xFE 0x7F]
127:       [0x7F]  16384:      [0xFF 0x00]
128:  [0x80 0x00]  16511: [0x80 0xFF 0x7F]
255:  [0x80 0x7F]  65535: [0x82 0xFD 0x7F]
2^32:           [0x8E 0xFE 0xFE 0xFF 0x00]
</pre>

==== Checkpoints ====

A ''checkpoint'' is defined for a block as a tuple of its hash and the
chain work:

{| class="wikitable"
! Field Name
! Data Type
! Byte Size
! Description
|-
| block_hash
| uint256
| 32
| The hash of the block
|-
| chain_work
| VarInt
| Variable(1-20)
| A delta between the total work in the chain at the checkpoint block
and a previous checkpoint, determined by context
|}

=== Service Bit ===

This BIP allocates a new service bit:

{| class="wikitable"
|-
| NODE_HEADERS_V2
| <code>1 << ?</code>
| If enabled, the node MUST respond to <code>getcheckpts</code> and
<code>getheaders2</code> queries
|}

=== New Messages ===

==== getcheckpts ====
<code>getcheckpts</code> is used to request block headers at a
specified distance from each other which serve as checkpoints during
parallel header download. The message contains the following fields:

{| class="wikitable"
! Field Name
! Data Type
! Byte Size
! Description
|-
| block_locator
| uint256[]
| Variable
| A vector of block hashes in descending order by height used to
identify the header chain of the requesting node
|-
| interval
| uint32_t
| 4
| The distance in block height between requested block hashes
|}

# Nodes SHOULD NOT send <code>getcheckpts</code> unless the peer has
set the <code>NODE_HEADERS_V2</code> service bit
# The hashes in <code>block_locator</code> MUST be in descending order
by block height
# The block locator SHOULD be generated as it is in
<code>getheaders</code> requests
# The receiving node MUST respond to valid requests with a
<code>checkpts</code> response where the interval is the same as in
the request and the first checkpoint hash matches the first common
block hash in the block locator

==== checkpts ====
<code>checkpts</code> is sent in response to <code>getcheckpts</code>,
listing block hashes at the specified interval. The message contains
the following fields:

{| class="wikitable"
! Field Name
! Data Type
! Byte Size
! Description
|-
| start_height
| uint32_t
| 4
| The height of the first block in the active chain matching the
request's block locator
|-
| end_height
| uint32_t
| 4
| The height of the last block in the active chain
|-
| start_checkpoint
| Checkpoint
| 48
| The checkpoint structure for the block in the active chain at height
start_height
|-
| end_checkpoint
| Checkpoint
| 48
| The checkpoint structure for the block in the active chain at height
end_height
|-
| interval
| uint32_t
| 4
| The distance in block height between checkpoints
|-
| checkpoints_length
| CompactSize
| Variable(1-5)
| The number of checkpoints to follow
|-
| checkpoints
| Checkpoint[]
| checkoints_length * Variable(33-52)
| The checkpoints as specified below
|}

# The interval SHOULD match the field in the <code>getcheckpts</code> request
# The start_checkpoint SHOULD correspond to the first block hash in
the locator from the <code>getcheckpts</code> request that is part of
the active chain
# The end_checkpoint SHOULD correspond to the tip of the node's active chain
# The start_height MOST be set to the block height of the start_checkpoint
# The end_height MOST be set to the block height of the end_checkpoint
# If the interval is zero, the checkpoints vector MUST be empty
# If the interval is non-zero, checkpoints MUST correspond to blocks
on the active chain between the start_checkpoint and the
end_checkpoint (exclusive), where the difference in block height
between each entry and the previous one is equal to the interval
# The checkpoints_length MUST be less than or equal to 2,000
# The node SHOULD include as many checkpoints on its active chain as
are available, up to the limit of 2,000
# The chain_work field in the first checkpoint MUST be the total work
in the chain ending at that block
# The chain_work field in each subsequent checkpoint MUST be the
difference in chain work between that block and the previous
checkpoint
# The chain_work field in each checkpoint MUST be a properly-encoded
VarInt, not exceeding 20 bytes

==== getheaders2 ====
<code>getheaders2</code> is used to request compressed headers for a
range of blocks. The message contains the following fields:

{| class="wikitable"
! Field Name
! Data Type
! Byte Size
! Description
|-
| max_version
| uint8_t
| 1
| The maximum supported encoding version of the headers
|-
| flags
| uint8_t
| 1
| A bitfield of message encoding flags
|-
| start_height
| uint32_t
| 4
| The height of the first block header in the requested range
|-
| end_hash
| uint256
| 32
| The hash of the last block header in the requested range
|}

# Nodes SHOULD NOT send <code>getheaders2</code> unless the peer has
set the <code>NODE_HEADERS_V2</code> service bit
# The height of the block with hash end_hash MUST be greater than or
equal to start_height, and the difference MUST be strictly less than
3,000
# The end_hash SHOULD match one in a previously received
<code>checkpts</code> message, otherwise the receiving node MAY
disconnect
# The 0th bit (least significant order) of the flags field MAY be set
to request the coinbase transaction and merkle branch for the block at
height start_height

==== headers2 ====
<code>headers2</code> is sent in response to <code>getheaders2</code>,
listing the compressed headers in the requested range. The message
contains the following fields:

{| class="wikitable"
! Field Name
! Data Type
! Byte Size
! Description
|-
| version
| uint8_t
| 1
| The encoding version of the headers
|-
| flags
| uint8_t
| 1
| A bitfield of message encoding flags
|-
| start_height
| uint32_t
| 4
| The height of the first block header returned
|-
| headers_length
| CompactSize
| 1-3
| The number of block headers to follow
|-
| headers
| CompressedHeader[]
| Variable
| The compressed block headers
|-
| start_block_coinbase_tx
| CTransaction
| Variable
| The coinbase transaction in the block at start_height
|-
| start_block_coinbase_branch
| uint256[]
| Variable
| A merkle branch linking the coinbase transaction in the block at
start_height to its header
|}

# The version MUST be less than or equal to the max_version field of
the <code>getheaders2</code> request
# Any bits set in the flags field of the <code>getheaders2</code>
request MAY be set in the response field
# Any bits not set in the flags field of the <code>getheaders2</code>
request MUST NOT be set in the response field
# The first header MUST be encoded with a 0-byte indicator (ie. the
header is uncompressed)
# start_height MUST be set to the block height of the first header
# The hash of the last block SHOULD equal the end_hash of the
<code>getheaders2</code> request, ''even if the block is no longer
part of the active chain''
# The length of the headers vector MUST be less than or equal to 3,000
# The headers MUST be sequential in order of height, with each header
a successor of the previous one
# Each header SHOULD be optimally compressed
# The start_block_coinbase_tx should be the serialized coinbase
transaction in the block corresponding to the first header
# The start_block_coinbase_branch should be a vector of
right-hand-side hashes in the merkle branch linking the coinbase
transaction to the first header, in order from bottom of the tree to
top
# If the 0th bit (least significant order) of the flags field is
unset, the start_block_coinbase_tx and start_block_coinbase_branch
fields MUST be omitted

=== Sync Strategies ===

The general header sync protocol for clients now is to first request
checkpoints from all peers with <code>getcheckpts</code>, then decide
which peers to fetch ranges of headers from and download them with
<code>getheaders2</code>.

==== Forward Sequential Syncing ====

Similar to the current sync protocol, a client may choose one peer to
download headers from, then fetch them in forward sequential order.
Once this peer is out of headers, the client performs the same routine
with any peers offering more headers.

With this strategy, the client is able to fully validate the block
headers in order and abort if the peer serves an invalid one. On the
other hand, the peer may be able to serve a longer, lower-work chain
than the global active chain, wasting the client's time, memory, and
storage space.

==== Parallel Header Download ====

In order to increase the throughput of header downloads, a node may
download multiple header ranges in parallel from all peers serving the
same checkpoints, then validate them in sequential order.

==== Random Sampling Proof-of-Work  ====

Similar the FlyClient<ref>https://www.youtube.com/watch?time_continue=8400&v=BPNs9EVxWrA</ref>
header download protocol, clients can select the peer claiming the
greatest total work chain and use random sampling to efficiently
determine if the peer is likely to be reporting its chain work
honestly.

The client treats the checkpoint message as a commitment to chain work
of intermediate ranges of headers, the client then randomly samples
ranges of headers weighted by total work to determine whether the
total chain work is valid before downloading all headers. To defend
against malicious peers attempting to reuse earlier headers later in
the chain to fake greater total work, the client should check the
block height in the coinbase transaction for all headers after the BIP
34 activation height. If the peer is found to be dishonest, they can
be banned before the client downloads too many headers, otherwise the
client chooses this as the primary sync peer for forward sequential
sync or parallel download.

== Rationale ==

* '''Why include the coinbase transaction in the headers messages?'''
The primary reason is that after BIP
34<ref>https://github.com/bitcoin/bips/blob/master/bip-0034.mediawiki</ref>
activation at block height 227,835, coinbase transactions constitute
cryptographic commitments to a block's height in the chain, which
mitigates certain attacks during header sync. Furthermore, the
<code>getheaders2</code> message can be used as a simple way of
requesting a coinbase transaction for a single header, which may be
independently useful.

* '''Why not omit nBits entirely?''' The compression is designed to
permit full decompression of all headers in a <code>headers2</code>
message ''without'' requiring any other chain context. This is
desirable so that proofs of work may be validated for arbitrary header
ranges. While nBits can be computed knowing previous headers, this
requires block headers that may not be sent in the same message.

== Compatibility ==

This is backwards compatible, as it defines new P2P messages which are
available if a service bit is signaled. There are no changes to
consensus rules.

== Acknowledgements ==

Thanks to Gregory Maxwell for suggestions on the compressed header
encoding and the DOS-resistant sync strategies. Thanks to Suhas
Daftuar for helpful discussions.

Credit for the VarInt encoding goes to Pieter Wuille.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180327/65c9ed79/attachment-0001.html>

From bram at chia.net  Wed Mar 28 03:19:48 2018
From: bram at chia.net (Bram Cohen)
Date: Tue, 27 Mar 2018 20:19:48 -0700
Subject: [bitcoin-dev] Soft-forks and schnorr signature aggregation
In-Reply-To: <20180327063433.GA24123@erisian.com.au>
References: <CAHUJnBAQwLfmvLyApcw8dKz1u8xX1KmdjiUmzSHXyw7npWUXHQ@mail.gmail.com>
	<20180327063433.GA24123@erisian.com.au>
Message-ID: <CAHUJnBBkXvFwgJH7OYkR1FWCCgerMH4SaUGv2kN3c54vqZFomg@mail.gmail.com>

On Mon, Mar 26, 2018 at 11:34 PM, Anthony Towns <aj at erisian.com.au> wrote:

> On Wed, Mar 21, 2018 at 05:47:01PM -0700, Bram Cohen via bitcoin-dev wrote:
> > [...] Most unused opcodes should be reclaimed as RETURN_VALID,
> > but there should still be one OP_NOP and there should be a 'real'
> RETURN_VALID,
> > which (a) is guaranteed to not be soft forked into something else in the
> > future, and (b) doesn't have any parsing weirdness.
>
> What's the reason for those? I could see an argument for RETURN_VALID, I
> guess:
>
>   confA IF condB IF condC IF [pathA] RETURN_VALID ENDIF ENDIF ENDIF [pathB]
>
> is probably simpler and saves 3 bytes compared to:
>
>   1 condA IF condB IF condC IF [pathA] NOT ENDIF ENDIF ENDIF IF [pathB]
> ENDIF
>
> but that doesn't seem crazy compelling?


Mostly yes it's for that case and also for:

   condA IF RETURN_VALID ENDIF condb IF RETURN_VALID ENDIF condc

Technically that can be done with fewer opcodes using OP_BOOLOR but maybe
in the future there will be some incentive for short circuit evaluation

But there's also the general principle that it's only one opcode and if
there are a lot of things which look like RETURN_VALID there should be one
thing which actually is RETURN_VALID


> I don't see a reason to just keep one OP_NOP though.
>

Mostly based on momentum because there are several of them there right now.
If noone else wants to defend it I won't either.


> > By far the most expedient option is (e) cause a RETURN_VALID at parse
> time.
> > There's even precedent for this sort of behavior in the other direction
> with
> > disabled opcodes causing failure at parse time even if they aren't being
> > executed.
>
> You're probably right. That still doesn't let you implement intercal's
> COMEFROM statement as a new opcode, of course. :)
>

That can be in the hardfork wishlist :-)


> > A lot can be said about all the options, but one thing I feel like
> snarking
> > about is that if you get rid of IFs using MAST, then it's highly unclear
> > whether OP_DEPTH should be nuked as well. My feeling is that it should
> and that
> > strict parsing should require that the bottom thing in the witness gets
> > referenced at some point.
>
> I guess when passing the script you could perhaps check if each witness
> item could have been replaced with OP_FALSE or OP_1 and still get the
> same result, and consider the transaction non-standard if so?
>

Essentially all opcodes including OP_PICK make clear at runtime how deep
they go and anything below the max depth can be safely eliminated (or used
as grounds for rejecting in strict mode). The big exception is OP_DEPTH
which totally mangles the assumptions. It's trivial to make scripts which
use OP_DEPTH which become invalid with things added below the stack then go
back to being valid again with more things added even though the individual
items are never even accessed.


>
> > Hacking in a multisig opcode isn't a horrible idea, but it is very stuck
> > specifically on m-of-n and doesn't support more complex formulas for how
> > signatures can be combined, which makes it feel hacky and weird.
>
> Hmm? The opcode I suggested works just as easily with arbitrary formulas,
> eg, "There must be at least 1 signer from pka{1,2,3}, and 3 signers all
> up, except each of pkb{1,2,3,4,5,6} only counts for half":
>
>   0 pkb6 pkb5 pkb4 pkb3 pkb2 pkb1 pka3 pka2 pka1 9 CHECK_AGGSIG_VERIFY
>     (declare pubkeys)
>   0b111 CHECK_AGG_SIGNERS VERIFY
>     (one of pka{1,2,3} must sign)
>   0b001 CHECK_AGG_SIGNERS
>   0b010 CHECK_AGG_SIGNERS ADD
>   0b100 CHECK_AGG_SIGNERS ADD
>   DUP ADD
>     (pka{1,2,3} count double)
>   0b000001000 CHECK_AGG_SIGNERS ADD
>   0b000010000 CHECK_AGG_SIGNERS ADD
>   0b000100000 CHECK_AGG_SIGNERS ADD
>   0b001000000 CHECK_AGG_SIGNERS ADD
>   0b010000000 CHECK_AGG_SIGNERS ADD
>   0b100000000 CHECK_AGG_SIGNERS ADD
>     (pkb{1..6} count single)
>   6 EQUAL
>     (summing to a total of 3 doubled)
>
> Not sure that saves it from being "hacky and weird" though...
>

That is very hacky and weird. Doing MAST on lots of possibilities is always
reasonably elegant, and it only gets problematic when the number of
possibilities is truly massive.

It's also the case that BLS can support complex key agreement schemes
without even giving away that it isn't a simple single signature. Just
saying.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180327/51bdea13/attachment.html>

From jtimon at jtimon.cc  Wed Mar 28 12:55:26 2018
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Wed, 28 Mar 2018 14:55:26 +0200
Subject: [bitcoin-dev] Soft Fork Activation & Enforcement w/o Signaling?
In-Reply-To: <CAAQZUuDEJeMFTxxJcgUEmTUQbxM_ZWkBD1k+UOvafsqbqj++Jg@mail.gmail.com>
References: <CAAQZUuDEJeMFTxxJcgUEmTUQbxM_ZWkBD1k+UOvafsqbqj++Jg@mail.gmail.com>
Message-ID: <CABm2gDq2pa_8T7Xhniuyh86eTi=PmSA_t=2Z0nYp1LhN=zc_NA@mail.gmail.com>

Yes, you can activate softforks at a given height.
I don't see any reason why you couldn't rebase to 0.16 directly.
The block version bumping was a mistake in bip34, you don't really
need to bump the version number. In any case, I would recommend
reading bip34 and what it activates in the code. IIRC the last thing
was bip65.

On Wed, Mar 21, 2018 at 11:04 PM, Samad Sajanlal via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
> Is it possible to activate soft forks such as BIP65 and BIP66 without prior
> signaling from miners? I noticed in chainparams.cpp that there are block
> heights where the enforcement begins.
>
> I understand this is already active on bitcoin. I'm working on a project
> that is a clone of a clone of bitcoin, and we currently do not have BIP65 or
> BIP66 enforced - no signaling of these soft forks either (most of the
> network is on a source code fork of bitcoin 0.9). This project does not and
> never intends to attempt to replace bitcoin - we know that without bitcoin
> our project could never exist, so we owe a great deal of gratitude to the
> bitcoin developers.
>
> If the entire network upgrades to the correct version of the software (based
> on bitcoin 0.15), which includes the block height that has enforcement, can
> we simply skip over the signaling and go straight into
> activation/enforcement?
>
> At this time we are lucky that our network is very small, so it is
> reasonable to assume that the whole network will upgrade their clients
> within a short window (~2 weeks). We would schedule the activation ~2 months
> out from when the client is released, just to ensure everyone has time to
> upgrade.
>
> We have been stuck on the 0.9 code branch and my goal is to bring it up to
> 0.15 at least, so that we can implement Segwit and other key features that
> bitcoin has introduced. The 0.15 client currently works with regards to
> sending and receiving transactions but the soft forks are not active. I
> understand that activating them will segregate the 0.15 clients onto their
> own fork, which is why I'd like to understand the repercussions of doing it
> without any signaling beforehand. I also would prefer not to have to make
> intermediate releases such as 0.10, 0.11.. etc to get the soft forks
> activated.
>
> Another related question - does the block version get bumped up
> automatically at the time that a soft fork activates, or is there additional
> stuff that I need to do within the code to ensure it bumps up at the same
> time? From what I saw in the code it appears that it will bump up
> automatically, but I would like some confirmation on that.
>
> Regards,
> Samad
>
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>

From belcher at riseup.net  Thu Mar 29 12:07:04 2018
From: belcher at riseup.net (Chris Belcher)
Date: Thu, 29 Mar 2018 13:07:04 +0100
Subject: [bitcoin-dev] Electrum Personal Server beta release
Message-ID: <4b1b0a84-48db-71ce-ddfd-7d7ba1386799@riseup.net>

Electrum Personal Server is an implementation of the Electrum wallet
server protocol that allows users to point their Electrum wallet at
their own full node. It is compatible resource-saving features like
pruning, blocksonly and disabled txindex. It is much less
resource-intensive than other Electrum servers because it only stores
the user's own addresses, instead of every address that was ever used.
As such it makes tradeoffs, namely losing Electrum's "instant on" feature.

Right now using Electrum Personal Server is the easiest way to use a
hardware wallet backed by your own full node. It is very lightweight,
being a small python script that barely uses any CPU or RAM; much less
than the full node it's connected to. Hopefully Electrum Personal Server
can be part of the solution in putting full node wallets into the hands
of as many people as possible.

The project is now in beta release:
https://github.com/chris-belcher/electrum-personal-server

It now has all the essential features to make it practical for use;
Merkle proofs, deterministic wallets, bech32 addresses, SSL, Core's
multi-wallet support. Along with the features that were in the alpha
release of tracking new transactions, confirmations, block headers,
importing addresses.

There is a caveat about pruning. Electrum Personal Server obtains merkle
proofs using the `gettxoutproof` RPC call, if pruning is enabled and
that block has been deleted then the RPC will return null and so the
Electrum wallet will display `Not Verified`. Everything else will still
work, and this shouldn't be a problem in most situations because
Electrum usually only requests merkle proofs for recent transactions and
pruning keeps recent blocks. But in the long term it needs some thought
on the best way to fix this. I've been thinking about adding code for
Bitcoin Core that stores merkle proofs for each of the wallet's own
transactions in wallet.dat.

Further Reading:
*
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-February/015707.html
* https://bitcointalk.org/index.php?topic=3167572.0

From riccardo.casatta at gmail.com  Thu Mar 29 08:17:12 2018
From: riccardo.casatta at gmail.com (Riccardo Casatta)
Date: Thu, 29 Mar 2018 10:17:12 +0200
Subject: [bitcoin-dev] Optimized Header Sync
In-Reply-To: <CADZtCSg7+x-sg-ysgacXobRexOVwT+k9fr6a9S-6xU2w8f8m3A@mail.gmail.com>
References: <CADZtCSg7+x-sg-ysgacXobRexOVwT+k9fr6a9S-6xU2w8f8m3A@mail.gmail.com>
Message-ID: <CADabwBAjTRdVqsL+V=YdQ+kr8+LtSPOeSXUJOzKoPNdKEbAOWQ@mail.gmail.com>

Hi Jim,

| <code>version[i] = version[i - ((bit[3] << 2) + (bit[4] << 1) +
bit[5])]</code>


Thought this wasn't effective in case overt asic boost get widely adopted,
but then I understood that at the moment only two bits of version get
scrambled by that technique so this looks fine, maybe add a comment about
this so the reader doesn't get the same initial doubt I got.

...downloading evenly spaced checkpoints throughout history (say every
> 1,000th) from all peers first...


My feeling is that encoding of the headers and checkpoints/parallel
download are separate subjects for two BIPS.
About the checkpoints I don't grasp why they are useful since an attacker
could lie about them but maybe I am missing something...

To take advantage of these possible savings, this document defines a
> variable-sized ''compressed encoding'' of block headers that occur in a
> range. Note that no savings are possible when serializing a single header;
> it should only be used for vectors of sequential headers. The full headers
> are reconstructed using data from previous headers in the range. The
> serialization begins with an ''encoding indicator'', which is a bitfield
> specifying how each field is serialized. The bits of the indicator have the
> following semantics:


Bitfield allows great savings, however the encoding depends on the headers
height a client ask for, this cause a little computational burden on the
node and the undesirable side effect of difficult caching. Variable length
encoding cause caching difficulties too...
A simpler approach could be to encode the headers in groups of 2016 headers
(the difficulty period) where the first header is complete and the others
2015 are missing the previous hash and the difficulty, this achieve
comparable savings ~45%, allows better caching and has fixed length
encoding. This could be useful for the node by caching headers on a single
file on disk and simply stream out the relative range when requested or to
serve the same encoded headers format in other context like http,
leveraging http caching infrastructure.



2018-03-28 1:31 GMT+02:00 Jim Posen via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org>:

> Based on some ideas that were thrown around in this thread (https://lists.
> linuxfoundation.org/pipermail/bitcoin-dev/2017-December/015385.html), I
> have been working on a P2P extension that will allow faster header sync
> mechanisms. The one-sentence summary is that by encoding headers more
> efficiently (eg. omitting prev_hash) and downloading evenly spaced
> checkpoints throughout history (say every 1,000th) from all peers first, we
> could speed up header sync, which would be a huge improvement for light
> clients. Here is a draft of the BIP: https://github.com/jimpo/
> bips/blob/headers-sync/headersv2.mediawiki. The full text is below as
> well.
>
> I'd love to hear any feedback people have.
>
> ----------------------------------------------------------
>
> == Abstract ==
>
> This BIP describes a P2P network extension enabling faster, more reliable methods for syncing the block header chain. New P2P messages are proposed as more efficient replacements for <code>getheaders</code> and <code>headers</code> during initial block download. The proposed header download protocol reduces bandwidth usage by ~40%-50% and supports downloading headers ranges from multiple peers in parallel, which is not possible with the current mechanism. This also enables sync strategies with better resistance to denial-of-service attacks.
>
> == Motivation ==
>
> Since 2015, optimized Bitcoin clients fetch all block headers before blocks themselves in order to avoid downloading ones that are not part of the most work chain. The protocol currently in use for fetching headers leaves room for further optimization, specifically by compressing header data and downloading more headers simulaneously<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-December/015385.html</ref>. Any savings here should have a large impact given that both full nodes and light clients must sync the header chain as a first step, and that the time to validate and index the headers is negligible compared to the time spent downloading them from the network. Furthermore, some current implementations of headers syncing rely on preconfigured checkpoints to discourage attackers attempting to fill up a victim's disk space with low-work headers. The proposed messages enable sync strategies that are resilient against these types of attacks. The P2P messages are designed to be flexible, supporting multiple header sync strategies and leaving room for future innovations, while also compact.
>
> == Definitions ==
>
> ''double-SHA256'' is a hash algorithm defined by two invocations of SHA-256: <code>double-SHA256(x) = SHA256(SHA256(x))</code>.
>
> == Specification ==
>
> The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT", "SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this document are to be interpreted as described in RFC 2119.
>
> === New Structures ===
>
> ==== Compressed Headers ====
>
> Bitcoin headers are serialized by default in 80 bytes as follows:
>
> {| class="wikitable"
> ! Field Name
> ! Data Type
> ! Byte Size
> ! Description
> |-
> | version
> | int32_t
> | 4
> | Block version information
> |-
> | prev_block
> | uint256
> | 32
> | The hash of the previous block
> |-
> | merkle_root
> | uint256
> | 32
> | The root hash of the transaction Merkle tree
> |-
> | timestamp
> | uint32_t
> | 4
> | A Unix timestamp of the block creation time, as reported by the miner
> |-
> | bits
> | uint32_t
> | 4
> | The calculated difficulty target for this block
> |-
> | nonce
> | uint32_t
> | 4
> | A nonce that is set such that the header's hash matches the difficulty target
> |}
>
> When deserializing a correctly-formed sequence of block headers encoded in this way, it can be noted that:
>
> * The prev_block field should always match the double-SHA256 hash of the previous header, making it redundant
> * According to Bitcoin consensus rules, the bits field only changes every 2016 blocks
> * The version often matches that of a recent ancestor block
> * The timestamp is often a small delta from the preceding header's timestamp
>
> To take advantage of these possible savings, this document defines a variable-sized ''compressed encoding'' of block headers that occur in a range. Note that no savings are possible when serializing a single header; it should only be used for vectors of sequential headers. The full headers are reconstructed using data from previous headers in the range. The serialization begins with an ''encoding indicator'', which is a bitfield specifying how each field is serialized. The bits of the indicator have the following semantics:
>
> {| class="wikitable"
> ! Bit Index
> ! Reconstruction
> ! Description
> |-
> | 0
> | <code>prev_block[i] = DSHA256(header[i-1])</code>
> | The prev_block field is ommitted and assigned to the double-SHA256 hash of the previous uncompressed header.
> |-
> | 1
> | <code>nbits[i] = nbits[i-1]</code>
> | The nbits field is omitted and matches that of the previous header.
> |-
> | 2
> | <code>timestamp[i] = timestamp[i-1] + value</code>
> | The timestamp is replaced by a 2-byte signed short int, representing an offset from the previous block's timestamp
> |-
> | 3
> |
> | Interpreted along with bits 4 & 5.
> |-
> | 4
> |
> | Interpreted along with bits 3 & 5.
> |-
> | 5
> | <code>version[i] = version[i - ((bit[3] << 2) + (bit[4] << 1) + bit[5])]</code>
> | Bits 3, 4, and 5 are first interpreted as a 3-bit offset, with bit index 3 as the most significant and bit index 5 as the least significant. If the offset is non-zero, the version field is omitted and assigned to the version of the block at the offset number of blocks prior.
> |-
> | 6
> |
> | Reserved.
> |-
> | 7
> |
> | Reserved. May be used in a future encoding version to signal another indicator byte.
> |}
>
> The compressed header format is versioned by a 256-bit unsigned integer. This document defines version 0.
>
> ==== VarInt ====
>
> ''VarInt'' is a variable-length unsigned integer encoding that supports a greater range of numbers than the standard ''CompactSize''. This encoding was introduced at the database layer in Bitcoin Core<ref>https://github.com/bitcoin/bitcoin/commit/4d6144f97faf9d2a6c89f41d7d2360f21f0b71e2</ref> in 2012, but is new to the Bitcoin P2P layer.
>
> This definition is per the code comments in Bitcoin Core written by Pieter Wuille:
>
> <pre>
> Variable-length integers: bytes are a MSB base-128 encoding of the number.
> The high bit in each byte signifies whether another digit follows. To make
> the encoding is one-to-one, one is subtracted from all but the last digit.
> Thus, the byte sequence a[] with length len, where all but the last byte
> has bit 128 set, encodes the number:
>
>   (a[len-1] & 0x7F) + sum(i=1..len-1, 128^i*((a[len-i-1] & 0x7F)+1))
>
> Properties:
> * Very small (0-127: 1 byte, 128-16511: 2 bytes, 16512-2113663: 3 bytes)
> * Every integer has exactly one encoding
> * Encoding does not depend on size of original integer type
> * No redundancy: every (infinite) byte sequence corresponds to a list
>   of encoded integers.
>
> 0:         [0x00]  256:        [0x81 0x00]
> 1:         [0x01]  16383:      [0xFE 0x7F]
> 127:       [0x7F]  16384:      [0xFF 0x00]
> 128:  [0x80 0x00]  16511: [0x80 0xFF 0x7F]
> 255:  [0x80 0x7F]  65535: [0x82 0xFD 0x7F]
> 2^32:           [0x8E 0xFE 0xFE 0xFF 0x00]
> </pre>
>
> ==== Checkpoints ====
>
> A ''checkpoint'' is defined for a block as a tuple of its hash and the chain work:
>
> {| class="wikitable"
> ! Field Name
> ! Data Type
> ! Byte Size
> ! Description
> |-
> | block_hash
> | uint256
> | 32
> | The hash of the block
> |-
> | chain_work
> | VarInt
> | Variable(1-20)
> | A delta between the total work in the chain at the checkpoint block and a previous checkpoint, determined by context
> |}
>
> === Service Bit ===
>
> This BIP allocates a new service bit:
>
> {| class="wikitable"
> |-
> | NODE_HEADERS_V2
> | <code>1 << ?</code>
> | If enabled, the node MUST respond to <code>getcheckpts</code> and <code>getheaders2</code> queries
> |}
>
> === New Messages ===
>
> ==== getcheckpts ====
> <code>getcheckpts</code> is used to request block headers at a specified distance from each other which serve as checkpoints during parallel header download. The message contains the following fields:
>
> {| class="wikitable"
> ! Field Name
> ! Data Type
> ! Byte Size
> ! Description
> |-
> | block_locator
> | uint256[]
> | Variable
> | A vector of block hashes in descending order by height used to identify the header chain of the requesting node
> |-
> | interval
> | uint32_t
> | 4
> | The distance in block height between requested block hashes
> |}
>
> # Nodes SHOULD NOT send <code>getcheckpts</code> unless the peer has set the <code>NODE_HEADERS_V2</code> service bit
> # The hashes in <code>block_locator</code> MUST be in descending order by block height
> # The block locator SHOULD be generated as it is in <code>getheaders</code> requests
> # The receiving node MUST respond to valid requests with a <code>checkpts</code> response where the interval is the same as in the request and the first checkpoint hash matches the first common block hash in the block locator
>
> ==== checkpts ====
> <code>checkpts</code> is sent in response to <code>getcheckpts</code>, listing block hashes at the specified interval. The message contains the following fields:
>
> {| class="wikitable"
> ! Field Name
> ! Data Type
> ! Byte Size
> ! Description
> |-
> | start_height
> | uint32_t
> | 4
> | The height of the first block in the active chain matching the request's block locator
> |-
> | end_height
> | uint32_t
> | 4
> | The height of the last block in the active chain
> |-
> | start_checkpoint
> | Checkpoint
> | 48
> | The checkpoint structure for the block in the active chain at height start_height
> |-
> | end_checkpoint
> | Checkpoint
> | 48
> | The checkpoint structure for the block in the active chain at height end_height
> |-
> | interval
> | uint32_t
> | 4
> | The distance in block height between checkpoints
> |-
> | checkpoints_length
> | CompactSize
> | Variable(1-5)
> | The number of checkpoints to follow
> |-
> | checkpoints
> | Checkpoint[]
> | checkoints_length * Variable(33-52)
> | The checkpoints as specified below
> |}
>
> # The interval SHOULD match the field in the <code>getcheckpts</code> request
> # The start_checkpoint SHOULD correspond to the first block hash in the locator from the <code>getcheckpts</code> request that is part of the active chain
> # The end_checkpoint SHOULD correspond to the tip of the node's active chain
> # The start_height MOST be set to the block height of the start_checkpoint
> # The end_height MOST be set to the block height of the end_checkpoint
> # If the interval is zero, the checkpoints vector MUST be empty
> # If the interval is non-zero, checkpoints MUST correspond to blocks on the active chain between the start_checkpoint and the end_checkpoint (exclusive), where the difference in block height between each entry and the previous one is equal to the interval
> # The checkpoints_length MUST be less than or equal to 2,000
> # The node SHOULD include as many checkpoints on its active chain as are available, up to the limit of 2,000
> # The chain_work field in the first checkpoint MUST be the total work in the chain ending at that block
> # The chain_work field in each subsequent checkpoint MUST be the difference in chain work between that block and the previous checkpoint
> # The chain_work field in each checkpoint MUST be a properly-encoded VarInt, not exceeding 20 bytes
>
> ==== getheaders2 ====
> <code>getheaders2</code> is used to request compressed headers for a range of blocks. The message contains the following fields:
>
> {| class="wikitable"
> ! Field Name
> ! Data Type
> ! Byte Size
> ! Description
> |-
> | max_version
> | uint8_t
> | 1
> | The maximum supported encoding version of the headers
> |-
> | flags
> | uint8_t
> | 1
> | A bitfield of message encoding flags
> |-
> | start_height
> | uint32_t
> | 4
> | The height of the first block header in the requested range
> |-
> | end_hash
> | uint256
> | 32
> | The hash of the last block header in the requested range
> |}
>
> # Nodes SHOULD NOT send <code>getheaders2</code> unless the peer has set the <code>NODE_HEADERS_V2</code> service bit
> # The height of the block with hash end_hash MUST be greater than or equal to start_height, and the difference MUST be strictly less than 3,000
> # The end_hash SHOULD match one in a previously received <code>checkpts</code> message, otherwise the receiving node MAY disconnect
> # The 0th bit (least significant order) of the flags field MAY be set to request the coinbase transaction and merkle branch for the block at height start_height
>
> ==== headers2 ====
> <code>headers2</code> is sent in response to <code>getheaders2</code>, listing the compressed headers in the requested range. The message contains the following fields:
>
> {| class="wikitable"
> ! Field Name
> ! Data Type
> ! Byte Size
> ! Description
> |-
> | version
> | uint8_t
> | 1
> | The encoding version of the headers
> |-
> | flags
> | uint8_t
> | 1
> | A bitfield of message encoding flags
> |-
> | start_height
> | uint32_t
> | 4
> | The height of the first block header returned
> |-
> | headers_length
> | CompactSize
> | 1-3
> | The number of block headers to follow
> |-
> | headers
> | CompressedHeader[]
> | Variable
> | The compressed block headers
> |-
> | start_block_coinbase_tx
> | CTransaction
> | Variable
> | The coinbase transaction in the block at start_height
> |-
> | start_block_coinbase_branch
> | uint256[]
> | Variable
> | A merkle branch linking the coinbase transaction in the block at start_height to its header
> |}
>
> # The version MUST be less than or equal to the max_version field of the <code>getheaders2</code> request
> # Any bits set in the flags field of the <code>getheaders2</code> request MAY be set in the response field
> # Any bits not set in the flags field of the <code>getheaders2</code> request MUST NOT be set in the response field
> # The first header MUST be encoded with a 0-byte indicator (ie. the header is uncompressed)
> # start_height MUST be set to the block height of the first header
> # The hash of the last block SHOULD equal the end_hash of the <code>getheaders2</code> request, ''even if the block is no longer part of the active chain''
> # The length of the headers vector MUST be less than or equal to 3,000
> # The headers MUST be sequential in order of height, with each header a successor of the previous one
> # Each header SHOULD be optimally compressed
> # The start_block_coinbase_tx should be the serialized coinbase transaction in the block corresponding to the first header
> # The start_block_coinbase_branch should be a vector of right-hand-side hashes in the merkle branch linking the coinbase transaction to the first header, in order from bottom of the tree to top
> # If the 0th bit (least significant order) of the flags field is unset, the start_block_coinbase_tx and start_block_coinbase_branch fields MUST be omitted
>
> === Sync Strategies ===
>
> The general header sync protocol for clients now is to first request checkpoints from all peers with <code>getcheckpts</code>, then decide which peers to fetch ranges of headers from and download them with <code>getheaders2</code>.
>
> ==== Forward Sequential Syncing ====
>
> Similar to the current sync protocol, a client may choose one peer to download headers from, then fetch them in forward sequential order. Once this peer is out of headers, the client performs the same routine with any peers offering more headers.
>
> With this strategy, the client is able to fully validate the block headers in order and abort if the peer serves an invalid one. On the other hand, the peer may be able to serve a longer, lower-work chain than the global active chain, wasting the client's time, memory, and storage space.
>
> ==== Parallel Header Download ====
>
> In order to increase the throughput of header downloads, a node may download multiple header ranges in parallel from all peers serving the same checkpoints, then validate them in sequential order.
>
> ==== Random Sampling Proof-of-Work  ====
>
> Similar the FlyClient<ref>https://www.youtube.com/watch?time_continue=8400&v=BPNs9EVxWrA</ref> header download protocol, clients can select the peer claiming the greatest total work chain and use random sampling to efficiently determine if the peer is likely to be reporting its chain work honestly.
>
> The client treats the checkpoint message as a commitment to chain work of intermediate ranges of headers, the client then randomly samples ranges of headers weighted by total work to determine whether the total chain work is valid before downloading all headers. To defend against malicious peers attempting to reuse earlier headers later in the chain to fake greater total work, the client should check the block height in the coinbase transaction for all headers after the BIP 34 activation height. If the peer is found to be dishonest, they can be banned before the client downloads too many headers, otherwise the client chooses this as the primary sync peer for forward sequential sync or parallel download.
>
> == Rationale ==
>
> * '''Why include the coinbase transaction in the headers messages?''' The primary reason is that after BIP 34<ref>https://github.com/bitcoin/bips/blob/master/bip-0034.mediawiki</ref> activation at block height 227,835, coinbase transactions constitute cryptographic commitments to a block's height in the chain, which mitigates certain attacks during header sync. Furthermore, the <code>getheaders2</code> message can be used as a simple way of requesting a coinbase transaction for a single header, which may be independently useful.
>
> * '''Why not omit nBits entirely?''' The compression is designed to permit full decompression of all headers in a <code>headers2</code> message ''without'' requiring any other chain context. This is desirable so that proofs of work may be validated for arbitrary header ranges. While nBits can be computed knowing previous headers, this requires block headers that may not be sent in the same message.
>
> == Compatibility ==
>
> This is backwards compatible, as it defines new P2P messages which are available if a service bit is signaled. There are no changes to consensus rules.
>
> == Acknowledgements ==
>
> Thanks to Gregory Maxwell for suggestions on the compressed header encoding and the DOS-resistant sync strategies. Thanks to Suhas Daftuar for helpful discussions.
>
> Credit for the VarInt encoding goes to Pieter Wuille.
>
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
>


-- 
Riccardo Casatta - @RCasatta <https://twitter.com/RCasatta>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180329/e96f51fb/attachment-0001.html>

From samad.sajanlal at gmail.com  Thu Mar 29 05:14:42 2018
From: samad.sajanlal at gmail.com (Samad Sajanlal)
Date: Thu, 29 Mar 2018 00:14:42 -0500
Subject: [bitcoin-dev] Soft Fork Activation & Enforcement w/o Signaling?
In-Reply-To: <CABm2gDq2pa_8T7Xhniuyh86eTi=PmSA_t=2Z0nYp1LhN=zc_NA@mail.gmail.com>
References: <CAAQZUuDEJeMFTxxJcgUEmTUQbxM_ZWkBD1k+UOvafsqbqj++Jg@mail.gmail.com>
	<CABm2gDq2pa_8T7Xhniuyh86eTi=PmSA_t=2Z0nYp1LhN=zc_NA@mail.gmail.com>
Message-ID: <CAAQZUuCW+dijXgLOkDwjx8sCnhFbygqaT-0gLxAwx7EEx=VcaQ@mail.gmail.com>

Excellent - Thanks for your response Jorge. This helps us plan out the
future upgrades properly.
Since I see 0.15 and 0.16 use block versions as 0x20000000, whereas the
current deployed codebase (based on bitcoin 0.9.4) makes versions
0x00000002 (as seen by a 0.15 client), it appears safe to activate soft
forks which require a minimum of version 3 and 4 blocks (0x00000003
and 0x00000004,
respectively). Would you agree?

On Wed, Mar 28, 2018 at 7:55 AM, Jorge Tim?n <jtimon at jtimon.cc> wrote:

> Yes, you can activate softforks at a given height.
> I don't see any reason why you couldn't rebase to 0.16 directly.
> The block version bumping was a mistake in bip34, you don't really
> need to bump the version number. In any case, I would recommend
> reading bip34 and what it activates in the code. IIRC the last thing
> was bip65.
>
> On Wed, Mar 21, 2018 at 11:04 PM, Samad Sajanlal via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org> wrote:
> > Is it possible to activate soft forks such as BIP65 and BIP66 without
> prior
> > signaling from miners? I noticed in chainparams.cpp that there are block
> > heights where the enforcement begins.
> >
> > I understand this is already active on bitcoin. I'm working on a project
> > that is a clone of a clone of bitcoin, and we currently do not have
> BIP65 or
> > BIP66 enforced - no signaling of these soft forks either (most of the
> > network is on a source code fork of bitcoin 0.9). This project does not
> and
> > never intends to attempt to replace bitcoin - we know that without
> bitcoin
> > our project could never exist, so we owe a great deal of gratitude to the
> > bitcoin developers.
> >
> > If the entire network upgrades to the correct version of the software
> (based
> > on bitcoin 0.15), which includes the block height that has enforcement,
> can
> > we simply skip over the signaling and go straight into
> > activation/enforcement?
> >
> > At this time we are lucky that our network is very small, so it is
> > reasonable to assume that the whole network will upgrade their clients
> > within a short window (~2 weeks). We would schedule the activation ~2
> months
> > out from when the client is released, just to ensure everyone has time to
> > upgrade.
> >
> > We have been stuck on the 0.9 code branch and my goal is to bring it up
> to
> > 0.15 at least, so that we can implement Segwit and other key features
> that
> > bitcoin has introduced. The 0.15 client currently works with regards to
> > sending and receiving transactions but the soft forks are not active. I
> > understand that activating them will segregate the 0.15 clients onto
> their
> > own fork, which is why I'd like to understand the repercussions of doing
> it
> > without any signaling beforehand. I also would prefer not to have to make
> > intermediate releases such as 0.10, 0.11.. etc to get the soft forks
> > activated.
> >
> > Another related question - does the block version get bumped up
> > automatically at the time that a soft fork activates, or is there
> additional
> > stuff that I need to do within the code to ensure it bumps up at the same
> > time? From what I saw in the code it appears that it will bump up
> > automatically, but I would like some confirmation on that.
> >
> > Regards,
> > Samad
> >
> >
> >
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev at lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180329/46f683eb/attachment.html>

From jim.posen at gmail.com  Fri Mar 30 00:50:30 2018
From: jim.posen at gmail.com (Jim Posen)
Date: Thu, 29 Mar 2018 17:50:30 -0700
Subject: [bitcoin-dev] Optimized Header Sync
In-Reply-To: <CADabwBAjTRdVqsL+V=YdQ+kr8+LtSPOeSXUJOzKoPNdKEbAOWQ@mail.gmail.com>
References: <CADZtCSg7+x-sg-ysgacXobRexOVwT+k9fr6a9S-6xU2w8f8m3A@mail.gmail.com>
	<CADabwBAjTRdVqsL+V=YdQ+kr8+LtSPOeSXUJOzKoPNdKEbAOWQ@mail.gmail.com>
Message-ID: <CADZtCSjmQfBZoaO=MCyRoEn-AYe4A=1kDhxSVxVMw+O4k7YJfQ@mail.gmail.com>

Thanks for giving it a read and for sparking the discussion with your
observation about the 40% savings from dropping prev_hash!


> Thought this wasn't effective in case overt asic boost get widely adopted,
> but then I understood that at the moment only two bits of version get
> scrambled by that technique so this looks fine, maybe add a comment about
> this so the reader doesn't get the same initial doubt I got.
>

I still need to compute for historical blocks how many could have an
omitted version. Will post back with that when I get results. If overt ASIC
Boost made this less effective, that would be unfortunate, but so be it.


> My feeling is that encoding of the headers and checkpoints/parallel
> download are separate subjects for two BIPS.
> About the checkpoints I don't grasp why they are useful since an attacker
> could lie about them but maybe I am missing something...
>

Yeah, I guess the background wasn't explained in the BIP itself. After your
original post on the mailing list, there were suggestions that instead of
modifying the format of existing messages, it would be better do create a
new headers message. And as long as we're designing a new headers message,
we should change the semantics to allow parallel download. But if you want
to download from peers in parallel, you need to get a summary of the blocks
that they have. Hence the checkpoints message. So that is why both of these
messages are in the same BIP -- only together can they perform an efficient
sync.

Regarding the reliability of the checkpoints, I think it's strictly better
than what we have now. Let's say a node is connected to 6 honest peers and
2 malicious peers. Even if the node does not know which ones are good or
bad until it validates the headers, it sees that 6 of the peers are on the
same chain, and can download those headers in parallel from 6 different
sources. So that's already a win.

Taken a step further though, I'm really interested in treating the
checkpoints as commitments to chain work and using random sampling to
detect lying peers before downloading all of their headers. So imagine you
are connected to two peers, one good one bad, where the good one claims a
chain with X total work and the bad one claims a chain with Y total work.
To determine quickly which is correct, you can randomly sample ranges of
headers and check the proofs of work to see whether it matches what the
peer claimed. So basically you pick a checkpoint at random (weighted by the
work delta) which commits to a total amount of work from the last
checkpoint, then request all headers in between. If the peer responds with
headers with the correct start hash, end hash, and start height (from the
coinbase tx of the first header), then you can be somewhat more confident
their total PoW matches the claimed amount.

How many times do you need to sample? I don't know yet, but I've heard
Benedikt Bunz is exploring this question with his research on FlyClients
[1], which was an inspiration for this.


> Bitfield allows great savings, however the encoding depends on the headers
> height a client ask for, this cause a little computational burden on the
> node and the undesirable side effect of difficult caching. Variable length
> encoding cause caching difficulties too...
> A simpler approach could be to encode the headers in groups of 2016
> headers (the difficulty period) where the first header is complete and the
> others 2015 are missing the previous hash and the difficulty, this achieve
> comparable savings ~45%, allows better caching and has fixed length
> encoding. This could be useful for the node by caching headers on a single
> file on disk and simply stream out the relative range when requested or to
> serve the same encoded headers format in other context like http,
> leveraging http caching infrastructure.
>

I don't see too much of a problem with caching. Most node implementations I
know of keep all headers in memory anyway, often in contiguous segments of
RAM for historical headers, so it should be fairly inexpensive to serve
queries. Beyond that, the response for a particular query (start_height,
end_hash, encoding version) can be cached, so if some service wants to
precompute max size responses for all start_height multiples of 1,000, they
could cache those.

-Jim

[1] https://www.youtube.com/watch?time_continue=8400&v=BPNs9EVxWrA


> 2018-03-28 1:31 GMT+02:00 Jim Posen via bitcoin-dev <bitcoin-dev at lists.
> linuxfoundation.org>:
>
>> Based on some ideas that were thrown around in this thread (
>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/
>> 2017-December/015385.html), I have been working on a P2P extension that
>> will allow faster header sync mechanisms. The one-sentence summary is that
>> by encoding headers more efficiently (eg. omitting prev_hash) and
>> downloading evenly spaced checkpoints throughout history (say every
>> 1,000th) from all peers first, we could speed up header sync, which would
>> be a huge improvement for light clients. Here is a draft of the BIP:
>> https://github.com/jimpo/bips/blob/headers-sync/headersv2.mediawiki. The
>> full text is below as well.
>>
>> I'd love to hear any feedback people have.
>>
>> ----------------------------------------------------------
>>
>> == Abstract ==
>>
>> This BIP describes a P2P network extension enabling faster, more reliable methods for syncing the block header chain. New P2P messages are proposed as more efficient replacements for <code>getheaders</code> and <code>headers</code> during initial block download. The proposed header download protocol reduces bandwidth usage by ~40%-50% and supports downloading headers ranges from multiple peers in parallel, which is not possible with the current mechanism. This also enables sync strategies with better resistance to denial-of-service attacks.
>>
>> == Motivation ==
>>
>> Since 2015, optimized Bitcoin clients fetch all block headers before blocks themselves in order to avoid downloading ones that are not part of the most work chain. The protocol currently in use for fetching headers leaves room for further optimization, specifically by compressing header data and downloading more headers simulaneously<ref>https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-December/015385.html</ref>. Any savings here should have a large impact given that both full nodes and light clients must sync the header chain as a first step, and that the time to validate and index the headers is negligible compared to the time spent downloading them from the network. Furthermore, some current implementations of headers syncing rely on preconfigured checkpoints to discourage attackers attempting to fill up a victim's disk space with low-work headers. The proposed messages enable sync strategies that are resilient against these types of attacks. The P2P messages are designed to be flexible, supporting multiple header sync strategies and leaving room for future innovations, while also compact.
>>
>> == Definitions ==
>>
>> ''double-SHA256'' is a hash algorithm defined by two invocations of SHA-256: <code>double-SHA256(x) = SHA256(SHA256(x))</code>.
>>
>> == Specification ==
>>
>> The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT", "SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this document are to be interpreted as described in RFC 2119.
>>
>> === New Structures ===
>>
>> ==== Compressed Headers ====
>>
>> Bitcoin headers are serialized by default in 80 bytes as follows:
>>
>> {| class="wikitable"
>> ! Field Name
>> ! Data Type
>> ! Byte Size
>> ! Description
>> |-
>> | version
>> | int32_t
>> | 4
>> | Block version information
>> |-
>> | prev_block
>> | uint256
>> | 32
>> | The hash of the previous block
>> |-
>> | merkle_root
>> | uint256
>> | 32
>> | The root hash of the transaction Merkle tree
>> |-
>> | timestamp
>> | uint32_t
>> | 4
>> | A Unix timestamp of the block creation time, as reported by the miner
>> |-
>> | bits
>> | uint32_t
>> | 4
>> | The calculated difficulty target for this block
>> |-
>> | nonce
>> | uint32_t
>> | 4
>> | A nonce that is set such that the header's hash matches the difficulty target
>> |}
>>
>> When deserializing a correctly-formed sequence of block headers encoded in this way, it can be noted that:
>>
>> * The prev_block field should always match the double-SHA256 hash of the previous header, making it redundant
>> * According to Bitcoin consensus rules, the bits field only changes every 2016 blocks
>> * The version often matches that of a recent ancestor block
>> * The timestamp is often a small delta from the preceding header's timestamp
>>
>> To take advantage of these possible savings, this document defines a variable-sized ''compressed encoding'' of block headers that occur in a range. Note that no savings are possible when serializing a single header; it should only be used for vectors of sequential headers. The full headers are reconstructed using data from previous headers in the range. The serialization begins with an ''encoding indicator'', which is a bitfield specifying how each field is serialized. The bits of the indicator have the following semantics:
>>
>> {| class="wikitable"
>> ! Bit Index
>> ! Reconstruction
>> ! Description
>> |-
>> | 0
>> | <code>prev_block[i] = DSHA256(header[i-1])</code>
>> | The prev_block field is ommitted and assigned to the double-SHA256 hash of the previous uncompressed header.
>> |-
>> | 1
>> | <code>nbits[i] = nbits[i-1]</code>
>> | The nbits field is omitted and matches that of the previous header.
>> |-
>> | 2
>> | <code>timestamp[i] = timestamp[i-1] + value</code>
>> | The timestamp is replaced by a 2-byte signed short int, representing an offset from the previous block's timestamp
>> |-
>> | 3
>> |
>> | Interpreted along with bits 4 & 5.
>> |-
>> | 4
>> |
>> | Interpreted along with bits 3 & 5.
>> |-
>> | 5
>> | <code>version[i] = version[i - ((bit[3] << 2) + (bit[4] << 1) + bit[5])]</code>
>> | Bits 3, 4, and 5 are first interpreted as a 3-bit offset, with bit index 3 as the most significant and bit index 5 as the least significant. If the offset is non-zero, the version field is omitted and assigned to the version of the block at the offset number of blocks prior.
>> |-
>> | 6
>> |
>> | Reserved.
>> |-
>> | 7
>> |
>> | Reserved. May be used in a future encoding version to signal another indicator byte.
>> |}
>>
>> The compressed header format is versioned by a 256-bit unsigned integer. This document defines version 0.
>>
>> ==== VarInt ====
>>
>> ''VarInt'' is a variable-length unsigned integer encoding that supports a greater range of numbers than the standard ''CompactSize''. This encoding was introduced at the database layer in Bitcoin Core<ref>https://github.com/bitcoin/bitcoin/commit/4d6144f97faf9d2a6c89f41d7d2360f21f0b71e2</ref> in 2012, but is new to the Bitcoin P2P layer.
>>
>> This definition is per the code comments in Bitcoin Core written by Pieter Wuille:
>>
>> <pre>
>> Variable-length integers: bytes are a MSB base-128 encoding of the number.
>> The high bit in each byte signifies whether another digit follows. To make
>> the encoding is one-to-one, one is subtracted from all but the last digit.
>> Thus, the byte sequence a[] with length len, where all but the last byte
>> has bit 128 set, encodes the number:
>>
>>   (a[len-1] & 0x7F) + sum(i=1..len-1, 128^i*((a[len-i-1] & 0x7F)+1))
>>
>> Properties:
>> * Very small (0-127: 1 byte, 128-16511: 2 bytes, 16512-2113663: 3 bytes)
>> * Every integer has exactly one encoding
>> * Encoding does not depend on size of original integer type
>> * No redundancy: every (infinite) byte sequence corresponds to a list
>>   of encoded integers.
>>
>> 0:         [0x00]  256:        [0x81 0x00]
>> 1:         [0x01]  16383:      [0xFE 0x7F]
>> 127:       [0x7F]  16384:      [0xFF 0x00]
>> 128:  [0x80 0x00]  16511: [0x80 0xFF 0x7F]
>> 255:  [0x80 0x7F]  65535: [0x82 0xFD 0x7F]
>> 2^32:           [0x8E 0xFE 0xFE 0xFF 0x00]
>> </pre>
>>
>> ==== Checkpoints ====
>>
>> A ''checkpoint'' is defined for a block as a tuple of its hash and the chain work:
>>
>> {| class="wikitable"
>> ! Field Name
>> ! Data Type
>> ! Byte Size
>> ! Description
>> |-
>> | block_hash
>> | uint256
>> | 32
>> | The hash of the block
>> |-
>> | chain_work
>> | VarInt
>> | Variable(1-20)
>> | A delta between the total work in the chain at the checkpoint block and a previous checkpoint, determined by context
>> |}
>>
>> === Service Bit ===
>>
>> This BIP allocates a new service bit:
>>
>> {| class="wikitable"
>> |-
>> | NODE_HEADERS_V2
>> | <code>1 << ?</code>
>> | If enabled, the node MUST respond to <code>getcheckpts</code> and <code>getheaders2</code> queries
>> |}
>>
>> === New Messages ===
>>
>> ==== getcheckpts ====
>> <code>getcheckpts</code> is used to request block headers at a specified distance from each other which serve as checkpoints during parallel header download. The message contains the following fields:
>>
>> {| class="wikitable"
>> ! Field Name
>> ! Data Type
>> ! Byte Size
>> ! Description
>> |-
>> | block_locator
>> | uint256[]
>> | Variable
>> | A vector of block hashes in descending order by height used to identify the header chain of the requesting node
>> |-
>> | interval
>> | uint32_t
>> | 4
>> | The distance in block height between requested block hashes
>> |}
>>
>> # Nodes SHOULD NOT send <code>getcheckpts</code> unless the peer has set the <code>NODE_HEADERS_V2</code> service bit
>> # The hashes in <code>block_locator</code> MUST be in descending order by block height
>> # The block locator SHOULD be generated as it is in <code>getheaders</code> requests
>> # The receiving node MUST respond to valid requests with a <code>checkpts</code> response where the interval is the same as in the request and the first checkpoint hash matches the first common block hash in the block locator
>>
>> ==== checkpts ====
>> <code>checkpts</code> is sent in response to <code>getcheckpts</code>, listing block hashes at the specified interval. The message contains the following fields:
>>
>> {| class="wikitable"
>> ! Field Name
>> ! Data Type
>> ! Byte Size
>> ! Description
>> |-
>> | start_height
>> | uint32_t
>> | 4
>> | The height of the first block in the active chain matching the request's block locator
>> |-
>> | end_height
>> | uint32_t
>> | 4
>> | The height of the last block in the active chain
>> |-
>> | start_checkpoint
>> | Checkpoint
>> | 48
>> | The checkpoint structure for the block in the active chain at height start_height
>> |-
>> | end_checkpoint
>> | Checkpoint
>> | 48
>> | The checkpoint structure for the block in the active chain at height end_height
>> |-
>> | interval
>> | uint32_t
>> | 4
>> | The distance in block height between checkpoints
>> |-
>> | checkpoints_length
>> | CompactSize
>> | Variable(1-5)
>> | The number of checkpoints to follow
>> |-
>> | checkpoints
>> | Checkpoint[]
>> | checkoints_length * Variable(33-52)
>> | The checkpoints as specified below
>> |}
>>
>> # The interval SHOULD match the field in the <code>getcheckpts</code> request
>> # The start_checkpoint SHOULD correspond to the first block hash in the locator from the <code>getcheckpts</code> request that is part of the active chain
>> # The end_checkpoint SHOULD correspond to the tip of the node's active chain
>> # The start_height MOST be set to the block height of the start_checkpoint
>> # The end_height MOST be set to the block height of the end_checkpoint
>> # If the interval is zero, the checkpoints vector MUST be empty
>> # If the interval is non-zero, checkpoints MUST correspond to blocks on the active chain between the start_checkpoint and the end_checkpoint (exclusive), where the difference in block height between each entry and the previous one is equal to the interval
>> # The checkpoints_length MUST be less than or equal to 2,000
>> # The node SHOULD include as many checkpoints on its active chain as are available, up to the limit of 2,000
>> # The chain_work field in the first checkpoint MUST be the total work in the chain ending at that block
>> # The chain_work field in each subsequent checkpoint MUST be the difference in chain work between that block and the previous checkpoint
>> # The chain_work field in each checkpoint MUST be a properly-encoded VarInt, not exceeding 20 bytes
>>
>> ==== getheaders2 ====
>> <code>getheaders2</code> is used to request compressed headers for a range of blocks. The message contains the following fields:
>>
>> {| class="wikitable"
>> ! Field Name
>> ! Data Type
>> ! Byte Size
>> ! Description
>> |-
>> | max_version
>> | uint8_t
>> | 1
>> | The maximum supported encoding version of the headers
>> |-
>> | flags
>> | uint8_t
>> | 1
>> | A bitfield of message encoding flags
>> |-
>> | start_height
>> | uint32_t
>> | 4
>> | The height of the first block header in the requested range
>> |-
>> | end_hash
>> | uint256
>> | 32
>> | The hash of the last block header in the requested range
>> |}
>>
>> # Nodes SHOULD NOT send <code>getheaders2</code> unless the peer has set the <code>NODE_HEADERS_V2</code> service bit
>> # The height of the block with hash end_hash MUST be greater than or equal to start_height, and the difference MUST be strictly less than 3,000
>> # The end_hash SHOULD match one in a previously received <code>checkpts</code> message, otherwise the receiving node MAY disconnect
>> # The 0th bit (least significant order) of the flags field MAY be set to request the coinbase transaction and merkle branch for the block at height start_height
>>
>> ==== headers2 ====
>> <code>headers2</code> is sent in response to <code>getheaders2</code>, listing the compressed headers in the requested range. The message contains the following fields:
>>
>> {| class="wikitable"
>> ! Field Name
>> ! Data Type
>> ! Byte Size
>> ! Description
>> |-
>> | version
>> | uint8_t
>> | 1
>> | The encoding version of the headers
>> |-
>> | flags
>> | uint8_t
>> | 1
>> | A bitfield of message encoding flags
>> |-
>> | start_height
>> | uint32_t
>> | 4
>> | The height of the first block header returned
>> |-
>> | headers_length
>> | CompactSize
>> | 1-3
>> | The number of block headers to follow
>> |-
>> | headers
>> | CompressedHeader[]
>> | Variable
>> | The compressed block headers
>> |-
>> | start_block_coinbase_tx
>> | CTransaction
>> | Variable
>> | The coinbase transaction in the block at start_height
>> |-
>> | start_block_coinbase_branch
>> | uint256[]
>> | Variable
>> | A merkle branch linking the coinbase transaction in the block at start_height to its header
>> |}
>>
>> # The version MUST be less than or equal to the max_version field of the <code>getheaders2</code> request
>> # Any bits set in the flags field of the <code>getheaders2</code> request MAY be set in the response field
>> # Any bits not set in the flags field of the <code>getheaders2</code> request MUST NOT be set in the response field
>> # The first header MUST be encoded with a 0-byte indicator (ie. the header is uncompressed)
>> # start_height MUST be set to the block height of the first header
>> # The hash of the last block SHOULD equal the end_hash of the <code>getheaders2</code> request, ''even if the block is no longer part of the active chain''
>> # The length of the headers vector MUST be less than or equal to 3,000
>> # The headers MUST be sequential in order of height, with each header a successor of the previous one
>> # Each header SHOULD be optimally compressed
>> # The start_block_coinbase_tx should be the serialized coinbase transaction in the block corresponding to the first header
>> # The start_block_coinbase_branch should be a vector of right-hand-side hashes in the merkle branch linking the coinbase transaction to the first header, in order from bottom of the tree to top
>> # If the 0th bit (least significant order) of the flags field is unset, the start_block_coinbase_tx and start_block_coinbase_branch fields MUST be omitted
>>
>> === Sync Strategies ===
>>
>> The general header sync protocol for clients now is to first request checkpoints from all peers with <code>getcheckpts</code>, then decide which peers to fetch ranges of headers from and download them with <code>getheaders2</code>.
>>
>> ==== Forward Sequential Syncing ====
>>
>> Similar to the current sync protocol, a client may choose one peer to download headers from, then fetch them in forward sequential order. Once this peer is out of headers, the client performs the same routine with any peers offering more headers.
>>
>> With this strategy, the client is able to fully validate the block headers in order and abort if the peer serves an invalid one. On the other hand, the peer may be able to serve a longer, lower-work chain than the global active chain, wasting the client's time, memory, and storage space.
>>
>> ==== Parallel Header Download ====
>>
>> In order to increase the throughput of header downloads, a node may download multiple header ranges in parallel from all peers serving the same checkpoints, then validate them in sequential order.
>>
>> ==== Random Sampling Proof-of-Work  ====
>>
>> Similar the FlyClient<ref>https://www.youtube.com/watch?time_continue=8400&v=BPNs9EVxWrA</ref> header download protocol, clients can select the peer claiming the greatest total work chain and use random sampling to efficiently determine if the peer is likely to be reporting its chain work honestly.
>>
>> The client treats the checkpoint message as a commitment to chain work of intermediate ranges of headers, the client then randomly samples ranges of headers weighted by total work to determine whether the total chain work is valid before downloading all headers. To defend against malicious peers attempting to reuse earlier headers later in the chain to fake greater total work, the client should check the block height in the coinbase transaction for all headers after the BIP 34 activation height. If the peer is found to be dishonest, they can be banned before the client downloads too many headers, otherwise the client chooses this as the primary sync peer for forward sequential sync or parallel download.
>>
>> == Rationale ==
>>
>> * '''Why include the coinbase transaction in the headers messages?''' The primary reason is that after BIP 34<ref>https://github.com/bitcoin/bips/blob/master/bip-0034.mediawiki</ref> activation at block height 227,835, coinbase transactions constitute cryptographic commitments to a block's height in the chain, which mitigates certain attacks during header sync. Furthermore, the <code>getheaders2</code> message can be used as a simple way of requesting a coinbase transaction for a single header, which may be independently useful.
>>
>> * '''Why not omit nBits entirely?''' The compression is designed to permit full decompression of all headers in a <code>headers2</code> message ''without'' requiring any other chain context. This is desirable so that proofs of work may be validated for arbitrary header ranges. While nBits can be computed knowing previous headers, this requires block headers that may not be sent in the same message.
>>
>> == Compatibility ==
>>
>> This is backwards compatible, as it defines new P2P messages which are available if a service bit is signaled. There are no changes to consensus rules.
>>
>> == Acknowledgements ==
>>
>> Thanks to Gregory Maxwell for suggestions on the compressed header encoding and the DOS-resistant sync strategies. Thanks to Suhas Daftuar for helpful discussions.
>>
>> Credit for the VarInt encoding goes to Pieter Wuille.
>>
>>
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>>
>
>
> --
> Riccardo Casatta - @RCasatta <https://twitter.com/RCasatta>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180329/4817a551/attachment-0001.html>

From aj at erisian.com.au  Fri Mar 30 06:14:18 2018
From: aj at erisian.com.au (Anthony Towns)
Date: Fri, 30 Mar 2018 16:14:18 +1000
Subject: [bitcoin-dev] Optimized Header Sync
In-Reply-To: <CADZtCSjmQfBZoaO=MCyRoEn-AYe4A=1kDhxSVxVMw+O4k7YJfQ@mail.gmail.com>
References: <CADZtCSg7+x-sg-ysgacXobRexOVwT+k9fr6a9S-6xU2w8f8m3A@mail.gmail.com>
	<CADabwBAjTRdVqsL+V=YdQ+kr8+LtSPOeSXUJOzKoPNdKEbAOWQ@mail.gmail.com>
	<CADZtCSjmQfBZoaO=MCyRoEn-AYe4A=1kDhxSVxVMw+O4k7YJfQ@mail.gmail.com>
Message-ID: <20180330061418.GA6017@erisian.com.au>

On Thu, Mar 29, 2018 at 05:50:30PM -0700, Jim Posen via bitcoin-dev wrote:
> Taken a step further though, I'm really interested in treating the checkpoints
> as commitments to chain work [...]

In that case, shouldn't the checkpoints just be every 2016 blocks and
include the corresponding bits value for that set of blocks?

That way every node commits to (approximately) how much work their entire
chain has by sending something like 10kB of data (currently), and you
could verify the deltas in each node's chain's target by downloading the
2016 headers between those checkpoints (~80kB with the proposed compact
encoding?) and checking the timestamps and proof of work match both the
old target and the new target from adjacent checkpoints.

(That probably still works fine even if there's a hardfork that allows
difficulty to adjust more frequently: a bits value at block n*2016 will
still enforce *some* lower limit on how much work blocks n*2016+{1..2016}
will have to contribute; so will still allow you to estimate how much work
will have been done, it may just be less precise than the estimate you could
generate now)

Cheers,
aj


From jtimon at jtimon.cc  Fri Mar 30 20:52:50 2018
From: jtimon at jtimon.cc (=?UTF-8?B?Sm9yZ2UgVGltw7Nu?=)
Date: Fri, 30 Mar 2018 22:52:50 +0200
Subject: [bitcoin-dev] Soft Fork Activation & Enforcement w/o Signaling?
In-Reply-To: <CAAQZUuCW+dijXgLOkDwjx8sCnhFbygqaT-0gLxAwx7EEx=VcaQ@mail.gmail.com>
References: <CAAQZUuDEJeMFTxxJcgUEmTUQbxM_ZWkBD1k+UOvafsqbqj++Jg@mail.gmail.com>
	<CABm2gDq2pa_8T7Xhniuyh86eTi=PmSA_t=2Z0nYp1LhN=zc_NA@mail.gmail.com>
	<CAAQZUuCW+dijXgLOkDwjx8sCnhFbygqaT-0gLxAwx7EEx=VcaQ@mail.gmail.com>
Message-ID: <CABm2gDpRY81cxyMmBnKyTefk=6d89YCxPVZjt3C2qwNq1sTK_g@mail.gmail.com>

Yes, in fact, you don't need to lose those bits like bitcoin by
imposing that the version is greater than that. But I guess just doing
the same is simpler.

On Thu, Mar 29, 2018 at 7:14 AM, Samad Sajanlal
<samad.sajanlal at gmail.com> wrote:
> Excellent - Thanks for your response Jorge. This helps us plan out the
> future upgrades properly.
> Since I see 0.15 and 0.16 use block versions as 0x20000000, whereas the
> current deployed codebase (based on bitcoin 0.9.4) makes versions 0x00000002
> (as seen by a 0.15 client), it appears safe to activate soft forks which
> require a minimum of version 3 and 4 blocks (0x00000003 and 0x00000004,
> respectively). Would you agree?
>
> On Wed, Mar 28, 2018 at 7:55 AM, Jorge Tim?n <jtimon at jtimon.cc> wrote:
>>
>> Yes, you can activate softforks at a given height.
>> I don't see any reason why you couldn't rebase to 0.16 directly.
>> The block version bumping was a mistake in bip34, you don't really
>> need to bump the version number. In any case, I would recommend
>> reading bip34 and what it activates in the code. IIRC the last thing
>> was bip65.
>>
>> On Wed, Mar 21, 2018 at 11:04 PM, Samad Sajanlal via bitcoin-dev
>> <bitcoin-dev at lists.linuxfoundation.org> wrote:
>> > Is it possible to activate soft forks such as BIP65 and BIP66 without
>> > prior
>> > signaling from miners? I noticed in chainparams.cpp that there are block
>> > heights where the enforcement begins.
>> >
>> > I understand this is already active on bitcoin. I'm working on a project
>> > that is a clone of a clone of bitcoin, and we currently do not have
>> > BIP65 or
>> > BIP66 enforced - no signaling of these soft forks either (most of the
>> > network is on a source code fork of bitcoin 0.9). This project does not
>> > and
>> > never intends to attempt to replace bitcoin - we know that without
>> > bitcoin
>> > our project could never exist, so we owe a great deal of gratitude to
>> > the
>> > bitcoin developers.
>> >
>> > If the entire network upgrades to the correct version of the software
>> > (based
>> > on bitcoin 0.15), which includes the block height that has enforcement,
>> > can
>> > we simply skip over the signaling and go straight into
>> > activation/enforcement?
>> >
>> > At this time we are lucky that our network is very small, so it is
>> > reasonable to assume that the whole network will upgrade their clients
>> > within a short window (~2 weeks). We would schedule the activation ~2
>> > months
>> > out from when the client is released, just to ensure everyone has time
>> > to
>> > upgrade.
>> >
>> > We have been stuck on the 0.9 code branch and my goal is to bring it up
>> > to
>> > 0.15 at least, so that we can implement Segwit and other key features
>> > that
>> > bitcoin has introduced. The 0.15 client currently works with regards to
>> > sending and receiving transactions but the soft forks are not active. I
>> > understand that activating them will segregate the 0.15 clients onto
>> > their
>> > own fork, which is why I'd like to understand the repercussions of doing
>> > it
>> > without any signaling beforehand. I also would prefer not to have to
>> > make
>> > intermediate releases such as 0.10, 0.11.. etc to get the soft forks
>> > activated.
>> >
>> > Another related question - does the block version get bumped up
>> > automatically at the time that a soft fork activates, or is there
>> > additional
>> > stuff that I need to do within the code to ensure it bumps up at the
>> > same
>> > time? From what I saw in the code it appears that it will bump up
>> > automatically, but I would like some confirmation on that.
>> >
>> > Regards,
>> > Samad
>> >
>> >
>> >
>> > _______________________________________________
>> > bitcoin-dev mailing list
>> > bitcoin-dev at lists.linuxfoundation.org
>> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>> >
>
>

From riccardo.casatta at gmail.com  Fri Mar 30 08:06:24 2018
From: riccardo.casatta at gmail.com (Riccardo Casatta)
Date: Fri, 30 Mar 2018 10:06:24 +0200
Subject: [bitcoin-dev] Optimized Header Sync
In-Reply-To: <20180330061418.GA6017@erisian.com.au>
References: <CADZtCSg7+x-sg-ysgacXobRexOVwT+k9fr6a9S-6xU2w8f8m3A@mail.gmail.com>
	<CADabwBAjTRdVqsL+V=YdQ+kr8+LtSPOeSXUJOzKoPNdKEbAOWQ@mail.gmail.com>
	<CADZtCSjmQfBZoaO=MCyRoEn-AYe4A=1kDhxSVxVMw+O4k7YJfQ@mail.gmail.com>
	<20180330061418.GA6017@erisian.com.au>
Message-ID: <CADabwBDiT2zNPHZ2=OyvCVrSY3Km2oyTnhRCHyMNsjW2vLMmOg@mail.gmail.com>

Yes, I think the checkpoints and the compressed headers streams should be
handled in chunks of 2016 headers and queried by chunk number instead of
height, falling back to current method if the chunk is not full yet.

This is cache friendly and allows to avoid bit 0 and bit 1 in the bitfield
(because they are always 1 after the first header in the chunk of 2016).

2018-03-30 8:14 GMT+02:00 Anthony Towns <aj at erisian.com.au>:

> On Thu, Mar 29, 2018 at 05:50:30PM -0700, Jim Posen via bitcoin-dev wrote:
> > Taken a step further though, I'm really interested in treating the
> checkpoints
> > as commitments to chain work [...]
>
> In that case, shouldn't the checkpoints just be every 2016 blocks and
> include the corresponding bits value for that set of blocks?
>
> That way every node commits to (approximately) how much work their entire
> chain has by sending something like 10kB of data (currently), and you
> could verify the deltas in each node's chain's target by downloading the
> 2016 headers between those checkpoints (~80kB with the proposed compact
> encoding?) and checking the timestamps and proof of work match both the
> old target and the new target from adjacent checkpoints.
>
> (That probably still works fine even if there's a hardfork that allows
> difficulty to adjust more frequently: a bits value at block n*2016 will
> still enforce *some* lower limit on how much work blocks n*2016+{1..2016}
> will have to contribute; so will still allow you to estimate how much work
> will have been done, it may just be less precise than the estimate you
> could
> generate now)
>
> Cheers,
> aj
>
>


-- 
Riccardo Casatta - @RCasatta <https://twitter.com/RCasatta>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180330/0aa5cd7e/attachment.html>

