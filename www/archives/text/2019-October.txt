From ZmnSCPxj at protonmail.com  Tue Oct  1 13:31:49 2019
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Tue, 01 Oct 2019 13:31:49 +0000
Subject: [bitcoin-dev] [Lightning-dev] Continuing the discussion about
	noinput / anyprevout
In-Reply-To: <CAFQwNuxdfMNBGyM5Y4nMb46GNigxFTFCv3X09jZd4fjNPckw4Q@mail.gmail.com>
References: <87wodp7w9f.fsf@gmail.com>
	<CAFQwNuxdfMNBGyM5Y4nMb46GNigxFTFCv3X09jZd4fjNPckw4Q@mail.gmail.com>
Message-ID: <bMt69zMSAH_2zekGjg56k6MWFMwWkjKMdUjqHQ5eN7c5ONixWZ0s2wW4HmILeVjImt6Z2K5fPa6GKGLP_HWThCzFIIu53wvEKTDrGg-YpOQ=@protonmail.com>

Good morning lists,

Let me summarize concerns brought up:

* Chris concern, is that an ordinary UTXO that is not allocated for `SIGHASH_NOINPUT` use, is inadvertently spent using `SIGHASH_NOINPUT`.
* My concern, is that unless a UTXO allocated for `SIGHASH_NOINPUT` use, is *indeed* used with SIGHASH_NOINPUT`, it should look exactly the same as any other SegWit v1 output.

I propose the below instead:

* Do ***NOT*** allocate SegWit v16 for `SIGHASH_NOINPUT`.
* Instead, allocate SegWit v1 Tapscript v16 for `SIGHASH_NOINPUT`.

Then, on usage:

* Exchange hoards can be protected by simple MuSig bip-schnorr SegWit v1 outputs, or a NUMS Taproot internal point with a MAST branch Tapscript v0 `OP_CHECKSIG_ADD` sequence.
* Decker-Russell-Osuntokun constructions are backed by a n-of-n MuSig Taproot internal point, with a MAST branch containing a Tapscript v16 with `OP_1 OP_CHECKSIG`.

This solves both concerns:

* Ordinary UTXOs not allocated for `SIGHASH_NOINPUT` use simply do not commit to any Taproot that has a Tapscript v16 branch, and thus `SIGHASH_NOINPUT` is unuseable to claim it.
* If a UTXO used for an offchain protocol ends up in a cooperative-resolution state, nobody has to know that a Tapscript v16 branch existed that could have used `SIGHASH_NOINPUT`.

Again, my objection to output tagging is that it is **publicly visible** as soon as the funding transaction is confirmed onchain that this is a special output used for a Decker-Russell-Osuntokun construction, greatly damaging privacy.
But if this fact is kept secret *unless* the very specific case of unilateral uncooperative enforcement, then it is quite fine with me.

Would this alternate proposal hold better muster?

Regards,
ZmnSCPxj



> I do have some concerns about SIGHASH_NOINPUT, mainly that it does introduce another footgun into the bitcoin protocol with address reuse. It's common practice for bitcoin businesses to re-use addresses. Many exchanges [1] reuse addresses for cold storage with very large sums of money that is stored in these addreses.
>
> It is my understanding with this part of BIP118
>
> >Using NOINPUT the input containing the signature no longer references a specific output. Any participant can take a transaction and rewrite it by changing the hash reference to the previous output, without invalidating the signatures. This allows transactions to be bound to any output that matches the value committed to in the witness and whose witnessProgram, combined with the spending transaction's witness returns true.
>
> if an exchange were to once produce a digital signature from that cold storage address with a SIGHASH_NOINPUT signature, that signature can be replayed again and again on the blockchain until their wallet is drained. This might be able to mitigated since the signatures commit to outputs, which may be small in value for the transaction that SIGHASH_NOINPUT was used. This means that an exchange could move coins from the address with a larger transaction that spends money to a new output (and presumably pays a higher fee than the smaller transactions).
>
> ### Why does this matter?
>
> It seems that SIGHASH_NOINPUT will be an extremely useful tool for offchain protocols like Lightning. This gives us the building blocks for enforcing specific offchain states to end up onchain [2].
>
> Since this tool is useful, we can presume that it will be integrated into the signing path of large economic entities in bitcoin -- namely exchanges. Many exchanges have specific signing procedures for transactions that are leaving an exchange that is custom software. Now -- presuming wide adoption of off chain protocols -- they will need to have a _second unique signing path that uses SIGHASH_NOINPUT_.
>
> It is imperative that this second signing path -- which uses SIGHASH_NOINPUT -- does NOT get mixed up with the first signing path that controls an exchanges onchain funds. If this were to happen, fund lost could occur if the exchange is reusing address, which seems to be common practice.
>
> This is stated here in BIP118:
>
> >This also means that particular care has to be taken in order to avoid unintentionally enabling this rebinding mechanism. NOINPUT MUST NOT be used, unless it is explicitly needed for the application, e.g., it MUST NOT be a default signing flag in a wallet implementation. Rebinding is only possible when the outputs the transaction may bind to all use the same public keys. Any public key that is used in a NOINPUT signature MUST only be used for outputs that the input may bind to, and they MUST NOT be used for transactions that the input may not bind to. For example an application SHOULD generate a new key-pair for the application instance using NOINPUT signatures and MUST NOT reuse them afterwards.
>
> This means we need to encourage onchain hot wallet signing procedures to be kept separate from offchain hot wallet signing procedures, which introduces more complexity for key management (two keychains).
>
> One (of the few) upsides of the current Lightning penalty mechanism is that fund loss can be contained to balance of the channel. You cannot do something in the current protocol that will effect your funds outside of that channel. With SIGHASH_NOINPUT, that property changes.
>
> ### A side note
> In general, i think we should start disallowing uses of the SIGHASH protocols that have unexpected behavior. The classic example of this is SIGHASH_SINGLE [3]. I get uneasy about adding more footguns to the protocol, which with current network behavior (address re-use) SIGHASH_NOINPUT would be a big one.
>
> [1] - https://bitinfocharts.com/top-100-richest-bitcoin-addresses.html
> [2] - https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-September/002136.html
> [3] - https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-May/016048.html
>
> On Mon, Sep 30, 2019 at 9:24 AM Christian Decker via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
>
> > With the recently renewed interest in eltoo, a proof-of-concept implementation
> > [1], and the discussions regarding clean abstractions for off-chain protocols
> > [2,3], I thought it might be time to revisit the `sighash_noinput` proposal
> > (BIP-118 [4]), and AJ's `bip-anyprevout` proposal [5].
> >
> > (sorry for the long e-mail. I wanted to give enough context and describe the
> > various tradeoffs so people don't have to stitch them together from memory. If
> > you're impatient there are a couple of open questions at the bottom)
> >
> > Both proposals are ways to allow rebinding of transactions to new outputs, by
> > adding a sighash flag that excludes the output when signing. This allows the
> > transaction to be bound to any output, without needing a new signature, as
> > long as output script and input script are compatible, e.g., the signature
> > matches the public key specified in the output.
> >
> > BIP-118 is limited to explaining the details of signature verification, and
> > omits anything related to deployment and dependency on other proposals. This
> > was done in order not to depend on bip-taproot which is also in draft-phase
> > currently, and to allow deployment alongside the next version of segwit
> > script. `bip-anyprevout` builds on top of BIP-118, adding integration with
> > `bip-taproot`, chaperone signatures, limits the use of the sighash flag to
> > script path spends, as well as a new pubkey serialization which uses the first
> > byte to signal opt-in.
> >
> > I'd like to stress that both proposals are complementary and not competing,
> > which is something that I've heard a couple of times.
> >
> > There remain a couple of unclear points which I hope we can address in the
> > coming days, to get this thing moving again, and hopefully get a new tool in
> > our toolbox soon(ish).
> >
> > In the following I will quote a couple of things that were discussed during
> > the CoreDev meeting earlier this year, but not everybody could join, and it is
> > important that we engage the wider community, to get a better picture, and I
> > think not everybody is up-to-date about the current state.
> >
> > ## Dangers of `sighash_noinput`
> >
> > An argument I have heard against noinput is that it is slightly less complex
> > or compute intensive than `sighash_all` signatures, which may encourage wallet
> > creators to only implement the noinput variant, and use it indiscrimi-
> > nately. This is certainly a good argument, and indeed we have seen at least
> > one developer proposing to use noinput for all transactions to discourage
> > address reuse.
> >
> > This was also mentioned at CoreDev [6]:
> >
> > > When [...] said he wanted to write a wallet that only used SIGHASH\_NOINPUT,
> > > that was pause for concern. Some people might want to use SIGHASH\_NOINPUT as a
> > > way to cheapen or reduce the complexity of making a wallet
> > > implementation. SIGHASH\_NOINPUT is from a purely procedural point of view
> > > easier than doing a SIGHASH\_ALL, that's all I'm saying. So you're hashing
> > > less. It's way faster. That concern has been brought to my attention and it's
> > > something I can see. Do we want to avoid people being stupid and shooting
> > > themselves and their customers in the foot? Or do we treat this as a special
> > > case where you mark we're aware of how it should be used and we just try to
> > > get that awareness out?
> >
> > Another issue that is sometimes brought up is that an external user may
> > attempt to send funds to a script that was really part of a higher-level
> > protocol. This leads to those funds becoming inaccessible unless you gather
> > all the participants and sign off on those funds. I don't believe this is
> > anything new, and if users really want to shoot themselves in the foot and
> > send funds to random addresses they fish out of a blockexplorer there's little
> > we can do. What we could do is make the scripts used internally in our
> > protocols unaddressable (see output tagging below), removing this issue
> > altogether.
> >
> > ## Chaperone signatures
> >
> > Chaperone signatures are signatures that ensure that there is no third-party
> > malleability of transactions. The idea is to have an additional signature,
> > that doesn't use noinput, or any of its variants, and therefore needs to be
> > authored by one of the pubkeys in the output script, i.e., one or more of the
> > participants of the contract the transaction belongs to. Concretely in eltoo
> > we'd be using a shared key known to all participants in the eltoo instance, so
> > any participant can sign an update to rebind it to the desired output.
> >
> > Chaperone signatures have a number of downsides however:
> >
> > -   Additional size: both the public key and the signature actually need to be
> >     stored along with the real noinput signature, resulting in transfer,
> >     computational and storage overhead. We can't reuse the same pubkey from the
> >     noinput signature since that'd require access to the matching privkey which
> >     is what we want to get rid of using noinput in the first place.
> > -   Protocols can still simply use a globally known privkey, voiding the
> >     benefit of chaperone signatures, since third-parties can sign again. I
> >     argue that third-party malleability is a subset of first-party
> >     malleability, and we should protect against first-party malleability first
> >     and foremost. My counterparty has the incentive to trick me, a third-party
> >     may not.
> >
> > On the plus side chaperone signatures certainly address the lazy-wallet-dev
> > scenario, and as AJ points out in [bip-anyprevout] we get back the same
> > security guarantees as we had without noinput.
> >
> > From what I remember and the transcript (thanks Kanzure for your awesome work
> > by the way), there was no strong support for chaperone signatures during the
> > meeting [6], but feedback from people that were not present is needed:
> >
> > > if everyone who wanted to use NOINPUT was convinced there was a problem, then
> > > they would pick the right thing, but clearly people aren't. It's not a
> > > foot-gun defense mechanism because it's easily bypassed, and it's easier to
> > > bypass it than to use it. Whereas for tagged outputs, it's that if you want
> > > any NOINPUT then you must tag.
> >
> > ## Output tagging
> >
> > One proposal that I found rather fascinating during the discussion in
> > Amsterdam was that we could achieve the same disincentive to use on
> > non-smart-contract cases by simply making the output scripts
> > unaddressable. This can be done by specifying a version of taproot outputs for
> > which the bech32 addressing scheme simply doesn't have a representation [6]:
> >
> > > The tagged outputs idea is that we don't have NOINPUT ANYPREVOUT supported for
> > > taproot v1 outputs, instead we have a segwit version 16 v16 that supports
> > > taproot. The reason for v16 is that we redefine bech32 to not cover
> > > v16. There's no addresses for this type of output. If you're an exchange and
> > > receive a bech32 address, you declare it invalid. You make it less user
> > > friendly here; and there shouldn't be an address anyway. You might want to see
> > > it on a block explorer, but you don't want to pass it around to anyone.
> >
> > We don't need addresses in our contract constructions because we deal directly
> > with the scripts. This would also have the desired effect of no allowing
> > generic wallets to send to these addresses, or users accidentally sending
> > funds to what was supposed to be a one-off script used internally in the
> > off-chain contract.
> >
> > Notice that this idea was already used by Russell O'Connor when performing a
> > transaction on elements using his new scripting language simplicity
> > [7]:
> >
> > > For this experimental development, we created an improper segwit version,
> > > "version 31" for Simplicity addresses. The payload of this segwit version 31
> > > address contains a commitment Merkle root of a Simplicity program to control
> > > the UTXO.
> >
> > The concern with output tagging is that it hurts fungibility, marking outputs
> > used in a contract as such and making them identifiable. But maybe it would be
> > a good idea to create two domains anyway: one for user-addressable
> > destinations which users can use with their general purpose wallets, and one
> > domain for contracts, which users cannot send to directly.
> >
> > This also came up during the CoreDev meeting [ams-coredev]:
> >
> > > these sort of NOINPUT signatures are only things that are within some
> > > application or within some protocol that gets negotiated between participants,
> > > but they don't cross-independent domains where you see a wallet or a protocol
> > > as a kind of domain. You can't tell the difference, is this an address I can
> > > give to someone else or not? It's all scripts, no real addresses. There are
> > > types of outputs that are completely insecure unconditionally; there are
> > > things that are protected and I can give to anyone, you don't want to reuse
> > > it, but there's no security issue from doing so. This is an additional class
> > > that is secure perfectly but only when used in the right way.
> >
> > ## Open questions
> >
> > The questions that remain to be addressed are the following:
> >
> > 1.  General agreement on the usefulness of noinput / anyprevoutanyscript /
> >     anyprevout. While at the CoreDev meeting I think everybody agreed that
> >     these proposals a useful, also beyond eltoo, not everybody could be
> >     there. I'd therefore like to elicit some feedback from the wider community.
> > 2.  Is there strong support or opposition to the chaperone signatures
> >     introduced in anyprevout / anyprevoutanyscript? I think it'd be best to
> >     formulate a concrete set of pros and contras, rather than talk about
> >     abstract dangers or advantages.
> > 3.  The same for output tagging / explicit opt-in. What are the advantages and
> >     disadvantages?
> > 4.  Shall we merge BIP-118 and bip-anyprevout. This would likely reduce the
> >     confusion and make for simpler discussions in the end.
> > 5.  Anything I forgot to mention :-)
> >
> > Cheers,
> > Christian
> >
> > [1] <https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-September/002131.html>
> > [2] <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-September/017285.html>
> > [3] <https://lists.linuxfoundation.org/pipermail/lightning-dev/2018-August/001383.html>
> > [4] <https://github.com/bitcoin/bips/blob/master/bip-0118.mediawiki>
> > [5] <https://github.com/ajtowns/bips/blob/bip-anyprevout/bip-anyprevout.mediawiki>
> > [6] <http://diyhpl.us/wiki/transcripts/bitcoin-core-dev-tech/2019-06-06-noinput-etc/>
> > [7] <https://lists.ozlabs.org/pipermail/simplicity/2019/000018.html>
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev at lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev



From decker.christian at gmail.com  Tue Oct  1 14:20:25 2019
From: decker.christian at gmail.com (Christian Decker)
Date: Tue, 01 Oct 2019 16:20:25 +0200
Subject: [bitcoin-dev] Continuing the discussion about noinput /
	anyprevout
In-Reply-To: <-5H29F71ID9UFqUGMaegQxPjKZSrF1mvdgfaaYtt_lwI7l1OTmN_8OgcooyoMt2_XuyZ5aDljL6gEup9C7skF8iuP_NbMW_81h0tJIGbJno=@protonmail.com>
References: <87wodp7w9f.fsf@gmail.com>
	<-5H29F71ID9UFqUGMaegQxPjKZSrF1mvdgfaaYtt_lwI7l1OTmN_8OgcooyoMt2_XuyZ5aDljL6gEup9C7skF8iuP_NbMW_81h0tJIGbJno=@protonmail.com>
Message-ID: <87tv8s7djq.fsf@gmail.com>

ZmnSCPxj <ZmnSCPxj at protonmail.com> writes:
> I rather strongly oppose output tagging.
>
> The entire point of for example Taproot was to reduce the variability
> of how outputs look like, so that unspent Taproot outputs look exactly
> like other unspent Taproot outputs regardless of the SCRIPT (or lack
> of SCRIPT) used to protect the outputs.  That is the reason why we
> would prefer to not support P2SH-wrapped Taproot even though
> P2SH-wrapping was intended to cover all future uses of SegWit,
> including SegWit v1 that Taproot will eventually get.

That is a bit reductive if you ask me. Taproot brings a number of
improvements such as the reduction of on-chain footprint in the
collaborative spend case, the hiding of complex logic in that case, and
yes, the uniformity of UTXOs that you mentioned. I do agree that it'd be
to make everything look identical to the outside observer, but saying
that separating outputs into two coarse-grained domains is equivalent to
throwing the baby out with the bath-water :-)

That being said, I should clarify that I would prefer not having to make
special accomodations on top of the raw sighash_noinput proposal, for
some perceived, but abstract danger that someone might shoot themselves
in the foot. I think we're all old enough not to need too much
handholding :-)

Output tagging is my second choice, since it minimizes the need for
people to get creative to work around other proposals, and minimizes the
on-chain footprint, and finally chaperone signatures are my least
preferred option due to its heavy-handed nature and the increased cost.

> Indeed, if it is output tagging that gets into Bitcoin base layer, I
> would strongly suggest the below for all Decker-Russell-Osuntokun
> implementations:
>
> * A standard MuSig 2-of-2 bip-schnorr SegWit v1 Funding Transaction Output, confirmed onchain
> * A "translator transaction" spending the above and paying out to a SegWit v16 output-tagged output, kept offchain.
> * Decker-Russell-Osuntokun update transaction, signed with `SIGHASH_NOINPUT` spending the translator transaction output.
> * Decker-Russell-Osuntokun state transaction, signed with `SIGHASH_NOINPUT` spending the update transaction output.

That is very much how I was planning to implement it anyway, using a
trigger transaction to separate timeout start and the actual
update/settlement pairs (cfr. eltoo paper Section 4.2). So for eltoo
there shouldn't be an issue here :-)

> The point regarding use of a commonly-known privkey to work around
> chaperone signatures is appropriate to the above, incidentally.  In
> short: this is a workaround, plain and simple, and one wonders the
> point of adding *either* chaperones *or* output tagging if we will, in
> practice, just work around them anyway.

Exactly, why introduce the extra burden of chaperone signatures or
output tagging if we're just going to sidestep it?

> Again, the *more* important point is that special blockchain
> constructions should only be used in the "bad" unilateral close case.
> In the cooperative case, we want to use simple plain
> bip-schnorr-signed outputs getting spent to further bip-schnor/Taproot
> SegWit v1 addresses, to increase the anonymity set of all uses of
> Decker-Russell-Osuntokun and other applications that might use
> `SIGHASH_NOINPUT` in some edge case (but which resolve down to simple
> bip-schnorr-signed n-of-n cases when the protocol is completed
> successfully by all participants).

While I do agree that we should keep outputs as unidentifiable as
possible, I am starting to question whether that is possible for
off-chain payment networks since we are gossiping about the existence of
channels and binding them to outpoints to prove their existence anyway.

Not the strongest argument I know, but there's little point in talking
ideal cases when we need to weaken that later again. 

>> Open questions
>>
>> ---------------
>>
>> The questions that remain to be addressed are the following:
>>
>> 1.  General agreement on the usefulness of noinput / anyprevoutanyscript /
>>     anyprevout. While at the CoreDev meeting I think everybody agreed that
>>     these proposals a useful, also beyond eltoo, not everybody could be
>>     there. I'd therefore like to elicit some feedback from the wider community.
>
> I strongly agree that `NOINPUT` is useful, and I was not able to attend CoreDev (at least, not with any human fleshbot already known to you --- I checked).

Great, good to know that I'm not shouting into the void, and that I'm
not just that crazy guy trying to get his hairbrained scheme to work :-)

>> 2.  Is there strong support or opposition to the chaperone signatures
>>     introduced in anyprevout / anyprevoutanyscript? I think it'd be best to
>>     formulate a concrete set of pros and contras, rather than talk about
>>     abstract dangers or advantages.
>
> No opposition, we will just work around this by publishing a common
> known private key to use for all chaperone signatures, since all the
> important security is in the `NOINPUT` signature anyway.
>
>>
>> 3.  The same for output tagging / explicit opt-in. What are the advantages and
>>     disadvantages?
>
> Strongly oppose, see above about my argument.
>
>>
>> 4.  Shall we merge BIP-118 and bip-anyprevout. This would likely reduce the
>>     confusion and make for simpler discussions in the end.
>
> Ambivalent, mildly support.
>
>>
>> 5.  Anything I forgot to mention :-)
>
> Cats are very interesting creatures, and are irrelevant to `SIGHASH_NOINPUT` discussion, but are extremely cute nonetheless.

Definitely agreed :+1:

From decker.christian at gmail.com  Tue Oct  1 14:26:39 2019
From: decker.christian at gmail.com (Christian Decker)
Date: Tue, 01 Oct 2019 16:26:39 +0200
Subject: [bitcoin-dev] Continuing the discussion about noinput /
	anyprevout
In-Reply-To: <gPtVJarazpIb7PaNu3ngXLKG2U4cIBfT9lb-04tltIrxufUUP4hMr08vU8Af19My-b5UeVwwo3BYhkDrVwEu1EjS_MMW5aXOx1sVub8MCIE=@protonmail.com>
References: <87wodp7w9f.fsf@gmail.com>
	<-5H29F71ID9UFqUGMaegQxPjKZSrF1mvdgfaaYtt_lwI7l1OTmN_8OgcooyoMt2_XuyZ5aDljL6gEup9C7skF8iuP_NbMW_81h0tJIGbJno=@protonmail.com>
	<gPtVJarazpIb7PaNu3ngXLKG2U4cIBfT9lb-04tltIrxufUUP4hMr08vU8Af19My-b5UeVwwo3BYhkDrVwEu1EjS_MMW5aXOx1sVub8MCIE=@protonmail.com>
Message-ID: <87r23w7d9c.fsf@gmail.com>

ZmnSCPxj <ZmnSCPxj at protonmail.com> writes:
> To elucidate further ---
>
> Suppose rather than `SIGHASH_NOINPUT`, we created a new opcode,
> `OP_CHECKSIG_WITHOUT_INPUT`.
>
> This new opcode ignores any `SIGHASH` flags, if present, on a
> signature, but instead hashes the current transaction without the
> input references, then checks that hash to the signature.
>
> This is equivalent to `SIGHASH_NOINPUT`.
>
> Yet as an opcode, it would be possible to embed in a Taproot script.
>
> For example, a Decker-Russell-Osuntokun would have an internal Taproot
> point be a 2-of-2, then have a script `OP_1
> OP_CHECKSIG_WITHOUT_INPUT`.  Unilateral closes would expose the hidden
> script, but cooperative closes would use the 2-of-2 directly.
>
> Of note, is that any special SCRIPT would already be supportable by Taproot.
> This includes SCRIPTs that may potentially lose funds for the user.
> Yet such SCRIPTs are already targetable by a Taproot address.
>
> If we are so concerned about `SIGHASH_NOINPUT` abuse, why are we not
> so concerned about Taproot abuse?

That would certainly be another possibility, which I have not explored
in detail so far. Due to the similarity between the various signature
checking op-codes it felt that it should be a sighash flag, and it
neatly slotted into the already existing flags. If we go for a separate
opcode we might end up reinventing the wheel, and to be honest I feared
that proposing a new opcode would get us into bikeshedding territory
(which I apparently failed to avoid with the sighash flag anyway...).

The advantage would be that with the sighash flag the spender is in
charge of specifying the flags, whereas with an opcode the output
dictates the signature verification modalities. The downside is the
increased design space.

What do others think? Would this be an acceptable opt-in mechanism that
addresses the main concerns?

Cheers,
Christian

From eth3rs at gmail.com  Tue Oct  1 14:27:21 2019
From: eth3rs at gmail.com (Ethan Heilman)
Date: Tue, 1 Oct 2019 10:27:21 -0400
Subject: [bitcoin-dev] [Lightning-dev] Continuing the discussion about
	noinput / anyprevout
In-Reply-To: <CACJVCgJ9PL-2jTS71--tXsa=QkK+f5_ciYLwv468WUno=XXAig@mail.gmail.com>
References: <87wodp7w9f.fsf@gmail.com>
	<CACJVCgJ9PL-2jTS71--tXsa=QkK+f5_ciYLwv468WUno=XXAig@mail.gmail.com>
Message-ID: <CAEM=y+Vm=UW4-UGV4zVJQY8mdT=9Ljr9kVcfQovtzjRcx33L=w@mail.gmail.com>

>I don't find too compelling the potential problem of a 'bad wallet designer', whether lazy or dogmatic, misusing noinput. I think there are simpler ways to cut corners and there will always be plenty of good wallet options people can choose.

I want to second this. The most expensive part of wallet design is
engineering time. Writing code that uses a new sighash or a custom
script with a OP_CODE is a very large barrier to use. How many wallets
support multisig or RBF? How much BTC has been stolen over the entire
history of Bitcoin because of sighash SIGHASH_NONE or SIGHASH_SINGLE
vs ECDSA nonce reuse?

On Tue, Oct 1, 2019 at 9:35 AM Richard Myers <rich at gotenna.com> wrote:
>
> Thanks Christian for pulling together this concise summary.
>
>> 1.  General agreement on the usefulness of noinput / anyprevoutanyscript /
>>     anyprevout.
>
>
> I certainly support the unification and adoption of the sighash_noinput and anyprevoutput* proposals to enable eltoo, but also to make possible better off-chain protocol designs generally.
>
> Among the various advantages previously discussed, the particular use case benefits from eltoo I want to take advantage of is less interactive payment channel negotiation.
>
> In talking with people about eltoo this summer, I found most people generally support adding this as an option to Lightning. The only general concern I heard, if any,  was the vague idea that rebindable transactions could be somehow misused or abused.
>
> I believe when these concerns are made more concrete they can be classified and addressed.
>
> I don't find too compelling the potential problem of a 'bad wallet designer', whether lazy or dogmatic, misusing noinput. I think there are simpler ways to cut corners and there will always be plenty of good wallet options people can choose.
>
> Because scripts signed with no_input signatures are only really exchanged and used for off-chain negotiations, very few should ever appear on chain. Those that do should represent non-cooperative situations that involve signing parties who know not to reuse or share scripts with these public keys again. No third party has any reason to spend value to a multisignature script they don't control, whether or not a no_input signature exists for it.
>
>> 2.  Is there strong support or opposition to the chaperone signatures
>>     introduced in anyprevout / anyprevoutanyscript? I think it'd be best to
>>     formulate a concrete set of pros and contras, rather than talk about
>>     abstract dangers or advantages.
>
>
> As I mentioned before, I don't think the lazy wallet designer advantage is enough to justify the downsides of chaperone signatures. One additional downside is the additional code complexity required to flag whether or not a chaperone output is included. By comparison, the code changes for creating a no_input digest that skips the prevout and prevscript parts of a tx is much less intrusive and easier to maintain.
>
>> 3.  The same for output tagging / explicit opt-in. What are the advantages and
>>     disadvantages?
>
>
> I see the point ZmnSCPxj makes about tagged outputs negatively impacting the anonymity set of taproot transactions. The suggested work around would impose a cost to unilateral closes of an additional translation transaction and not using the work around would cause a hit to anonymity for off-chain script users. I feel both costs are too high relative to the benefit gained of preventing sloppy reuse of public keys.
>
>> 4.  Shall we merge BIP-118 and bip-anyprevout. This would likely reduce the
>>     confusion and make for simpler discussions in the end.
>
>
> I believe they should be merged. I also think both chaperone signatures and output tagging should become part of the discussion of security alternatives, but not part of the initial specification.
>
> I understand the desire to be conservative with protocol changes that could be misused. However, with just taproot and taproot public key types the anyprevout functionality is already very opt-in and not something that might accidentally get used. Belt-and-suspender protections like chaperone signatures and tagged outputs have their own impacts on code complexity, on-chain transaction sizes and transaction anonymity that also must be considered.
>
> I believe efforts like descriptors will help people follow best practices when working with complex scripts without pushing extra complexity for safety into the consensus layer of bitcoin. Anywhere we can make core code simpler, and handle foot-guns in higher level non-consensus code, the better.
>
> _______________________________________________
>>
>> Lightning-dev mailing list
>> Lightning-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev
>
> _______________________________________________
> Lightning-dev mailing list
> Lightning-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev

From chris at suredbits.com  Tue Oct  1 12:23:47 2019
From: chris at suredbits.com (Chris Stewart)
Date: Tue, 1 Oct 2019 07:23:47 -0500
Subject: [bitcoin-dev] Continuing the discussion about noinput /
	anyprevout
In-Reply-To: <87wodp7w9f.fsf@gmail.com>
References: <87wodp7w9f.fsf@gmail.com>
Message-ID: <CAFQwNuxdfMNBGyM5Y4nMb46GNigxFTFCv3X09jZd4fjNPckw4Q@mail.gmail.com>

I do have some concerns about SIGHASH_NOINPUT, mainly that it does
introduce another footgun into the bitcoin protocol with address reuse.
It's common practice for bitcoin businesses to re-use addresses. Many
exchanges [1] reuse addresses for cold storage with very large sums of
money that is stored in these addreses.

It is my understanding with this part of BIP118

>Using NOINPUT the input containing the signature no longer references a
specific output. Any participant can take a transaction and rewrite it by
changing the hash reference to the previous output, without invalidating
the signatures. This allows transactions to be bound to any output that
matches the value committed to in the witness and whose witnessProgram,
combined with the spending transaction's witness returns true.

if an exchange were to once produce a digital signature from that cold
storage address with a SIGHASH_NOINPUT signature, that signature can be
replayed again and again on the blockchain until their wallet is drained.
This might be able to mitigated since the signatures commit to outputs,
which may be small in value for the transaction that SIGHASH_NOINPUT was
used. This means that an exchange could move coins from the address with a
larger transaction that spends money to a new output (and presumably pays a
higher fee than the smaller transactions).

### Why does this matter?

It seems that SIGHASH_NOINPUT will be an extremely useful tool for offchain
protocols like Lightning. This gives us the building blocks for enforcing
specific offchain states to end up onchain [2].

Since this tool is useful, we can presume that it will be integrated into
the signing path of large economic entities in bitcoin -- namely exchanges.
Many exchanges have specific signing procedures for transactions that are
leaving an exchange that is custom software. Now -- presuming wide adoption
of off chain protocols -- they will need to have a _second unique signing
path that uses SIGHASH_NOINPUT_.

It is imperative that this second signing path -- which uses
SIGHASH_NOINPUT -- does NOT get mixed up with the first signing path that
controls an exchanges onchain funds. If this were to happen, fund lost
could occur if the exchange is reusing address, which seems to be common
practice.

This is stated here in BIP118:

>This also means that particular care has to be taken in order to avoid
unintentionally enabling this rebinding mechanism. NOINPUT MUST NOT be
used, unless it is explicitly needed for the application, e.g., it MUST NOT
be a default signing flag in a wallet implementation. Rebinding is only
possible when the outputs the transaction may bind to all use the same
public keys. Any public key that is used in a NOINPUT signature MUST only
be used for outputs that the input may bind to, and they MUST NOT be used
for transactions that the input may not bind to. For example an application
SHOULD generate a new key-pair for the application instance using NOINPUT
signatures and MUST NOT reuse them afterwards.

This means we need to encourage onchain hot wallet signing procedures to be
kept separate from offchain hot wallet signing procedures, which introduces
more complexity for key management (two keychains).

One (of the few) upsides of the current Lightning penalty mechanism is that
fund loss can be contained to balance of the channel. You cannot do
something in the current protocol that will effect your funds outside of
that channel. With SIGHASH_NOINPUT, that property changes.

### A side note
In general, i think we should start disallowing uses of the SIGHASH
protocols that have unexpected behavior. The classic example of this is
SIGHASH_SINGLE [3]. I get uneasy about adding more footguns to the
protocol, which with current network behavior (address re-use)
SIGHASH_NOINPUT would be a big one.


[1] - https://bitinfocharts.com/top-100-richest-bitcoin-addresses.html
[2] -
https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-September/002136.html
[3] -
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-May/016048.html

On Mon, Sep 30, 2019 at 9:24 AM Christian Decker via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> With the recently renewed interest in eltoo, a proof-of-concept
> implementation
> [1], and the discussions regarding clean abstractions for off-chain
> protocols
> [2,3], I thought it might be time to revisit the `sighash_noinput` proposal
> (BIP-118 [4]), and AJ's `bip-anyprevout` proposal [5].
>
> (sorry for the long e-mail. I wanted to give enough context and describe
> the
> various tradeoffs so people don't have to stitch them together from
> memory. If
> you're impatient there are a couple of open questions at the bottom)
>
> Both proposals are ways to allow rebinding of transactions to new outputs,
> by
> adding a sighash flag that excludes the output when signing. This allows
> the
> transaction to be bound to any output, without needing a new signature, as
> long as output script and input script are compatible, e.g., the signature
> matches the public key specified in the output.
>
> BIP-118 is limited to explaining the details of signature verification, and
> omits anything related to deployment and dependency on other proposals.
> This
> was done in order not to depend on bip-taproot which is also in draft-phase
> currently, and to allow deployment alongside the next version of segwit
> script. `bip-anyprevout` builds on top of BIP-118, adding integration with
> `bip-taproot`, chaperone signatures, limits the use of the sighash flag to
> script path spends, as well as a new pubkey serialization which uses the
> first
> byte to signal opt-in.
>
> I'd like to stress that both proposals are complementary and not competing,
> which is something that I've heard a couple of times.
>
> There remain a couple of unclear points which I hope we can address in the
> coming days, to get this thing moving again, and hopefully get a new tool
> in
> our toolbox soon(ish).
>
> In the following I will quote a couple of things that were discussed during
> the CoreDev meeting earlier this year, but not everybody could join, and
> it is
> important that we engage the wider community, to get a better picture, and
> I
> think not everybody is up-to-date about the current state.
>
>
> ## Dangers of `sighash_noinput`
>
> An argument I have heard against noinput is that it is slightly less
> complex
> or compute intensive than `sighash_all` signatures, which may encourage
> wallet
> creators to only implement the noinput variant, and use it indiscrimi-
> nately. This is certainly a good argument, and indeed we have seen at least
> one developer proposing to use noinput for all transactions to discourage
> address reuse.
>
> This was also mentioned at CoreDev [6]:
>
> > When [...] said he wanted to write a wallet that only used
> SIGHASH\_NOINPUT,
> > that was pause for concern. Some people might want to use
> SIGHASH\_NOINPUT as a
> > way to cheapen or reduce the complexity of making a wallet
> > implementation. SIGHASH\_NOINPUT is from a purely procedural point of
> view
> > easier than doing a SIGHASH\_ALL, that's all I'm saying. So you're
> hashing
> > less. It's way faster. That concern has been brought to my attention and
> it's
> > something I can see. Do we want to avoid people being stupid and shooting
> > themselves and their customers in the foot? Or do we treat this as a
> special
> > case where you mark we're aware of how it should be used and we just try
> to
> > get that awareness out?
>
> Another issue that is sometimes brought up is that an external user may
> attempt to send funds to a script that was really part of a higher-level
> protocol. This leads to those funds becoming inaccessible unless you gather
> all the participants and sign off on those funds. I don't believe this is
> anything new, and if users really want to shoot themselves in the foot and
> send funds to random addresses they fish out of a blockexplorer there's
> little
> we can do. What we could do is make the scripts used internally in our
> protocols unaddressable (see output tagging below), removing this issue
> altogether.
>
>
> ## Chaperone signatures
>
> Chaperone signatures are signatures that ensure that there is no
> third-party
> malleability of transactions. The idea is to have an additional signature,
> that doesn't use noinput, or any of its variants, and therefore needs to be
> authored by one of the pubkeys in the output script, i.e., one or more of
> the
> participants of the contract the transaction belongs to. Concretely in
> eltoo
> we'd be using a shared key known to all participants in the eltoo
> instance, so
> any participant can sign an update to rebind it to the desired output.
>
> Chaperone signatures have a number of downsides however:
>
> -   Additional size: both the public key and the signature actually need
> to be
>     stored along with the real noinput signature, resulting in transfer,
>     computational and storage overhead. We can't reuse the same pubkey
> from the
>     noinput signature since that'd require access to the matching privkey
> which
>     is what we want to get rid of using noinput in the first place.
> -   Protocols can still simply use a globally known privkey, voiding the
>     benefit of chaperone signatures, since third-parties can sign again. I
>     argue that third-party malleability is a subset of first-party
>     malleability, and we should protect against first-party malleability
> first
>     and foremost. My counterparty has the incentive to trick me, a
> third-party
>     may not.
>
> On the plus side chaperone signatures certainly address the lazy-wallet-dev
> scenario, and as AJ points out in [bip-anyprevout] we get back the same
> security guarantees as we had without noinput.
>
> From what I remember and the transcript (thanks Kanzure for your awesome
> work
> by the way), there was no strong support for chaperone signatures during
> the
> meeting [6], but feedback from people that were not present is needed:
>
> > if everyone who wanted to use NOINPUT was convinced there was a problem,
> then
> > they would pick the right thing, but clearly people aren't. It's not a
> > foot-gun defense mechanism because it's easily bypassed, and it's easier
> to
> > bypass it than to use it. Whereas for tagged outputs, it's that if you
> want
> > any NOINPUT then you must tag.
>
>
> ## Output tagging
>
> One proposal that I found rather fascinating during the discussion in
> Amsterdam was that we could achieve the same disincentive to use on
> non-smart-contract cases by simply making the output scripts
> unaddressable. This can be done by specifying a version of taproot outputs
> for
> which the bech32 addressing scheme simply doesn't have a representation
> [6]:
>
> > The tagged outputs idea is that we don't have NOINPUT ANYPREVOUT
> supported for
> > taproot v1 outputs, instead we have a segwit version 16 v16 that supports
> > taproot. The reason for v16 is that we redefine bech32 to not cover
> > v16. There's no addresses for this type of output. If you're an exchange
> and
> > receive a bech32 address, you declare it invalid. You make it less user
> > friendly here; and there shouldn't be an address anyway. You might want
> to see
> > it on a block explorer, but you don't want to pass it around to anyone.
>
> We don't need addresses in our contract constructions because we deal
> directly
> with the scripts. This would also have the desired effect of no allowing
> generic wallets to send to these addresses, or users accidentally sending
> funds to what was supposed to be a one-off script used internally in the
> off-chain contract.
>
> Notice that this idea was already used by Russell O'Connor when performing
> a
> transaction on elements using his new scripting language simplicity
> [7]:
>
> > For this experimental development, we created an improper segwit version,
> > "version 31" for Simplicity addresses. The payload of this segwit
> version 31
> > address contains a commitment Merkle root of a Simplicity program to
> control
> > the UTXO.
>
> The concern with output tagging is that it hurts fungibility, marking
> outputs
> used in a contract as such and making them identifiable. But maybe it
> would be
> a good idea to create two domains anyway: one for user-addressable
> destinations which users can use with their general purpose wallets, and
> one
> domain for contracts, which users cannot send to directly.
>
> This also came up during the CoreDev meeting [ams-coredev]:
>
> > these sort of NOINPUT signatures are only things that are within some
> > application or within some protocol that gets negotiated between
> participants,
> > but they don't cross-independent domains where you see a wallet or a
> protocol
> > as a kind of domain. You can't tell the difference, is this an address I
> can
> > give to someone else or not? It's all scripts, no real addresses. There
> are
> > types of outputs that are completely insecure unconditionally; there are
> > things that are protected and I can give to anyone, you don't want to
> reuse
> > it, but there's no security issue from doing so. This is an additional
> class
> > that is secure perfectly but only when used in the right way.
>
>
> ## Open questions
>
> The questions that remain to be addressed are the following:
>
> 1.  General agreement on the usefulness of noinput / anyprevoutanyscript /
>     anyprevout. While at the CoreDev meeting I think everybody agreed that
>     these proposals a useful, also beyond eltoo, not everybody could be
>     there. I'd therefore like to elicit some feedback from the wider
> community.
> 2.  Is there strong support or opposition to the chaperone signatures
>     introduced in anyprevout / anyprevoutanyscript? I think it'd be best to
>     formulate a concrete set of pros and contras, rather than talk about
>     abstract dangers or advantages.
> 3.  The same for output tagging / explicit opt-in. What are the advantages
> and
>     disadvantages?
> 4.  Shall we merge BIP-118 and bip-anyprevout. This would likely reduce the
>     confusion and make for simpler discussions in the end.
> 5.  Anything I forgot to mention :-)
>
> Cheers,
> Christian
>
> [1] <
> https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-September/002131.html
> >
> [2] <
> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-September/017285.html
> >
> [3] <
> https://lists.linuxfoundation.org/pipermail/lightning-dev/2018-August/001383.html
> >
> [4] <https://github.com/bitcoin/bips/blob/master/bip-0118.mediawiki>
> [5] <
> https://github.com/ajtowns/bips/blob/bip-anyprevout/bip-anyprevout.mediawiki
> >
> [6] <
> http://diyhpl.us/wiki/transcripts/bitcoin-core-dev-tech/2019-06-06-noinput-etc/
> >
> [7] <https://lists.ozlabs.org/pipermail/simplicity/2019/000018.html>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191001/6260f1ba/attachment-0001.html>

From aj at erisian.com.au  Tue Oct  1 14:45:48 2019
From: aj at erisian.com.au (Anthony Towns)
Date: Wed, 2 Oct 2019 00:45:48 +1000
Subject: [bitcoin-dev] Continuing the discussion about noinput /
 anyprevout
In-Reply-To: <gPtVJarazpIb7PaNu3ngXLKG2U4cIBfT9lb-04tltIrxufUUP4hMr08vU8Af19My-b5UeVwwo3BYhkDrVwEu1EjS_MMW5aXOx1sVub8MCIE=@protonmail.com>
References: <87wodp7w9f.fsf@gmail.com>
	<-5H29F71ID9UFqUGMaegQxPjKZSrF1mvdgfaaYtt_lwI7l1OTmN_8OgcooyoMt2_XuyZ5aDljL6gEup9C7skF8iuP_NbMW_81h0tJIGbJno=@protonmail.com>
	<gPtVJarazpIb7PaNu3ngXLKG2U4cIBfT9lb-04tltIrxufUUP4hMr08vU8Af19My-b5UeVwwo3BYhkDrVwEu1EjS_MMW5aXOx1sVub8MCIE=@protonmail.com>
Message-ID: <20191001144548.hrne6mlhmof7tpkr@erisian.com.au>

On Mon, Sep 30, 2019 at 11:28:43PM +0000, ZmnSCPxj via bitcoin-dev wrote:
> Suppose rather than `SIGHASH_NOINPUT`, we created a new opcode, `OP_CHECKSIG_WITHOUT_INPUT`.

I don't think there's any meaningful difference between making a new
opcode and making a new tapscript public key type; the difference is
just one of encoding:

   3301<key>AC   [CHECKSIG of public key type 0x01]
   32<key>B3     [CHECKSIG_WITHOUT_INPUT (replacing NOP4) of key]

> This new opcode ignores any `SIGHASH` flags, if present, on a signature,

(How sighash flags are treated can be redefined by new public key types;
if that's not obvious already)

Cheers,
aj


From ZmnSCPxj at protonmail.com  Tue Oct  1 15:35:34 2019
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Tue, 01 Oct 2019 15:35:34 +0000
Subject: [bitcoin-dev] Continuing the discussion about noinput /
	anyprevout
In-Reply-To: <87tv8s7djq.fsf@gmail.com>
References: <87wodp7w9f.fsf@gmail.com>
	<-5H29F71ID9UFqUGMaegQxPjKZSrF1mvdgfaaYtt_lwI7l1OTmN_8OgcooyoMt2_XuyZ5aDljL6gEup9C7skF8iuP_NbMW_81h0tJIGbJno=@protonmail.com>
	<87tv8s7djq.fsf@gmail.com>
Message-ID: <4zx7e_vHQr58myY5w_-bAjTk04LTGNknZudZs4wbUiOIoVKhL69M7k1eELCSuoBND2CtVXXzDFBHW4351cttIh80eP8jiaoO8cmbSefZmj4=@protonmail.com>

Good morning Christian,

> > -   A standard MuSig 2-of-2 bip-schnorr SegWit v1 Funding Transaction Output, confirmed onchain
> > -   A "translator transaction" spending the above and paying out to a SegWit v16 output-tagged output, kept offchain.
> > -   Decker-Russell-Osuntokun update transaction, signed with `SIGHASH_NOINPUT` spending the translator transaction output.
> > -   Decker-Russell-Osuntokun state transaction, signed with `SIGHASH_NOINPUT` spending the update transaction output.
>
> That is very much how I was planning to implement it anyway, using a
> trigger transaction to separate timeout start and the actual
> update/settlement pairs (cfr. eltoo paper Section 4.2). So for eltoo
> there shouldn't be an issue here :-)

My understanding is that a trigger transaction is not in fact necessary for Decker-Russell-Osuntokun: any update transaction could spend the funding transaction output directly, and thereby start the relative timelock.
At least, if we could arrange the funding transaction output to be spendable directly using `SIGHASH_NOINPUT` or variants thereof.


> > Again, the more important point is that special blockchain
> > constructions should only be used in the "bad" unilateral close case.
> > In the cooperative case, we want to use simple plain
> > bip-schnorr-signed outputs getting spent to further bip-schnor/Taproot
> > SegWit v1 addresses, to increase the anonymity set of all uses of
> > Decker-Russell-Osuntokun and other applications that might use
> > `SIGHASH_NOINPUT` in some edge case (but which resolve down to simple
> > bip-schnorr-signed n-of-n cases when the protocol is completed
> > successfully by all participants).
>
> While I do agree that we should keep outputs as unidentifiable as
> possible, I am starting to question whether that is possible for
> off-chain payment networks since we are gossiping about the existence of
> channels and binding them to outpoints to prove their existence anyway.

* Lightning supports unpublished channels, so we do not gossip some outpoints even though they are in fact channels underneath.
  * I confess the existence of unpublished channels in the spec fails to summon any reaction other than incredulity from me, but they exist nonetheless, my incredulity notwithstanding.
* Historical channels that have been cooperatively closed are no longer normally gossiped, so the fact that they used to be channels is no longer widely broadcast, and may eventually be forgotten by most or all of the network.
  * This means anyone who wants to record the historical use of Lightning will have to retain the information themselves, rather than delegating it to fullnodes everywhere.

>
> Not the strongest argument I know, but there's little point in talking
> ideal cases when we need to weaken that later again.

The point of ideal cases is to strive to approach them, not necessarily achieve them.
Just as a completely unbiased rational reasoner is almost impossible to achieve, does not mean we should give up all attempts to reduce bias.

Outpoints that used to be channels, but have now been closed using cooperative closes, will potentially no longer be widely gossiped as having once been channels, thus it may happen that they will eventually be forgotten by most of the network as once having been channels.
But if the outpoints of those channels are specially marked, then that cannot be forgotten, as the initial block download thereafter will have that history indelibly etched forevermore.

Regards,
ZmnSCPxj

From ZmnSCPxj at protonmail.com  Tue Oct  1 15:42:08 2019
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Tue, 01 Oct 2019 15:42:08 +0000
Subject: [bitcoin-dev] Continuing the discussion about noinput /
	anyprevout
In-Reply-To: <20191001144548.hrne6mlhmof7tpkr@erisian.com.au>
References: <87wodp7w9f.fsf@gmail.com>
	<-5H29F71ID9UFqUGMaegQxPjKZSrF1mvdgfaaYtt_lwI7l1OTmN_8OgcooyoMt2_XuyZ5aDljL6gEup9C7skF8iuP_NbMW_81h0tJIGbJno=@protonmail.com>
	<gPtVJarazpIb7PaNu3ngXLKG2U4cIBfT9lb-04tltIrxufUUP4hMr08vU8Af19My-b5UeVwwo3BYhkDrVwEu1EjS_MMW5aXOx1sVub8MCIE=@protonmail.com>
	<20191001144548.hrne6mlhmof7tpkr@erisian.com.au>
Message-ID: <cF916RaV0ndCBiZcXM0Cl6QOtCWn-bWz8Fs3MAKIIck85dlGEZFQkmWJXPQQ-342viaXHx8mA5xLYuD-fnoIYdIc3eyk5lNXgEj1hhx36mQ=@protonmail.com>

Good morning aj,


> On Mon, Sep 30, 2019 at 11:28:43PM +0000, ZmnSCPxj via bitcoin-dev wrote:
>
> > Suppose rather than `SIGHASH_NOINPUT`, we created a new opcode, `OP_CHECKSIG_WITHOUT_INPUT`.
>
> I don't think there's any meaningful difference between making a new
> opcode and making a new tapscript public key type; the difference is
> just one of encoding:
>
> 3301<key>AC [CHECKSIG of public key type 0x01]
> 32<key>B3 [CHECKSIG_WITHOUT_INPUT (replacing NOP4) of key]
>
> > This new opcode ignores any `SIGHASH` flags, if present, on a signature,
>
> (How sighash flags are treated can be redefined by new public key types;
> if that's not obvious already)


Thank you for this thought,
I believe under tapscript v0 we can give `OP_1` as the public key to `OP_CHECKSIG` to mean to reuse the internal Taproot pubkey, would it be possible to have some similar mechanism here, to copy the internal Taproot pubkey but also to enable new `SIGHASH` flag for this particular script only?

This seems fine, as then a Decker-Russell-Osuntokun funding tx output between nodes A, B, and C would have:

* Taproot internal key: `P = MuSig(A, B, C)`
* Script 1: leaf version 0, `<MuSig(A,B,C) + pubkeytype 1> OP_CHECKSIG`

Then, update transactions could use `MuSig(A,B,C)` for signing along the "update" path, with unique "state" keys.
And cooperative closes would sign using `P + h(P | MAST(<MuSig(A,B,C) + pubkeytype 1> OPCHECKSIG)) * G`, not revealing the fact that this was in fact a Decker-Russell-Osuntokun output.

Regards,
ZmnSCPxj

From aj at erisian.com.au  Tue Oct  1 15:59:29 2019
From: aj at erisian.com.au (Anthony Towns)
Date: Wed, 2 Oct 2019 01:59:29 +1000
Subject: [bitcoin-dev] Continuing the discussion about noinput /
 anyprevout
In-Reply-To: <87wodp7w9f.fsf@gmail.com>
References: <87wodp7w9f.fsf@gmail.com>
Message-ID: <20191001155929.e2yznsetqesx2jxo@erisian.com.au>

On Mon, Sep 30, 2019 at 03:23:56PM +0200, Christian Decker via bitcoin-dev wrote:
> With the recently renewed interest in eltoo, a proof-of-concept implementation
> [1], and the discussions regarding clean abstractions for off-chain protocols
> [2,3], I thought it might be time to revisit the `sighash_noinput` proposal
> (BIP-118 [4]), and AJ's `bip-anyprevout` proposal [5].

Hey Christian, thanks for the write up!

> ## Open questions
> The questions that remain to be addressed are the following:
> 1.  General agreement on the usefulness of noinput / anyprevoutanyscript /
>     anyprevout[?]
> 2.  Is there strong support or opposition to the chaperone signatures[?]
> 3.  The same for output tagging / explicit opt-in[?]
> 4.  Shall we merge BIP-118 and bip-anyprevout. This would likely reduce the
>     confusion and make for simpler discussions in the end.

I think there's an important open question you missed from this list:
(1.5) do we really understand what the dangers of noinput/anyprevout-style
constructions actually are?

My impression on the first 3.5 q's is: (1) yes, (1.5) not really,
(2) weak opposition for requiring chaperone sigs, (3) mixed (weak)
support/opposition for output tagging.

My thinking at the moment (subject to change!) is:

 * anyprevout signatures make the address you're signing for less safe,
   which may cause you to lose funds when additional coins are sent to
   the same address; this can be avoided if handled with care (or if you
   don't care about losing funds in the event of address reuse)

 * being able to guarantee that an address can never be signed for with
   an anyprevout signature is therefore valuable; so having it be opt-in
   at the tapscript level, rather than a sighash flag available for
   key-path spends is valuable (I call this "opt-in", but it's hidden
   until use via taproot rather than "explicit" as output tagging
   would be)

 * receiving funds spent via an anyprevout signature does not involve any
   qualitatively new double-spending/malleability risks.
   
   (eltoo is unavoidably malleable if there are multiple update
   transactions (and chaperone signatures aren't used or are used with
   well known keys), but while it is better to avoid this where possible,
   it's something that's already easily dealt with simply by waiting
   for confirmations, and whether a transaction is malleable is always
   under the control of the sender not the receiver)

 * as such, output tagging is also unnecessary, and there is also no
   need for users to mark anyprevout spends as "tainted" in order to
   wait for more confirmations than normal before considering those funds
   "safe"

I think it might be good to have a public testnet (based on Richard Myers
et al's signet2 work?) where we have some fake exchanges/merchants/etc
and scheduled reorgs, and demo every weird noinput/anyprevout case anyone
can think of, and just work out if we need any extra code/tagging/whatever
to keep those fake exchanges/merchants from losing money (and write up
the weird cases we've found in a wiki or a paper so people can easily
tell if we missed something obvious).

Cheers,
aj


From chris at suredbits.com  Tue Oct  1 15:14:56 2019
From: chris at suredbits.com (Chris Stewart)
Date: Tue, 1 Oct 2019 10:14:56 -0500
Subject: [bitcoin-dev] [Lightning-dev] Continuing the discussion about
	noinput / anyprevout
In-Reply-To: <CACJVCgJ9PL-2jTS71--tXsa=QkK+f5_ciYLwv468WUno=XXAig@mail.gmail.com>
References: <87wodp7w9f.fsf@gmail.com>
	<CACJVCgJ9PL-2jTS71--tXsa=QkK+f5_ciYLwv468WUno=XXAig@mail.gmail.com>
Message-ID: <CAFQwNuxRcwOh9AUWXMonDM=7AgiHMym-TtuaHS_-6RFKcnNgZQ@mail.gmail.com>

> I don't find too compelling the potential problem of a 'bad wallet
designer', whether lazy or dogmatic, misusing noinput. I think there are
simpler ways to cut corners and there will always be plenty of good wallet
options people can choose.

In my original post, the business that I am talking about don't use "off
the shelf" wallet options. It isn't a "let's switch from wallet A to wallet
B" kind of situation. Usually this involves design from ground up with
security considerations that businesses of scale need to consider (signing
procedures and key handling being the most important!).

>Because scripts signed with no_input signatures are only really exchanged
and used for off-chain negotiations, very few should ever appear on chain.
Those that do should represent non-cooperative situations that involve
signing parties who know not to reuse or share scripts with these public
keys again. No third party has any reason to spend value to a
multisignature script they don't control, whether or not a no_input
signature exists for it.

Just because some one is your friend today, doesn't mean they aren't
necessarily your adversary tomorrow. I don't think a signature being
onchain really matters, as you have to give it to your counterparty
regardless. How do you know your counterparty won't replay that
SIGHASH_NOINPUT signature later? Offchain protocols shouldn't rely on
"good-will" for their counter parties for security.

>As I mentioned before, I don't think the lazy wallet designer advantage is
enough to justify the downsides of chaperone signatures. One additional
downside is the additional code complexity required to flag whether or not
a chaperone output is included. By comparison, the code changes for
creating a no_input digest that skips the prevout and prevscript parts of a
tx is much less intrusive and easier to maintain.

>I want to second this. The most expensive part of wallet design is
engineering time. Writing code that uses a new sighash or a custom
script with a OP_CODE is a very large barrier to use. How many wallets
support multisig or RBF? How much BTC has been stolen over the entire
history of Bitcoin because of sighash SIGHASH_NONE or SIGHASH_SINGLE
vs ECDSA nonce reuse

I actually think lazy wallet designer is a really compelling reason to fix
footguns in the bitcoin protocol. Mt Gox is allegedly a product of lazy
wallet design. Now we have non-malleable transactions in the form of segwit
(yay!) that prevent this exploit. We can wish that the Mt Gox wallet
designers were more aware of bitcoin protocol vulnerabilities, but at the
end of the day the best thing to do was offering an alternative that
circumvents the vulnerability all together.

Ethan made a great point about SIGHASH_NONE or SIGHASH_SINGLE -- which have
virtually no use AFAIK -- vs the ECDSA nonce reuse which is used in nearly
every transaction. The feature -- ECDSA in this case -- was managed to be
done wrong by wallet developers causing fund loss. Unfortunately we can't
protect against this type of bug in the protocol.

If things aren't used -- such as SIGHASH_NONE or SIGHASH_SINGLE -- it
doesn't matter if they are secure or insecure. I'm hopefully that offchain
protocols will achieve wide adoption, and I would hate to see money lost
because of this. Even though they aren't used, in my OP I do advocate for
fixing these.

> understand the desire to be conservative with protocol changes that could
be misused. However, with just taproot and taproot public key types the
anyprevout functionality is already very opt-in and not something that
might accidentally get used. Belt-and-suspender protections like chaperone
signatures and tagged outputs have their own impacts on code complexity,
on-chain transaction sizes and transaction anonymity that also must be
considered.

I'm making the assumption that the business has decided to use this
feature, and in my OP I talk about the engineering decisions that will have
to be made support this. I'm hoping the "lazy wallet designers" -- or
perhaps people that don't follow bitcoin protocol development as rabidly as
us :-) -- can see that nuance.

-Chris



On Tue, Oct 1, 2019 at 8:36 AM Richard Myers <rich at gotenna.com> wrote:

> Thanks Christian for pulling together this concise summary.
>
> 1.  General agreement on the usefulness of noinput / anyprevoutanyscript /
>>     anyprevout.
>>
>
> I certainly support the unification and adoption of the sighash_noinput
> and anyprevoutput* proposals to enable eltoo, but also to make possible
> better off-chain protocol designs generally.
>
> Among the various advantages previously discussed, the particular use case
> benefits from eltoo I want to take advantage of is less interactive payment
> channel negotiation.
>
> In talking with people about eltoo this summer, I found most people
> generally support adding this as an option to Lightning. The only general
> concern I heard, if any,  was the vague idea that rebindable transactions
> could be somehow misused or abused.
>
> I believe when these concerns are made more concrete they can be
> classified and addressed.
>
> I don't find too compelling the potential problem of a 'bad wallet
> designer', whether lazy or dogmatic, misusing noinput. I think there are
> simpler ways to cut corners and there will always be plenty of good wallet
> options people can choose.
>
> Because scripts signed with no_input signatures are only really exchanged
> and used for off-chain negotiations, very few should ever appear on chain.
> Those that do should represent non-cooperative situations that involve
> signing parties who know not to reuse or share scripts with these public
> keys again. No third party has any reason to spend value to a
> multisignature script they don't control, whether or not a no_input
> signature exists for it.
>
> 2.  Is there strong support or opposition to the chaperone signatures
>>     introduced in anyprevout / anyprevoutanyscript? I think it'd be best
>> to
>>     formulate a concrete set of pros and contras, rather than talk about
>>     abstract dangers or advantages.
>>
>
> As I mentioned before, I don't think the lazy wallet designer advantage is
> enough to justify the downsides of chaperone signatures. One additional
> downside is the additional code complexity required to flag whether or not
> a chaperone output is included. By comparison, the code changes for
> creating a no_input digest that skips the prevout and prevscript parts of a
> tx is much less intrusive and easier to maintain.
>
> 3.  The same for output tagging / explicit opt-in. What are the advantages
>> and
>>     disadvantages?
>>
>
> I see the point ZmnSCPxj makes about tagged outputs negatively impacting
> the anonymity set of taproot transactions. The suggested work around would
> impose a cost to unilateral closes of an additional translation transaction
> and not using the work around would cause a hit to anonymity for off-chain
> script users. I feel both costs are too high relative to the benefit gained
> of preventing sloppy reuse of public keys.
>
> 4.  Shall we merge BIP-118 and bip-anyprevout. This would likely reduce the
>>     confusion and make for simpler discussions in the end.
>
>
> I believe they should be merged. I also think both chaperone signatures
> and output tagging should become part of the discussion of security
> alternatives, but not part of the initial specification.
>
> I understand the desire to be conservative with protocol changes that
> could be misused. However, with just taproot and taproot public key types
> the anyprevout functionality is already very opt-in and not something that
> might accidentally get used. Belt-and-suspender protections like chaperone
> signatures and tagged outputs have their own impacts on code complexity,
> on-chain transaction sizes and transaction anonymity that also must be
> considered.
>
> I believe efforts like descriptors will help people follow best practices
> when working with complex scripts without pushing extra complexity for
> safety into the consensus layer of bitcoin. Anywhere we can make core code
> simpler, and handle foot-guns in higher level non-consensus code, the
> better.
>
> _______________________________________________
>
>> Lightning-dev mailing list
>> Lightning-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev
>>
> _______________________________________________
> Lightning-dev mailing list
> Lightning-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191001/4f71dd93/attachment-0001.html>

From ZmnSCPxj at protonmail.com  Wed Oct  2 02:03:43 2019
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Wed, 02 Oct 2019 02:03:43 +0000
Subject: [bitcoin-dev] Continuing the discussion about noinput /
	anyprevout
In-Reply-To: <20191001155929.e2yznsetqesx2jxo@erisian.com.au>
References: <87wodp7w9f.fsf@gmail.com>
	<20191001155929.e2yznsetqesx2jxo@erisian.com.au>
Message-ID: <CR-etCjXB-JWkvecjDog4Pkq1SuLUgndtSrZo-V4f4EGcNXzNCeAHRvCZGrxDWw7aHVdDY0pAF92jNLb_Hct0bMb3ew6JEpB9AfIm1tSGaQ=@protonmail.com>

Good morning lists,

Let me propose the below radical idea:

* `SIGHASH` flags attached to signatures are a misdesign, sadly retained from the original BitCoin 0.1.0 Alpha for Windows design, on par with:
  * 1 RETURN
  * higher-`nSequence` replacement
  * DER-encoded pubkeys
  * unrestricted `scriptPubKey`
  * Payee-security-paid-by-payer (i.e. lack of P2SH)
  * `OP_CAT` and `OP_MULT` and `OP_ADD` and friends
  * transaction malleability
  * probably many more

So let me propose the more radical excision, starting with SegWit v1:

* Remove `SIGHASH` from signatures.
* Put `SIGHASH` on public keys.

Public keys are now encoded as either 33-bytes (implicit `SIGHASH_ALL`) or 34-bytes (`SIGHASH` byte, followed by pubkey type, followed by pubkey coordinate).
`OP_CHECKSIG` and friends then look at the *public key* to determine sighash algorithm rather than the signature.

As we expect public keys to be indirectly committed to on every output `scriptPubKey`, this is automatically output tagging to allow particular `SIGHASH`.
However, we can then utilize the many many ways to hide public keys away until they are needed, exemplified in MAST-inside-Taproot.

I propose also the addition of the opcode:

    <sighash> <pubkey> OP_SETPUBKEYSIGHASH

* `sighash` must be one byte.
* `pubkey` may be the special byte `0x1`, meaning "just use the Taproot internal pubkey".
* `pubkey` may be 33-byte public key, in which case the `sighash` byte is just prepended to it.
* `pubkey` may be 34-byte public key with sighash, in which case the first byte is replaced with `sighash` byte.
* If `sighash` is `0x00` then the result is a 33-byte public key (the sighash byte is removed) i.e. `SIGHASH_ALL` implicit.

This retains the old feature where the sighash is selected at time-of-spending rather than time-of-payment.
This is done by using the script:

    <pubkey> OP_SETPUBKEYSIGHASH OP_CHECKSIG

Then the sighash can be put in the witness stack after the signature, letting the `SIGHASH` flag be selected at time-of-signing, but only if the SCRIPT specifically is formed to do so.
This is malleability-safe as the signature still commits to the `SIGHASH` it was created for.

However, by default, public keys will not have an attached `SIGHASH` byte, implying `SIGHASH_ALL` (and disallowing-by-default non-`SIGHASH_ALL`).

This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as they are allowed only if the output specifically says they are allowed.

Would this not be a superior solution?

Regards,
ZmnSCPxj

From s7r at sky-ip.org  Wed Oct  2 15:11:25 2019
From: s7r at sky-ip.org (s7r)
Date: Wed, 2 Oct 2019 18:11:25 +0300
Subject: [bitcoin-dev] Continuing the discussion about noinput /
 anyprevout
In-Reply-To: <20191001155929.e2yznsetqesx2jxo@erisian.com.au>
References: <87wodp7w9f.fsf@gmail.com>
	<20191001155929.e2yznsetqesx2jxo@erisian.com.au>
Message-ID: <51d80db6-83d9-2473-d007-f3601c940f28@sky-ip.org>

Anthony Towns via bitcoin-dev wrote:
[SNIP]
> 
> My thinking at the moment (subject to change!) is:
> 
>  * anyprevout signatures make the address you're signing for less safe,
>    which may cause you to lose funds when additional coins are sent to
>    the same address; this can be avoided if handled with care (or if you
>    don't care about losing funds in the event of address reuse)
> 

It's not necessarily like this. Address re-use is many times OUTSIDE the
control of the address owner. Say I give my address to a counterparty.
They send me a transaction which I successfully spend. So far so good.

After that, I have no control over that counterparty. If they decide to
re-use that address, it does not mean I wanted to re-use it and it also
does not mean that I don't care about those funds being lost.

This could create a lot of problems in the industry and I think it
should be avoided. Address re-use has been strongly discouraged ever
since I can remember, and all (proper) wallet implementations try as
hard as possible to enforce it, but it's not always possible. A
counterparty that decides to re-use an address, either accidentally or
not, is not under the control of the user who handed out the address in
the first place.

There are also a lot of use cases with P2SH addresses that are some
smart contracts particularly designed to be re-used multiple times over
time.

My 2 cents are that this is not a good way to go. If you try to index
the entire blockchain until now you'll see that address re-use is more
common than we'd want it to be and there's no clear way to prevent this
from further happening without hurting the economic interests of the users.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 488 bytes
Desc: OpenPGP digital signature
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191002/282e8910/attachment.sig>

From aj at erisian.com.au  Thu Oct  3 01:47:58 2019
From: aj at erisian.com.au (Anthony Towns)
Date: Thu, 3 Oct 2019 11:47:58 +1000
Subject: [bitcoin-dev] [Lightning-dev] Continuing the discussion about
 noinput / anyprevout
In-Reply-To: <CR-etCjXB-JWkvecjDog4Pkq1SuLUgndtSrZo-V4f4EGcNXzNCeAHRvCZGrxDWw7aHVdDY0pAF92jNLb_Hct0bMb3ew6JEpB9AfIm1tSGaQ=@protonmail.com>
References: <87wodp7w9f.fsf@gmail.com>
	<20191001155929.e2yznsetqesx2jxo@erisian.com.au>
	<CR-etCjXB-JWkvecjDog4Pkq1SuLUgndtSrZo-V4f4EGcNXzNCeAHRvCZGrxDWw7aHVdDY0pAF92jNLb_Hct0bMb3ew6JEpB9AfIm1tSGaQ=@protonmail.com>
Message-ID: <20191003014758.gtgfge5yokcxkfsj@erisian.com.au>

On Wed, Oct 02, 2019 at 02:03:43AM +0000, ZmnSCPxj via Lightning-dev wrote:
> So let me propose the more radical excision, starting with SegWit v1:
> * Remove `SIGHASH` from signatures.
> * Put `SIGHASH` on public keys.
>     <sighash> <pubkey> OP_SETPUBKEYSIGHASH

I don't think you could reasonably do this for key path spends -- if
you included the sighash as part of the scriptpubkey explicitly, that
would lose some of the indistinguishability of taproot addresses, and be
more expensive than having the sighash be in witness data. So I think
that means sighashes would still be included in key path signatures,
which would make the behaviour a little confusingly different between
signing for key path and script path spends.

> This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as they are allowed only if the output specifically says they are allowed.

I don't think the problems with NONE and SINGLE are any worse than using
SIGHASH_ALL to pay to "1*G" -- someone may steal the money you send,
but that's as far as it goes. NOINPUT/ANYPREVOUT is worse in that if
you use it, someone may steal funds from other UTXOs too -- similar
to nonce-reuse. So I think having to commit to enabling NOINPUT for an
address may make sense; but I don't really see the need for doing the
same for other sighashes generally.

FWIW, one way of looking at a transaction spending UTXO "U" to address
"A" is something like:

 * "script" lets you enforce conditions on the transaction when you
   create "A" [0]
 * "sighash" lets you enforce conditions on the transaction when
   you sign the transaction
 * nlocktime, nsequence, taproot annex are ways you express conditions
   on the transaction

In that view, "sighash" is actually an *extremely* simple scripting
language itself (with a total of six possible scripts).

That doesn't seem like a bad design to me, fwiw.

Cheers,
aj

[0] "graftroot" lets you update those conditions for address "A" after
    the fact

From ZmnSCPxj at protonmail.com  Thu Oct  3 03:07:55 2019
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Thu, 03 Oct 2019 03:07:55 +0000
Subject: [bitcoin-dev] [Lightning-dev] Continuing the discussion about
	noinput / anyprevout
In-Reply-To: <20191003014758.gtgfge5yokcxkfsj@erisian.com.au>
References: <87wodp7w9f.fsf@gmail.com>
	<20191001155929.e2yznsetqesx2jxo@erisian.com.au>
	<CR-etCjXB-JWkvecjDog4Pkq1SuLUgndtSrZo-V4f4EGcNXzNCeAHRvCZGrxDWw7aHVdDY0pAF92jNLb_Hct0bMb3ew6JEpB9AfIm1tSGaQ=@protonmail.com>
	<20191003014758.gtgfge5yokcxkfsj@erisian.com.au>
Message-ID: <iKVHkY_BWc4A9e7AiuGiyYUAu9TM4yGNlTL7HQklrw6_1pW4ZpwRd-qKLox7jpmQ8QgWrj3Ovrq9sDQijMD3Q_dNivchgNfxy8zcchFYkQ4=@protonmail.com>

> > let me propose the more radical excision, starting with SegWit v1:
> >
> > -   Remove `SIGHASH` from signatures.
> > -   Put `SIGHASH` on public keys.
> >     <sighash> <pubkey> OP_SETPUBKEYSIGHASH
> >
>
> I don't think you could reasonably do this for key path spends -- if
> you included the sighash as part of the scriptpubkey explicitly, that
> would lose some of the indistinguishability of taproot addresses, and be
> more expensive than having the sighash be in witness data.

Nonexistence of sighash byte implies `SIGHASH_ALL`, and for offchain anyway the desired path is to end up with an n-of-n MuSig `SIGHASH_ALL` signed mutual close transaction.
Indeed we can even restrict keypath spends to not having a sighash byte and just implicitly requiring `SIGHASH_ALL` with no loss of privacy for offchain while attaining safety against `SIGHASH_NOINPUT` for MuSig and VSSS multisignature adresses.


> So I think
> that means sighashes would still be included in key path signatures,
> which would make the behaviour a little confusingly different between
> signing for key path and script path spends.
>
> > This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as they are allowed only if the output specifically says they are allowed.
>
> I don't think the problems with NONE and SINGLE are any worse than using
> SIGHASH_ALL to pay to "1*G" -- someone may steal the money you send,
> but that's as far as it goes. NOINPUT/ANYPREVOUT is worse in that if
> you use it, someone may steal funds from other UTXOs too -- similar
> to nonce-reuse. So I think having to commit to enabling NOINPUT for an
> address may make sense; but I don't really see the need for doing the
> same for other sighashes generally.

As the existing sighashes are not particularly used anyway, additional restrictions on them are relatively immaterial.

>
> FWIW, one way of looking at a transaction spending UTXO "U" to address
> "A" is something like:
>
> -   "script" lets you enforce conditions on the transaction when you
>     create "A" [0]
>
> -   "sighash" lets you enforce conditions on the transaction when
>     you sign the transaction
>
> -   nlocktime, nsequence, taproot annex are ways you express conditions
>     on the transaction
>
>     In that view, "sighash" is actually an extremely simple scripting
>     language itself (with a total of six possible scripts).
>
>     That doesn't seem like a bad design to me, fwiw.


Only one of the scripts is widely used, another has an edge use it sucks at (assurance contracts).

Does not seem to be good design, rather legacy cruft.

Regards,
ZmnSCPxj

>
>     Cheers,
>     aj
>
>     [0] "graftroot" lets you update those conditions for address "A" after
>     the fact
>



From decker.christian at gmail.com  Thu Oct  3 09:42:00 2019
From: decker.christian at gmail.com (Christian Decker)
Date: Thu, 03 Oct 2019 11:42:00 +0200
Subject: [bitcoin-dev] Continuing the discussion about noinput /
	anyprevout
In-Reply-To: <4zx7e_vHQr58myY5w_-bAjTk04LTGNknZudZs4wbUiOIoVKhL69M7k1eELCSuoBND2CtVXXzDFBHW4351cttIh80eP8jiaoO8cmbSefZmj4=@protonmail.com>
References: <87wodp7w9f.fsf@gmail.com>
	<-5H29F71ID9UFqUGMaegQxPjKZSrF1mvdgfaaYtt_lwI7l1OTmN_8OgcooyoMt2_XuyZ5aDljL6gEup9C7skF8iuP_NbMW_81h0tJIGbJno=@protonmail.com>
	<87tv8s7djq.fsf@gmail.com>
	<4zx7e_vHQr58myY5w_-bAjTk04LTGNknZudZs4wbUiOIoVKhL69M7k1eELCSuoBND2CtVXXzDFBHW4351cttIh80eP8jiaoO8cmbSefZmj4=@protonmail.com>
Message-ID: <87o8yy6u8n.fsf@gmail.com>

ZmnSCPxj <ZmnSCPxj at protonmail.com> writes:
>> That is very much how I was planning to implement it anyway, using a
>> trigger transaction to separate timeout start and the actual
>> update/settlement pairs (cfr. eltoo paper Section 4.2). So for eltoo
>> there shouldn't be an issue here :-)
>
> My understanding is that a trigger transaction is not in fact
> necessary for Decker-Russell-Osuntokun: any update transaction could
> spend the funding transaction output directly, and thereby start the
> relative timelock.  At least, if we could arrange the funding
> transaction output to be spendable directly using `SIGHASH_NOINPUT` or
> variants thereof.

This is the case in which we don't have a pre-signed settlement
transaction (or in this case refund transaction) that uses a relative
timelock. In order to have a refund transaction we would need to have
the first update and settlement pair be signed before funding (otherwise
the funder isn't sure she is getting her funds back). Since that first
update and settlement pair do not need to be rebound (they can only ever
be bound to the funding transaction) they can be signed without
noinput/anyprevoutanyscript. If we use output tagging we would mandate
that this first update must be published, so that the funding output is
indistinguishable from a normal output, and the first update switches
from non-noinput/anyprevoutanyscript to enabling it. Collaborative
closes are still indistinguishable, unilateral closes require the
switch, but then would be identifiable anyway.

The one downside I can see is that we now mandate that unilateral closes
also publish the first update, which is a bit annoying.

>> While I do agree that we should keep outputs as unidentifiable as
>> possible, I am starting to question whether that is possible for
>> off-chain payment networks since we are gossiping about the existence of
>> channels and binding them to outpoints to prove their existence anyway.
>
> * Lightning supports unpublished channels, so we do not gossip some outpoints even though they are in fact channels underneath.
>   * I confess the existence of unpublished channels in the spec fails to summon any reaction other than incredulity from me, but they exist nonetheless, my incredulity notwithstanding.

That is true, we do however selectively tell others about the channel's
existence (in invoices, our peers, ...) so I wouldn't consider that to
be the most secret information :-)

As for why they exist: nodes need to have the option of not announcing
their channels to reduce the noise in the network with channels that are
unlikely to be useable in order to forward payments. If every node were
to announce their channels we'd have a much larger routing table, mostly
consisting of unusable channels going to leafs in the
network. Furthermore, the sheer threat that there might be unannounced
channels adds uncertainty for attackers trying to profile nodes: "I see
only my channel with my peer, but he might have unannounced channels, so
I can't really tell whether the payment I forwarded to it is destined
for it or one of its unannounced peers".

> * Historical channels that have been cooperatively closed are no longer normally gossiped, so the fact that they used to be channels is no longer widely broadcast, and may eventually be forgotten by most or all of the network.
>   * This means anyone who wants to record the historical use of Lightning will have to retain the information themselves, rather than delegating it to fullnodes everywhere.

Good point, it requires storing the ephemeral data from gossip, that's
not all that hard, but I agree that it puts up a small barrier for
newcomers.

From decker.christian at gmail.com  Thu Oct  3 09:57:05 2019
From: decker.christian at gmail.com (Christian Decker)
Date: Thu, 03 Oct 2019 11:57:05 +0200
Subject: [bitcoin-dev] [Lightning-dev] Continuing the discussion about
	noinput / anyprevout
In-Reply-To: <CAFQwNuxdfMNBGyM5Y4nMb46GNigxFTFCv3X09jZd4fjNPckw4Q@mail.gmail.com>
References: <87wodp7w9f.fsf@gmail.com>
	<CAFQwNuxdfMNBGyM5Y4nMb46GNigxFTFCv3X09jZd4fjNPckw4Q@mail.gmail.com>
Message-ID: <87lfu26tji.fsf@gmail.com>

Chris Stewart <chris at suredbits.com> writes:

> I do have some concerns about SIGHASH_NOINPUT, mainly that it does
> introduce another footgun into the bitcoin protocol with address reuse.
> It's common practice for bitcoin businesses to re-use addresses. Many
> exchanges [1] reuse addresses for cold storage with very large sums of
> money that is stored in these addreses.
>
> It is my understanding with this part of BIP118
>
>>Using NOINPUT the input containing the signature no longer references a
> specific output. Any participant can take a transaction and rewrite it by
> changing the hash reference to the previous output, without invalidating
> the signatures. This allows transactions to be bound to any output that
> matches the value committed to in the witness and whose witnessProgram,
> combined with the spending transaction's witness returns true.
>
> if an exchange were to once produce a digital signature from that cold
> storage address with a SIGHASH_NOINPUT signature, that signature can be
> replayed again and again on the blockchain until their wallet is drained.
> This might be able to mitigated since the signatures commit to outputs,
> which may be small in value for the transaction that SIGHASH_NOINPUT was
> used. This means that an exchange could move coins from the address with a
> larger transaction that spends money to a new output (and presumably pays a
> higher fee than the smaller transactions).

Thanks for sharing your concerns Chris, I do agree that noinput and
friends are a very sharp knife that needs to be treated carefully, but
ultimately it's exactly its sharpness that makes it useful :-)

> ### Why does this matter?
>
> It seems that SIGHASH_NOINPUT will be an extremely useful tool for offchain
> protocols like Lightning. This gives us the building blocks for enforcing
> specific offchain states to end up onchain [2].
>
> Since this tool is useful, we can presume that it will be integrated into
> the signing path of large economic entities in bitcoin -- namely exchanges.
> Many exchanges have specific signing procedures for transactions that are
> leaving an exchange that is custom software. Now -- presuming wide adoption
> of off chain protocols -- they will need to have a _second unique signing
> path that uses SIGHASH_NOINPUT_.
>
> It is imperative that this second signing path -- which uses
> SIGHASH_NOINPUT -- does NOT get mixed up with the first signing path that
> controls an exchanges onchain funds. If this were to happen, fund lost
> could occur if the exchange is reusing address, which seems to be common
> practice.

Totally agreed, and as you point out, BIP118 is careful to mandate
separate private keys be used for off-chain contracts and that the
off-chain contract never be mixed with the remainder of your funds. The
way eltoo uses noinput we selectively open us up to replay attacks
(because that's what the update mechanism is after all) by controlling
the way the transactions can be replayed very carefully, and any other
use of noinput would need to make sure to have the same guarantees.
However, once we have separated the two domains, we can simply use a
separate (hardened) derivation path from a seed key, and never mix them
afterwards. We never exchange any private keys, so even leaking info
across derived keys is not an issue here.

> This is stated here in BIP118:
>
>>This also means that particular care has to be taken in order to avoid
> unintentionally enabling this rebinding mechanism. NOINPUT MUST NOT be
> used, unless it is explicitly needed for the application, e.g., it MUST NOT
> be a default signing flag in a wallet implementation. Rebinding is only
> possible when the outputs the transaction may bind to all use the same
> public keys. Any public key that is used in a NOINPUT signature MUST only
> be used for outputs that the input may bind to, and they MUST NOT be used
> for transactions that the input may not bind to. For example an application
> SHOULD generate a new key-pair for the application instance using NOINPUT
> signatures and MUST NOT reuse them afterwards.
>
> This means we need to encourage onchain hot wallet signing procedures to be
> kept separate from offchain hot wallet signing procedures, which introduces
> more complexity for key management (two keychains).

This is already the case: off-chain systems always require access to the
signing key in real-time in order to be useful. If any state change is
performed in a channel, even just adjusting fees or receiving a payment,
requires the signature from the key associated with the channel. With
high security on-chain systems on the other hand you should never have a
hot key that automatically signs off on transfers without human
intervention. So I find it unlikely that mandating the on-chain keys to
be kept separate from off-chain keys is any harder than what should be
done with the current systems.

> One (of the few) upsides of the current Lightning penalty mechanism is that
> fund loss can be contained to balance of the channel. You cannot do
> something in the current protocol that will effect your funds outside of
> that channel. With SIGHASH_NOINPUT, that property changes.

Good point, but if the key hygiene is maintained as detailed in BIP118,
i.e., off-chain keys must be kept separate from on-chain keys, and that
each off-chain contract instance uses a separate set of keys, that
property is maintained.

Regards,
Christian

From decker.christian at gmail.com  Thu Oct  3 10:01:58 2019
From: decker.christian at gmail.com (Christian Decker)
Date: Thu, 03 Oct 2019 12:01:58 +0200
Subject: [bitcoin-dev] [Lightning-dev] Continuing the discussion about
	noinput / anyprevout
In-Reply-To: <bMt69zMSAH_2zekGjg56k6MWFMwWkjKMdUjqHQ5eN7c5ONixWZ0s2wW4HmILeVjImt6Z2K5fPa6GKGLP_HWThCzFIIu53wvEKTDrGg-YpOQ=@protonmail.com>
References: <87wodp7w9f.fsf@gmail.com>
	<CAFQwNuxdfMNBGyM5Y4nMb46GNigxFTFCv3X09jZd4fjNPckw4Q@mail.gmail.com>
	<bMt69zMSAH_2zekGjg56k6MWFMwWkjKMdUjqHQ5eN7c5ONixWZ0s2wW4HmILeVjImt6Z2K5fPa6GKGLP_HWThCzFIIu53wvEKTDrGg-YpOQ=@protonmail.com>
Message-ID: <87imp66tbd.fsf@gmail.com>

ZmnSCPxj via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> writes:

> Good morning lists,
>
> Let me summarize concerns brought up:
>
> * Chris concern, is that an ordinary UTXO that is not allocated for `SIGHASH_NOINPUT` use, is inadvertently spent using `SIGHASH_NOINPUT`.
> * My concern, is that unless a UTXO allocated for `SIGHASH_NOINPUT` use, is *indeed* used with SIGHASH_NOINPUT`, it should look exactly the same as any other SegWit v1 output.
>
> I propose the below instead:
>
> * Do ***NOT*** allocate SegWit v16 for `SIGHASH_NOINPUT`.
> * Instead, allocate SegWit v1 Tapscript v16 for `SIGHASH_NOINPUT`.
>
> Then, on usage:
>
> * Exchange hoards can be protected by simple MuSig bip-schnorr SegWit v1 outputs, or a NUMS Taproot internal point with a MAST branch Tapscript v0 `OP_CHECKSIG_ADD` sequence.
> * Decker-Russell-Osuntokun constructions are backed by a n-of-n MuSig Taproot internal point, with a MAST branch containing a Tapscript v16 with `OP_1 OP_CHECKSIG`.
>
> This solves both concerns:
>
> * Ordinary UTXOs not allocated for `SIGHASH_NOINPUT` use simply do not commit to any Taproot that has a Tapscript v16 branch, and thus `SIGHASH_NOINPUT` is unuseable to claim it.
> * If a UTXO used for an offchain protocol ends up in a cooperative-resolution state, nobody has to know that a Tapscript v16 branch existed that could have used `SIGHASH_NOINPUT`.
>
> Again, my objection to output tagging is that it is **publicly visible** as soon as the funding transaction is confirmed onchain that this is a special output used for a Decker-Russell-Osuntokun construction, greatly damaging privacy.
> But if this fact is kept secret *unless* the very specific case of unilateral uncooperative enforcement, then it is quite fine with me.
>
> Would this alternate proposal hold better muster?

Intriguing idea, this would be an invisible tagging, since the opt-in to
noinput and friends is hidden inside the committed script, which only
gets revealed whenever we actually need it.

For eltoo this would mean that the funding output would be invisibly
tagged, and the cooperative close would use the taproot pubkey, while
the uncooperative close, which would require noinput opt-in, reveals the
script, proving prior opt-in, and provides a matching signature.

If I'm not mistaken this would require AJ's alternative pubkey encoding
(0x01 or 0x00 prefixed pubkey) to make the opt-in visible, correct?

From decker.christian at gmail.com  Thu Oct  3 10:30:03 2019
From: decker.christian at gmail.com (Christian Decker)
Date: Thu, 03 Oct 2019 12:30:03 +0200
Subject: [bitcoin-dev] [Lightning-dev] Continuing the discussion about
	noinput / anyprevout
In-Reply-To: <CAFQwNuxRcwOh9AUWXMonDM=7AgiHMym-TtuaHS_-6RFKcnNgZQ@mail.gmail.com>
References: <87wodp7w9f.fsf@gmail.com>
	<CACJVCgJ9PL-2jTS71--tXsa=QkK+f5_ciYLwv468WUno=XXAig@mail.gmail.com>
	<CAFQwNuxRcwOh9AUWXMonDM=7AgiHMym-TtuaHS_-6RFKcnNgZQ@mail.gmail.com>
Message-ID: <87eezu6s0k.fsf@gmail.com>

Chris Stewart <chris at suredbits.com> writes:

>> I don't find too compelling the potential problem of a 'bad wallet
> designer', whether lazy or dogmatic, misusing noinput. I think there are
> simpler ways to cut corners and there will always be plenty of good wallet
> options people can choose.
>
> In my original post, the business that I am talking about don't use "off
> the shelf" wallet options. It isn't a "let's switch from wallet A to wallet
> B" kind of situation. Usually this involves design from ground up with
> security considerations that businesses of scale need to consider (signing
> procedures and key handling being the most important!).

In this case I'd hope that the custom wallet designers/developers are
well-versed in the issues they might encounter when implementing their
wallet. This is especially true if they decide to opt into using some
lesser known sighash flags, such as noinput, that come with huge warning
signs (I forgot to mention that renaming noinput to noinput_dangerous is
also still on the table).

>>Because scripts signed with no_input signatures are only really exchanged
> and used for off-chain negotiations, very few should ever appear on chain.
> Those that do should represent non-cooperative situations that involve
> signing parties who know not to reuse or share scripts with these public
> keys again. No third party has any reason to spend value to a
> multisignature script they don't control, whether or not a no_input
> signature exists for it.
>
> Just because some one is your friend today, doesn't mean they aren't
> necessarily your adversary tomorrow. I don't think a signature being
> onchain really matters, as you have to give it to your counterparty
> regardless. How do you know your counterparty won't replay that
> SIGHASH_NOINPUT signature later? Offchain protocols shouldn't rely on
> "good-will" for their counter parties for security.
>
>>As I mentioned before, I don't think the lazy wallet designer advantage is
> enough to justify the downsides of chaperone signatures. One additional
> downside is the additional code complexity required to flag whether or not
> a chaperone output is included. By comparison, the code changes for
> creating a no_input digest that skips the prevout and prevscript parts of a
> tx is much less intrusive and easier to maintain.
>
>>I want to second this. The most expensive part of wallet design is
> engineering time. Writing code that uses a new sighash or a custom
> script with a OP_CODE is a very large barrier to use. How many wallets
> support multisig or RBF? How much BTC has been stolen over the entire
> history of Bitcoin because of sighash SIGHASH_NONE or SIGHASH_SINGLE
> vs ECDSA nonce reuse
>
> I actually think lazy wallet designer is a really compelling reason to fix
> footguns in the bitcoin protocol. Mt Gox is allegedly a product of lazy
> wallet design. Now we have non-malleable transactions in the form of segwit
> (yay!) that prevent this exploit. We can wish that the Mt Gox wallet
> designers were more aware of bitcoin protocol vulnerabilities, but at the
> end of the day the best thing to do was offering an alternative that
> circumvents the vulnerability all together.

It's worth pointing out that the transaction malleability issue and the
introduction of a new sighash flag are fundamentally different: a wallet
developer has to take active measures to guard against transaction
malleability since it was present even for the most minimal
implementation, whereas with sighash flags the developers have to
actively add support for it. Where transaction malleability you just had
to know that it might be an issue, with noinput you actively have to do
work yo expose yourself to it.

I'd argue that you have to have a very compelling reason to opt into
supporting noinput, and that's usually because you want to support a
more complex protocol such as an off-chain contract anyway, at which
point I'd hope you know about the tradeoffs of various sighash flags :-)

> Ethan made a great point about SIGHASH_NONE or SIGHASH_SINGLE -- which have
> virtually no use AFAIK -- vs the ECDSA nonce reuse which is used in nearly
> every transaction. The feature -- ECDSA in this case -- was managed to be
> done wrong by wallet developers causing fund loss. Unfortunately we can't
> protect against this type of bug in the protocol.
>
> If things aren't used -- such as SIGHASH_NONE or SIGHASH_SINGLE -- it
> doesn't matter if they are secure or insecure. I'm hopefully that offchain
> protocols will achieve wide adoption, and I would hate to see money lost
> because of this. Even though they aren't used, in my OP I do advocate for
> fixing these.

I do share the feeling that we better make a commonly used sighash flag
as useable and safe as possible, but it's rather unrealistic to have a
developer that is able to implement a complex off-chain system, but
fails to understand the importance of using the correct sighash flags in
their wallet. That being said, I think this concern would be addressed
by any form of explicit opt-in on the output side (whether hidden or
not), right?


Cheers,
Christian

From decker.christian at gmail.com  Thu Oct  3 11:08:29 2019
From: decker.christian at gmail.com (Christian Decker)
Date: Thu, 03 Oct 2019 13:08:29 +0200
Subject: [bitcoin-dev] Continuing the discussion about noinput /
	anyprevout
In-Reply-To: <20191001155929.e2yznsetqesx2jxo@erisian.com.au>
References: <87wodp7w9f.fsf@gmail.com>
	<20191001155929.e2yznsetqesx2jxo@erisian.com.au>
Message-ID: <877e5m6q8i.fsf@gmail.com>

Anthony Towns <aj at erisian.com.au> writes:

> On Mon, Sep 30, 2019 at 03:23:56PM +0200, Christian Decker via bitcoin-dev wrote:
>> With the recently renewed interest in eltoo, a proof-of-concept implementation
>> [1], and the discussions regarding clean abstractions for off-chain protocols
>> [2,3], I thought it might be time to revisit the `sighash_noinput` proposal
>> (BIP-118 [4]), and AJ's `bip-anyprevout` proposal [5].
>
> Hey Christian, thanks for the write up!
>
>> ## Open questions
>> The questions that remain to be addressed are the following:
>> 1.  General agreement on the usefulness of noinput / anyprevoutanyscript /
>>     anyprevout[?]
>> 2.  Is there strong support or opposition to the chaperone signatures[?]
>> 3.  The same for output tagging / explicit opt-in[?]
>> 4.  Shall we merge BIP-118 and bip-anyprevout. This would likely reduce the
>>     confusion and make for simpler discussions in the end.
>
> I think there's an important open question you missed from this list:
> (1.5) do we really understand what the dangers of noinput/anyprevout-style
> constructions actually are?
>
> My impression on the first 3.5 q's is: (1) yes, (1.5) not really,
> (2) weak opposition for requiring chaperone sigs, (3) mixed (weak)
> support/opposition for output tagging.
>
> My thinking at the moment (subject to change!) is:
>
>  * anyprevout signatures make the address you're signing for less safe,
>    which may cause you to lose funds when additional coins are sent to
>    the same address; this can be avoided if handled with care (or if you
>    don't care about losing funds in the event of address reuse)
>
>  * being able to guarantee that an address can never be signed for with
>    an anyprevout signature is therefore valuable; so having it be opt-in
>    at the tapscript level, rather than a sighash flag available for
>    key-path spends is valuable (I call this "opt-in", but it's hidden
>    until use via taproot rather than "explicit" as output tagging
>    would be)
>
>  * receiving funds spent via an anyprevout signature does not involve any
>    qualitatively new double-spending/malleability risks.
>    
>    (eltoo is unavoidably malleable if there are multiple update
>    transactions (and chaperone signatures aren't used or are used with
>    well known keys), but while it is better to avoid this where possible,
>    it's something that's already easily dealt with simply by waiting
>    for confirmations, and whether a transaction is malleable is always
>    under the control of the sender not the receiver)
>
>  * as such, output tagging is also unnecessary, and there is also no
>    need for users to mark anyprevout spends as "tainted" in order to
>    wait for more confirmations than normal before considering those funds
>    "safe"

Excellent points, I had missed the hidden nature of the opt-in via
pubkey prefix while reading your proposal. I'm starting to like that
option more and more. In that case we'd only ever be revealing that we
opted into anyprevout when we're revealing the entire script anyway, at
which point all fungibility concerns go out the window anyway.

Would this scheme be extendable to opt into all sighash flags the
outpoint would like to allow (e.g., adding opt-in for sighash_none and
sighash_anyonecanpay as well)? That way the pubkey prefix could act as a
mask for the sighash flags and fail verification if they don't match.

> I think it might be good to have a public testnet (based on Richard Myers
> et al's signet2 work?) where we have some fake exchanges/merchants/etc
> and scheduled reorgs, and demo every weird noinput/anyprevout case anyone
> can think of, and just work out if we need any extra code/tagging/whatever
> to keep those fake exchanges/merchants from losing money (and write up
> the weird cases we've found in a wiki or a paper so people can easily
> tell if we missed something obvious).

That'd be great, however even that will not ensure that every possible
corner case is handled and from experience it seems that people are
unwilling to invest a lot of time testing on a network unless their
money is on the line. That's not to say that we shouldn't try, we
absolutely should, I'm just not sure it alone is enough to dispell all
remaining doubts :-)

Cheers,
Christian

From eth3rs at gmail.com  Thu Oct  3 15:05:52 2019
From: eth3rs at gmail.com (Ethan Heilman)
Date: Thu, 3 Oct 2019 11:05:52 -0400
Subject: [bitcoin-dev] OP_CAT was Re: Continuing the discussion about
 noinput / anyprevout
In-Reply-To: <CR-etCjXB-JWkvecjDog4Pkq1SuLUgndtSrZo-V4f4EGcNXzNCeAHRvCZGrxDWw7aHVdDY0pAF92jNLb_Hct0bMb3ew6JEpB9AfIm1tSGaQ=@protonmail.com>
References: <87wodp7w9f.fsf@gmail.com>
	<20191001155929.e2yznsetqesx2jxo@erisian.com.au>
	<CR-etCjXB-JWkvecjDog4Pkq1SuLUgndtSrZo-V4f4EGcNXzNCeAHRvCZGrxDWw7aHVdDY0pAF92jNLb_Hct0bMb3ew6JEpB9AfIm1tSGaQ=@protonmail.com>
Message-ID: <CAEM=y+XbP3Dn7X8rHu7h0vbX6DkKA0vFK5nQqzcJ_V+D4EVMmw@mail.gmail.com>

To avoid derailing the NO_INPUT conversation, I have changed the
subject to OP_CAT.

Responding to:
"""
* `SIGHASH` flags attached to signatures are a misdesign, sadly
retained from the original BitCoin 0.1.0 Alpha for Windows design, on
par with:
[..]
* `OP_CAT` and `OP_MULT` and `OP_ADD` and friends
[..]
"""

OP_CAT is an extremely valuable op code. I understand why it was
removed as the situation at the time with scripts was dire. However
most of the protocols I've wanted to build on Bitcoin run into the
limitation that stack values can not be concatenated. For instance
TumbleBit would have far smaller transaction sizes if OP_CAT was
supported in Bitcoin. If it happens to me as a researcher it is
probably holding other people back as well. If I could wave a magic
wand and turn on one of the disabled op codes it would be OP_CAT.  Of
course with the change that size of each concatenated value must be 64
Bytes or less.


On Tue, Oct 1, 2019 at 10:04 PM ZmnSCPxj via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
>
> Good morning lists,
>
> Let me propose the below radical idea:
>
> * `SIGHASH` flags attached to signatures are a misdesign, sadly retained from the original BitCoin 0.1.0 Alpha for Windows design, on par with:
>   * 1 RETURN
>   * higher-`nSequence` replacement
>   * DER-encoded pubkeys
>   * unrestricted `scriptPubKey`
>   * Payee-security-paid-by-payer (i.e. lack of P2SH)
>   * `OP_CAT` and `OP_MULT` and `OP_ADD` and friends
>   * transaction malleability
>   * probably many more
>
> So let me propose the more radical excision, starting with SegWit v1:
>
> * Remove `SIGHASH` from signatures.
> * Put `SIGHASH` on public keys.
>
> Public keys are now encoded as either 33-bytes (implicit `SIGHASH_ALL`) or 34-bytes (`SIGHASH` byte, followed by pubkey type, followed by pubkey coordinate).
> `OP_CHECKSIG` and friends then look at the *public key* to determine sighash algorithm rather than the signature.
>
> As we expect public keys to be indirectly committed to on every output `scriptPubKey`, this is automatically output tagging to allow particular `SIGHASH`.
> However, we can then utilize the many many ways to hide public keys away until they are needed, exemplified in MAST-inside-Taproot.
>
> I propose also the addition of the opcode:
>
>     <sighash> <pubkey> OP_SETPUBKEYSIGHASH
>
> * `sighash` must be one byte.
> * `pubkey` may be the special byte `0x1`, meaning "just use the Taproot internal pubkey".
> * `pubkey` may be 33-byte public key, in which case the `sighash` byte is just prepended to it.
> * `pubkey` may be 34-byte public key with sighash, in which case the first byte is replaced with `sighash` byte.
> * If `sighash` is `0x00` then the result is a 33-byte public key (the sighash byte is removed) i.e. `SIGHASH_ALL` implicit.
>
> This retains the old feature where the sighash is selected at time-of-spending rather than time-of-payment.
> This is done by using the script:
>
>     <pubkey> OP_SETPUBKEYSIGHASH OP_CHECKSIG
>
> Then the sighash can be put in the witness stack after the signature, letting the `SIGHASH` flag be selected at time-of-signing, but only if the SCRIPT specifically is formed to do so.
> This is malleability-safe as the signature still commits to the `SIGHASH` it was created for.
>
> However, by default, public keys will not have an attached `SIGHASH` byte, implying `SIGHASH_ALL` (and disallowing-by-default non-`SIGHASH_ALL`).
>
> This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as they are allowed only if the output specifically says they are allowed.
>
> Would this not be a superior solution?
>
> Regards,
> ZmnSCPxj
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From jlrubin at mit.edu  Thu Oct  3 23:22:47 2019
From: jlrubin at mit.edu (Jeremy)
Date: Thu, 3 Oct 2019 16:22:47 -0700
Subject: [bitcoin-dev] OP_SECURETHEBAG (supersedes OP_CHECKOUTPUTSVERIFY)
In-Reply-To: <20190708152624.39c33985@simplexum.com>
References: <CAD5xwhjSj82YYuQHHbwgSLvUNV2RDY0b=yMYeLj-p6j7PpS9-Q@mail.gmail.com>
	<20190605093039.xfo7lcylqkhsfncv@erisian.com.au>
	<im0q8670MxshmvMLmoJU0dv4rFhwWZNvQeQYv7i4fBWJOx0ghAdH8fYuQSqNxO2z8uxXGV-kurinUDfl0FsLWD0knw_U_h3zVZ0xy7vmn8o=@protonmail.com>
	<CAMZUoK=ZB06jwAbuX2D=aN8ztAqr_jSgEXS1z1ABjQYVawKCBQ@mail.gmail.com>
	<20190620220552.metrqaul3iporwma@erisian.com.au>
	<CAD5xwhgEQKdTCSLQn4-X_atT5_aE-1hEKk0xd1wm1m0qotwYXQ@mail.gmail.com>
	<20190708152624.39c33985@simplexum.com>
Message-ID: <CAD5xwhiUe1H+4mHzHyFH9dOr68nTSP4iVv4gHOf5h_+1FjM-zQ@mail.gmail.com>

I've updated the BIP to no longer be based on Taproot, and instead based on
a OP_NOP upgrade. The example implementation and tests have also been
updated.

BIP:
https://github.com/JeremyRubin/bips/blob/op-secure-the-bag/bip-secure-the-bag.mediawiki
Implementation:
https://github.com/bitcoin/bitcoin/compare/master...JeremyRubin:securethebag_master

The BIP defines OP_NOP4 with the same semantics as previously presented.
This enables OP_SECURETHEBAG for segwit and bare script, but not p2sh
(because of hash cycle, it's impossible to put the redeemscript on the
scriptSig without changing the bag hash). The implementation also makes a
bare OP_SECURETHEBAG script standard as that is a common use case.

To address Russel's feedback, once Tapscript is fully prepared (with more
thorough script parsing improvements), multibyte opcodes can be more
cleanly specified.

Best,

Jeremy

n.b. the prior BIP version remains at
https://github.com/JeremyRubin/bips/blob/op-secure-the-bag-taproot/bip-secure-the-bag.mediawiki
--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>


On Mon, Jul 8, 2019 at 3:25 AM Dmitry Petukhov <dp at simplexum.com> wrote:

> If you make ANYPREVOUTANYSCRIPT signature not the only signature that
> controls this UTXO, but use it solely for restricting the spending
> conditions such as the set of outputs, and require another signature
> that would commit to the whole transaction, you can eliminate
> malleability, for the price of additional signature, of course.
>
> <control-sig> <control-P> CHECKSIG <P> CHECKSIG
>
> (CHECKMULTISIG/CHECKSIGADD might be used instead)
>
> where control-P can even be a pubkey of a key that is publicly known,
> and the whole purpose of control-sig would be to restrict the outputs
> (control-sig would be created with flags meaning ANYPREVOUTANYSCRIPT).
> Because control-sig does not depend on the script and on the current
> input, there should be no circular dependency, and it can be part of
> the redeem script.
>
> P would be the pubkey of the actual key that is needed to spend this
> UTXO, and the signature of P can commit to all the inputs and outputs,
> preventing malleability.
>
> I would like to add that it may make sense to just have 2 additional
> flags for sighash: NOPREVOUT and NOSCRIPT.
>
> NOPREVOUT would mean that previous output is not committed to, and when
> combined with ANYONECANPAY, this will mean ANYPREVOUT/NOINPUT:
> ANYONECANPAY means exclude all inputs except the current, and NOPREVOUT
> means exclude the current input. Thus NOPREVOUT|ANYONECANPAY = NOINPUT
>
> With NOPREVOUT|ANYONECANPAY|NOSCRIPT you would have ANYPREVOUTANYSCRIPT
>
> with NOPREVOUT|NOSCRIPT you can commit to "all the inputs beside the
> current one", which would allow to create a spending restriction like
> "this UTXO, and also one (or more) other UTXO", which might be useful
> to retroactively revoke or transfer the rights to spend certain UTXO
> without actually moving it:
>
> think 'vault' UTXO that is controlled by Alice, but requires additional
> 'control' UTXO to spend. Alice have keys for both 'vault' UTXO, and
> 'control' UTXO, but Bob have only key for 'control' UTXO.
>
> If Bob learnsthat Alice's vault UTXO key is at risk of compromize,
> he spends the control UTXO, and then Alice's ability to spend vault
> UTXO vanishes.
>
> You can use this mechanism to transfer this right to spend if you
> prepare a number of taproot branches with different pairs of (vault,
> control) keys and a chain of transactions that each spend the previous
> control UTXO such that the newly created UTXO becomes controlled by the
> control key of the next pair, together with vault key in that pair.
>
> ? Sat, 22 Jun 2019 23:43:22 -0700
> Jeremy via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
>
> > This is insufficient: sequences must be committed to because they
> > affect TXID. As with scriptsigs (witness data fine to ignore). NUM_IN
> > too.
> >
> > Any malleability makes this much less useful.
> > --
> > @JeremyRubin <https://twitter.com/JeremyRubin>
> > <https://twitter.com/JeremyRubin>
> >
> >
> > On Fri, Jun 21, 2019 at 10:31 AM Anthony Towns via bitcoin-dev <
> > bitcoin-dev at lists.linuxfoundation.org> wrote:
> >
> > > On Tue, Jun 18, 2019 at 04:57:34PM -0400, Russell O'Connor wrote:
> > > > So with regards to OP_SECURETHEBAG, I am also "not really seeing
> > > > any
> > > reason to
> > > > complicate the spec to ensure the digest is precommitted as part
> > > > of the opcode."
> > >
> > > Also, I think you can simulate OP_SECURETHEBAG with an ANYPREVOUT
> > > (NOINPUT) sighash (Johnson Lau's mentioned this before, but not
> > > sure if it's been spelled out anywhere); ie instead of constructing
> > >
> > >   X = Hash_BagHash( version, locktime, [outputs], [sequences],
> > > num_in )
> > >
> > > and having the script be "<X> OP_SECURETHEBAG" you calculate an
> > > ANYPREVOUT sighash for SIGHASH_ANYPREVOUTANYSCRIPT | SIGHASH_ALL:
> > >
> > >   Y = Hash_TapSighash( 0, 0xc1, version, locktime, [outputs], 0,
> > >                        amount, sequence)
> > >
> > > and calculate a signature sig = Schnorr(P,m) for some pubkey P, and
> > > make your script be "<sig> <P> CHECKSIG".
> > >
> > > That loses the ability to commit to the number of inputs or restrict
> > > the nsequence of other inputs, and requires a bigger script (sig
> > > and P are ~96 bytes instead of X's 32 bytes), but is otherwise
> > > pretty much the same as far as I can tell. Both scripts are
> > > automatically satisfied when revealed (with the correct set of
> > > outputs), and don't need any additional witness data.
> > >
> > > If you wanted to construct "X" via script instead of hardcoding a
> > > value because it got you generalised covenants or whatever; I think
> > > you could get the same effect with CAT,LEFT, and RIGHT: you'd
> > > construct Y in much the same way you construct X, but you'd then
> > > need to turn that into a signature. You could do so by using pubkey
> > > P=G and nonce R=G, which means you need to calculate
> > > s=1+hash(G,G,Y)*1 -- calculating the hash part is easy, multiplying
> > > it by 1 is easy, and to add 1 you can probably do something along
> > > the lines of:
> > >
> > >     OP_DUP 4 OP_RIGHT 1 OP_ADD OP_SWAP 28 OP_LEFT OP_SWAP OP_CAT
> > >
> > > (ie, take the last 4 bytes, increment it using 4-byte arithmetic,
> > > then cat the first 28 bytes and the result. There's overflow issues,
> > > but I think they can be worked around either by allowing you to
> > > choose different locktimes, or by more complicated script)
> > >
> > > Cheers,
> > > aj
> > >
> > > _______________________________________________
> > > bitcoin-dev mailing list
> > > bitcoin-dev at lists.linuxfoundation.org
> > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> > >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191003/216d81a2/attachment.html>

From ZmnSCPxj at protonmail.com  Thu Oct  3 23:42:25 2019
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Thu, 03 Oct 2019 23:42:25 +0000
Subject: [bitcoin-dev] [Lightning-dev] OP_CAT was Re: Continuing the
	discussion about noinput / anyprevout
In-Reply-To: <CAEM=y+XbP3Dn7X8rHu7h0vbX6DkKA0vFK5nQqzcJ_V+D4EVMmw@mail.gmail.com>
References: <87wodp7w9f.fsf@gmail.com>
	<20191001155929.e2yznsetqesx2jxo@erisian.com.au>
	<CR-etCjXB-JWkvecjDog4Pkq1SuLUgndtSrZo-V4f4EGcNXzNCeAHRvCZGrxDWw7aHVdDY0pAF92jNLb_Hct0bMb3ew6JEpB9AfIm1tSGaQ=@protonmail.com>
	<CAEM=y+XbP3Dn7X8rHu7h0vbX6DkKA0vFK5nQqzcJ_V+D4EVMmw@mail.gmail.com>
Message-ID: <C1OLL5FLxdOgfQ_A15mf88wIyztDapkyXJ2HZ0HxwmQADhRXGRe3le7Veso4tMIlbis6I0qiCd22xug5_GCKtgrjGnBtojWxOCMgn1UldkE=@protonmail.com>

Good morning Ethan,


> To avoid derailing the NO_INPUT conversation, I have changed the
> subject to OP_CAT.
>
> Responding to:
> """
>
> -   `SIGHASH` flags attached to signatures are a misdesign, sadly
>     retained from the original BitCoin 0.1.0 Alpha for Windows design, on
>     par with:
>     [..]
>
> -   `OP_CAT` and `OP_MULT` and `OP_ADD` and friends
>     [..]
>     """
>
>     OP_CAT is an extremely valuable op code. I understand why it was
>     removed as the situation at the time with scripts was dire. However
>     most of the protocols I've wanted to build on Bitcoin run into the
>     limitation that stack values can not be concatenated. For instance
>     TumbleBit would have far smaller transaction sizes if OP_CAT was
>     supported in Bitcoin. If it happens to me as a researcher it is
>     probably holding other people back as well. If I could wave a magic
>     wand and turn on one of the disabled op codes it would be OP_CAT. Of
>     course with the change that size of each concatenated value must be 64
>     Bytes or less.

Why 64 bytes in particular?

It seems obvious to me that this 64 bytes is most suited for building Merkle trees, being the size of two SHA256 hashes.

However we have had issues with the use of Merkle trees in Bitcoin blocks.
Specifically, it is difficult to determine if a hash on a Merkle node is the hash of a Merkle subnode, or a leaf transaction.
My understanding is that this is the reason for now requiring transactions to be at least 80 bytes.

The obvious fix would be to prepend the type of the hashed object, i.e. add at least one byte to determine this type.
Taproot for example uses tagged hash functions, with a different tag for leaves, and tagged hashes are just prepend-this-32-byte-constant-twice-before-you-SHA256.

This seems to indicate that to check merkle tree proofs, an `OP_CAT` with only 64 bytes max output size would not be sufficient.

Or we could implement tagged SHA256 as a new opcode...

Regards,
ZmnSCPxj


>
>     On Tue, Oct 1, 2019 at 10:04 PM ZmnSCPxj via bitcoin-dev
>     bitcoin-dev at lists.linuxfoundation.org wrote:
>
>
> > Good morning lists,
> > Let me propose the below radical idea:
> >
> > -   `SIGHASH` flags attached to signatures are a misdesign, sadly retained from the original BitCoin 0.1.0 Alpha for Windows design, on par with:
> >     -   1 RETURN
> >     -   higher-`nSequence` replacement
> >     -   DER-encoded pubkeys
> >     -   unrestricted `scriptPubKey`
> >     -   Payee-security-paid-by-payer (i.e. lack of P2SH)
> >     -   `OP_CAT` and `OP_MULT` and `OP_ADD` and friends
> >     -   transaction malleability
> >     -   probably many more
> >
> > So let me propose the more radical excision, starting with SegWit v1:
> >
> > -   Remove `SIGHASH` from signatures.
> > -   Put `SIGHASH` on public keys.
> >
> > Public keys are now encoded as either 33-bytes (implicit `SIGHASH_ALL`) or 34-bytes (`SIGHASH` byte, followed by pubkey type, followed by pubkey coordinate).
> > `OP_CHECKSIG` and friends then look at the public key to determine sighash algorithm rather than the signature.
> > As we expect public keys to be indirectly committed to on every output `scriptPubKey`, this is automatically output tagging to allow particular `SIGHASH`.
> > However, we can then utilize the many many ways to hide public keys away until they are needed, exemplified in MAST-inside-Taproot.
> > I propose also the addition of the opcode:
> >
> >     <sighash> <pubkey> OP_SETPUBKEYSIGHASH
> >
> >
> > -   `sighash` must be one byte.
> > -   `pubkey` may be the special byte `0x1`, meaning "just use the Taproot internal pubkey".
> > -   `pubkey` may be 33-byte public key, in which case the `sighash` byte is just prepended to it.
> > -   `pubkey` may be 34-byte public key with sighash, in which case the first byte is replaced with `sighash` byte.
> > -   If `sighash` is `0x00` then the result is a 33-byte public key (the sighash byte is removed) i.e. `SIGHASH_ALL` implicit.
> >
> > This retains the old feature where the sighash is selected at time-of-spending rather than time-of-payment.
> > This is done by using the script:
> >
> >     <pubkey> OP_SETPUBKEYSIGHASH OP_CHECKSIG
> >
> >
> > Then the sighash can be put in the witness stack after the signature, letting the `SIGHASH` flag be selected at time-of-signing, but only if the SCRIPT specifically is formed to do so.
> > This is malleability-safe as the signature still commits to the `SIGHASH` it was created for.
> > However, by default, public keys will not have an attached `SIGHASH` byte, implying `SIGHASH_ALL` (and disallowing-by-default non-`SIGHASH_ALL`).
> > This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as they are allowed only if the output specifically says they are allowed.
> > Would this not be a superior solution?
> > Regards,
> > ZmnSCPxj
> >
> > bitcoin-dev mailing list
> > bitcoin-dev at lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
> Lightning-dev mailing list
> Lightning-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev



From eth3rs at gmail.com  Fri Oct  4 00:48:17 2019
From: eth3rs at gmail.com (Ethan Heilman)
Date: Thu, 3 Oct 2019 20:48:17 -0400
Subject: [bitcoin-dev] [Lightning-dev] OP_CAT was Re: Continuing the
 discussion about noinput / anyprevout
In-Reply-To: <C1OLL5FLxdOgfQ_A15mf88wIyztDapkyXJ2HZ0HxwmQADhRXGRe3le7Veso4tMIlbis6I0qiCd22xug5_GCKtgrjGnBtojWxOCMgn1UldkE=@protonmail.com>
References: <87wodp7w9f.fsf@gmail.com>
	<20191001155929.e2yznsetqesx2jxo@erisian.com.au>
	<CR-etCjXB-JWkvecjDog4Pkq1SuLUgndtSrZo-V4f4EGcNXzNCeAHRvCZGrxDWw7aHVdDY0pAF92jNLb_Hct0bMb3ew6JEpB9AfIm1tSGaQ=@protonmail.com>
	<CAEM=y+XbP3Dn7X8rHu7h0vbX6DkKA0vFK5nQqzcJ_V+D4EVMmw@mail.gmail.com>
	<C1OLL5FLxdOgfQ_A15mf88wIyztDapkyXJ2HZ0HxwmQADhRXGRe3le7Veso4tMIlbis6I0qiCd22xug5_GCKtgrjGnBtojWxOCMgn1UldkE=@protonmail.com>
Message-ID: <CAEM=y+WCGSF_=WXpgXJUZCZcGUQhxzXF6Wv1_iX+VwEyYSWypg@mail.gmail.com>

I hope you are having an great afternoon ZmnSCPxj,

You make an excellent point!

I had thought about doing the following to tag nodes

|| means OP_CAT

`node = SHA256(type||SHA256(data))`
so a subnode would be
`subnode1 = SHA256(1||SHA256(subnode2||subnode3))`
and a leaf node would be
`leafnode = SHA256(0||SHA256(leafdata))`

Yet, I like your idea better. Increasing the size of the two inputs to
OP_CAT to be 260 Bytes each where 520 Bytes is the maximum allowable
size of object on the stack seems sensible and also doesn't special
case the logic of OP_CAT.

It would also increase performance. SHA256(tag||subnode2||subnode3)
requires 2 compression function calls whereas
SHA256(1||SHA256(subnode2||subnode3)) requires 2+1=3 compression
function calls (due to padding).

>Or we could implement tagged SHA256 as a new opcode...

I agree that tagged SHA256 as an op code that would certainty be
useful, but OP_CAT provides far more utility and is a simpler change.

Thanks,
Ethan

On Thu, Oct 3, 2019 at 7:42 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:
>
> Good morning Ethan,
>
>
> > To avoid derailing the NO_INPUT conversation, I have changed the
> > subject to OP_CAT.
> >
> > Responding to:
> > """
> >
> > -   `SIGHASH` flags attached to signatures are a misdesign, sadly
> >     retained from the original BitCoin 0.1.0 Alpha for Windows design, on
> >     par with:
> >     [..]
> >
> > -   `OP_CAT` and `OP_MULT` and `OP_ADD` and friends
> >     [..]
> >     """
> >
> >     OP_CAT is an extremely valuable op code. I understand why it was
> >     removed as the situation at the time with scripts was dire. However
> >     most of the protocols I've wanted to build on Bitcoin run into the
> >     limitation that stack values can not be concatenated. For instance
> >     TumbleBit would have far smaller transaction sizes if OP_CAT was
> >     supported in Bitcoin. If it happens to me as a researcher it is
> >     probably holding other people back as well. If I could wave a magic
> >     wand and turn on one of the disabled op codes it would be OP_CAT. Of
> >     course with the change that size of each concatenated value must be 64
> >     Bytes or less.
>
> Why 64 bytes in particular?
>
> It seems obvious to me that this 64 bytes is most suited for building Merkle trees, being the size of two SHA256 hashes.
>
> However we have had issues with the use of Merkle trees in Bitcoin blocks.
> Specifically, it is difficult to determine if a hash on a Merkle node is the hash of a Merkle subnode, or a leaf transaction.
> My understanding is that this is the reason for now requiring transactions to be at least 80 bytes.
>
> The obvious fix would be to prepend the type of the hashed object, i.e. add at least one byte to determine this type.
> Taproot for example uses tagged hash functions, with a different tag for leaves, and tagged hashes are just prepend-this-32-byte-constant-twice-before-you-SHA256.
>
> This seems to indicate that to check merkle tree proofs, an `OP_CAT` with only 64 bytes max output size would not be sufficient.
>
> Or we could implement tagged SHA256 as a new opcode...
>
> Regards,
> ZmnSCPxj
>
>
> >
> >     On Tue, Oct 1, 2019 at 10:04 PM ZmnSCPxj via bitcoin-dev
> >     bitcoin-dev at lists.linuxfoundation.org wrote:
> >
> >
> > > Good morning lists,
> > > Let me propose the below radical idea:
> > >
> > > -   `SIGHASH` flags attached to signatures are a misdesign, sadly retained from the original BitCoin 0.1.0 Alpha for Windows design, on par with:
> > >     -   1 RETURN
> > >     -   higher-`nSequence` replacement
> > >     -   DER-encoded pubkeys
> > >     -   unrestricted `scriptPubKey`
> > >     -   Payee-security-paid-by-payer (i.e. lack of P2SH)
> > >     -   `OP_CAT` and `OP_MULT` and `OP_ADD` and friends
> > >     -   transaction malleability
> > >     -   probably many more
> > >
> > > So let me propose the more radical excision, starting with SegWit v1:
> > >
> > > -   Remove `SIGHASH` from signatures.
> > > -   Put `SIGHASH` on public keys.
> > >
> > > Public keys are now encoded as either 33-bytes (implicit `SIGHASH_ALL`) or 34-bytes (`SIGHASH` byte, followed by pubkey type, followed by pubkey coordinate).
> > > `OP_CHECKSIG` and friends then look at the public key to determine sighash algorithm rather than the signature.
> > > As we expect public keys to be indirectly committed to on every output `scriptPubKey`, this is automatically output tagging to allow particular `SIGHASH`.
> > > However, we can then utilize the many many ways to hide public keys away until they are needed, exemplified in MAST-inside-Taproot.
> > > I propose also the addition of the opcode:
> > >
> > >     <sighash> <pubkey> OP_SETPUBKEYSIGHASH
> > >
> > >
> > > -   `sighash` must be one byte.
> > > -   `pubkey` may be the special byte `0x1`, meaning "just use the Taproot internal pubkey".
> > > -   `pubkey` may be 33-byte public key, in which case the `sighash` byte is just prepended to it.
> > > -   `pubkey` may be 34-byte public key with sighash, in which case the first byte is replaced with `sighash` byte.
> > > -   If `sighash` is `0x00` then the result is a 33-byte public key (the sighash byte is removed) i.e. `SIGHASH_ALL` implicit.
> > >
> > > This retains the old feature where the sighash is selected at time-of-spending rather than time-of-payment.
> > > This is done by using the script:
> > >
> > >     <pubkey> OP_SETPUBKEYSIGHASH OP_CHECKSIG
> > >
> > >
> > > Then the sighash can be put in the witness stack after the signature, letting the `SIGHASH` flag be selected at time-of-signing, but only if the SCRIPT specifically is formed to do so.
> > > This is malleability-safe as the signature still commits to the `SIGHASH` it was created for.
> > > However, by default, public keys will not have an attached `SIGHASH` byte, implying `SIGHASH_ALL` (and disallowing-by-default non-`SIGHASH_ALL`).
> > > This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as they are allowed only if the output specifically says they are allowed.
> > > Would this not be a superior solution?
> > > Regards,
> > > ZmnSCPxj
> > >
> > > bitcoin-dev mailing list
> > > bitcoin-dev at lists.linuxfoundation.org
> > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> >
> > Lightning-dev mailing list
> > Lightning-dev at lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev
>
>

From dscotese at litmocracy.com  Fri Oct  4 01:37:33 2019
From: dscotese at litmocracy.com (Dave Scotese)
Date: Thu, 3 Oct 2019 18:37:33 -0700
Subject: [bitcoin-dev] Smaller "Bitcoin address" accounts in the blockchain.
Message-ID: <CAGLBAheDMFkFtJG1gLf61ZZ9U3rasZ_T6f_sAHWWRiAcNWiO8A@mail.gmail.com>

Currently, bitcoin must be redeemed by providing input to a script which
results in the required output.  This causes the attached amount of bitcoin
to become available for use in the outputs of a transaction.  Is there any
work on creating a shorter "transaction" which, instead of creating a new
output, points to (creates a virtual copy of) an existing (unspent) output
with a larger amount attached to it?  This would invalidate the smaller,
earlier UTXO and replace it with the new one without requiring the earlier
one to be redeemed, and also without requiring the original script to be
duplicated.  It is a method for aggregating bitcoin to a UTXO which may
otherwise not be economically viable.

The idea is that there already exists a script that must be satisfied to
spend X1, and if the owner of X1 would like to have the same requirements
for spending X2, this would be a transaction that does that using fewer
data bytes.  Since the script already exists, the transaction can simply
point to it instead of duplicating it.

This would also enable the capacity of lightning channels to be increased
on the fly without closing the existing channel and re-opening a new one.
The LN layer would have to cope with the possibility that the "short
channel ID" could change.

Dave.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191003/4b3fcfb3/attachment.html>

From jaejoon at gmail.com  Thu Oct  3 20:14:22 2019
From: jaejoon at gmail.com (Jimmy Song)
Date: Thu, 3 Oct 2019 15:14:22 -0500
Subject: [bitcoin-dev] PSBT global key for network
Message-ID: <CAJR7vkqjWs2PqLhS2P4LGFc5A5Sh3ZFMOnVPVS9S_8C9HHD=GQ@mail.gmail.com>

Hey all,

I wanted to propose a new key in the global context for BIP174,
Partially-Signed Bitcoin Transactions.

= Rationale

Each signer should make sure that the inputs being referenced in the PSBT
exist (with the exception of a Proof-of-Reserves input). In order to do
this, it's critical to know which network the coins are on (mainnet or
testnet). This could potentially be extended to other networks should they
want to use something like PSBT, much in the same way that HD keys from
BIP0044 reserved 0' and 1' as coins for mainnet Bitcoin and testnet Bitcoin
respectively.

= Proposal

Add the key 0x03 for network in the global key-value store. Value is a
variable integer with 0x00 indicating Bitcoin mainnet and 0x01 indicating
Bitcoin testnet. Other coins that want to use the PSBT should use the coin
network number from SLIP-0044 with the high bit removed.

---------------------------

Best,

Jimmy
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191003/36eecd03/attachment.html>

From braydon at purse.io  Fri Oct  4 00:38:36 2019
From: braydon at purse.io (Braydon Fuller)
Date: Thu, 3 Oct 2019 17:38:36 -0700
Subject: [bitcoin-dev] Chain width expansion
Message-ID: <42cd5ffd-63e8-b738-c4ea-13d0699b1268@purse.io>

Hi everyone,

We would like to share a paper for broad discussion, it is titled
"Bitcoin Chain Width Expansion Denial-of-Service Attacks".

>From the abstract: The attacks leverage unprotected resources for a
denial-of-service by filling the disk and exhausting the CPU with
unnecessary header and block data. This forces the node to halt
operation. The attack difficulty ranges from difficult to easy. There
are currently limited guards for some of the attacks that require
checkpoints to be enabled. This paper describes a solution that does not
require enabling or maintaining checkpoints and provides improved security.

As the checkpoints in Bitcoin Core have not been maintained or updated
since mid 2014, this is especially relevant. Bitcoin Core implements
headers-first synchronization, since 2014, that provides the base for
the further improvements upon that design.

The paper is available at:
https://bcoin.io/papers/bitcoin-chain-expansion.pdf

The proposed solution has been implemented in Bcoin and is available at:
https://github.com/bcoin-org/bcoin/tree/chain-expansion

Best,
Braydon Fuller


From jlrubin at mit.edu  Fri Oct  4 05:02:14 2019
From: jlrubin at mit.edu (Jeremy)
Date: Thu, 3 Oct 2019 22:02:14 -0700
Subject: [bitcoin-dev] [Lightning-dev] OP_CAT was Re: Continuing the
 discussion about noinput / anyprevout
In-Reply-To: <CAEM=y+WCGSF_=WXpgXJUZCZcGUQhxzXF6Wv1_iX+VwEyYSWypg@mail.gmail.com>
References: <87wodp7w9f.fsf@gmail.com>
	<20191001155929.e2yznsetqesx2jxo@erisian.com.au>
	<CR-etCjXB-JWkvecjDog4Pkq1SuLUgndtSrZo-V4f4EGcNXzNCeAHRvCZGrxDWw7aHVdDY0pAF92jNLb_Hct0bMb3ew6JEpB9AfIm1tSGaQ=@protonmail.com>
	<CAEM=y+XbP3Dn7X8rHu7h0vbX6DkKA0vFK5nQqzcJ_V+D4EVMmw@mail.gmail.com>
	<C1OLL5FLxdOgfQ_A15mf88wIyztDapkyXJ2HZ0HxwmQADhRXGRe3le7Veso4tMIlbis6I0qiCd22xug5_GCKtgrjGnBtojWxOCMgn1UldkE=@protonmail.com>
	<CAEM=y+WCGSF_=WXpgXJUZCZcGUQhxzXF6Wv1_iX+VwEyYSWypg@mail.gmail.com>
Message-ID: <CAD5xwhi7=5eiv1jjf72-rUezZMfj3caR+PGfZEa8i8rjNjodFg@mail.gmail.com>

Awhile back, Ethan and I discussed having, rather than OP_CAT, an
OP_SHA256STREAM that uses the streaming properties of a SHA256 hash
function to allow concatenation of an unlimited amount of data, provided
the only use is to hash it.

You can then use it perhaps as follows:

// start a new hash with item
OP_SHA256STREAM  (-1) -> [state]
// Add item to the hash in state
OP_SHA256STREAM n [item] [state] -> [state]
// Finalize
OP_SHA256STREAM (-2) [state] -> [Hash]

<-1> OP_SHA256STREAM <tag> <subnode 2> <subnode 3> <3> OP_SHA256STREAM <-2>
OP_SHA256STREAM


Or it coul



--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>


On Thu, Oct 3, 2019 at 8:04 PM Ethan Heilman <eth3rs at gmail.com> wrote:

> I hope you are having an great afternoon ZmnSCPxj,
>
> You make an excellent point!
>
> I had thought about doing the following to tag nodes
>
> || means OP_CAT
>
> `node = SHA256(type||SHA256(data))`
> so a subnode would be
> `subnode1 = SHA256(1||SHA256(subnode2||subnode3))`
> and a leaf node would be
> `leafnode = SHA256(0||SHA256(leafdata))`
>
> Yet, I like your idea better. Increasing the size of the two inputs to
> OP_CAT to be 260 Bytes each where 520 Bytes is the maximum allowable
> size of object on the stack seems sensible and also doesn't special
> case the logic of OP_CAT.
>
> It would also increase performance. SHA256(tag||subnode2||subnode3)
> requires 2 compression function calls whereas
> SHA256(1||SHA256(subnode2||subnode3)) requires 2+1=3 compression
> function calls (due to padding).
>
> >Or we could implement tagged SHA256 as a new opcode...
>
> I agree that tagged SHA256 as an op code that would certainty be
> useful, but OP_CAT provides far more utility and is a simpler change.
>
> Thanks,
> Ethan
>
> On Thu, Oct 3, 2019 at 7:42 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:
> >
> > Good morning Ethan,
> >
> >
> > > To avoid derailing the NO_INPUT conversation, I have changed the
> > > subject to OP_CAT.
> > >
> > > Responding to:
> > > """
> > >
> > > -   `SIGHASH` flags attached to signatures are a misdesign, sadly
> > >     retained from the original BitCoin 0.1.0 Alpha for Windows design,
> on
> > >     par with:
> > >     [..]
> > >
> > > -   `OP_CAT` and `OP_MULT` and `OP_ADD` and friends
> > >     [..]
> > >     """
> > >
> > >     OP_CAT is an extremely valuable op code. I understand why it was
> > >     removed as the situation at the time with scripts was dire. However
> > >     most of the protocols I've wanted to build on Bitcoin run into the
> > >     limitation that stack values can not be concatenated. For instance
> > >     TumbleBit would have far smaller transaction sizes if OP_CAT was
> > >     supported in Bitcoin. If it happens to me as a researcher it is
> > >     probably holding other people back as well. If I could wave a magic
> > >     wand and turn on one of the disabled op codes it would be OP_CAT.
> Of
> > >     course with the change that size of each concatenated value must
> be 64
> > >     Bytes or less.
> >
> > Why 64 bytes in particular?
> >
> > It seems obvious to me that this 64 bytes is most suited for building
> Merkle trees, being the size of two SHA256 hashes.
> >
> > However we have had issues with the use of Merkle trees in Bitcoin
> blocks.
> > Specifically, it is difficult to determine if a hash on a Merkle node is
> the hash of a Merkle subnode, or a leaf transaction.
> > My understanding is that this is the reason for now requiring
> transactions to be at least 80 bytes.
> >
> > The obvious fix would be to prepend the type of the hashed object, i.e.
> add at least one byte to determine this type.
> > Taproot for example uses tagged hash functions, with a different tag for
> leaves, and tagged hashes are just
> prepend-this-32-byte-constant-twice-before-you-SHA256.
> >
> > This seems to indicate that to check merkle tree proofs, an `OP_CAT`
> with only 64 bytes max output size would not be sufficient.
> >
> > Or we could implement tagged SHA256 as a new opcode...
> >
> > Regards,
> > ZmnSCPxj
> >
> >
> > >
> > >     On Tue, Oct 1, 2019 at 10:04 PM ZmnSCPxj via bitcoin-dev
> > >     bitcoin-dev at lists.linuxfoundation.org wrote:
> > >
> > >
> > > > Good morning lists,
> > > > Let me propose the below radical idea:
> > > >
> > > > -   `SIGHASH` flags attached to signatures are a misdesign, sadly
> retained from the original BitCoin 0.1.0 Alpha for Windows design, on par
> with:
> > > >     -   1 RETURN
> > > >     -   higher-`nSequence` replacement
> > > >     -   DER-encoded pubkeys
> > > >     -   unrestricted `scriptPubKey`
> > > >     -   Payee-security-paid-by-payer (i.e. lack of P2SH)
> > > >     -   `OP_CAT` and `OP_MULT` and `OP_ADD` and friends
> > > >     -   transaction malleability
> > > >     -   probably many more
> > > >
> > > > So let me propose the more radical excision, starting with SegWit v1:
> > > >
> > > > -   Remove `SIGHASH` from signatures.
> > > > -   Put `SIGHASH` on public keys.
> > > >
> > > > Public keys are now encoded as either 33-bytes (implicit
> `SIGHASH_ALL`) or 34-bytes (`SIGHASH` byte, followed by pubkey type,
> followed by pubkey coordinate).
> > > > `OP_CHECKSIG` and friends then look at the public key to determine
> sighash algorithm rather than the signature.
> > > > As we expect public keys to be indirectly committed to on every
> output `scriptPubKey`, this is automatically output tagging to allow
> particular `SIGHASH`.
> > > > However, we can then utilize the many many ways to hide public keys
> away until they are needed, exemplified in MAST-inside-Taproot.
> > > > I propose also the addition of the opcode:
> > > >
> > > >     <sighash> <pubkey> OP_SETPUBKEYSIGHASH
> > > >
> > > >
> > > > -   `sighash` must be one byte.
> > > > -   `pubkey` may be the special byte `0x1`, meaning "just use the
> Taproot internal pubkey".
> > > > -   `pubkey` may be 33-byte public key, in which case the `sighash`
> byte is just prepended to it.
> > > > -   `pubkey` may be 34-byte public key with sighash, in which case
> the first byte is replaced with `sighash` byte.
> > > > -   If `sighash` is `0x00` then the result is a 33-byte public key
> (the sighash byte is removed) i.e. `SIGHASH_ALL` implicit.
> > > >
> > > > This retains the old feature where the sighash is selected at
> time-of-spending rather than time-of-payment.
> > > > This is done by using the script:
> > > >
> > > >     <pubkey> OP_SETPUBKEYSIGHASH OP_CHECKSIG
> > > >
> > > >
> > > > Then the sighash can be put in the witness stack after the
> signature, letting the `SIGHASH` flag be selected at time-of-signing, but
> only if the SCRIPT specifically is formed to do so.
> > > > This is malleability-safe as the signature still commits to the
> `SIGHASH` it was created for.
> > > > However, by default, public keys will not have an attached `SIGHASH`
> byte, implying `SIGHASH_ALL` (and disallowing-by-default non-`SIGHASH_ALL`).
> > > > This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as
> they are allowed only if the output specifically says they are allowed.
> > > > Would this not be a superior solution?
> > > > Regards,
> > > > ZmnSCPxj
> > > >
> > > > bitcoin-dev mailing list
> > > > bitcoin-dev at lists.linuxfoundation.org
> > > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> > >
> > > Lightning-dev mailing list
> > > Lightning-dev at lists.linuxfoundation.org
> > > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev
> >
> >
> _______________________________________________
> Lightning-dev mailing list
> Lightning-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191003/e3a709c9/attachment.html>

From kinoshitajona at gmail.com  Fri Oct  4 05:54:52 2019
From: kinoshitajona at gmail.com (=?UTF-8?B?5pyo44OO5LiL44GY44KH44Gq?=)
Date: Fri, 4 Oct 2019 14:54:52 +0900
Subject: [bitcoin-dev] PSBT global key for network
In-Reply-To: <CAJR7vkqjWs2PqLhS2P4LGFc5A5Sh3ZFMOnVPVS9S_8C9HHD=GQ@mail.gmail.com>
References: <CAJR7vkqjWs2PqLhS2P4LGFc5A5Sh3ZFMOnVPVS9S_8C9HHD=GQ@mail.gmail.com>
Message-ID: <CACvEmnH4Qcm3EDNBqQb1GPu0ct5R-JbbZGfUgdiGh_63OMbvgw@mail.gmail.com>

Hi Jimmy,

The only time I could see this being a problem is in the case of a
fork-coin.
Otherwise the likelihood that two unrelated networks could have a tx with
an id that is identical are low.

Everything included in PSBT thus far is info for verifying something
helpful, and providing the information needed for signing and verifying
what will be signed.

Adding a network section will be the inverse of that. The info doesn't help
you verify anything, since I could lie about the network, and you will need
to go out and check the network is as the PSBT says it is anyways.
Network is also not needed for signing.

In fact, come to think of it, even if there was a fork-coin incident, even
if you were able to separate PSBTs via network info, it won't matter if
there's no replay protection anyways, so giving a false sense of security
in thinking "I have explicitly stated my network so I should be ok"
(developers will think this, I guarantee) is actually a security minus IMO.

Currently BitcoinJS only uses network parameters to allow for the use of
addresses in addOutput... but I'm starting to think we should remove it...
not sure...

Cheers,
Jon

2019?10?4?(?) 11:04 Jimmy Song via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org>:

> Hey all,
>
> I wanted to propose a new key in the global context for BIP174,
> Partially-Signed Bitcoin Transactions.
>
> = Rationale
>
> Each signer should make sure that the inputs being referenced in the PSBT
> exist (with the exception of a Proof-of-Reserves input). In order to do
> this, it's critical to know which network the coins are on (mainnet or
> testnet). This could potentially be extended to other networks should they
> want to use something like PSBT, much in the same way that HD keys from
> BIP0044 reserved 0' and 1' as coins for mainnet Bitcoin and testnet Bitcoin
> respectively.
>
> = Proposal
>
> Add the key 0x03 for network in the global key-value store. Value is a
> variable integer with 0x00 indicating Bitcoin mainnet and 0x01 indicating
> Bitcoin testnet. Other coins that want to use the PSBT should use the coin
> network number from SLIP-0044 with the high bit removed.
>
> ---------------------------
>
> Best,
>
> Jimmy
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>


-- 
-----BEGIN PGP PUBLIC KEY BLOCK-----
Comment: http://openpgpjs.org

xsBNBFTmJ8oBB/9rd+7XLxZG/x/KnhkVK2WBG8ySx91fs+qQfHIK1JrakSV3
x6x0cK3XLClASLLDomm7Od3Q/fMFzdwCEqj6z60T8wgKxsjWYSGL3mq8ucdv
iBjC3wGauk5dQKtT7tkCFyQQbX/uMsBM4ccGBICoDmIJlwJIj7fAZVqGxGOM
bO1RhYb4dbQA2qxYP7wSsHJ6/ZNAXyEphOj6blUzdqO0exAbCOZWWF+E/1SC
EuKO4RmL7Imdep7uc2Qze1UpJCZx7ASHl2IZ4UD0G3Qr3pI6/jvNlaqCTa3U
3/YeJwEubFsd0AVy0zs809RcKKgX3W1q+hVDTeWinem9RiOG/vT+Eec/ABEB
AAHNI2tpbm9zaGl0YSA8a2lub3NoaXRham9uYUBnbWFpbC5jb20+wsByBBAB
CAAmBQJU5ifRBgsJCAcDAgkQRB9iZ30dlisEFQgCCgMWAgECGwMCHgEAAC6Z
B/9otobf0ASHYdlUBeIPXdDopyjQhR2RiZGYaS0VZ5zzHYLDDMW6ZIYm5CjO
Fc09ETLGKFxH2RcCOK2dzwz+KRU4xqOrt/l5gyd50cFE1nOhUN9+/XaPgrou
WhyT9xLeGit7Xqhht93z2+VanTtJAG6lWbAZLIZAMGMuLX6sJDCO0GiO5zxa
02Q2D3kh5GL57A5+oVOna12JBRaIA5eBGKVCp3KToT/z48pxBe3WAmLo0zXr
hEgTSzssfb2zTwtB3Ogoedj+cU2bHJvJ8upS/jMr3TcdguySmxJlGpocVC/e
qxq12Njv+LiETOrD8atGmXCnA+nFNljBkz+l6ADl93jHzsBNBFTmJ9EBCACu
Qq9ZnP+aLU/Rt6clAfiHfTFBsJvLKsdIKeE6qHzsU1E7A7bGQKTtLEnhCCQE
W+OQP+sgbOWowIdH9PpwLJ3Op+NhvLlMxRvbT36LwCmBL0yD7bMqxxmmVj8n
vlMMRSe4wDSIG19Oy7701imnHZPm/pnPlneg/Meu/UffpcDWYBbAFX8nrXPY
vkVULcI/qTcCxW/+S9fwoXjQhWHaiJJ6y3cYOSitN31W9zgcMvLwLX3JgDxE
flkwq/M+ZkfCYnS3GAPEt8GkVKy2eHtCJuNkGFlCAmKMX0yWzHRAkqOMN5KP
LFbkKY2GQl13ztWp82QYJZpj5af6dmyUosurn6AZABEBAAHCwF8EGAEIABMF
AlTmJ9QJEEQfYmd9HZYrAhsMAABKbgf/Ulu5JAk4fXgH0DtkMmdkFiKEFdkW
0Wkw7Vhd5eZ4NzeP9kOkD01OGweT9hqzwhfT2CNXCGxh4UnvEM1ZMFypIKdq
0XpLLJMrDOQO021UjAa56vHZPAVmAM01z5VzHJ7ekjgwrgMLmVkm0jWKEKaO
n/MW7CyphG7QcZ6cJX2f6uJcekBlZRw9TNYRnojMjkutlOVhYJ3J78nc/k0p
kcgV63GB6D7wHRF4TVe4xIBqKpbBhhN+ISwFN1z+gx3lfyRMSmiTSrGdKEQe
XSIQKG8XZQZUDhLNkqPS+7EMV1g7+lOfT4GhLL68dUXDa1e9YxGH6zkpVECw
Spe3vsHZr6CqFg==
=/vUJ
-----END PGP PUBLIC KEY BLOCK-----
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191004/8730e81e/attachment-0001.html>

From ZmnSCPxj at protonmail.com  Fri Oct  4 06:45:35 2019
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Fri, 04 Oct 2019 06:45:35 +0000
Subject: [bitcoin-dev] Smaller "Bitcoin address" accounts in the
	blockchain.
In-Reply-To: <CAGLBAheDMFkFtJG1gLf61ZZ9U3rasZ_T6f_sAHWWRiAcNWiO8A@mail.gmail.com>
References: <CAGLBAheDMFkFtJG1gLf61ZZ9U3rasZ_T6f_sAHWWRiAcNWiO8A@mail.gmail.com>
Message-ID: <BoxdwCcItqGY7JSaG245SNsgO6ijaeruqzdSUvSomOJLMHM33L8lRXPKE4e38g3DewzUEwbGNu7o2V5l9z2A48HrRpubm0U4zgpUecaXW60=@protonmail.com>

Good morning David,

> Currently, bitcoin must be redeemed by providing input to a script which results in the required output.? This causes the attached amount of bitcoin to become available for use in the outputs of a transaction.? Is there any work on creating a shorter "transaction" which, instead of creating a new output, points to (creates a virtual copy of) an existing (unspent) output with a larger amount attached to it?? This would invalidate the smaller, earlier UTXO and replace it with the new one without requiring the earlier one to be redeemed, and also without requiring the original script to be duplicated.? It is a method for aggregating bitcoin to a UTXO which may otherwise not be economically viable.
>
> The idea is that there already exists a script that must be satisfied to spend X1, and if the owner of X1 would like to have the same requirements for spending X2, this would be a transaction that does that using fewer data bytes.? Since the script already exists, the transaction can simply point to it instead of duplicating it.
>
> This would also enable the capacity of lightning channels to be increased on the fly without closing the existing channel and re-opening a new one.? The LN layer would have to cope with the possibility that the "short channel ID" could change.
>
> Dave.

This moves us closer to an "account"-style rather than "UTXO"-style.
The advantage of UTXO-style is that it becomes easy to validate a transaction as valid when putting it into the mempool, and as long as the UTXO it consumes remains valid, revalidation of the transaction when it is seen in a block is unnecessary.

Admittedly, the issue with account-style is when the account is overdrawn --- with UTXOs every spend drains the entire "account" and the "account" subsequently is definitely no longer spendable, whereas with accounts, every fullnode has to consider what would happen if two or more transactions spend from the account.
In your case, it seems to just *add* to the amount of a UTXO.

In any case, this might not be easy to implement in current Bitcoin.
The UTXO-style is deeply ingrained to Bitcoin design, and cannot be easily hacked in a softfork.

See also https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-July/017135.html and its thread for the difficulties involved with "just copy some existing `scriptPubKey`" and why such a thing will be very unlikely to come in Bitcoin.


But I think this can be done, in spirit, by pay-to-endpoint / payjoin.

In P2EP/Payjoin, the payer contacts the payee and offers to coinjoin simultaneously to the payment.
This does what you want:

* Refers to a previous UTXO owned by the payee, and deletes it (by normal transaction spending rules).
* Creates a new UTO, owned by the payee, which contains the total value of the below:
  * The above old UTXO.
  * The value to be transferred from payer to payee.

The only issues are that:

* Payee has to be online and cooperate.
* Payee has to provide signatures for the old UTXO, adding more blockchain data.
* New UTXO has to publish a SCRIPT too.
  * In terms of *privacy*, of course you *have* to use a new SCRIPT with a new public key anyway.
    Thus this is superior to your proposal where the pubkey is reused, as P2EP/Payjoin preserves privacy.


Regards,
ZmnSCPXj

From ZmnSCPxj at protonmail.com  Fri Oct  4 07:00:13 2019
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Fri, 04 Oct 2019 07:00:13 +0000
Subject: [bitcoin-dev] [Lightning-dev] OP_CAT was Re: Continuing the
	discussion about noinput / anyprevout
In-Reply-To: <CAD5xwhi7=5eiv1jjf72-rUezZMfj3caR+PGfZEa8i8rjNjodFg@mail.gmail.com>
References: <87wodp7w9f.fsf@gmail.com>
	<20191001155929.e2yznsetqesx2jxo@erisian.com.au>
	<CR-etCjXB-JWkvecjDog4Pkq1SuLUgndtSrZo-V4f4EGcNXzNCeAHRvCZGrxDWw7aHVdDY0pAF92jNLb_Hct0bMb3ew6JEpB9AfIm1tSGaQ=@protonmail.com>
	<CAEM=y+XbP3Dn7X8rHu7h0vbX6DkKA0vFK5nQqzcJ_V+D4EVMmw@mail.gmail.com>
	<C1OLL5FLxdOgfQ_A15mf88wIyztDapkyXJ2HZ0HxwmQADhRXGRe3le7Veso4tMIlbis6I0qiCd22xug5_GCKtgrjGnBtojWxOCMgn1UldkE=@protonmail.com>
	<CAEM=y+WCGSF_=WXpgXJUZCZcGUQhxzXF6Wv1_iX+VwEyYSWypg@mail.gmail.com>
	<CAD5xwhi7=5eiv1jjf72-rUezZMfj3caR+PGfZEa8i8rjNjodFg@mail.gmail.com>
Message-ID: <NVDIhcpuRV6VduUgfMNyGSU1U24ErWPuxpA07fMkIIgXqxEL2aX1_oYJ189Wf5ZigGDBa860gJ-p8d3iAMAWsjHx-5tZHZLjXr7gEmQbL0c=@protonmail.com>

Good morning Jeremy,

> Awhile back, Ethan and I discussed having, rather than OP_CAT, an OP_SHA256STREAM that uses the streaming properties of a SHA256 hash function to allow concatenation of an unlimited amount of data, provided the only use is to hash it.
>
> You can then use it perhaps as follows:
>
> // start a new hash with item
> OP_SHA256STREAM? (-1) -> [state]
> // Add item to the hash in state
> OP_SHA256STREAM n [item] [state] -> [state]
> // Finalize
> OP_SHA256STREAM (-2) [state] -> [Hash]
>
> <-1> OP_SHA256STREAM <tag> <subnode 2> <subnode 3> <3> OP_SHA256STREAM <-2> OP_SHA256STREAM
>
> Or it coul
>

This seems a good idea.

Though it brings up the age-old tension between:

* Generically-useable components, but due to generalization are less efficient.
* Specific-use components, which are efficient, but which may end up not being useable in the future.

In particular, `OP_SHA256STREAM` would no longer be useable if SHA256 eventually is broken, while the `OP_CAT` will still be useable in the indefinite future.
In the future a new hash function can simply be defined and the same technique with `OP_CAT` would still be useable.


Regards,
ZmnSCPxj

> --
> @JeremyRubin
>
> On Thu, Oct 3, 2019 at 8:04 PM Ethan Heilman <eth3rs at gmail.com> wrote:
>
> > I hope you are having an great afternoon ZmnSCPxj,
> >
> > You make an excellent point!
> >
> > I had thought about doing the following to tag nodes
> >
> > || means OP_CAT
> >
> > `node = SHA256(type||SHA256(data))`
> > so a subnode would be
> > `subnode1 = SHA256(1||SHA256(subnode2||subnode3))`
> > and a leaf node would be
> > `leafnode = SHA256(0||SHA256(leafdata))`
> >
> > Yet, I like your idea better. Increasing the size of the two inputs to
> > OP_CAT to be 260 Bytes each where 520 Bytes is the maximum allowable
> > size of object on the stack seems sensible and also doesn't special
> > case the logic of OP_CAT.
> >
> > It would also increase performance. SHA256(tag||subnode2||subnode3)
> > requires 2 compression function calls whereas
> > SHA256(1||SHA256(subnode2||subnode3)) requires 2+1=3 compression
> > function calls (due to padding).
> >
> > >Or we could implement tagged SHA256 as a new opcode...
> >
> > I agree that tagged SHA256 as an op code that would certainty be
> > useful, but OP_CAT provides far more utility and is a simpler change.
> >
> > Thanks,
> > Ethan
> >
> > On Thu, Oct 3, 2019 at 7:42 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:
> > >
> > > Good morning Ethan,
> > >
> > >
> > > > To avoid derailing the NO_INPUT conversation, I have changed the
> > > > subject to OP_CAT.
> > > >
> > > > Responding to:
> > > > """
> > > >
> > > > -? ?`SIGHASH` flags attached to signatures are a misdesign, sadly
> > > >? ? ?retained from the original BitCoin 0.1.0 Alpha for Windows design, on
> > > >? ? ?par with:
> > > >? ? ?[..]
> > > >
> > > > -? ?`OP_CAT` and `OP_MULT` and `OP_ADD` and friends
> > > >? ? ?[..]
> > > >? ? ?"""
> > > >
> > > >? ? ?OP_CAT is an extremely valuable op code. I understand why it was
> > > >? ? ?removed as the situation at the time with scripts was dire. However
> > > >? ? ?most of the protocols I've wanted to build on Bitcoin run into the
> > > >? ? ?limitation that stack values can not be concatenated. For instance
> > > >? ? ?TumbleBit would have far smaller transaction sizes if OP_CAT was
> > > >? ? ?supported in Bitcoin. If it happens to me as a researcher it is
> > > >? ? ?probably holding other people back as well. If I could wave a magic
> > > >? ? ?wand and turn on one of the disabled op codes it would be OP_CAT. Of
> > > >? ? ?course with the change that size of each concatenated value must be 64
> > > >? ? ?Bytes or less.
> > >
> > > Why 64 bytes in particular?
> > >
> > > It seems obvious to me that this 64 bytes is most suited for building Merkle trees, being the size of two SHA256 hashes.
> > >
> > > However we have had issues with the use of Merkle trees in Bitcoin blocks.
> > > Specifically, it is difficult to determine if a hash on a Merkle node is the hash of a Merkle subnode, or a leaf transaction.
> > > My understanding is that this is the reason for now requiring transactions to be at least 80 bytes.
> > >
> > > The obvious fix would be to prepend the type of the hashed object, i.e. add at least one byte to determine this type.
> > > Taproot for example uses tagged hash functions, with a different tag for leaves, and tagged hashes are just prepend-this-32-byte-constant-twice-before-you-SHA256.
> > >
> > > This seems to indicate that to check merkle tree proofs, an `OP_CAT` with only 64 bytes max output size would not be sufficient.
> > >
> > > Or we could implement tagged SHA256 as a new opcode...
> > >
> > > Regards,
> > > ZmnSCPxj
> > >
> > >
> > > >
> > > >? ? ?On Tue, Oct 1, 2019 at 10:04 PM ZmnSCPxj via bitcoin-dev
> > > >? ? ?bitcoin-dev at lists.linuxfoundation.org wrote:
> > > >
> > > >
> > > > > Good morning lists,
> > > > > Let me propose the below radical idea:
> > > > >
> > > > > -? ?`SIGHASH` flags attached to signatures are a misdesign, sadly retained from the original BitCoin 0.1.0 Alpha for Windows design, on par with:
> > > > >? ? ?-? ?1 RETURN
> > > > >? ? ?-? ?higher-`nSequence` replacement
> > > > >? ? ?-? ?DER-encoded pubkeys
> > > > >? ? ?-? ?unrestricted `scriptPubKey`
> > > > >? ? ?-? ?Payee-security-paid-by-payer (i.e. lack of P2SH)
> > > > >? ? ?-? ?`OP_CAT` and `OP_MULT` and `OP_ADD` and friends
> > > > >? ? ?-? ?transaction malleability
> > > > >? ? ?-? ?probably many more
> > > > >
> > > > > So let me propose the more radical excision, starting with SegWit v1:
> > > > >
> > > > > -? ?Remove `SIGHASH` from signatures.
> > > > > -? ?Put `SIGHASH` on public keys.
> > > > >
> > > > > Public keys are now encoded as either 33-bytes (implicit `SIGHASH_ALL`) or 34-bytes (`SIGHASH` byte, followed by pubkey type, followed by pubkey coordinate).
> > > > > `OP_CHECKSIG` and friends then look at the public key to determine sighash algorithm rather than the signature.
> > > > > As we expect public keys to be indirectly committed to on every output `scriptPubKey`, this is automatically output tagging to allow particular `SIGHASH`.
> > > > > However, we can then utilize the many many ways to hide public keys away until they are needed, exemplified in MAST-inside-Taproot.
> > > > > I propose also the addition of the opcode:
> > > > >
> > > > >? ? ?<sighash> <pubkey> OP_SETPUBKEYSIGHASH
> > > > >
> > > > >
> > > > > -? ?`sighash` must be one byte.
> > > > > -? ?`pubkey` may be the special byte `0x1`, meaning "just use the Taproot internal pubkey".
> > > > > -? ?`pubkey` may be 33-byte public key, in which case the `sighash` byte is just prepended to it.
> > > > > -? ?`pubkey` may be 34-byte public key with sighash, in which case the first byte is replaced with `sighash` byte.
> > > > > -? ?If `sighash` is `0x00` then the result is a 33-byte public key (the sighash byte is removed) i.e. `SIGHASH_ALL` implicit.
> > > > >
> > > > > This retains the old feature where the sighash is selected at time-of-spending rather than time-of-payment.
> > > > > This is done by using the script:
> > > > >
> > > > >? ? ?<pubkey> OP_SETPUBKEYSIGHASH OP_CHECKSIG
> > > > >
> > > > >
> > > > > Then the sighash can be put in the witness stack after the signature, letting the `SIGHASH` flag be selected at time-of-signing, but only if the SCRIPT specifically is formed to do so.
> > > > > This is malleability-safe as the signature still commits to the `SIGHASH` it was created for.
> > > > > However, by default, public keys will not have an attached `SIGHASH` byte, implying `SIGHASH_ALL` (and disallowing-by-default non-`SIGHASH_ALL`).
> > > > > This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as they are allowed only if the output specifically says they are allowed.
> > > > > Would this not be a superior solution?
> > > > > Regards,
> > > > > ZmnSCPxj
> > > > >
> > > > > bitcoin-dev mailing list
> > > > > bitcoin-dev at lists.linuxfoundation.org
> > > > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> > > >
> > > > Lightning-dev mailing list
> > > > Lightning-dev at lists.linuxfoundation.org
> > > > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev
> > >
> > >
> > _______________________________________________
> > Lightning-dev mailing list
> > Lightning-dev at lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev



From dave at dtrt.org  Fri Oct  4 08:20:31 2019
From: dave at dtrt.org (David A. Harding)
Date: Thu, 3 Oct 2019 22:20:31 -1000
Subject: [bitcoin-dev] Chain width expansion
In-Reply-To: <42cd5ffd-63e8-b738-c4ea-13d0699b1268@purse.io>
References: <42cd5ffd-63e8-b738-c4ea-13d0699b1268@purse.io>
Message-ID: <20191004082031.ns3pgzwh2zz2mxyc@ganymede>

On Thu, Oct 03, 2019 at 05:38:36PM -0700, Braydon Fuller via bitcoin-dev wrote:
> This paper describes a solution [to DoS attacks] that does not
> require enabling or maintaining checkpoints and provides improved security.
> [...] 
> The paper is available at:
> https://bcoin.io/papers/bitcoin-chain-expansion.pdf

Hi Braydon,

Thank you for researching this important issue.  An alternative solution
proposed some time ago (I believe originally by Gregory Maxwell) was a
soft fork to raise the minimum difficulty.  You can find discussion of
it in various old IRC conversations[1,2] as well as in related changes
to Bitcoin Core such as PR #9053 addining minimum chain work[3] and the
assumed-valid change added in Bitcoin Core 0.14.0[4].

[1] http://www.erisian.com.au/meetbot/bitcoin-core-dev/2016/bitcoin-core-dev.2016-10-27-19.01.log.html#l-121
[2] http://www.erisian.com.au/meetbot/bitcoin-core-dev/2017/bitcoin-core-dev.2017-03-02-19.01.log.html#l-57
[3] https://github.com/bitcoin/bitcoin/pull/9053/commits/fd46136dfaf68a7046cf7b8693824d73ac6b1caf
[4] https://bitcoincore.org/en/2017/03/08/release-0.14.0/#assumed-valid-blocks

The solutions proposed in section 4.2 and 4.3 of your paper have the
advantage of not requiring any consensus changes.  However, I find it
hard to analyze the full consequences of the throttling solution in
4.3 and the pruning solution in 4.2.  If we assume a node is on the
most-PoW valid chain and that a huge fork is unlikely, it seems fine.
But I worry that the mechanisms could also be used to keep a node that
synced to a long-but-lower-PoW chain on that false chain (or other false
chain) indefinitely even if it had connections to honest peers that
tried to tell it about the most-PoW chain.

For example, with your maximum throttle of 5 seconds between
`getheaders` requests and the `headers` P2P message maximum of 2,000
headers per instance, it would take about half an hour to get a full
chain worth of headers.  If a peer was disconnected before sending
enough headers to establish they were on the most-PoW chain, your
pruning solution would delete whatever progress was made, forcing the
next peer to start from genesis and taking them at least half an hour
too.  On frequently-suspended laptops or poor connections, it's possible
a node could be be operational for a long time before it kept the same
connection open for half an hour.  All that time, it would be on a
dishonest chain.

By comparison, I find it easy to analyze the effect of raising the
minimum difficulty.  It is a change to the consensus rules, so it's
something we should be careful about, but it's the kind of
basically-one-line change that I expect should be easy for a large
number of people to review directly.  Assuming the choice of a new
minimum (and what point in the chain to use it) is sane, I think it
would be easy to get acceptance, and I think it would further be easy
increase it again every five years or so as overall hashrate increases.

-Dave

From saulo at astrotown.de  Fri Oct  4 09:15:40 2019
From: saulo at astrotown.de (Saulo Fonseca)
Date: Fri, 4 Oct 2019 11:15:40 +0200
Subject: [bitcoin-dev] ChainWallet - A way to prevent loss of funds by
	physical violence
Message-ID: <4E84E4B0-7354-4681-985F-3DBFAA4E856F@astrotown.de>

Hi everyone

If you are a hodler, I like to propose the creation of a key stretching as a new layer of protection over your current wallet. I call it ChainWallet. Whatever is the method used to generate your private key, we can do the following:

newPrivKey = sha256(sha256(sha256(?sha256(privKey)?)))
NewWallet = PubAddress(newPrivKey)
In this way we create a chain of hashes over your private key and generate a new wallet from it. If the chain is very long (billions or trillions of hashes) it will take a long time to be created. If you don?t keep the newPrivKey, the only way to move coins in the NewWallet is to generate the chain again.

The length of the chain can be easy memorized as an exponent such as 2^40 or 10^12.

What is that gut for? You will not be able to move your coins in an unplanned way such as being tortured by a kidnaper. You can create a wallet that takes days or even months to return the final address.

Comparison with a BrainWallet

If the first privKey is the hash of a password, your ChainWallet can be compared to a BrainWallet with a chain added to it. BrainWallets have a bad reputation because it is possible to create a brute-force attack against it. There are reports where the attacker was able to guess the password by generating hundreds of thousands of hashes per second. But, if you use a ChainWallet that takes one second to be generated, it means that the speed of an attack would be reduced to one guess per second. This makes a brute force attack practically impossible.

Entropy

The ChainWallet adds only a few bits of entropy to your key. The idea here is not to increase the entropy, but to add ?time? as part of the puzzle.

SHA-256

I am suggesting the use of SHA-256 because it is the most popular hash algorithm in the crypto community. But you could use SHA-512 or a slower hash algorithm such as Bcrypt to do it. But keep in mind that other hash algorithms can reduce the entropy.

The idea is to add time to the key generation. If you use many SHA-256 or a few SHA-512, as long as both need the same time to be generated, there is no difference.

Other hashes have the advantage that a hardware implementation of it is not widespread.

ASICs

Someone could mention that ASICs get more and more powerful and could crack a ChainWallet. But they have a huge hash rate because they calculate it in parallel. A ChainWallet requires that the output of a hash would be the input of the next calculation. This dramatically reduces the speed of a hardware implementation of such algorithms.

Let?s pick an example:  The Bitfury Clarke has 8.154 cores and runs 120 Gh/s. This means that each core can perform about 14.72 Mh/s. This speed is all that you can get with one of the best ASIC on the market. 17.72 Mh/s is only about 17,7 times faster than a typical computer. This speed can only increase slowly, as technology needs time to make the transistors run faster. So, the best way to generate a ChainWallet is by using such an ASIC core.

Misuse

Someone could argue that people would misuse it by picking easy to remember passwords or small chain length. A wallet implementation could solve it by forcing a minimum length for the chain and block commonly used words for the password. It is a matter of design.

Theft

The major advantage of a ChainWallet is the ability to avoid a theft. If your wallet takes a really long time to be generated and someone tries to force you to give your private key, you would not be able to do it, even if you really want. You could also give away a wrong password or chain length and he/she is not able to verify it. The chances are very small that he/she will wait weeks of months for the chain generation of even that he/she is able to do the chain calculation.

Final Thoughts

A ChainWallet could be used as an alternative to BIP39. Instead of keeping 24 words, you would have a password and two numbers, a base and an exponent, that defines the length of the chain. This is easier to memorize, so you do not need to write it down.

This is only meant as an additional option along with all others available in the crypto environment, such as multisig and smart contracts. As for those other ideas, the ChainWallet is not applicable in every case.

When the day arrives at which you want to stop hodling and transferring your coins to another location, you should re-generate your wallet in a planned way with the same original private key and length of the chain. Then, after waiting until the program concludes, you will get the new private key back.

Web Links
	
The original idea can be found on this post:

https://www.reddit.com/user/sauloqf/comments/a3q8dt/chainwallet <https://www.reddit.com/user/sauloqf/comments/a3q8dt/chainwallet>

A proof of concept in C++ can be found on this link:

https://github.com/Saulo-Fonseca/ChainWallet <https://github.com/Saulo-Fonseca/ChainWallet>

The community is testing the concept for a while. You can find discussions on this links:

https://www.reddit.com/r/Bitcoin/comments/cya467/chainwallet_challenge_get_01_btc_if_you_solve_it <https://www.reddit.com/r/Bitcoin/comments/cya467/chainwallet_challenge_get_01_btc_if_you_solve_it>

https://www.reddit.com/r/Bitcoin/comments/d9ltec/does_someone_know_how_to_submit_a_bip_for_bitcoin <https://www.reddit.com/r/Bitcoin/comments/d9ltec/does_someone_know_how_to_submit_a_bip_for_bitcoin>

Saulo Fonseca

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191004/b7a50f18/attachment-0001.html>

From kanzure at gmail.com  Fri Oct  4 10:02:59 2019
From: kanzure at gmail.com (Bryan Bishop)
Date: Fri, 4 Oct 2019 05:02:59 -0500
Subject: [bitcoin-dev] ChainWallet - A way to prevent loss of funds by
 physical violence
In-Reply-To: <4E84E4B0-7354-4681-985F-3DBFAA4E856F@astrotown.de>
References: <4E84E4B0-7354-4681-985F-3DBFAA4E856F@astrotown.de>
Message-ID: <CABaSBayF+nq+MdK3TY4_0Wkgn4qNMAWJub+q8e9amuig6gRE0Q@mail.gmail.com>

Since the user can't prove that they are using this technique, or
petertodd's timelock encryption for that matter, an attacker has little
incentive to stop physically attacking until they have a spendable UTXO.

I believe you can get the same effect with on-chain timelocks, or
delete-the-bits plus a rangeproof and a zero-knowledge proof that the
rangeproof corresponds to some secret that can be used to derive the
expected public key. I think Jeremy Rubin had an idea for such a proof.

Also, adam3us has described a similar thought here:
https://bitcointalk.org/index.php?topic=311000.0

- Bryan

On Fri, Oct 4, 2019, 4:43 AM Saulo Fonseca via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Hi everyone
>
> If you are a hodler, I like to propose the creation of a key stretching as
> a new layer of protection over your current wallet.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191004/1473ba33/attachment.html>

From pete at petertodd.org  Fri Oct  4 11:15:36 2019
From: pete at petertodd.org (Peter Todd)
Date: Fri, 4 Oct 2019 07:15:36 -0400
Subject: [bitcoin-dev] [Lightning-dev] OP_CAT was Re: Continuing the
 discussion about noinput / anyprevout
In-Reply-To: <CAD5xwhi7=5eiv1jjf72-rUezZMfj3caR+PGfZEa8i8rjNjodFg@mail.gmail.com>
References: <87wodp7w9f.fsf@gmail.com>
	<20191001155929.e2yznsetqesx2jxo@erisian.com.au>
	<CR-etCjXB-JWkvecjDog4Pkq1SuLUgndtSrZo-V4f4EGcNXzNCeAHRvCZGrxDWw7aHVdDY0pAF92jNLb_Hct0bMb3ew6JEpB9AfIm1tSGaQ=@protonmail.com>
	<CAEM=y+XbP3Dn7X8rHu7h0vbX6DkKA0vFK5nQqzcJ_V+D4EVMmw@mail.gmail.com>
	<C1OLL5FLxdOgfQ_A15mf88wIyztDapkyXJ2HZ0HxwmQADhRXGRe3le7Veso4tMIlbis6I0qiCd22xug5_GCKtgrjGnBtojWxOCMgn1UldkE=@protonmail.com>
	<CAEM=y+WCGSF_=WXpgXJUZCZcGUQhxzXF6Wv1_iX+VwEyYSWypg@mail.gmail.com>
	<CAD5xwhi7=5eiv1jjf72-rUezZMfj3caR+PGfZEa8i8rjNjodFg@mail.gmail.com>
Message-ID: <20191004111536.w7snbgpoe27xutfu@petertodd.org>

On Thu, Oct 03, 2019 at 10:02:14PM -0700, Jeremy via bitcoin-dev wrote:
> Awhile back, Ethan and I discussed having, rather than OP_CAT, an
> OP_SHA256STREAM that uses the streaming properties of a SHA256 hash
> function to allow concatenation of an unlimited amount of data, provided
> the only use is to hash it.
> 
> You can then use it perhaps as follows:
> 
> // start a new hash with item
> OP_SHA256STREAM  (-1) -> [state]
> // Add item to the hash in state
> OP_SHA256STREAM n [item] [state] -> [state]
> // Finalize
> OP_SHA256STREAM (-2) [state] -> [Hash]
> 
> <-1> OP_SHA256STREAM <tag> <subnode 2> <subnode 3> <3> OP_SHA256STREAM <-2>
> OP_SHA256STREAM

One issue with this is the simplest implementation where the state is just raw
bytes would expose raw SHA256 midstates, allowing people to use them directly;
preventing that would require adding types to the stack. Specifically I could
write a script that rather than initializing the state correctly from the
official IV, instead takes an untrusted state as input.

SHA256 isn't designed to be used in situations where adversaries control the
initialization vector. I personally don't know one way or the other if anyone
has analyzed this in detail, but I'd be surprised if that's secure. I
considered adding midstate support to OpenTimestamps but decided against it for
exactly that reason.

I don't have the link handy but there's even an example of an experienced
cryptographer on this very list (bitcoin-dev) proposing a design that falls
victim to this attack. It's a subtle issue and we probably don't want to
encourage it.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191004/9c383745/attachment.sig>

From jlrubin at mit.edu  Fri Oct  4 18:33:09 2019
From: jlrubin at mit.edu (Jeremy)
Date: Fri, 4 Oct 2019 11:33:09 -0700
Subject: [bitcoin-dev] [Lightning-dev] OP_CAT was Re: Continuing the
 discussion about noinput / anyprevout
In-Reply-To: <NVDIhcpuRV6VduUgfMNyGSU1U24ErWPuxpA07fMkIIgXqxEL2aX1_oYJ189Wf5ZigGDBa860gJ-p8d3iAMAWsjHx-5tZHZLjXr7gEmQbL0c=@protonmail.com>
References: <87wodp7w9f.fsf@gmail.com>
	<20191001155929.e2yznsetqesx2jxo@erisian.com.au>
	<CR-etCjXB-JWkvecjDog4Pkq1SuLUgndtSrZo-V4f4EGcNXzNCeAHRvCZGrxDWw7aHVdDY0pAF92jNLb_Hct0bMb3ew6JEpB9AfIm1tSGaQ=@protonmail.com>
	<CAEM=y+XbP3Dn7X8rHu7h0vbX6DkKA0vFK5nQqzcJ_V+D4EVMmw@mail.gmail.com>
	<C1OLL5FLxdOgfQ_A15mf88wIyztDapkyXJ2HZ0HxwmQADhRXGRe3le7Veso4tMIlbis6I0qiCd22xug5_GCKtgrjGnBtojWxOCMgn1UldkE=@protonmail.com>
	<CAEM=y+WCGSF_=WXpgXJUZCZcGUQhxzXF6Wv1_iX+VwEyYSWypg@mail.gmail.com>
	<CAD5xwhi7=5eiv1jjf72-rUezZMfj3caR+PGfZEa8i8rjNjodFg@mail.gmail.com>
	<NVDIhcpuRV6VduUgfMNyGSU1U24ErWPuxpA07fMkIIgXqxEL2aX1_oYJ189Wf5ZigGDBa860gJ-p8d3iAMAWsjHx-5tZHZLjXr7gEmQbL0c=@protonmail.com>
Message-ID: <CAD5xwhh_WbpSvou7sORjG9JeVonU8UR3qR0Bc9cmhp5sep34OA@mail.gmail.com>

Good point -- in our discussion, we called it OP_FFS -- Fold Functional
Stream, and it could be initialized with a different integer to select for
different functions. Therefore the stream processing opcodes would be
generic, but extensible.
--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>


On Fri, Oct 4, 2019 at 12:00 AM ZmnSCPxj via Lightning-dev <
lightning-dev at lists.linuxfoundation.org> wrote:

> Good morning Jeremy,
>
> > Awhile back, Ethan and I discussed having, rather than OP_CAT, an
> OP_SHA256STREAM that uses the streaming properties of a SHA256 hash
> function to allow concatenation of an unlimited amount of data, provided
> the only use is to hash it.
> >
> > You can then use it perhaps as follows:
> >
> > // start a new hash with item
> > OP_SHA256STREAM  (-1) -> [state]
> > // Add item to the hash in state
> > OP_SHA256STREAM n [item] [state] -> [state]
> > // Finalize
> > OP_SHA256STREAM (-2) [state] -> [Hash]
> >
> > <-1> OP_SHA256STREAM <tag> <subnode 2> <subnode 3> <3> OP_SHA256STREAM
> <-2> OP_SHA256STREAM
> >
> > Or it coul
> >
>
> This seems a good idea.
>
> Though it brings up the age-old tension between:
>
> * Generically-useable components, but due to generalization are less
> efficient.
> * Specific-use components, which are efficient, but which may end up not
> being useable in the future.
>
> In particular, `OP_SHA256STREAM` would no longer be useable if SHA256
> eventually is broken, while the `OP_CAT` will still be useable in the
> indefinite future.
> In the future a new hash function can simply be defined and the same
> technique with `OP_CAT` would still be useable.
>
>
> Regards,
> ZmnSCPxj
>
> > --
> > @JeremyRubin
> >
> > On Thu, Oct 3, 2019 at 8:04 PM Ethan Heilman <eth3rs at gmail.com> wrote:
> >
> > > I hope you are having an great afternoon ZmnSCPxj,
> > >
> > > You make an excellent point!
> > >
> > > I had thought about doing the following to tag nodes
> > >
> > > || means OP_CAT
> > >
> > > `node = SHA256(type||SHA256(data))`
> > > so a subnode would be
> > > `subnode1 = SHA256(1||SHA256(subnode2||subnode3))`
> > > and a leaf node would be
> > > `leafnode = SHA256(0||SHA256(leafdata))`
> > >
> > > Yet, I like your idea better. Increasing the size of the two inputs to
> > > OP_CAT to be 260 Bytes each where 520 Bytes is the maximum allowable
> > > size of object on the stack seems sensible and also doesn't special
> > > case the logic of OP_CAT.
> > >
> > > It would also increase performance. SHA256(tag||subnode2||subnode3)
> > > requires 2 compression function calls whereas
> > > SHA256(1||SHA256(subnode2||subnode3)) requires 2+1=3 compression
> > > function calls (due to padding).
> > >
> > > >Or we could implement tagged SHA256 as a new opcode...
> > >
> > > I agree that tagged SHA256 as an op code that would certainty be
> > > useful, but OP_CAT provides far more utility and is a simpler change.
> > >
> > > Thanks,
> > > Ethan
> > >
> > > On Thu, Oct 3, 2019 at 7:42 PM ZmnSCPxj <ZmnSCPxj at protonmail.com>
> wrote:
> > > >
> > > > Good morning Ethan,
> > > >
> > > >
> > > > > To avoid derailing the NO_INPUT conversation, I have changed the
> > > > > subject to OP_CAT.
> > > > >
> > > > > Responding to:
> > > > > """
> > > > >
> > > > > -   `SIGHASH` flags attached to signatures are a misdesign, sadly
> > > > >     retained from the original BitCoin 0.1.0 Alpha for Windows
> design, on
> > > > >     par with:
> > > > >     [..]
> > > > >
> > > > > -   `OP_CAT` and `OP_MULT` and `OP_ADD` and friends
> > > > >     [..]
> > > > >     """
> > > > >
> > > > >     OP_CAT is an extremely valuable op code. I understand why it
> was
> > > > >     removed as the situation at the time with scripts was dire.
> However
> > > > >     most of the protocols I've wanted to build on Bitcoin run into
> the
> > > > >     limitation that stack values can not be concatenated. For
> instance
> > > > >     TumbleBit would have far smaller transaction sizes if OP_CAT
> was
> > > > >     supported in Bitcoin. If it happens to me as a researcher it is
> > > > >     probably holding other people back as well. If I could wave a
> magic
> > > > >     wand and turn on one of the disabled op codes it would be
> OP_CAT. Of
> > > > >     course with the change that size of each concatenated value
> must be 64
> > > > >     Bytes or less.
> > > >
> > > > Why 64 bytes in particular?
> > > >
> > > > It seems obvious to me that this 64 bytes is most suited for
> building Merkle trees, being the size of two SHA256 hashes.
> > > >
> > > > However we have had issues with the use of Merkle trees in Bitcoin
> blocks.
> > > > Specifically, it is difficult to determine if a hash on a Merkle
> node is the hash of a Merkle subnode, or a leaf transaction.
> > > > My understanding is that this is the reason for now requiring
> transactions to be at least 80 bytes.
> > > >
> > > > The obvious fix would be to prepend the type of the hashed object,
> i.e. add at least one byte to determine this type.
> > > > Taproot for example uses tagged hash functions, with a different tag
> for leaves, and tagged hashes are just
> prepend-this-32-byte-constant-twice-before-you-SHA256.
> > > >
> > > > This seems to indicate that to check merkle tree proofs, an `OP_CAT`
> with only 64 bytes max output size would not be sufficient.
> > > >
> > > > Or we could implement tagged SHA256 as a new opcode...
> > > >
> > > > Regards,
> > > > ZmnSCPxj
> > > >
> > > >
> > > > >
> > > > >     On Tue, Oct 1, 2019 at 10:04 PM ZmnSCPxj via bitcoin-dev
> > > > >     bitcoin-dev at lists.linuxfoundation.org wrote:
> > > > >
> > > > >
> > > > > > Good morning lists,
> > > > > > Let me propose the below radical idea:
> > > > > >
> > > > > > -   `SIGHASH` flags attached to signatures are a misdesign,
> sadly retained from the original BitCoin 0.1.0 Alpha for Windows design, on
> par with:
> > > > > >     -   1 RETURN
> > > > > >     -   higher-`nSequence` replacement
> > > > > >     -   DER-encoded pubkeys
> > > > > >     -   unrestricted `scriptPubKey`
> > > > > >     -   Payee-security-paid-by-payer (i.e. lack of P2SH)
> > > > > >     -   `OP_CAT` and `OP_MULT` and `OP_ADD` and friends
> > > > > >     -   transaction malleability
> > > > > >     -   probably many more
> > > > > >
> > > > > > So let me propose the more radical excision, starting with
> SegWit v1:
> > > > > >
> > > > > > -   Remove `SIGHASH` from signatures.
> > > > > > -   Put `SIGHASH` on public keys.
> > > > > >
> > > > > > Public keys are now encoded as either 33-bytes (implicit
> `SIGHASH_ALL`) or 34-bytes (`SIGHASH` byte, followed by pubkey type,
> followed by pubkey coordinate).
> > > > > > `OP_CHECKSIG` and friends then look at the public key to
> determine sighash algorithm rather than the signature.
> > > > > > As we expect public keys to be indirectly committed to on every
> output `scriptPubKey`, this is automatically output tagging to allow
> particular `SIGHASH`.
> > > > > > However, we can then utilize the many many ways to hide public
> keys away until they are needed, exemplified in MAST-inside-Taproot.
> > > > > > I propose also the addition of the opcode:
> > > > > >
> > > > > >     <sighash> <pubkey> OP_SETPUBKEYSIGHASH
> > > > > >
> > > > > >
> > > > > > -   `sighash` must be one byte.
> > > > > > -   `pubkey` may be the special byte `0x1`, meaning "just use
> the Taproot internal pubkey".
> > > > > > -   `pubkey` may be 33-byte public key, in which case the
> `sighash` byte is just prepended to it.
> > > > > > -   `pubkey` may be 34-byte public key with sighash, in which
> case the first byte is replaced with `sighash` byte.
> > > > > > -   If `sighash` is `0x00` then the result is a 33-byte public
> key (the sighash byte is removed) i.e. `SIGHASH_ALL` implicit.
> > > > > >
> > > > > > This retains the old feature where the sighash is selected at
> time-of-spending rather than time-of-payment.
> > > > > > This is done by using the script:
> > > > > >
> > > > > >     <pubkey> OP_SETPUBKEYSIGHASH OP_CHECKSIG
> > > > > >
> > > > > >
> > > > > > Then the sighash can be put in the witness stack after the
> signature, letting the `SIGHASH` flag be selected at time-of-signing, but
> only if the SCRIPT specifically is formed to do so.
> > > > > > This is malleability-safe as the signature still commits to the
> `SIGHASH` it was created for.
> > > > > > However, by default, public keys will not have an attached
> `SIGHASH` byte, implying `SIGHASH_ALL` (and disallowing-by-default
> non-`SIGHASH_ALL`).
> > > > > > This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`,
> as they are allowed only if the output specifically says they are allowed.
> > > > > > Would this not be a superior solution?
> > > > > > Regards,
> > > > > > ZmnSCPxj
> > > > > >
> > > > > > bitcoin-dev mailing list
> > > > > > bitcoin-dev at lists.linuxfoundation.org
> > > > > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> > > > >
> > > > > Lightning-dev mailing list
> > > > > Lightning-dev at lists.linuxfoundation.org
> > > > > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev
> > > >
> > > >
> > > _______________________________________________
> > > Lightning-dev mailing list
> > > Lightning-dev at lists.linuxfoundation.org
> > > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev
>
>
> _______________________________________________
> Lightning-dev mailing list
> Lightning-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191004/709217b1/attachment.html>

From jlrubin at mit.edu  Fri Oct  4 18:40:53 2019
From: jlrubin at mit.edu (Jeremy)
Date: Fri, 4 Oct 2019 11:40:53 -0700
Subject: [bitcoin-dev] [Lightning-dev] OP_CAT was Re: Continuing the
 discussion about noinput / anyprevout
In-Reply-To: <20191004111536.w7snbgpoe27xutfu@petertodd.org>
References: <87wodp7w9f.fsf@gmail.com>
	<20191001155929.e2yznsetqesx2jxo@erisian.com.au>
	<CR-etCjXB-JWkvecjDog4Pkq1SuLUgndtSrZo-V4f4EGcNXzNCeAHRvCZGrxDWw7aHVdDY0pAF92jNLb_Hct0bMb3ew6JEpB9AfIm1tSGaQ=@protonmail.com>
	<CAEM=y+XbP3Dn7X8rHu7h0vbX6DkKA0vFK5nQqzcJ_V+D4EVMmw@mail.gmail.com>
	<C1OLL5FLxdOgfQ_A15mf88wIyztDapkyXJ2HZ0HxwmQADhRXGRe3le7Veso4tMIlbis6I0qiCd22xug5_GCKtgrjGnBtojWxOCMgn1UldkE=@protonmail.com>
	<CAEM=y+WCGSF_=WXpgXJUZCZcGUQhxzXF6Wv1_iX+VwEyYSWypg@mail.gmail.com>
	<CAD5xwhi7=5eiv1jjf72-rUezZMfj3caR+PGfZEa8i8rjNjodFg@mail.gmail.com>
	<20191004111536.w7snbgpoe27xutfu@petertodd.org>
Message-ID: <CAD5xwhhLd9Ufv50kOi+yaJ5dTX9LhB1dPsK_0bqjz038tChcjw@mail.gmail.com>

Interesting point.

The script is under your control, so you should be able to ensure that you
are always using a correctly constructed midstate, e.g., something like:

scriptPubKey: <-1> OP_SHA256STREAM DEPTH OP_SHA256STREAM <-2>
OP_SHA256STREAM
<hash> OP_EQUALVERIFY

would hash all the elements on the stack and compare to a known hash.
How is that sort of thing weak to midstateattacks?


--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>


On Fri, Oct 4, 2019 at 4:16 AM Peter Todd <pete at petertodd.org> wrote:

> On Thu, Oct 03, 2019 at 10:02:14PM -0700, Jeremy via bitcoin-dev wrote:
> > Awhile back, Ethan and I discussed having, rather than OP_CAT, an
> > OP_SHA256STREAM that uses the streaming properties of a SHA256 hash
> > function to allow concatenation of an unlimited amount of data, provided
> > the only use is to hash it.
> >
> > You can then use it perhaps as follows:
> >
> > // start a new hash with item
> > OP_SHA256STREAM  (-1) -> [state]
> > // Add item to the hash in state
> > OP_SHA256STREAM n [item] [state] -> [state]
> > // Finalize
> > OP_SHA256STREAM (-2) [state] -> [Hash]
> >
> > <-1> OP_SHA256STREAM <tag> <subnode 2> <subnode 3> <3> OP_SHA256STREAM
> <-2>
> > OP_SHA256STREAM
>
> One issue with this is the simplest implementation where the state is just
> raw
> bytes would expose raw SHA256 midstates, allowing people to use them
> directly;
> preventing that would require adding types to the stack. Specifically I
> could
> write a script that rather than initializing the state correctly from the
> official IV, instead takes an untrusted state as input.
>
> SHA256 isn't designed to be used in situations where adversaries control
> the
> initialization vector. I personally don't know one way or the other if
> anyone
> has analyzed this in detail, but I'd be surprised if that's secure. I
> considered adding midstate support to OpenTimestamps but decided against
> it for
> exactly that reason.
>
> I don't have the link handy but there's even an example of an experienced
> cryptographer on this very list (bitcoin-dev) proposing a design that falls
> victim to this attack. It's a subtle issue and we probably don't want to
> encourage it.
>
> --
> https://petertodd.org 'peter'[:-1]@petertodd.org
> _______________________________________________
> Lightning-dev mailing list
> Lightning-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191004/bda6e24c/attachment-0001.html>

From braydon at purse.io  Fri Oct  4 19:51:26 2019
From: braydon at purse.io (Braydon Fuller)
Date: Fri, 4 Oct 2019 12:51:26 -0700
Subject: [bitcoin-dev] Chain width expansion
In-Reply-To: <20191004082031.ns3pgzwh2zz2mxyc@ganymede>
References: <42cd5ffd-63e8-b738-c4ea-13d0699b1268@purse.io>
	<20191004082031.ns3pgzwh2zz2mxyc@ganymede>
Message-ID: <e9b9c972-7357-690c-2522-1928e6f84d48@purse.io>

On 10/4/19 1:20 AM, David A. Harding wrote:

> On Thu, Oct 03, 2019 at 05:38:36PM -0700, Braydon Fuller via bitcoin-dev wrote:
>> This paper describes a solution [to DoS attacks] that does not
>> require enabling or maintaining checkpoints and provides improved security.
>> [...] 
>> The paper is available at:
>> https://bcoin.io/papers/bitcoin-chain-expansion.pdf
> [..] But I worry that the mechanisms could also be used to keep a node that
> synced to a long-but-lower-PoW chain on that false chain (or other false
> chain) indefinitely even if it had connections to honest peers that
> tried to tell it about the most-PoW chain.

Here is an example: An attacker eclipses a target node during the
initial block download; all of the target's outgoing peers are the
attacker. The attacker has a low work chain that is sent to the target.
The total chainwork for the low work chain is 0x09104210421039 at a
height of 593,975. The target is now in the state of a fully validated
low work dishonest chain. The target node then connects to an honest
peer and learns about the honest chain. The chainwork of the honest
chain is 0x085b67d9e07a751e53679d68 at a height of 593,975. The first
69,500 headers of the honest chain would have a delay, however the
remaining 52,4475 would not be delayed. Given a maximum of 5 seconds,
this would be a total delay of only 157 seconds.



From tier.nolan at gmail.com  Fri Oct  4 23:31:18 2019
From: tier.nolan at gmail.com (Tier Nolan)
Date: Sat, 5 Oct 2019 00:31:18 +0100
Subject: [bitcoin-dev] Chain width expansion
In-Reply-To: <42cd5ffd-63e8-b738-c4ea-13d0699b1268@purse.io>
References: <42cd5ffd-63e8-b738-c4ea-13d0699b1268@purse.io>
Message-ID: <CAE-z3OV_LL+Jww3e=gO6t=02VW7m9PK+8EaYoEVLy9NKNMiSaQ@mail.gmail.com>

Are you assuming no network protocol changes?

At root, the requirement is that peers can prove their total chain POW.

Since each block has the height in the coinbase, a peer can send a short
proof of height for a disconnected header and could assert the POW for that
header.

Each peer could send the the N strongest headers (lowest digest/most POW)
for their main chain and prove the height of each one.

The total chain work can be estimated as N times the POW for the lowest in
the list.  This is an interesting property of how POW works.  The 10th best
POW block will have about 10% of the total POW.

The N blocks would be spread along the chain and the peer could ask for all
headers between any 2 of them and check the different in claimed POW.  If
dishonesty is discovered, the peer can be banned and all info from that
peer wiped.

You can apply the rule hierarchically.  The honest peers would have a much
higher POW chain.  You could ask the peer to give you the N strongest
headers between 2 headers that they gave for their best chain.  You can
check that their height is between the two limits.

The peer would effectively be proving their total POW recursively.

This would require a new set of messages so you can request info about the
best chain.

It also has the nice feature that it allows you to see if multiple peers
are on the same chain, since they will have the same best blocks.

The most elegant would be something like using SNARKS to directly prove
that your chain tip has a particular POW.  The download would go tip to
genesis, unlike now when it is in the other direction.

------------------------------------------------------------------------

In regard to your proposal, I think the key is to limit things by peer,
rather than globally.

The limit to header width should be split between peers.  If you have N
outgoing peers, they get 1/N of your header download resources each.

You store the current best/most POW header chain and at least one
alternative chain per outgoing peer.

You could still prune old chains based on POW, but the best chain and the
current chain for each outgoing peer should not be pruned.

The security assumption is that a node is connected to at least one honest
node.

If you split resources between all peers, then it prevents the dishonest
nodes from flooding and wiping out the progress for the honest peer.

- Message Limiting -

I have the same objection here.  The message limiting should be per peer.

An honest peer who has just been connected to shouldn't suffer a penalty.

Your point that it is only a few minutes anyway may make this point moot
though.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191005/ea973505/attachment.html>

From aj at erisian.com.au  Sat Oct  5 10:06:15 2019
From: aj at erisian.com.au (Anthony Towns)
Date: Sat, 5 Oct 2019 20:06:15 +1000
Subject: [bitcoin-dev] Continuing the discussion about noinput /
 anyprevout
In-Reply-To: <877e5m6q8i.fsf@gmail.com>
References: <87wodp7w9f.fsf@gmail.com>
	<20191001155929.e2yznsetqesx2jxo@erisian.com.au>
	<877e5m6q8i.fsf@gmail.com>
Message-ID: <20191005100615.q6z2nklq257xbqfp@erisian.com.au>

On Thu, Oct 03, 2019 at 01:08:29PM +0200, Christian Decker wrote:
> >  * anyprevout signatures make the address you're signing for less safe,
> >    which may cause you to lose funds when additional coins are sent to
> >    the same address; this can be avoided if handled with care (or if you
> >    don't care about losing funds in the event of address reuse)
> Excellent points, I had missed the hidden nature of the opt-in via
> pubkey prefix while reading your proposal. I'm starting to like that
> option more and more. In that case we'd only ever be revealing that we
> opted into anyprevout when we're revealing the entire script anyway, at
> which point all fungibility concerns go out the window anyway.
>
> Would this scheme be extendable to opt into all sighash flags the
> outpoint would like to allow (e.g., adding opt-in for sighash_none and
> sighash_anyonecanpay as well)? That way the pubkey prefix could act as a
> mask for the sighash flags and fail verification if they don't match.

For me, the thing that distinguishes ANYPREVOUT/NOINPUT as warranting
an opt-in step is that it affects the security of potentially many
UTXOs at once; whereas all the other combinations (ALL,SINGLE,NONE
cross ALL,ANYONECANPAY) still commit to the specific UTXO being spent,
so at most you only risk somehow losing the funds from the specific UTXO
you're working with (apart from the SINGLE bug, which taproot doesn't
support anyway).

Having a meaningful prefix on the taproot scriptpubkey (ie paying to
"[SIGHASH_SINGLE][32B pubkey]") seems like it would make it a bit easier
to distinguish wallets, which taproot otherwise avoids -- "oh this address
is going to be a SIGHASH_SINGLE? probably some hacker, let's ban it".

> > I think it might be good to have a public testnet (based on Richard Myers
> > et al's signet2 work?) where we have some fake exchanges/merchants/etc
> > and scheduled reorgs, and demo every weird noinput/anyprevout case anyone
> > can think of, and just work out if we need any extra code/tagging/whatever
> > to keep those fake exchanges/merchants from losing money (and write up
> > the weird cases we've found in a wiki or a paper so people can easily
> > tell if we missed something obvious).
> That'd be great, however even that will not ensure that every possible
> corner case is handled [...]

Well, sure. I'm thinking of it more as a *necessary* step than a
*sufficient* one, though. If we can't demonstrate that we can deal with
the theoretical attacks people have dreamt up in a "laboratory" setting,
then it doesn't make much sense to deploy things in a real world setting,
does it?

I think if it turns out that we can handle every case we can think of
easily, that will be good evidence that output tagging and the like isn't
necessary; and conversely if it turns out we can't handle them easily,
it at least gives us a chance to see how output tagging (or chaperone
sigs, or whatever else) would actually work, and if they'd provide any
meaningful protection at all. At the moment the best we've got is ideas
and handwaving...

Cheers,
aj


From pete at petertodd.org  Sat Oct  5 15:49:02 2019
From: pete at petertodd.org (Peter Todd)
Date: Sat, 5 Oct 2019 11:49:02 -0400
Subject: [bitcoin-dev] [Lightning-dev] OP_CAT was Re: Continuing the
 discussion about noinput / anyprevout
In-Reply-To: <CAD5xwhhLd9Ufv50kOi+yaJ5dTX9LhB1dPsK_0bqjz038tChcjw@mail.gmail.com>
References: <87wodp7w9f.fsf@gmail.com>
	<20191001155929.e2yznsetqesx2jxo@erisian.com.au>
	<CR-etCjXB-JWkvecjDog4Pkq1SuLUgndtSrZo-V4f4EGcNXzNCeAHRvCZGrxDWw7aHVdDY0pAF92jNLb_Hct0bMb3ew6JEpB9AfIm1tSGaQ=@protonmail.com>
	<CAEM=y+XbP3Dn7X8rHu7h0vbX6DkKA0vFK5nQqzcJ_V+D4EVMmw@mail.gmail.com>
	<C1OLL5FLxdOgfQ_A15mf88wIyztDapkyXJ2HZ0HxwmQADhRXGRe3le7Veso4tMIlbis6I0qiCd22xug5_GCKtgrjGnBtojWxOCMgn1UldkE=@protonmail.com>
	<CAEM=y+WCGSF_=WXpgXJUZCZcGUQhxzXF6Wv1_iX+VwEyYSWypg@mail.gmail.com>
	<CAD5xwhi7=5eiv1jjf72-rUezZMfj3caR+PGfZEa8i8rjNjodFg@mail.gmail.com>
	<20191004111536.w7snbgpoe27xutfu@petertodd.org>
	<CAD5xwhhLd9Ufv50kOi+yaJ5dTX9LhB1dPsK_0bqjz038tChcjw@mail.gmail.com>
Message-ID: <20191005154902.ck236q65xha25ore@petertodd.org>

On Fri, Oct 04, 2019 at 11:40:53AM -0700, Jeremy wrote:
> Interesting point.
> 
> The script is under your control, so you should be able to ensure that you
> are always using a correctly constructed midstate, e.g., something like:
> 
> scriptPubKey: <-1> OP_SHA256STREAM DEPTH OP_SHA256STREAM <-2>
> OP_SHA256STREAM
> <hash> OP_EQUALVERIFY
> 
> would hash all the elements on the stack and compare to a known hash.
> How is that sort of thing weak to midstateattacks?

Obviously with care you can get the computation right. But at that point what's
the actual advantage over OP_CAT?

We're limited by the size of the script anyway; if the OP_CAT output size limit
is comparable to that for almost anything you could use SHA256STREAM on you
could just as easily use OP_CAT, followed by a single OP_SHA256.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191005/213e4e81/attachment.sig>

From me at emilengler.com  Sat Oct  5 21:57:48 2019
From: me at emilengler.com (Emil Engler)
Date: Sat, 5 Oct 2019 23:57:48 +0200
Subject: [bitcoin-dev] BIPable-idea: Consistent and better definition of the
	term 'address'
Message-ID: <58e44347-6eee-a0c3-3b8a-965c7450780e@emilengler.com>

Hello dear mailing list subscribers.
Before I'll explain my idea here, I need to define a term first

'address':
When I use the terms address, pubkey, etc., I mean the same: The Base58
string

Ok now let's get into it:
As you should know, sending bitcoins to an address more than once is a
very bad approach.
In my opinion the problem why so many people are still doing this is
because of the term 'address' which is used in lots of wallets,
implementations, BIP 21 and so on. It is a design issue.
With the term 'address' most people identify things that are fixed and
don't change really often (e.g postal address, IP address [depends on
provider], Domain, E-Mail address, ...).
Because of this most people compare bitcoin addresses with e-mail
addresses and use this address to send the recipient money multiple times.

My suggestion would be to change the term address in wallets, the URI
scheme and so on to something of the following options by a
Informational/Process BIP:

* Payment Password
* Transaction Password
* ...

The guideline for the term should indicate that it is:
* temporary
* Something that identifies the recipient

I've chosen 'password' because they can be used as a pseudonym to
identify a person.
This is already used in stuff like bank transfers where something like
the transaction id should be used as the purpose or at universities
there are student numbers.
The first is probably a better example because student numbers aren't
temporary.

What do you think? Should I write a BIP for this or use another term?
Feedback is most welcome :)

Greetings,
Emil Engler

-------------- next part --------------
A non-text attachment was scrubbed...
Name: pEpkey.asc
Type: application/pgp-keys
Size: 3147 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191005/6875962d/attachment-0001.bin>

From jw at mathbot.com  Sat Oct  5 22:51:20 2019
From: jw at mathbot.com (JW Weatherman)
Date: Sat, 05 Oct 2019 22:51:20 +0000
Subject: [bitcoin-dev] Human readable format for private keys
Message-ID: <9QeoIpWbdIBmLIvDI1SnwIIUSu84IDb1DqCNIn60CrKbnwZkOk0aFZY8uNCv5hCC6QK2nChJ3lQdvGwftGwhBYTayzVqIgQ3L1UC4nw5Y2Y=@mathbot.com>

Hey Guys,

I?d like to propose a feature to bitcoin to solve the following problems:

- When people read or write private keys it is very easy to mistake a letter or number.
- When entering a private key a mistake isn?t identified until the entire key is entered.
- When an error is made in providing a private key the location of the error isn?t indicated within the private key.
- Private keys stored on paper can be lost if a single character is damaged or poorly transcribed.

The solution I?m proposing has two parts.

First provide an option to use to the NATO phonetic alphabet when displaying or entertaining private keys. To indicate lower case the word should not be capitalized. Capital letters and numbers should be capitalized.

The nato phonetic alphabet is a long-standing international standard (as international as the use of letters and numbers already used in base58) and has been designed to make each letter easily distinguishable when spoken and written.

By using whole words, that are easily distinguishable and from a very short word database (58 well known words that are either the English numbers or words that begin with the letter indicated) the likelihood of errors in recovery are reduced.

The second part of the solution is to insert checksum letters. If every 5th word is actually a checksum for the previous 4 words, you end up with 13 sentences such as:

ALFA tango THREE SIX bravo

In this case bravo is actually a checksum for the previous 4 words and can be calculated and verified as the private key is entered. If the user accidentally trumped BRAVO instead of bravo the checksum would immediately indicate an error within these 5 words (in most cases) making for a greatly improved user experience.

An additional side effect of this is that even if an entire word is lost on multiple lines, the  checksum would probably make guessing the correct words relatively easy.

I realize some of these issues have been discussed in relation to bip39, but I hope this is more likely to be adopted by bitcoin core as it uses existing private keys, has no impact on keygen, does not require a standardized and well known word list for every language, and is essential just a display format that hopefully wouldn?t require invasive code changes.

Thanks in advance for your feedback.

-JW
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191005/ccdde54b/attachment-0001.html>

From lloyd.fourn at gmail.com  Sun Oct  6 07:02:52 2019
From: lloyd.fourn at gmail.com (Lloyd Fournier)
Date: Sun, 6 Oct 2019 10:02:52 +0300
Subject: [bitcoin-dev] [Lightning-dev] OP_CAT was Re: Continuing the
 discussion about noinput / anyprevout
In-Reply-To: <CAEM=y+XbP3Dn7X8rHu7h0vbX6DkKA0vFK5nQqzcJ_V+D4EVMmw@mail.gmail.com>
References: <87wodp7w9f.fsf@gmail.com>
	<20191001155929.e2yznsetqesx2jxo@erisian.com.au>
	<CR-etCjXB-JWkvecjDog4Pkq1SuLUgndtSrZo-V4f4EGcNXzNCeAHRvCZGrxDWw7aHVdDY0pAF92jNLb_Hct0bMb3ew6JEpB9AfIm1tSGaQ=@protonmail.com>
	<CAEM=y+XbP3Dn7X8rHu7h0vbX6DkKA0vFK5nQqzcJ_V+D4EVMmw@mail.gmail.com>
Message-ID: <CAH5Bsr1DP-7bEUYVhffCmNXjxTb2KJLASQd6NTz8O9P9OpSKCQ@mail.gmail.com>

Hi Thread,

I made a reply to the OP but didn't "reply all" so it just went directly to
Ethan. Since the comments were interesting I'll attempt to salvage them by
posting them in full:

== Lloyd's post ==
Hi Ethan,

I'd be interested to know what protocols you need OP_CAT for. I'm trying to
figure out if there really exists any script based protocol that doesn't
have a more efficient scriptless counterpart.  For example,
A?L[1] achieves the same thing as Tumblebit but requires no script. I can
imagine paying based on a merkle path could be useful, but a protocol was
recently suggested on lightning-dev [2] that does this but without OP_CAT
(and without any script!).


[1] https://eprint.iacr.org/2019/589.pdf
[2]
https://www.mail-archive.com/lightning-dev at lists.linuxfoundation.org/msg01427.html
(*I linked to the wrong thread in the original email*).

LL

== Ethan's response ==
Hi Lloyd,

Thanks for your response. I am not sure if you intended to take this off
list or not.

I plan to at some point to enumerate in detail protocols that OP_CAT would
benefit. A more important point is that OP_CAT is a basic building block
and that we don't know what future protocols it would allow. In my own
research I have avoiding going down certain paths because it isn't worth
the time to investigate knowing that OP_CAT wouldn't make the protocol
practical.

In regards to scriptless scripts they almost always require an interactive
protocol and sometimes ZKPs. A2L is very impressive but like TumbleBit it
places a large burden on the developer. Additionally I am aware of no way
to reveal a subset of preimages with scriptless scripts, do a conditioned
reveal i.e. these preimages can only spend under these two pubkeys and
timelockA where after timelockZ this other pubkey can spend without a
preimages. Scriptless scripts are a fantastic tool but they shouldn't be
the only tool that we have.

I'm not sure I follow what you are saying with [2]

This brings me back a philosophical point:
Bitcoin should give people basic tools to build protocols without first
knowing what all those protocols are especially when those tools have very
little downside.

I really appreciate your comments.

Thanks,
Ethan
==

*Back to normal thread*

Hi Ethan,

Thanks for the insightful reply and sorry for my mailing list errors.

> I plan to at some point to enumerate in detail protocols that OP_CAT
would benefit.

Sweet. Thanks.

> Additionally I am aware of no way to reveal a subset of preimages with
scriptless scripts, do a conditioned reveal i.e. these preimages can only
spend under these two pubkeys and timelockA where after timelockZ this
other pubkey can spend without a preimages. Scriptless scripts are a
fantastic tool but they shouldn't be the only tool that we have.

Yes. With adaptor signatures there is no way to reveal more than one
pre-image; you are limited to revealing a single scalar. But you can have
multiple transactions spending from the same output, each with a different
set of scriptless conditions (absolute time locks, relative time locks and
pre-image reveal). This is enough to achieve what I think you are
describing. FWIW there's a growing consensus that you can do lightning
without script [1]. Perhaps we can't do everything with this technique. My
current focus is figuring out what useful things we can't do like this
(even if we were to go wild and add whatever opcodes we wanted). So far it
looks like covenants are the main exception.

> I'm not sure I follow what you are saying with [2]

That is perfectly understandable as I linked the wrong thread (sorry!).
Here's the right one:
https://www.mail-archive.com/lightning-dev at lists.linuxfoundation.org/msg01427.html

I was pointing to the surprising result that you can actually pay for a
merkle path with a particular merkle root leading to a particular leaf that
you're interested in without validating the merkle path on chain (e.g.
OP_CAT and OP_SHA256). The catch is that the leaves have to be pedersen
commitments and you prove the existence of your data in the merkle root by
showing an opening to the leaf pedersen commitment. This may not be general
enough to cover every merkle tree use case (but I'm not sure what those
are!).

> This brings me back a philosophical point:
> Bitcoin should give people basic tools to build protocols without first
knowing what all those protocols are especially when those tools have very
little downside.

This is a really powerful idea. But I've started feeling like you have to
just design the layer 2 protocols first and then design layer 1! It seems
like almost every protocol that people want to make requires very
particular fundamental changes: SegWit for LN-penalty and NOINPUT for eltoo
for example. On top of that it seems like just having the right signature
scheme (schnorr) at layer 1 is enough to enable most useful stuff in an
elegant way.

[1]
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-September/017309.html

Cheers,

LL

On Fri, Oct 4, 2019 at 1:08 AM Ethan Heilman <eth3rs at gmail.com> wrote:

> To avoid derailing the NO_INPUT conversation, I have changed the
> subject to OP_CAT.
>
> Responding to:
> """
> * `SIGHASH` flags attached to signatures are a misdesign, sadly
> retained from the original BitCoin 0.1.0 Alpha for Windows design, on
> par with:
> [..]
> * `OP_CAT` and `OP_MULT` and `OP_ADD` and friends
> [..]
> """
>
> OP_CAT is an extremely valuable op code. I understand why it was
> removed as the situation at the time with scripts was dire. However
> most of the protocols I've wanted to build on Bitcoin run into the
> limitation that stack values can not be concatenated. For instance
> TumbleBit would have far smaller transaction sizes if OP_CAT was
> supported in Bitcoin. If it happens to me as a researcher it is
> probably holding other people back as well. If I could wave a magic
> wand and turn on one of the disabled op codes it would be OP_CAT.  Of
> course with the change that size of each concatenated value must be 64
> Bytes or less.
>
>
> On Tue, Oct 1, 2019 at 10:04 PM ZmnSCPxj via bitcoin-dev
> <bitcoin-dev at lists.linuxfoundation.org> wrote:
> >
> > Good morning lists,
> >
> > Let me propose the below radical idea:
> >
> > * `SIGHASH` flags attached to signatures are a misdesign, sadly retained
> from the original BitCoin 0.1.0 Alpha for Windows design, on par with:
> >   * 1 RETURN
> >   * higher-`nSequence` replacement
> >   * DER-encoded pubkeys
> >   * unrestricted `scriptPubKey`
> >   * Payee-security-paid-by-payer (i.e. lack of P2SH)
> >   * `OP_CAT` and `OP_MULT` and `OP_ADD` and friends
> >   * transaction malleability
> >   * probably many more
> >
> > So let me propose the more radical excision, starting with SegWit v1:
> >
> > * Remove `SIGHASH` from signatures.
> > * Put `SIGHASH` on public keys.
> >
> > Public keys are now encoded as either 33-bytes (implicit `SIGHASH_ALL`)
> or 34-bytes (`SIGHASH` byte, followed by pubkey type, followed by pubkey
> coordinate).
> > `OP_CHECKSIG` and friends then look at the *public key* to determine
> sighash algorithm rather than the signature.
> >
> > As we expect public keys to be indirectly committed to on every output
> `scriptPubKey`, this is automatically output tagging to allow particular
> `SIGHASH`.
> > However, we can then utilize the many many ways to hide public keys away
> until they are needed, exemplified in MAST-inside-Taproot.
> >
> > I propose also the addition of the opcode:
> >
> >     <sighash> <pubkey> OP_SETPUBKEYSIGHASH
> >
> > * `sighash` must be one byte.
> > * `pubkey` may be the special byte `0x1`, meaning "just use the Taproot
> internal pubkey".
> > * `pubkey` may be 33-byte public key, in which case the `sighash` byte
> is just prepended to it.
> > * `pubkey` may be 34-byte public key with sighash, in which case the
> first byte is replaced with `sighash` byte.
> > * If `sighash` is `0x00` then the result is a 33-byte public key (the
> sighash byte is removed) i.e. `SIGHASH_ALL` implicit.
> >
> > This retains the old feature where the sighash is selected at
> time-of-spending rather than time-of-payment.
> > This is done by using the script:
> >
> >     <pubkey> OP_SETPUBKEYSIGHASH OP_CHECKSIG
> >
> > Then the sighash can be put in the witness stack after the signature,
> letting the `SIGHASH` flag be selected at time-of-signing, but only if the
> SCRIPT specifically is formed to do so.
> > This is malleability-safe as the signature still commits to the
> `SIGHASH` it was created for.
> >
> > However, by default, public keys will not have an attached `SIGHASH`
> byte, implying `SIGHASH_ALL` (and disallowing-by-default non-`SIGHASH_ALL`).
> >
> > This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as they
> are allowed only if the output specifically says they are allowed.
> >
> > Would this not be a superior solution?
> >
> > Regards,
> > ZmnSCPxj
> > _______________________________________________
> > bitcoin-dev mailing list
> > bitcoin-dev at lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> _______________________________________________
> Lightning-dev mailing list
> Lightning-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191006/f0e27c02/attachment-0001.html>

From ZmnSCPxj at protonmail.com  Sun Oct  6 08:46:59 2019
From: ZmnSCPxj at protonmail.com (ZmnSCPxj)
Date: Sun, 06 Oct 2019 08:46:59 +0000
Subject: [bitcoin-dev] [Lightning-dev] OP_CAT was Re: Continuing the
	discussion about noinput / anyprevout
In-Reply-To: <20191005154902.ck236q65xha25ore@petertodd.org>
References: <87wodp7w9f.fsf@gmail.com>
	<20191001155929.e2yznsetqesx2jxo@erisian.com.au>
	<CR-etCjXB-JWkvecjDog4Pkq1SuLUgndtSrZo-V4f4EGcNXzNCeAHRvCZGrxDWw7aHVdDY0pAF92jNLb_Hct0bMb3ew6JEpB9AfIm1tSGaQ=@protonmail.com>
	<CAEM=y+XbP3Dn7X8rHu7h0vbX6DkKA0vFK5nQqzcJ_V+D4EVMmw@mail.gmail.com>
	<C1OLL5FLxdOgfQ_A15mf88wIyztDapkyXJ2HZ0HxwmQADhRXGRe3le7Veso4tMIlbis6I0qiCd22xug5_GCKtgrjGnBtojWxOCMgn1UldkE=@protonmail.com>
	<CAEM=y+WCGSF_=WXpgXJUZCZcGUQhxzXF6Wv1_iX+VwEyYSWypg@mail.gmail.com>
	<CAD5xwhi7=5eiv1jjf72-rUezZMfj3caR+PGfZEa8i8rjNjodFg@mail.gmail.com>
	<20191004111536.w7snbgpoe27xutfu@petertodd.org>
	<CAD5xwhhLd9Ufv50kOi+yaJ5dTX9LhB1dPsK_0bqjz038tChcjw@mail.gmail.com>
	<20191005154902.ck236q65xha25ore@petertodd.org>
Message-ID: <g0JWqeAd0tg8PbmrwuBRz7VP9h_-63H11oMxWzS8pQE7-awPkLlzSGmhVANp2ssbo19KJU_waNUg846YFbvh0WVSejnOMSoVo-eDl-aytpg=@protonmail.com>

Good morning Peter, Jeremy, and lists,

> On Fri, Oct 04, 2019 at 11:40:53AM -0700, Jeremy wrote:
>
> > Interesting point.
> > The script is under your control, so you should be able to ensure that you
> > are always using a correctly constructed midstate, e.g., something like:
> > scriptPubKey: <-1> OP_SHA256STREAM DEPTH OP_SHA256STREAM <-2>
> > OP_SHA256STREAM
> > <hash> OP_EQUALVERIFY
> > would hash all the elements on the stack and compare to a known hash.
> > How is that sort of thing weak to midstateattacks?
>
> Obviously with care you can get the computation right. But at that point what's
> the actual advantage over OP_CAT?
>
> We're limited by the size of the script anyway; if the OP_CAT output size limit
> is comparable to that for almost anything you could use SHA256STREAM on you
> could just as easily use OP_CAT, followed by a single OP_SHA256.

Theoretically, `OP_CAT` is less efficient.

In cases where the memory area used to back the data cannot be resized, new backing memory must be allocated elsewhere and the existing data copied.
This leads to possible O( n^2 ) behavior for `OP_CAT` (degenerate case where we add 1 byte per `OP_CAT` and each time find that the memory area currently in use is exactly fitting the data and cannot be resized in-place).

`OP_SHASTREAM` would not require new allocations once the stream state is in place and would not require any copying.


This may be relevant in considering the cost of executing `OP_CAT`.

Admittedly a sufficiently-limited  maximum `OP_CAT` output would be helpful in reducing the worst-case `OP_CAT` behavior.
The question is what limit would be reasonable.
64 bytes feels too small if one considers Merkle tree proofs, due to mentioned issues of lack of typechecking.


Regards,
ZmnSCPxj


>
> --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
>
> https://petertodd.org 'peter'[:-1]@petertodd.org
>
> Lightning-dev mailing list
> Lightning-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev



From pete at petertodd.org  Sun Oct  6 09:12:21 2019
From: pete at petertodd.org (Peter Todd)
Date: Sun, 6 Oct 2019 05:12:21 -0400
Subject: [bitcoin-dev] [Lightning-dev] OP_CAT was Re: Continuing the
 discussion about noinput / anyprevout
In-Reply-To: <g0JWqeAd0tg8PbmrwuBRz7VP9h_-63H11oMxWzS8pQE7-awPkLlzSGmhVANp2ssbo19KJU_waNUg846YFbvh0WVSejnOMSoVo-eDl-aytpg=@protonmail.com>
References: <20191001155929.e2yznsetqesx2jxo@erisian.com.au>
	<CR-etCjXB-JWkvecjDog4Pkq1SuLUgndtSrZo-V4f4EGcNXzNCeAHRvCZGrxDWw7aHVdDY0pAF92jNLb_Hct0bMb3ew6JEpB9AfIm1tSGaQ=@protonmail.com>
	<CAEM=y+XbP3Dn7X8rHu7h0vbX6DkKA0vFK5nQqzcJ_V+D4EVMmw@mail.gmail.com>
	<C1OLL5FLxdOgfQ_A15mf88wIyztDapkyXJ2HZ0HxwmQADhRXGRe3le7Veso4tMIlbis6I0qiCd22xug5_GCKtgrjGnBtojWxOCMgn1UldkE=@protonmail.com>
	<CAEM=y+WCGSF_=WXpgXJUZCZcGUQhxzXF6Wv1_iX+VwEyYSWypg@mail.gmail.com>
	<CAD5xwhi7=5eiv1jjf72-rUezZMfj3caR+PGfZEa8i8rjNjodFg@mail.gmail.com>
	<20191004111536.w7snbgpoe27xutfu@petertodd.org>
	<CAD5xwhhLd9Ufv50kOi+yaJ5dTX9LhB1dPsK_0bqjz038tChcjw@mail.gmail.com>
	<20191005154902.ck236q65xha25ore@petertodd.org>
	<g0JWqeAd0tg8PbmrwuBRz7VP9h_-63H11oMxWzS8pQE7-awPkLlzSGmhVANp2ssbo19KJU_waNUg846YFbvh0WVSejnOMSoVo-eDl-aytpg=@protonmail.com>
Message-ID: <20191006091221.pq4utwocwwzqir3h@petertodd.org>

On Sun, Oct 06, 2019 at 08:46:59AM +0000, ZmnSCPxj wrote:
> > Obviously with care you can get the computation right. But at that point what's
> > the actual advantage over OP_CAT?
> >
> > We're limited by the size of the script anyway; if the OP_CAT output size limit
> > is comparable to that for almost anything you could use SHA256STREAM on you
> > could just as easily use OP_CAT, followed by a single OP_SHA256.
> 
> Theoretically, `OP_CAT` is less efficient.
> 
> In cases where the memory area used to back the data cannot be resized, new backing memory must be allocated elsewhere and the existing data copied.
> This leads to possible O( n^2 ) behavior for `OP_CAT` (degenerate case where we add 1 byte per `OP_CAT` and each time find that the memory area currently in use is exactly fitting the data and cannot be resized in-place).

In even that degenerate case allocators also free memory.

Anyway, every execution step in script evaluation has a maximum output size,
and the number of steps is limited. At worst you can allocate the entire
possible stack up-front for relatively little cost (eg fitting in the MB or two
that is a common size for L2 cache).

> Admittedly a sufficiently-limited  maximum `OP_CAT` output would be helpful in reducing the worst-case `OP_CAT` behavior.
> The question is what limit would be reasonable.
> 64 bytes feels too small if one considers Merkle tree proofs, due to mentioned issues of lack of typechecking.

256 bytes is more than enough for even the most complex summed merkle tree with
512-byte hashes and full-sized sum commitments. Yet that's still less than the
~500byte limit proposed elsewhere.

-- 
https://petertodd.org 'peter'[:-1]@petertodd.org
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191006/13e0402e/attachment.sig>

From luke at dashjr.org  Sun Oct  6 11:32:33 2019
From: luke at dashjr.org (Luke Dashjr)
Date: Sun, 6 Oct 2019 11:32:33 +0000
Subject: [bitcoin-dev] BIPable-idea: Consistent and better definition of
	the term 'address'
In-Reply-To: <58e44347-6eee-a0c3-3b8a-965c7450780e@emilengler.com>
References: <58e44347-6eee-a0c3-3b8a-965c7450780e@emilengler.com>
Message-ID: <201910061132.39215.luke@dashjr.org>

On Saturday 05 October 2019 21:57:48 Emil Engler via bitcoin-dev wrote:
> Hello dear mailing list subscribers.
> Before I'll explain my idea here, I need to define a term first
>
> 'address':
> When I use the terms address, pubkey, etc., I mean the same: The Base58
> string

But a pubkey is not a Base58 string, and fundamentally different from an 
address. An address identifies the recipient and the purpose of the payment; 
a pubkey does not. The pubkey remains with the UTXO; an address does not.

> Ok now let's get into it:
> As you should know, sending bitcoins to an address more than once is a
> very bad approach.
> In my opinion the problem why so many people are still doing this is
> because of the term 'address' which is used in lots of wallets,
> implementations, BIP 21 and so on. It is a design issue.
> With the term 'address' most people identify things that are fixed and
> don't change really often (e.g postal address, IP address [depends on
> provider], Domain, E-Mail address, ...).
> Because of this most people compare bitcoin addresses with e-mail
> addresses and use this address to send the recipient money multiple times.

That problem would require using a different term than "address" to address.
A BIP is unlikely to do the job (though it may help).

> My suggestion would be to change the term address in wallets, the URI
> scheme and so on to something of the following options by a
> Informational/Process BIP:
>
> * Payment Password
> * Transaction Password
> * ...

Neither the address nor pubkey are a password...

Some possible alternative terms would be "invoice id", "payment token", etc.

> The guideline for the term should indicate that it is:
> * temporary
> * Something that identifies the recipient
>
> I've chosen 'password' because they can be used as a pseudonym to
> identify a person.
> This is already used in stuff like bank transfers where something like
> the transaction id should be used as the purpose or at universities
> there are student numbers.
> The first is probably a better example because student numbers aren't
> temporary.
>
> What do you think? Should I write a BIP for this or use another term?
> Feedback is most welcome :)
>
> Greetings,
> Emil Engler


From me at emilengler.com  Sun Oct  6 16:06:27 2019
From: me at emilengler.com (Emil Engler)
Date: Sun, 6 Oct 2019 18:06:27 +0200
Subject: [bitcoin-dev] BIPable-idea: Consistent and better definition of
 the term 'address'
In-Reply-To: <201910061132.39215.luke@dashjr.org>
References: <58e44347-6eee-a0c3-3b8a-965c7450780e@emilengler.com>
	<201910061132.39215.luke@dashjr.org>
Message-ID: <ea664ec6-4485-d925-ada1-d302d6f81eb1@emilengler.com>

> But a pubkey is not a Base58 string, and fundamentally different from an 
> address. An address identifies the recipient and the purpose of the payment; 
> a pubkey does not. The pubkey remains with the UTXO; an address does not.

I don't know much about this topic in Bitcoin so far, but I meant the
address :)

> That problem would require using a different term than "address" to address.

I meant this the whole time, sorry for misunderstandings.

> A BIP is unlikely to do the job (though it may help).

Why? They are specifications and implementations need them (even if some
do not implement everything).
Also BIP 21 would need an alternative/separate parameter with the new term.

> Neither the address nor pubkey are a password...

In Germany at least (where I live), you need to provide a 'Password'
(the german translation of thin word of course) in the purpose of the
transaction if you pay with a bank transfer. They are used as IDs but I
agree, ID is definitely a better suffix.

> Some possible alternative terms would be "invoice id", "payment token", etc.

ACK

Greetings, Emil Engler
-------------- next part --------------
A non-text attachment was scrubbed...
Name: pEpkey.asc
Type: application/pgp-keys
Size: 3147 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191006/5c829744/attachment.bin>

From apoelstra at wpsoftware.net  Wed Oct  9 16:56:51 2019
From: apoelstra at wpsoftware.net (Andrew Poelstra)
Date: Wed, 9 Oct 2019 16:56:51 +0000
Subject: [bitcoin-dev] [Lightning-dev] OP_CAT was Re: Continuing the
 discussion about noinput / anyprevout
In-Reply-To: <CAEM=y+XbP3Dn7X8rHu7h0vbX6DkKA0vFK5nQqzcJ_V+D4EVMmw@mail.gmail.com>
References: <87wodp7w9f.fsf@gmail.com>
	<20191001155929.e2yznsetqesx2jxo@erisian.com.au>
	<CR-etCjXB-JWkvecjDog4Pkq1SuLUgndtSrZo-V4f4EGcNXzNCeAHRvCZGrxDWw7aHVdDY0pAF92jNLb_Hct0bMb3ew6JEpB9AfIm1tSGaQ=@protonmail.com>
	<CAEM=y+XbP3Dn7X8rHu7h0vbX6DkKA0vFK5nQqzcJ_V+D4EVMmw@mail.gmail.com>
Message-ID: <20191009165651.GN13224@boulet>

On Thu, Oct 03, 2019 at 11:05:52AM -0400, Ethan Heilman wrote:
> To avoid derailing the NO_INPUT conversation, I have changed the
> subject to OP_CAT.
> 
> Responding to:
> """
> * `SIGHASH` flags attached to signatures are a misdesign, sadly
> retained from the original BitCoin 0.1.0 Alpha for Windows design, on
> par with:
> [..]
> * `OP_CAT` and `OP_MULT` and `OP_ADD` and friends
> [..]
> """
> 
> OP_CAT is an extremely valuable op code. I understand why it was
> removed as the situation at the time with scripts was dire. However
> most of the protocols I've wanted to build on Bitcoin run into the
> limitation that stack values can not be concatenated. For instance
> TumbleBit would have far smaller transaction sizes if OP_CAT was
> supported in Bitcoin. If it happens to me as a researcher it is
> probably holding other people back as well. If I could wave a magic
> wand and turn on one of the disabled op codes it would be OP_CAT.  Of
> course with the change that size of each concatenated value must be 64
> Bytes or less.
>

Just throwing my two cents in here - as others have noted, OP_CAT
lets you create Merkle trees (allowing e.g. log-sized accountable
threshold sigs, at least in a post-Schnorr future).

It also allows manipulating signatures - e.g. forcing the revelation
of discrete logs by requiring the user use the (1/2) point as a nonce
(this starts with 11 zero bytes, which no other computationally
accessible point does), or by requiring two sigs with the same nonce.

It also lets you do proof-of-work-like computations on hashes or
curvepoints; or enforce that EC points come from a hash and have
no known discrete log. You can also switch on hashes, something
currently impossible because of the 4-byte limitation on numeric
opcodes. I don't have specific application of these in mind but
definitely have cut off many lines of inquiry because they were
impossible.

You could build a crappy Lamport signature, though the key would
be so big that you'd never do this pre-MAST :P.


-- 
Andrew Poelstra
Director of Research, Blockstream
Email: apoelstra at wpsoftware.net
Web:   https://www.wpsoftware.net/andrew

The sun is always shining in space
    -Justin Lewis-Webster

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 488 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191009/8edaa1f9/attachment.sig>

From belcher at riseup.net  Wed Oct  9 19:32:13 2019
From: belcher at riseup.net (Chris Belcher)
Date: Wed, 9 Oct 2019 20:32:13 +0100
Subject: [bitcoin-dev] BIPable-idea: Consistent and better definition of
 the term 'address'
In-Reply-To: <58e44347-6eee-a0c3-3b8a-965c7450780e@emilengler.com>
References: <58e44347-6eee-a0c3-3b8a-965c7450780e@emilengler.com>
Message-ID: <6fe67006-7131-a861-61fa-65392d5be069@riseup.net>

This is an excellent idea and I hope something like this happens.

I've had the idea of using an intermediate name to make the transition
easier, for example "Bitcoin address" becomes "Bitcoin invoice address"
which after 10 years becomes "Bitcoin invoice" (or "Bitcoin invoice").

"Invoice" would also work well because Lightning uses the name invoice
for the object which is used to receive payments. So it's easy to
imagine that the GUI text input presented to users can be called
"invoice" and users can put both kinds of invoices there leaving the
wallet to easily figure out whether it makes an on-chain transaction or
a Lightning Network transaction.

Changing a commonly-used name like this could be very hard, but the
gains in terms of privacy are immense.

On 05/10/2019 22:57, Emil Engler via bitcoin-dev wrote:
> Hello dear mailing list subscribers.
> Before I'll explain my idea here, I need to define a term first
> 
> 'address':
> When I use the terms address, pubkey, etc., I mean the same: The Base58
> string
> 
> Ok now let's get into it:
> As you should know, sending bitcoins to an address more than once is a
> very bad approach.
> In my opinion the problem why so many people are still doing this is
> because of the term 'address' which is used in lots of wallets,
> implementations, BIP 21 and so on. It is a design issue.
> With the term 'address' most people identify things that are fixed and
> don't change really often (e.g postal address, IP address [depends on
> provider], Domain, E-Mail address, ...).
> Because of this most people compare bitcoin addresses with e-mail
> addresses and use this address to send the recipient money multiple times.
> 
> My suggestion would be to change the term address in wallets, the URI
> scheme and so on to something of the following options by a
> Informational/Process BIP:
> 
> * Payment Password
> * Transaction Password
> * ...
> 
> The guideline for the term should indicate that it is:
> * temporary
> * Something that identifies the recipient
> 
> I've chosen 'password' because they can be used as a pseudonym to
> identify a person.
> This is already used in stuff like bank transfers where something like
> the transaction id should be used as the purpose or at universities
> there are student numbers.
> The first is probably a better example because student numbers aren't
> temporary.
> 
> What do you think? Should I write a BIP for this or use another term?
> Feedback is most welcome :)
> 
> Greetings,
> Emil Engler
> 
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 

From pieter.wuille at gmail.com  Wed Oct  9 21:34:32 2019
From: pieter.wuille at gmail.com (Pieter Wuille)
Date: Wed, 9 Oct 2019 14:34:32 -0700
Subject: [bitcoin-dev] Taproot updates
Message-ID: <CAPg+sBi9CYyz7O3ToEvoDwEbykUZAW2A-jwuR0aAA769Pb0=tA@mail.gmail.com>

Hi all,

I wanted to give an update on some of the changes we've made to the
bip-schnorr/taproot/tapscript drafts following discussions on this
list:
* The original post:
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-May/016914.html
and follow-ups
* Using 2 or 4 byte indexes:
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-June/017046.html
* 32-byte public keys:
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-August/017247.html
* Resource limits:
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-September/017306.html
* P2SH support or not:
https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-September/017297.html).

We've made the following semantical changes to the proposal:
* 32-byte public keys everywhere instead of 33-byte ones: dropping one
byte that provably does not contribute to security, while remaining
compatible with existing BIP32 and other key generation algorithms.
* No more P2SH support: more efficient chain usage, no gratuitous
fungibility loss from having 2 versions, no mode limited to 80-bit
security for non-interactive multiuser constructs; however senders
will need bech32 support to send to Taproot outputs.
* 32-bit txin position and codesep position indexes instead of 16-bits ones.
* Tagged hashes also in bip-schnorr: the signature and nonce
generation now also use tagged hashes, rather than direct SHA256
(previously tagged hashes were only used in bip-taproot and
bip-tapscript)
* Dropping the 10000 byte script limit and 201 non-push opcode limit:
as no operations remain whose validation performance depends on the
size of scripts or number of executed opcodes, these limits serve no
purpose, but complicate creation of Scripts.
* Increased the limit on the depth of Merkle trees from 32 to 128: a
limit of 32 would necessitate suboptimal trees in some cases, but more
than 128 levels are only necessary when dealing with leaves that have
a chance of ~1/2^128 of being executed, which our security level
treats as impossible anyway.

See the updated documents:
* https://github.com/sipa/bips/blob/bip-schnorr/bip-schnorr.mediawiki
* https://github.com/sipa/bips/blob/bip-schnorr/bip-taproot.mediawiki
* https://github.com/sipa/bips/blob/bip-schnorr/bip-tapscript.mediawiki

In addition, a lot of clarifications and rationales were added. The
reference implementation on
https://github.com/sipa/bitcoin/commits/taproot was also updated to
reflect these changes, has a cleaner commit history now, and improved
tests (though those can still use a lot of work).

Cheers,

-- 
Pieter

From me at emilengler.com  Thu Oct 10 15:05:36 2019
From: me at emilengler.com (Emil Engler)
Date: Thu, 10 Oct 2019 17:05:36 +0200
Subject: [bitcoin-dev] BIPable-idea: Consistent and better definition of
 the term 'address'
In-Reply-To: <6fe67006-7131-a861-61fa-65392d5be069@riseup.net>
References: <58e44347-6eee-a0c3-3b8a-965c7450780e@emilengler.com>
	<6fe67006-7131-a861-61fa-65392d5be069@riseup.net>
Message-ID: <20824fa5-e3fa-8d4e-1678-4c2048b49b6b@emilengler.com>

* Sorry if this mail was sent multiple times, my E-Mail client went crazy *

Thanks for all your feedback.
I came to the decision to write a BIP for this, even if it might not be
implemented by many wallets, a standardization is never wrong and this
would be the first step in the correct direction for better on-chain
privacy.

However currently we still need a good term for the 'address' replacement.

The current suggestions are:
* Invoice ID
* Payment Token
* Bitcoin invoice (address)
* Bitcoin invoice (path)

Because of the LN term invoice I really like the term 'Bitcoin Invoice'
by Chris Belcher.

So how do find a consensus about these terms?

Greetings
Emil Engler
-------------- next part --------------
A non-text attachment was scrubbed...
Name: pEpkey.asc
Type: application/pgp-keys
Size: 3147 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191010/5b380c47/attachment.bin>

From braydon at purse.io  Thu Oct 10 16:16:16 2019
From: braydon at purse.io (Braydon Fuller)
Date: Thu, 10 Oct 2019 09:16:16 -0700
Subject: [bitcoin-dev] Chain width expansion
In-Reply-To: <CAE-z3OV_LL+Jww3e=gO6t=02VW7m9PK+8EaYoEVLy9NKNMiSaQ@mail.gmail.com>
References: <42cd5ffd-63e8-b738-c4ea-13d0699b1268@purse.io>
	<CAE-z3OV_LL+Jww3e=gO6t=02VW7m9PK+8EaYoEVLy9NKNMiSaQ@mail.gmail.com>
Message-ID: <e9c5e519-ea8a-f0e2-d8fb-c955b5c2de40@purse.io>

On 10/4/19 4:31 PM, Tier Nolan via bitcoin-dev wrote

> [..] At root, the requirement is that peers can prove their total chain POW. [...]
Indeed, it's currently necessary to receive all of the chain headers to
determine. It would be interesting to have a succinct chainwork proof
for all cases. Chainwork being a sum of the total proof-of-work in a
chain. Such proofs currently only require a few headers for common cases
and the other cases can be identified.
> In regard to your proposal, I think the key is to limit things by peer, rather than globally. [...]

Yeah, there should be enough width available for every active
connection, only one chain of headers is requested at a time per peer.
Peer based limiting is susceptible to Sybil attacks; A peer could
broadcast a few low-work header chains, reconnect and repeat ad nauseam.

The delay for the next set of headers is based on the chainwork of the
last received headers from the peer. The peer could change identity and
run into the same limit. The unrequested header rate is tracked per peer.

A header chain with more chainwork will be requested at a faster rate
than a header chain with less chainwork. The chainwork is compared to
the current fully validated chain. Honest peers with more chainwork will
have a time advantage over dishonest peers with less chainwork.

For example, let's assume a case that the initial chain of headers was
dishonest and with low chainwork. The initial block download retrieves
the header chain from a single loader peer first. Once recent time is
reached, header chains are downloaded from all outgoing peers. A single
honest peer will have an advantage over many dishonest peers. Thus, as
you have mentioned, there is a security assumption that there is at
least one connected honest node.


From karljohan-alm at garage.co.jp  Fri Oct 11 02:00:51 2019
From: karljohan-alm at garage.co.jp (Karl-Johan Alm)
Date: Fri, 11 Oct 2019 11:00:51 +0900
Subject: [bitcoin-dev] BIPable-idea: Consistent and better definition of
 the term 'address'
In-Reply-To: <20824fa5-e3fa-8d4e-1678-4c2048b49b6b@emilengler.com>
References: <58e44347-6eee-a0c3-3b8a-965c7450780e@emilengler.com>
	<6fe67006-7131-a861-61fa-65392d5be069@riseup.net>
	<20824fa5-e3fa-8d4e-1678-4c2048b49b6b@emilengler.com>
Message-ID: <CALJw2w6FeQDpiFZ9+j-tmho_HMFCyi-0wLrGqze9jD5iSLfuVw@mail.gmail.com>

I've proposed bitcoin invoice for awhile now. See
https://twitter.com/kallewoof/status/1165841566105079808

I like bitcoin invoice address into bitcoin address as proposed by Chris.


On Fri, Oct 11, 2019 at 12:45 AM Emil Engler via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
>
> * Sorry if this mail was sent multiple times, my E-Mail client went crazy *
>
> Thanks for all your feedback.
> I came to the decision to write a BIP for this, even if it might not be
> implemented by many wallets, a standardization is never wrong and this
> would be the first step in the correct direction for better on-chain
> privacy.
>
> However currently we still need a good term for the 'address' replacement.
>
> The current suggestions are:
> * Invoice ID
> * Payment Token
> * Bitcoin invoice (address)
> * Bitcoin invoice (path)
>
> Because of the LN term invoice I really like the term 'Bitcoin Invoice'
> by Chris Belcher.
>
> So how do find a consensus about these terms?
>
> Greetings
> Emil Engler
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From lloyd.fourn at gmail.com  Fri Oct 11 01:13:40 2019
From: lloyd.fourn at gmail.com (Lloyd Fournier)
Date: Fri, 11 Oct 2019 12:13:40 +1100
Subject: [bitcoin-dev] BIPable-idea: Consistent and better definition of
 the term 'address'
In-Reply-To: <20824fa5-e3fa-8d4e-1678-4c2048b49b6b@emilengler.com>
References: <58e44347-6eee-a0c3-3b8a-965c7450780e@emilengler.com>
	<6fe67006-7131-a861-61fa-65392d5be069@riseup.net>
	<20824fa5-e3fa-8d4e-1678-4c2048b49b6b@emilengler.com>
Message-ID: <CAH5Bsr2mtec+QT3wmnoSkRPwuHu2_4qn4zysqChkpFdaeqP1Cg@mail.gmail.com>

Hi Thread,

This may not be the most practical information, but there actually did
exist an almost perfect analogy for Bitcoin addresses from the ancient
world: From wikipedia https://en.wikipedia.org/wiki/Bulla_(seal)

"Transactions for trading needed to be accounted for efficiently, so the
clay tokens were placed in a clay ball (bulla), which helped with
dishonesty and kept all the tokens together. In order to account for the
tokens, the bulla would have to be crushed to reveal their content. This
introduced the idea of impressing the token onto the wet bulla before it
dried, to insure trust that the tokens hadn't been tampered with and for
anyone to know what exactly was in the bulla without having to break it."

You could only use the bulla once because it had to be destroyed in order
to get the tokens out! I think there are even examples of bulla with a kind
of "signature" on them (an imprint with the seal of a noble family etc).

"send me a Bitcoin bulla" has a nice ring to it!

Sincerely,

LL





On Fri, Oct 11, 2019 at 2:44 AM Emil Engler via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> * Sorry if this mail was sent multiple times, my E-Mail client went crazy *
>
> Thanks for all your feedback.
> I came to the decision to write a BIP for this, even if it might not be
> implemented by many wallets, a standardization is never wrong and this
> would be the first step in the correct direction for better on-chain
> privacy.
>
> However currently we still need a good term for the 'address' replacement.
>
> The current suggestions are:
> * Invoice ID
> * Payment Token
> * Bitcoin invoice (address)
> * Bitcoin invoice (path)
>
> Because of the LN term invoice I really like the term 'Bitcoin Invoice'
> by Chris Belcher.
>
> So how do find a consensus about these terms?
>
> Greetings
> Emil Engler
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191011/e2d43243/attachment-0001.html>

From arielluaces at gmail.com  Fri Oct 11 04:22:03 2019
From: arielluaces at gmail.com (Ariel Lorenzo-Luaces)
Date: Thu, 10 Oct 2019 21:22:03 -0700
Subject: [bitcoin-dev] BIPable-idea: Consistent and better definition of
	the term 'address'
In-Reply-To: <CALJw2w6FeQDpiFZ9+j-tmho_HMFCyi-0wLrGqze9jD5iSLfuVw@mail.gmail.com>
References: <58e44347-6eee-a0c3-3b8a-965c7450780e@emilengler.com>
	<6fe67006-7131-a861-61fa-65392d5be069@riseup.net>
	<20824fa5-e3fa-8d4e-1678-4c2048b49b6b@emilengler.com>
	<CALJw2w6FeQDpiFZ9+j-tmho_HMFCyi-0wLrGqze9jD5iSLfuVw@mail.gmail.com>
Message-ID: <ceb671aa-c85f-47d3-9084-aa66005640a9@gmail.com>

I would propose a term different than all the others mentioned so far.

I propose Funding Codes.

The word funding can be used as a verb or a noun and this works for the nature of Bitcoin transactions. Funding can be seen as what someone needs to do with the code as well as referring to it as the code that has funding once bitcoins have been transfered to it "give me a funding code".

The word code is fitting since codes are what addresses ultimately describe, the signature script, a piece of code. When speaking about the lightning transaction graph it's easy to talk about transactions making bitcoins flow from code to code, each code different for a different purpose "funding is sent from this code to that code" and "funding is kept in this code for 144 blocks".

- Payment tokens feel like they themselves hold the value and can be transfered but anyone can generate as many payment tokens as they desire. This name conflicts with other cryptocurrencies that focus their blockchain on payments and refer to their currency as tokens.

- Invoices are problematic because they imply that they hold bitcoins and they specify an amount. However addresses don't specify any amounts while they themselves can be included inside a real invoice. I think it is wrong to imply that the "thing" we are sending money to is temporary, because it isn't. Lightning channels can stay open forever until closed but money doesn't stay in an invoice for any amount of time.

- I actually prefer Addresses over both Payment Tokens and Invoices. It's actually very natural to send something to an address and an address can hold something for a long time. However addresses fall short due to people only having one. This makes them think that they have to memorize a bitcoin address. There are also all the other reasons people have mentioned.

The word code fits well in the divide between technical and non-technical people. To the layman a code is just a string of characters and that is what we can visually see and check and compare when one of these funding codes are transfered between two parties "does your finding code end with xyz?". To programmers code is something that can be executed which is exactly what addresses are. Especially ones with complicated logic and lots of conditions "this funding code can only be unlocked by Alice or Bob plus Charlie or Dave after 1000 blocks".

Funding codes work even better when thinking about self executing smart contracts "they are funded and running code with their funds". Contracts don't make sense at all when it's an autonomous thing that didn't strike any contract with anyone. Contracts should only be referred to the transactions people send to funding codes or "smart" codes.

Addresses also fail when transferring between two of your own wallets because it doesn't make sense for someone to have so many addresses but it does make sense for someone to have many codes.

Lightning already has "funding addresses" and "funding transactions". With schnorr and taproot coming it will be possible to create more complex situations and they all need funding codes so that funds can be sent to it and be locked up in the code (sigscript).

One criticism might be that funding codes sound like they are funding something which doesn't appear to be true. But indeed they are! Funding codes fund a situation, a setup. The common setup is that you can unlock them at any time. Other setups demand multi-party coordination. The funding code is what funds all these setups.

Funding codes are also two words and three syllables which is great because using only one word is not descriptive enough and using four or more syllables is way too long. Having the second word "code" makes for easy abbreviation when the conversation is already about Bitcoin "which code did you send them to?"

People will naturally use funding code and bitcoin code interchangeably. This is a good thing because bitcoin is a type of fund, so there is no contradiction. The more common term should still be funding code because there is more than one type of "code" in Bitcoin 

Most importantly funding codes sound good when spoken. This goes for both singular and plural.

"First a receiver must generate a funding code to have a sender fund it with their? from their own funding code which had been previously funded"

Cheers
Ariel Lorenzo-Luaces



On Oct 10, 2019, 7:20 PM, at 7:20 PM, Karl-Johan Alm via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
>I've proposed bitcoin invoice for awhile now. See
>https://twitter.com/kallewoof/status/1165841566105079808
>
>I like bitcoin invoice address into bitcoin address as proposed by
>Chris.
>
>
>On Fri, Oct 11, 2019 at 12:45 AM Emil Engler via bitcoin-dev
><bitcoin-dev at lists.linuxfoundation.org> wrote:
>>
>> * Sorry if this mail was sent multiple times, my E-Mail client went
>crazy *
>>
>> Thanks for all your feedback.
>> I came to the decision to write a BIP for this, even if it might not
>be
>> implemented by many wallets, a standardization is never wrong and
>this
>> would be the first step in the correct direction for better on-chain
>> privacy.
>>
>> However currently we still need a good term for the 'address'
>replacement.
>>
>> The current suggestions are:
>> * Invoice ID
>> * Payment Token
>> * Bitcoin invoice (address)
>> * Bitcoin invoice (path)
>>
>> Because of the LN term invoice I really like the term 'Bitcoin
>Invoice'
>> by Chris Belcher.
>>
>> So how do find a consensus about these terms?
>>
>> Greetings
>> Emil Engler
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>_______________________________________________
>bitcoin-dev mailing list
>bitcoin-dev at lists.linuxfoundation.org
>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191010/25216e06/attachment.html>

From dev at jonasschnelli.ch  Fri Oct 11 15:44:54 2019
From: dev at jonasschnelli.ch (Jonas Schnelli)
Date: Fri, 11 Oct 2019 17:44:54 +0200
Subject: [bitcoin-dev] Block Batch Filters for Light Clients
In-Reply-To: <E9935F93-B5D2-48FD-96D2-88EF605ADA4B@bitaps.com>
References: <mailman.22.1569240010.14875.bitcoin-dev@lists.linuxfoundation.org>
	<E9935F93-B5D2-48FD-96D2-88EF605ADA4B@bitaps.com>
Message-ID: <1C1A9686-C852-4513-A05C-64F518105516@jonasschnelli.ch>

Hi Aleksey

> BIP 157 unlike BIP 37 not allow apply filters to mempool and check zero confirmation transactions.
> Light client that refused to use BIP 37 due to privacy leaks can process unconfirmed transactions only one way and this is loading the entire mempool transaction flow.
> 
> https://github.com/bitaps-com/bips/blob/master/bip-mempool-transactions-filters.mediawiki <https://github.com/bitaps-com/bips/blob/master/bip-mempool-transactions-filters.mediawiki>

Instead of using a per tx filter, would it be possible to allow retrieving a complete compact filter of the whole mempool (similar to BIP35)? Maybe using a maximum size of the filter (ordered by feerate).
In general, I guess the concerns are DOS and fingerprinting.

Maybe DOS could be mitigated via a dynamic filter construction (append elements during the time between blocks, though unsure if possible).
The update-interval of a such filter could also be timebases rather than on every new tx in the mempool.

Unsure about fingerprinting defence measures.

I would expect the following process:
* peer generates mempool filter
* [timespan A (say 3 seconds)]
* light client connects to peer
* light client requests mempool-filter
* peers sends mempool filter
* light client processes filter for relevant txns
* eventually, light client sends getdata of relevant txns

a) after the initial retrieve...
* light client inspects all new txns (inv/getdata) received from peers from this point on (filterless unconfirmed tx detection)

Of if a) is to bandwidth expansive, request the mempool filter again after a timeout

/jonas

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191011/f88698ba/attachment.html>

From me at emilengler.com  Fri Oct 11 21:03:45 2019
From: me at emilengler.com (Emil Engler)
Date: Fri, 11 Oct 2019 23:03:45 +0200
Subject: [bitcoin-dev] BIPable-idea: Consistent and better definition of
 the term 'address'
In-Reply-To: <ceb671aa-c85f-47d3-9084-aa66005640a9@gmail.com>
References: <58e44347-6eee-a0c3-3b8a-965c7450780e@emilengler.com>
	<6fe67006-7131-a861-61fa-65392d5be069@riseup.net>
	<20824fa5-e3fa-8d4e-1678-4c2048b49b6b@emilengler.com>
	<CALJw2w6FeQDpiFZ9+j-tmho_HMFCyi-0wLrGqze9jD5iSLfuVw@mail.gmail.com>
	<ceb671aa-c85f-47d3-9084-aa66005640a9@gmail.com>
Message-ID: <a5c28fd3-4ffd-94f8-f80c-c55c202a3f2c@emilengler.com>

> This may not be the most practical information, but there actually did exist an almost perfect analogy for Bitcoin addresses from the ancient world: From wikipedia https://en.wikipedia.org/wiki/Bulla_(seal)

I personally do not like the term bulla, it might be a perfect analogy
but I personally don't believe that this term is well known. Tbh I
didn't knew what a 'bulla' was before.

> I propose Funding Codes.

I'm neutral on this. With code I associate something that is slightly
more permanent like a code to unlock your mobile phone or a code to
unlock your bike if you know what I mean. These kind of codes change
sometimes but not as often as a bitcoin address (should).
I also agree that Payment Tokens might confuse with other currencies and
block chain.

> 
> - Invoices are problematic because they imply that they hold bitcoins and they specify an amount. However addresses don't specify any amounts while they themselves can be included inside a real invoice. I think it is wrong to imply that the "thing" we are sending money to is temporary, because it isn't. Lightning channels can stay open forever until closed but money doesn't stay in an invoice for any amount of time.

What is with 'Bitcoin Invoice Address'?
This is the best of both worlds because it implies the temporary factor
with 'invoice' and the way that you send something to something.

Also, this is more a personal opinion but I thunk that 'funding' implies
more to donate to something. I think this could lead to misunderstandings.

To summarize it up, here are the following suggested terms:
* Invoice ID
* Payment Token
* Bitcoin invoice (address)
* Bitcoin invoice (path)
* Bulla
* Funding code

Greetings,
Emil Engler
-------------- next part --------------
A non-text attachment was scrubbed...
Name: pEpkey.asc
Type: application/pgp-keys
Size: 3147 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191011/5cac5674/attachment-0001.bin>

From braydon at purse.io  Fri Oct 11 21:24:27 2019
From: braydon at purse.io (Braydon Fuller)
Date: Fri, 11 Oct 2019 14:24:27 -0700
Subject: [bitcoin-dev] Chain width expansion
In-Reply-To: <20191004082031.ns3pgzwh2zz2mxyc@ganymede>
References: <42cd5ffd-63e8-b738-c4ea-13d0699b1268@purse.io>
	<20191004082031.ns3pgzwh2zz2mxyc@ganymede>
Message-ID: <69e6c777-ae56-0f33-85a6-84108a38e1ab@purse.io>

On 10/4/19 1:20 AM, David A. Harding wrote:

> On Thu, Oct 03, 2019 at 05:38:36PM -0700, Braydon Fuller via bitcoin-dev wrote:
>> This paper describes a solution [to DoS attacks] that does not
>> require enabling or maintaining checkpoints and provides improved security.
>> [...] 
>> The paper is available at:
>> https://bcoin.io/papers/bitcoin-chain-expansion.pdf
> Hi Braydon,
>
> Thank you for researching this important issue.  An alternative solution
> proposed some time ago (I believe originally by Gregory Maxwell) was a
> soft fork to raise the minimum difficulty.  You can find discussion of
> it in various old IRC conversations[1,2] as well as in related changes
> to Bitcoin Core such as PR #9053 addining minimum chain work[3] and the
> assumed-valid change added in Bitcoin Core 0.14.0[4].
>
> [1] http://www.erisian.com.au/meetbot/bitcoin-core-dev/2016/bitcoin-core-dev.2016-10-27-19.01.log.html#l-121
> [2] http://www.erisian.com.au/meetbot/bitcoin-core-dev/2017/bitcoin-core-dev.2017-03-02-19.01.log.html#l-57
> [3] https://github.com/bitcoin/bitcoin/pull/9053/commits/fd46136dfaf68a7046cf7b8693824d73ac6b1caf
> [4] https://bitcoincore.org/en/2017/03/08/release-0.14.0/#assumed-valid-blocks
>
Okay, here is an overview of what I have found for the minimum
difficulty proposal:

It describes having a new consensus rule to not fork or accept headers
prior to, or below, a minimum difficulty once the best chain work is
achieved at release time of the software. This would be instead of the
rule to not fork before the last checkpoint, as checkpoints are removed.

It has an advantage to the existing checkpoint solution as it does not
require checkpoints to be enabled. This is not a surprise as the
proposal was to remove checkpoints entirely. It would increase the cost
of the attack without checkpoints. Long header chains would need to be
built using this minimum difficulty, instead of the current lowest
difficulty of the genesis block. The exact cost of that is not yet
calculated.

There are a few caveats with the approach mentioned; nodes are
vulnerable if the initial loader peer is the attacker, it could leave
minority hashpower without an ability to softfork away during a
contentious hardfork, and requires period consensus changes to continue
to maintain:
? - Nodes are vulnerable during the initial sync when joining the
network until the minimum chainwork is achieved. This is possible if the
loader peer is the attacker. To mitigate this there would need to be a
minimum chainwork defined based on the current chainwork. However, such
could also be used to prevent nodes from joining the network as it's
rejecting rather that throttling.
? - A contentious hardfork could leave a minority hashpower without an
ability to softfork away without agreeing on a hardfork. This was the
reason why the minimum difficulty was about 10 devices instead of 10,000.
? - It's technically a consensus change each time the minimum difficulty
or best chainwork is updated. It is a similar consensus change as
maintaining the last checkpoint, as it's used to prevent forking prior
to the last checkpoint.

I think the solution proposed in the Bitcoin Chain Width Expansion paper
solves those issues by limiting chain width and throttling based on
chainwork, instead of rejecting blocks based on the minimum difficulty.



From tier.nolan at gmail.com  Sat Oct 12 16:27:42 2019
From: tier.nolan at gmail.com (Tier Nolan)
Date: Sat, 12 Oct 2019 17:27:42 +0100
Subject: [bitcoin-dev] Chain width expansion
In-Reply-To: <e9c5e519-ea8a-f0e2-d8fb-c955b5c2de40@purse.io>
References: <42cd5ffd-63e8-b738-c4ea-13d0699b1268@purse.io>
	<CAE-z3OV_LL+Jww3e=gO6t=02VW7m9PK+8EaYoEVLy9NKNMiSaQ@mail.gmail.com>
	<e9c5e519-ea8a-f0e2-d8fb-c955b5c2de40@purse.io>
Message-ID: <CAE-z3OXyTc0aoJJVNLS5MReE7+Nhckyrjf22+yCSjXF8=bNbXQ@mail.gmail.com>

On Thu, Oct 10, 2019 at 5:20 PM Braydon Fuller via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

>  It would be interesting to have a succinct chainwork proof
> for all cases. Chainwork being a sum of the total proof-of-work in a
> chain. Such proofs currently only require a few headers for common cases
> and the other cases can be identified.
>

I wonder if a "seed" based system would be useful.

A seed is defined as a header with a very low digest.

When a new peer connects, you ask him to send you the header with the
lowest digest on his main chain.

Chains ending at the strongest seeds are kept preferentially when
discarding chains.

This requires a way to download chains backwards, which the protocol
doesn't support at the moment.

The most chain work chain is overwhelmingly likely to contain the header
with the strongest digest.

This means that the honest peer's chain would be kept preferentially.

It also means that a node that is synced to the main chain can easily
discard noise from dishonest peers.  Before downloading, they could ask the
peer to provide a header with at least 1% of the POW of the best header on
the main chain starting at the fork point.  If they can't then their fork
probably has less POW than the main chain.


> A peer could
> broadcast a few low-work header chains, reconnect and repeat ad nauseam.
>

I meant connected peer rather than peer.  If a peer disconnects and then
reconnects as a new peer, then their allocation of bandwidth/RAM resets to
zero.

Each peer would be allocated a certain bandwidth per minute for headers as
in a token bucket system.   New peers would start with empty buckets.

If an active (outgoing) peer is building on a header chain, then that chain
is preferentially kept.  Essentially, the last chain that each outgoing
peer built on may not be discarded.

In retrospect, that works out as the same as throttling peer download, just
with a different method for throttling.

In your system, peers who extend the best chain don't get throttled, but
the other peers do (but with a gradual transition).

This could be accomplished by adding 80 bytes into the peers bucket if it
extends the main chain.


> For example, let's assume a case that the initial chain of headers was
> dishonest and with low chainwork. The initial block download retrieves
> the header chain from a single loader peer first. Once recent time is
> reached, header chains are downloaded from all outgoing peers.


The key it that it must not be possible to prevent a single honest peer
from making progress by flooding with other peers and getting the honest
peer's chain discarded.

I think parallel downloading would be better than focusing on one peer
initially.  Otherwise, a dishonest peer can slowly send their headers to
prevent moving to parallel mode.

Each connected peer is given a bandwidth and RAM allowance.  If a connected
peer forks off their own chain before reaching current time, then the fork
is just discarded.

The RAM allowance would be sufficient to hold one header per minute since
genesis.

The header chains are relatively small (50MB), so it is not unreasonable to
expect the honest peer to send the entire chain in one go.

I wonder if there is a formula that gives the minimum chain work required
to have a particular chain length by now.

1 minute per header would mean that the difficulty would increase every
adjustment, so it couldn't be maintained without an exponentially rising
total chain work.

On Sat, Oct 12, 2019 at 2:41 AM Braydon Fuller via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

>   - Nodes are vulnerable during the initial sync when joining the
> network until the minimum chainwork is achieved.


Nodes should stay "headers-only" until they have hit the threshold.

It isn't really any different from a checkpoint anyway.

Download headers until you hit this header is about the same as download
headers until you hit this chain work.

It would be different if header chains were downloaded from the final
checkpoint backwards.

You would start at a final checkpoint and work backwards.  Each ancestor
header is committed to by the final checkpoint, so it would not be possible
a dishonest peer to fool the node during IBD.


> This is possible if the
> loader peer is the attacker. To mitigate this there would need to be a
> minimum chainwork defined based on the current chainwork. However, such
> could also be used to prevent nodes from joining the network as it's
> rejecting rather that throttling.
>

I think mixing two different concepts makes this problem more complex than
needed.

It looks like they are aiming for hard-coding

A) "The main chain has at least C chainwork"
B) "All blocks after A is satisfied have at least X POW"

To me, this is equivalent to a checkpoint, without it having it be called a
checkpoint.

The point about excluding checkpoints is that it means that (in theory) two
clients can't end up on incompatible forks due to different checkpoints.

The "checkpoint" is replaced by a statement by the dev team that

"There exists at least one valid chain with C chainwork"

which is equivalent to

"The longest valid chain has at least C chainwork"

Two client making those statements can't cause a permanent
incompatibility.  If they pick a different C, then eventually, once the
main chain has more than the larger chain work, they will agree again.

Checkpoints don't automatically heal.

Adding in a minimum POW requirement could break the requirement for that to
happen.

Just because B was met on the original main chain, a fork isn't required to
meet it.

  - It's technically a consensus change each time the minimum difficulty
> or best chainwork is updated. It is a similar consensus change as
> maintaining the last checkpoint, as it's used to prevent forking prior
> to the last checkpoint.
>

I agree on the min difficulty being a consensus change.

The minimum chain work is just the devs making a true statement and then
using it to optimize things.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191012/24bb4eaa/attachment.html>

From admin at bitaps.com  Sat Oct 12 13:46:43 2019
From: admin at bitaps.com (admin at bitaps.com)
Date: Sat, 12 Oct 2019 17:46:43 +0400
Subject: [bitcoin-dev] Block Batch Filters for Light Clients (Jonas
	Schnelli)
In-Reply-To: <mailman.4271.1570844414.8631.bitcoin-dev@lists.linuxfoundation.org>
References: <mailman.4271.1570844414.8631.bitcoin-dev@lists.linuxfoundation.org>
Message-ID: <D8F63ECB-83D6-4F98-BEE3-4E467D8E2379@bitaps.com>

Hi Jonas,

Let's review case when we create filters for part of mempool or whole mempool. 

After new block is mined we have to verify what transactions is confirmed from mempool. Mempool filter in design with set of transactions make impossible use it do block filter reconstruction because we have no mapping filter elements with transactions.
This means that we need to download block filter to check what exactly transactions are affected.

Which does not give us advantages in terms of traffic savings.

The idea of draft for mempool transactions filters is make possible to reconstruct correct block filters using unconfirmed tx filters and archive additional savings.

At this moment I  am working on Block Batch Filters implementation and have same changes which will be updated in drafts as soon as I complete all tests. Regarding unconfirmed TX filters, I came to the conclusion that the filter
should be the first 6 bytes of the address hash, this takes up a bit more space, but allows to construct block filter.



What do you mean by  fingerprinting defence ?

Please let me know what, in your opinion, are the disadvantages of the per tx filter approach?

Why use whole/part mempool is better?


With regards Aleksey

 

 

> 12 ???. 2019 ?., ? 5:40, bitcoin-dev-request at lists.linuxfoundation.org ???????(?):
> 
> Re: Block Batch Filters for Light Clients (Jonas Schnelli)

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191012/2d5f3da2/attachment-0001.html>

From tier.nolan at gmail.com  Sat Oct 12 20:46:40 2019
From: tier.nolan at gmail.com (Tier Nolan)
Date: Sat, 12 Oct 2019 21:46:40 +0100
Subject: [bitcoin-dev] Chain width expansion
In-Reply-To: <H_Yq1W3SffFweLPPXiUiA4EeU2yU7c8LVcqw5AbajovWTWMt5hKQARKglEQwCjPpXvjiBfvmTnaXJwivkGkT8BDha8k303DNbFB-ECes0d4=@protonmail.com>
References: <42cd5ffd-63e8-b738-c4ea-13d0699b1268@purse.io>
	<CAE-z3OV_LL+Jww3e=gO6t=02VW7m9PK+8EaYoEVLy9NKNMiSaQ@mail.gmail.com>
	<e9c5e519-ea8a-f0e2-d8fb-c955b5c2de40@purse.io>
	<CAE-z3OXyTc0aoJJVNLS5MReE7+Nhckyrjf22+yCSjXF8=bNbXQ@mail.gmail.com>
	<H_Yq1W3SffFweLPPXiUiA4EeU2yU7c8LVcqw5AbajovWTWMt5hKQARKglEQwCjPpXvjiBfvmTnaXJwivkGkT8BDha8k303DNbFB-ECes0d4=@protonmail.com>
Message-ID: <CAE-z3OVqU5n8H9vKj9Pn7k80guMTsxt_CkN9qwpfCK8HB4PDgQ@mail.gmail.com>

On Sat, Oct 12, 2019 at 6:56 PM Joachim Str?mbergson <
joachimstr at protonmail.com> wrote:

> I like the backwards syncing idea. First you provide proof of your best
> block height via coinbase, then sync backwards. It solves lots of related
> problems. You know how much you can expect from the given peer.
>

It shows you which nodes are on the same chain too.

If you have 8 peers and you ask the 8 of them for their 8 best, then they
should all agree on most of them.

You can then ask each of the 8 to start sending you headers backwards from
one of the 8 seeds.

They will all roughly split the chain into 8 equal pieces, though the split
will be based on work rather than header height.

If there is disagreement, you can give priority to the node(s) with the
lowest headers until they have completed their download.

It requires a network protocol change to allow reverse block downloads
though (and messages to indicate lowest headers etc.)

On different note, one of the problems that I haven't seen mentioned here
> yet is the timewarp attack. It is relevant to some of the proposed
> solutions. It should be possible, IIRC, for a malicious node to generate
> much longer chain with superslow timestamp increase (~5 blocks in 1 second)
> without increasing difficulty (i.e. staying at min. diff.). This could
> produce chain that is ~2500 times longer than main chain without having
> multiple branches.
>

That is a good point.  It answers my question about formula for maximum
number of blocks.

5 * 60 * 60 * 24 * 365 = 157,680,000

That's around 150 million blocks per year at that rate.

I assume the 5 per second limit is that it is greater that the median of
the last 11 rather than greater or equal?

The timewarp bug can be fixed by a basic soft fork.  You just need to limit
the maximum difference between the timestamp for the first header in a
period and the last header in the previous period.

An alternative would be to soft fork in a maximum block rate.  In addition
to the current rules, you could limit it to a maximum of 1 block every 2
mins.  That rule shouldn't activate normally.

   block.height <= (block.timestamp - genesis.timestamp) / (2 mins)

It could have some weird incentives if it actually activated though.
Miners would have to shutdown mining if they were finding blocks to fast.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191012/557b7e19/attachment.html>

From joachimstr at protonmail.com  Sat Oct 12 17:56:51 2019
From: joachimstr at protonmail.com (=?UTF-8?Q?Joachim_Str=C3=B6mbergson?=)
Date: Sat, 12 Oct 2019 17:56:51 +0000
Subject: [bitcoin-dev] Chain width expansion
In-Reply-To: <CAE-z3OXyTc0aoJJVNLS5MReE7+Nhckyrjf22+yCSjXF8=bNbXQ@mail.gmail.com>
References: <42cd5ffd-63e8-b738-c4ea-13d0699b1268@purse.io>
	<CAE-z3OV_LL+Jww3e=gO6t=02VW7m9PK+8EaYoEVLy9NKNMiSaQ@mail.gmail.com>
	<e9c5e519-ea8a-f0e2-d8fb-c955b5c2de40@purse.io>
	<CAE-z3OXyTc0aoJJVNLS5MReE7+Nhckyrjf22+yCSjXF8=bNbXQ@mail.gmail.com>
Message-ID: <H_Yq1W3SffFweLPPXiUiA4EeU2yU7c8LVcqw5AbajovWTWMt5hKQARKglEQwCjPpXvjiBfvmTnaXJwivkGkT8BDha8k303DNbFB-ECes0d4=@protonmail.com>

I like the backwards syncing idea. First you provide proof of your best block height via coinbase, then sync backwards. It solves lots of related problems. You know how much you can expect from the given peer.

On different note, one of the problems that I haven't seen mentioned here yet is the timewarp attack. It is relevant to some of the proposed solutions. It should be possible, IIRC, for a malicious node to generate much longer chain with superslow timestamp increase (~5 blocks in 1 second) without increasing difficulty (i.e. staying at min. diff.). This could produce chain that is ~2500 times longer than main chain without having multiple branches.

I also agree that there is no big difference between hash checkpoints and "min. diff. checkpoints".

Joachim

Sent with [ProtonMail](https://protonmail.com) Secure Email.

??????? Original Message ???????
On Saturday, October 12, 2019 4:27 PM, Tier Nolan via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:

> On Thu, Oct 10, 2019 at 5:20 PM Braydon Fuller via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>>  It would be interesting to have a succinct chainwork proof
>> for all cases. Chainwork being a sum of the total proof-of-work in a
>> chain. Such proofs currently only require a few headers for common cases
>> and the other cases can be identified.
>
> I wonder if a "seed" based system would be useful.
>
> A seed is defined as a header with a very low digest.
>
> When a new peer connects, you ask him to send you the header with the lowest digest on his main chain.
>
> Chains ending at the strongest seeds are kept preferentially when discarding chains.
>
> This requires a way to download chains backwards, which the protocol doesn't support at the moment.
>
> The most chain work chain is overwhelmingly likely to contain the header with the strongest digest.
>
> This means that the honest peer's chain would be kept preferentially.
>
> It also means that a node that is synced to the main chain can easily discard noise from dishonest peers.  Before downloading, they could ask the peer to provide a header with at least 1% of the POW of the best header on the main chain starting at the fork point.  If they can't then their fork probably has less POW than the main chain.
>
>> A peer could
>> broadcast a few low-work header chains, reconnect and repeat ad nauseam.
>
> I meant connected peer rather than peer.  If a peer disconnects and then reconnects as a new peer, then their allocation of bandwidth/RAM resets to zero.
>
> Each peer would be allocated a certain bandwidth per minute for headers as in a token bucket system.   New peers would start with empty buckets.
>
> If an active (outgoing) peer is building on a header chain, then that chain is preferentially kept.  Essentially, the last chain that each outgoing peer built on may not be discarded.
>
> In retrospect, that works out as the same as throttling peer download, just with a different method for throttling.
>
> In your system, peers who extend the best chain don't get throttled, but the other peers do (but with a gradual transition).
>
> This could be accomplished by adding 80 bytes into the peers bucket if it extends the main chain.
>
>> For example, let's assume a case that the initial chain of headers was
>> dishonest and with low chainwork. The initial block download retrieves
>> the header chain from a single loader peer first. Once recent time is
>> reached, header chains are downloaded from all outgoing peers.
>
> The key it that it must not be possible to prevent a single honest peer from making progress by flooding with other peers and getting the honest peer's chain discarded.
>
> I think parallel downloading would be better than focusing on one peer initially.  Otherwise, a dishonest peer can slowly send their headers to prevent moving to parallel mode.
>
> Each connected peer is given a bandwidth and RAM allowance.  If a connected peer forks off their own chain before reaching current time, then the fork is just discarded.
>
> The RAM allowance would be sufficient to hold one header per minute since genesis.
>
> The header chains are relatively small (50MB), so it is not unreasonable to expect the honest peer to send the entire chain in one go.
>
> I wonder if there is a formula that gives the minimum chain work required to have a particular chain length by now.
>
> 1 minute per header would mean that the difficulty would increase every adjustment, so it couldn't be maintained without an exponentially rising total chain work.
>
> On Sat, Oct 12, 2019 at 2:41 AM Braydon Fuller via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>>   - Nodes are vulnerable during the initial sync when joining the
>> network until the minimum chainwork is achieved.
>
> Nodes should stay "headers-only" until they have hit the threshold.
>
> It isn't really any different from a checkpoint anyway.
>
> Download headers until you hit this header is about the same as download headers until you hit this chain work.
>
> It would be different if header chains were downloaded from the final checkpoint backwards.
>
> You would start at a final checkpoint and work backwards.  Each ancestor header is committed to by the final checkpoint, so it would not be possible a dishonest peer to fool the node during IBD.
>
>> This is possible if the
>> loader peer is the attacker. To mitigate this there would need to be a
>> minimum chainwork defined based on the current chainwork. However, such
>> could also be used to prevent nodes from joining the network as it's
>> rejecting rather that throttling.
>
> I think mixing two different concepts makes this problem more complex than needed.
>
> It looks like they are aiming for hard-coding
>
> A) "The main chain has at least C chainwork"
> B) "All blocks after A is satisfied have at least X POW"
>
> To me, this is equivalent to a checkpoint, without it having it be called a checkpoint.
>
> The point about excluding checkpoints is that it means that (in theory) two clients can't end up on incompatible forks due to different checkpoints.
>
> The "checkpoint" is replaced by a statement by the dev team that
>
> "There exists at least one valid chain with C chainwork"
>
> which is equivalent to
>
> "The longest valid chain has at least C chainwork"
>
> Two client making those statements can't cause a permanent incompatibility.  If they pick a different C, then eventually, once the main chain has more than the larger chain work, they will agree again.
>
> Checkpoints don't automatically heal.
>
> Adding in a minimum POW requirement could break the requirement for that to happen.
>
> Just because B was met on the original main chain, a fork isn't required to meet it.
>
>>   - It's technically a consensus change each time the minimum difficulty
>> or best chainwork is updated. It is a similar consensus change as
>> maintaining the last checkpoint, as it's used to prevent forking prior
>> to the last checkpoint.
>
> I agree on the min difficulty being a consensus change.
>
> The minimum chain work is just the devs making a true statement and then using it to optimize things.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191012/d2a9a41c/attachment-0001.html>

From karljohan-alm at garage.co.jp  Tue Oct 15 02:54:28 2019
From: karljohan-alm at garage.co.jp (Karl-Johan Alm)
Date: Tue, 15 Oct 2019 11:54:28 +0900
Subject: [bitcoin-dev] Is Signet Bitcoin?
Message-ID: <CALJw2w5h3jjKAtvzwup=wbGWU7Yf=byuJzzvJm16CMXQAoTmtg@mail.gmail.com>

Hello,

The pull request to the bips repository for Signet has stalled, as the
maintainer isn't sure Signet should have a BIP at all, i.e. "is Signet
Bitcoin?".

My argument is that Signet is indeed Bitcoin and should have a BIP, as
this facilitates the interoperability between different software in
the Bitcoin space.

Feedback welcome, here or on the pull request itself:
https://github.com/bitcoin/bips/pull/803

From lf-lists at mattcorallo.com  Tue Oct 15 05:33:53 2019
From: lf-lists at mattcorallo.com (Matt Corallo)
Date: Mon, 14 Oct 2019 22:33:53 -0700
Subject: [bitcoin-dev] Is Signet Bitcoin?
In-Reply-To: <CALJw2w5h3jjKAtvzwup=wbGWU7Yf=byuJzzvJm16CMXQAoTmtg@mail.gmail.com>
References: <CALJw2w5h3jjKAtvzwup=wbGWU7Yf=byuJzzvJm16CMXQAoTmtg@mail.gmail.com>
Message-ID: <DC6D6BE7-8924-4E7D-B40B-D0FE3BB1A057@mattcorallo.com>

Indeed, Signet is no less (or more) Bitcoin than a seed format or BIP 32. It?s ?not Bitcoin? but it?s certainly ?interoperability for how to build good testing for Bitcoin?.

> On Oct 14, 2019, at 19:55, Karl-Johan Alm via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
> 
> ?Hello,
> 
> The pull request to the bips repository for Signet has stalled, as the
> maintainer isn't sure Signet should have a BIP at all, i.e. "is Signet
> Bitcoin?".
> 
> My argument is that Signet is indeed Bitcoin and should have a BIP, as
> this facilitates the interoperability between different software in
> the Bitcoin space.
> 
> Feedback welcome, here or on the pull request itself:
> https://github.com/bitcoin/bips/pull/803
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


From braydon at purse.io  Tue Oct 15 00:38:55 2019
From: braydon at purse.io (Braydon Fuller)
Date: Mon, 14 Oct 2019 17:38:55 -0700
Subject: [bitcoin-dev] Chain width expansion
In-Reply-To: <CAE-z3OXyTc0aoJJVNLS5MReE7+Nhckyrjf22+yCSjXF8=bNbXQ@mail.gmail.com>
References: <42cd5ffd-63e8-b738-c4ea-13d0699b1268@purse.io>
	<CAE-z3OV_LL+Jww3e=gO6t=02VW7m9PK+8EaYoEVLy9NKNMiSaQ@mail.gmail.com>
	<e9c5e519-ea8a-f0e2-d8fb-c955b5c2de40@purse.io>
	<CAE-z3OXyTc0aoJJVNLS5MReE7+Nhckyrjf22+yCSjXF8=bNbXQ@mail.gmail.com>
Message-ID: <e71301df-dd2b-c30c-129e-430d344fa6c4@purse.io>

On 10/12/19 9:27 AM, Tier Nolan via bitcoin-dev wrote:

> [...]
>
> I think parallel downloading would be better than focusing on one peer
> initially.  Otherwise, a dishonest peer can slowly send their headers to
> prevent moving to parallel mode.
>
> [...]

As implemented, there is a timeout for that loader peer based on the
amount of time it should take to request all the headers. The time
period is defined as a base time plus the number of expected headers
times an expected amount of time per header. For example, the timeout
would be 25 minutes with a base time of 15 minutes, 1 millisecond per
header and an expected 600000 headers.



From braydon at purse.io  Tue Oct 15 00:42:06 2019
From: braydon at purse.io (Braydon Fuller)
Date: Mon, 14 Oct 2019 17:42:06 -0700
Subject: [bitcoin-dev] Chain width expansion
In-Reply-To: <H_Yq1W3SffFweLPPXiUiA4EeU2yU7c8LVcqw5AbajovWTWMt5hKQARKglEQwCjPpXvjiBfvmTnaXJwivkGkT8BDha8k303DNbFB-ECes0d4=@protonmail.com>
References: <42cd5ffd-63e8-b738-c4ea-13d0699b1268@purse.io>
	<CAE-z3OV_LL+Jww3e=gO6t=02VW7m9PK+8EaYoEVLy9NKNMiSaQ@mail.gmail.com>
	<e9c5e519-ea8a-f0e2-d8fb-c955b5c2de40@purse.io>
	<CAE-z3OXyTc0aoJJVNLS5MReE7+Nhckyrjf22+yCSjXF8=bNbXQ@mail.gmail.com>
	<H_Yq1W3SffFweLPPXiUiA4EeU2yU7c8LVcqw5AbajovWTWMt5hKQARKglEQwCjPpXvjiBfvmTnaXJwivkGkT8BDha8k303DNbFB-ECes0d4=@protonmail.com>
Message-ID: <93649df9-27ab-abaf-00f3-da6c528344cc@purse.io>

On 10/12/19 10:56 AM, Joachim Str?mbergson via bitcoin-dev wrote:

> [...] First you provide proof of your best block height via coinbase [...]

So I don't think you can use the height in the coinbase for that
purpose, as it's not possible to validate it without the previous
headers. That's common for more than just the height.

> [...] to generate much longer chain with superslow timestamp increase (~5 blocks in 1 second) without increasing difficulty (i.e. staying at min. diff.). [...]

In that case, it would take about 7 minutes of block time seconds for
the next retarget period, every 2016 blocks, and the difficulty would
adjust. The difficulty would adjust in that case as if 2 weeks of blocks
had been mined in 7 minutes. For the difficulty to remain the same the
time between blocks needs to be 10 minutes.



From junderwood at bitcoinbank.co.jp  Tue Oct 15 04:38:29 2019
From: junderwood at bitcoinbank.co.jp (Jonathan Underwood)
Date: Tue, 15 Oct 2019 13:38:29 +0900
Subject: [bitcoin-dev] Is Signet Bitcoin?
In-Reply-To: <CALJw2w5h3jjKAtvzwup=wbGWU7Yf=byuJzzvJm16CMXQAoTmtg@mail.gmail.com>
References: <CALJw2w5h3jjKAtvzwup=wbGWU7Yf=byuJzzvJm16CMXQAoTmtg@mail.gmail.com>
Message-ID: <CAMpN3mJ67BpGk-WGJLMcaOEfcLPQrrjEnygqQHmR8xfS5fQC=A@mail.gmail.com>

I would also like to agree that Signet should be a BIP.

Problem: Testnet is unreliable. *Testnet is used often for development of
Bitcoin*.
Proposal: To improve the dev environment for Bitcoin, let's create a new
kind of testnet that is more reliable.

I would also like to hear the logic behind "Testnet is Bitcoin" but "Signet
is not Bitcoin"... both are not 100% compatible with mainnet consensus
processes.

-Jon
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191015/c00dcd1e/attachment-0001.html>

From joachimstr at protonmail.com  Tue Oct 15 07:20:07 2019
From: joachimstr at protonmail.com (=?UTF-8?Q?Joachim_Str=C3=B6mbergson?=)
Date: Tue, 15 Oct 2019 07:20:07 +0000
Subject: [bitcoin-dev] Chain width expansion
In-Reply-To: <93649df9-27ab-abaf-00f3-da6c528344cc@purse.io>
References: <42cd5ffd-63e8-b738-c4ea-13d0699b1268@purse.io>
	<CAE-z3OV_LL+Jww3e=gO6t=02VW7m9PK+8EaYoEVLy9NKNMiSaQ@mail.gmail.com>
	<e9c5e519-ea8a-f0e2-d8fb-c955b5c2de40@purse.io>
	<CAE-z3OXyTc0aoJJVNLS5MReE7+Nhckyrjf22+yCSjXF8=bNbXQ@mail.gmail.com>
	<H_Yq1W3SffFweLPPXiUiA4EeU2yU7c8LVcqw5AbajovWTWMt5hKQARKglEQwCjPpXvjiBfvmTnaXJwivkGkT8BDha8k303DNbFB-ECes0d4=@protonmail.com>
	<93649df9-27ab-abaf-00f3-da6c528344cc@purse.io>
Message-ID: <yKuDn4LKCA9-olIsgnJO7VqN3RE9BgZWEzLu6-k4wG30iHYDIJa6wjTBoYXU0zWCRyNCEvBb4xQw7qp0BDGBCR4zccI6kJ_4AnlsB9vc9L4=@protonmail.com>


> > [...] First you provide proof of your best block height via coinbase [...]
>
> So I don't think you can use the height in the coinbase for that
> purpose, as it's not possible to validate it without the previous
> headers. That's common for more than just the height.

You can fake it of course, but it means you are spending money for PoW and I can't see viable attack here. You can commit to either lower height than actual or higher height, if you are malicious. If it is lower, then your chain will look weaker with some heuristic based on PoW of the tip and the chain length. So you are not going to do that. It thus seem it only makes sense to commit to higher than actual height. OK, this could convince me to be interested in your chain over others. So let's say the real chain is 600k blocks, you claim 1m blocks, and you prove 1m height block to me. So I can ask you about

1) 2000 headers from the tip down
2) AND another proof of height for the block at 1m-2000.

To be able to provide that you need to have such a chain and you can reuse any real subchain from mainnet. It's still possible that you can deliver that, but not for high difficulty.

Now if time warp attack is blocked, you will have pretty hard time to create such a chain that would look strongest in cumulative work. I don't have actual numbers though, so I can't say this would mitigate everything.


> > [...] to generate much longer chain with superslow timestamp increase (~5 blocks in 1 second) without increasing difficulty (i.e. staying at min. diff.). [...]
>
> In that case, it would take about 7 minutes of block time seconds for
> the next retarget period, every 2016 blocks, and the difficulty would
> adjust. The difficulty would adjust in that case as if 2 weeks of blocks
> had been mined in 7 minutes. For the difficulty to remain the same the
> time between blocks needs to be 10 minutes.

This calculation does not apply under time warp attack. You can fake timestamps of all blocks except for those relevant to the retarget calculation. Those are only the first and the last block in the 2016 block window.

From braydon at purse.io  Tue Oct 15 08:12:09 2019
From: braydon at purse.io (Braydon Fuller)
Date: Tue, 15 Oct 2019 01:12:09 -0700
Subject: [bitcoin-dev] Chain width expansion
In-Reply-To: <yKuDn4LKCA9-olIsgnJO7VqN3RE9BgZWEzLu6-k4wG30iHYDIJa6wjTBoYXU0zWCRyNCEvBb4xQw7qp0BDGBCR4zccI6kJ_4AnlsB9vc9L4=@protonmail.com>
References: <42cd5ffd-63e8-b738-c4ea-13d0699b1268@purse.io>
	<CAE-z3OV_LL+Jww3e=gO6t=02VW7m9PK+8EaYoEVLy9NKNMiSaQ@mail.gmail.com>
	<e9c5e519-ea8a-f0e2-d8fb-c955b5c2de40@purse.io>
	<CAE-z3OXyTc0aoJJVNLS5MReE7+Nhckyrjf22+yCSjXF8=bNbXQ@mail.gmail.com>
	<H_Yq1W3SffFweLPPXiUiA4EeU2yU7c8LVcqw5AbajovWTWMt5hKQARKglEQwCjPpXvjiBfvmTnaXJwivkGkT8BDha8k303DNbFB-ECes0d4=@protonmail.com>
	<93649df9-27ab-abaf-00f3-da6c528344cc@purse.io>
	<yKuDn4LKCA9-olIsgnJO7VqN3RE9BgZWEzLu6-k4wG30iHYDIJa6wjTBoYXU0zWCRyNCEvBb4xQw7qp0BDGBCR4zccI6kJ_4AnlsB9vc9L4=@protonmail.com>
Message-ID: <7b0d0bff-d898-10ff-5fdb-c982b82054a1@purse.io>

On 10/15/19 12:20 AM, Joachim Str?mbergson wrote:

>>> [...] to generate much longer chain with superslow timestamp increase (~5 blocks in 1 second) without increasing difficulty (i.e. staying at min. diff.). [...]
>> In that case, it would take about 7 minutes of block time seconds for
>> the next retarget period, every 2016 blocks, and the difficulty would
>> adjust. The difficulty would adjust in that case as if 2 weeks of blocks
>> had been mined in 7 minutes. For the difficulty to remain the same the
>> time between blocks needs to be 10 minutes.
> This calculation does not apply under time warp attack. You can fake timestamps of all blocks except for those relevant to the retarget calculation. Those are only the first and the last block in the 2016 block window.

This must be in reference to the non-overlapping difficulty calculation
and off-by-one bug?



From joachimstr at protonmail.com  Tue Oct 15 15:50:29 2019
From: joachimstr at protonmail.com (=?UTF-8?Q?Joachim_Str=C3=B6mbergson?=)
Date: Tue, 15 Oct 2019 15:50:29 +0000
Subject: [bitcoin-dev] Chain width expansion
In-Reply-To: <7b0d0bff-d898-10ff-5fdb-c982b82054a1@purse.io>
References: <42cd5ffd-63e8-b738-c4ea-13d0699b1268@purse.io>
	<CAE-z3OV_LL+Jww3e=gO6t=02VW7m9PK+8EaYoEVLy9NKNMiSaQ@mail.gmail.com>
	<e9c5e519-ea8a-f0e2-d8fb-c955b5c2de40@purse.io>
	<CAE-z3OXyTc0aoJJVNLS5MReE7+Nhckyrjf22+yCSjXF8=bNbXQ@mail.gmail.com>
	<H_Yq1W3SffFweLPPXiUiA4EeU2yU7c8LVcqw5AbajovWTWMt5hKQARKglEQwCjPpXvjiBfvmTnaXJwivkGkT8BDha8k303DNbFB-ECes0d4=@protonmail.com>
	<93649df9-27ab-abaf-00f3-da6c528344cc@purse.io>
	<yKuDn4LKCA9-olIsgnJO7VqN3RE9BgZWEzLu6-k4wG30iHYDIJa6wjTBoYXU0zWCRyNCEvBb4xQw7qp0BDGBCR4zccI6kJ_4AnlsB9vc9L4=@protonmail.com>
	<7b0d0bff-d898-10ff-5fdb-c982b82054a1@purse.io>
Message-ID: <V_DCmFvCt-jxYr-fmfwy9JvDApaJWrBz9_QESEs85FkBjiq579w8SOSex7I_aJyL_cN2idZ3SuR9qt0Eer456GD6eV7BAlq0k9Em5JIOPmM=@protonmail.com>


> > > > [...] to generate much longer chain with superslow timestamp increase (~5 blocks in 1 second) without increasing difficulty (i.e. staying at min. diff.). [...]
> > > > In that case, it would take about 7 minutes of block time seconds for
> > > > the next retarget period, every 2016 blocks, and the difficulty would
> > > > adjust. The difficulty would adjust in that case as if 2 weeks of blocks
> > > > had been mined in 7 minutes. For the difficulty to remain the same the
> > > > time between blocks needs to be 10 minutes.
> > > > This calculation does not apply under time warp attack. You can fake timestamps of all blocks except for those relevant to the retarget calculation. Those are only the first and the last block in the 2016 block window.
>
> This must be in reference to the non-overlapping difficulty calculation
> and off-by-one bug?

Indeed.


From tier.nolan at gmail.com  Tue Oct 15 18:30:58 2019
From: tier.nolan at gmail.com (Tier Nolan)
Date: Tue, 15 Oct 2019 19:30:58 +0100
Subject: [bitcoin-dev] Chain width expansion
In-Reply-To: <93649df9-27ab-abaf-00f3-da6c528344cc@purse.io>
References: <42cd5ffd-63e8-b738-c4ea-13d0699b1268@purse.io>
	<CAE-z3OV_LL+Jww3e=gO6t=02VW7m9PK+8EaYoEVLy9NKNMiSaQ@mail.gmail.com>
	<e9c5e519-ea8a-f0e2-d8fb-c955b5c2de40@purse.io>
	<CAE-z3OXyTc0aoJJVNLS5MReE7+Nhckyrjf22+yCSjXF8=bNbXQ@mail.gmail.com>
	<H_Yq1W3SffFweLPPXiUiA4EeU2yU7c8LVcqw5AbajovWTWMt5hKQARKglEQwCjPpXvjiBfvmTnaXJwivkGkT8BDha8k303DNbFB-ECes0d4=@protonmail.com>
	<93649df9-27ab-abaf-00f3-da6c528344cc@purse.io>
Message-ID: <CAE-z3OWRv1aw4_meRRon+5OA1rxerTr9=0DLWC5d7Xhyg8veBQ@mail.gmail.com>

On Tue, Oct 15, 2019 at 7:29 AM Braydon Fuller via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> So I don't think you can use the height in the coinbase for that
> purpose, as it's not possible to validate it without the previous
> headers. That's common for more than just the height.
>

It is a property of blockchains that the lowest digest for a chain
represents the total chainwork.

Estimate total hash count = N * (2^256) / (Nth lowest (i.e. strongest)
digest over all headers)

To produce a fake set of 10 headers that give a higher work estimate than
the main chain would require around the same effort as went into the main
chain in the first place.  You might as well completely build an
alternative chain.

Working backwards for one of those headers, you have to follow the actual
chain back to genesis.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191015/449fd686/attachment.html>

From john at johnnewbery.com  Wed Oct 16 16:43:53 2019
From: john at johnnewbery.com (John Newbery)
Date: Wed, 16 Oct 2019 12:43:53 -0400
Subject: [bitcoin-dev] Removal of reject network messages from Bitcoin
 Core (BIP61)
In-Reply-To: <CAK51vgDO2Tg38XbW0pqAnO3ETJ_qf8owRsUYsTXmrf7H2yGZtw@mail.gmail.com>
References: <CAK51vgDO2Tg38XbW0pqAnO3ETJ_qf8owRsUYsTXmrf7H2yGZtw@mail.gmail.com>
Message-ID: <CAFmfg2u3cLwG4h=tSF1+ho__1n2n4xyBGH+mwQgVYE9c_s+EMw@mail.gmail.com>

Following discussion on this mailing list, support for BIP 61 REJECT
messages was not removed from Bitcoin Core in V0.19. The behaviour in that
upcoming release is that REJECT messages are disabled by default and can be
enabled using the `-enablebip61` command line option.

Support for REJECT messages will be removed entirely in Bitcoin Core V0.20,
expected for release in mid 2020. The PR to remove support was merged into
Bitcoin Core's master branch this week.

Adoption of new Bitcoin Core versions across reachable nodes generally
takes several months. https://bitnodes.earn.com/dashboard/?days=365 shows
that although v0.18 was released in May 2019, there are still several
hundred reachable nodes on V0.17, V0.16, V0.15 and earlier software.
Software that currently use REJECT messages from public nodes for
troubleshooting issues therefore have plenty of time to transition to one
of the methods listed by Marco in the email above.

John

On Tue, Mar 5, 2019 at 10:28 PM Marco Falke via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> wrote:

> Bitcoin Core may send "reject" messages as response to "tx", "block" or
> "version" messages from a network peer when the message could not be
> accepted.
>
> This feature is toggled by the `-enablebip61` command line option and has
> been
> disabled by default since Bitcoin Core version 0.18.0 (not yet released as
> of
> time of writing). Nodes on the network can not generally be trusted to send
> valid ("reject") messages, so this should only ever be used when connected
> to a
> trusted node. At this time, I am not aware of any software that requires
> this
> feature, and I would like to remove if from Bitcoin Core to make the
> codebase
> slimmer, easier to understand and maintain. Let us know if your application
> relies on this feature and you can not use any of the recommended
> alternatives:
>
> * Testing or debugging of implementations of the Bitcoin P2P network
> protocol
>   should be done by inspecting the log messages that are produced by a
> recent
>   version of Bitcoin Core. Bitcoin Core logs debug messages
>   (`-debug=<category>`) to a stream (`-printtoconsole`) or to a file
>   (`-debuglogfile=<debug.log>`).
>
> * Testing the validity of a block can be achieved by specific RPCs:
>   - `submitblock`
>   - `getblocktemplate` with `'mode'` set to `'proposal'` for blocks with
>     potentially invalid POW
>
> * Testing the validity of a transaction can be achieved by specific RPCs:
>   - `sendrawtransaction`
>   - `testmempoolaccept`
>
> * Wallets should not use the absence of "reject" messages to indicate a
>   transaction has propagated the network, nor should wallets use "reject"
>   messages to set transaction fees. Wallets should rather use fee
> estimation
>   to determine transaction fees and set replace-by-fee if desired. Thus,
> they
>   could wait until the transaction has confirmed (taking into account the
> fee
>   target they set (compare the RPC `estimatesmartfee`)) or listen for the
>   transaction announcement by other network peers to check for propagation.
>
> I propose to remove "reject" messages from Bitcoin Core 0.19.0 unless
> there are
> valid concerns about its removal.
>
> Marco
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191016/4664e5b3/attachment.html>

From braydon at purse.io  Wed Oct 16 19:07:25 2019
From: braydon at purse.io (Braydon Fuller)
Date: Wed, 16 Oct 2019 12:07:25 -0700
Subject: [bitcoin-dev] Chain width expansion
In-Reply-To: <CAE-z3OVqU5n8H9vKj9Pn7k80guMTsxt_CkN9qwpfCK8HB4PDgQ@mail.gmail.com>
References: <42cd5ffd-63e8-b738-c4ea-13d0699b1268@purse.io>
	<CAE-z3OV_LL+Jww3e=gO6t=02VW7m9PK+8EaYoEVLy9NKNMiSaQ@mail.gmail.com>
	<e9c5e519-ea8a-f0e2-d8fb-c955b5c2de40@purse.io>
	<CAE-z3OXyTc0aoJJVNLS5MReE7+Nhckyrjf22+yCSjXF8=bNbXQ@mail.gmail.com>
	<H_Yq1W3SffFweLPPXiUiA4EeU2yU7c8LVcqw5AbajovWTWMt5hKQARKglEQwCjPpXvjiBfvmTnaXJwivkGkT8BDha8k303DNbFB-ECes0d4=@protonmail.com>
	<CAE-z3OVqU5n8H9vKj9Pn7k80guMTsxt_CkN9qwpfCK8HB4PDgQ@mail.gmail.com>
Message-ID: <8d1e2696-0245-638a-61fa-8f79845136d5@purse.io>

On 10/12/19 1:46 PM, Tier Nolan via bitcoin-dev wrote:

> On Sat, Oct 12, 2019 at 6:56 PM Joachim Str?mbergson <joachimstr at protonmail.com> wrote:
>> On different note, one of the problems that I haven't seen mentioned here
>> yet is the timewarp attack. It is relevant to some of the proposed
>> solutions. It should be possible, IIRC, for a malicious node to generate
>> much longer chain with superslow timestamp increase (~5 blocks in 1 second)
>> without increasing difficulty (i.e. staying at min. diff.). This could
>> produce chain that is ~2500 times longer than main chain without having
>> multiple branches.
>>
> [..]
>
> The timewarp bug can be fixed by a basic soft fork.  You just need to limit
> the maximum difference between the timestamp for the first header in a
> period and the last header in the previous period.

Yeah, that makes sense as it corrects the off-by-one error. I think this
solution has been included in a draft proposal "The Great Consensus
Cleanup". It would need to be effective for not only the main chain but
also for any future forked chain.



From braydon at purse.io  Wed Oct 16 19:25:31 2019
From: braydon at purse.io (Braydon Fuller)
Date: Wed, 16 Oct 2019 12:25:31 -0700
Subject: [bitcoin-dev] Chain width expansion
In-Reply-To: <V_DCmFvCt-jxYr-fmfwy9JvDApaJWrBz9_QESEs85FkBjiq579w8SOSex7I_aJyL_cN2idZ3SuR9qt0Eer456GD6eV7BAlq0k9Em5JIOPmM=@protonmail.com>
References: <42cd5ffd-63e8-b738-c4ea-13d0699b1268@purse.io>
	<CAE-z3OV_LL+Jww3e=gO6t=02VW7m9PK+8EaYoEVLy9NKNMiSaQ@mail.gmail.com>
	<e9c5e519-ea8a-f0e2-d8fb-c955b5c2de40@purse.io>
	<CAE-z3OXyTc0aoJJVNLS5MReE7+Nhckyrjf22+yCSjXF8=bNbXQ@mail.gmail.com>
	<H_Yq1W3SffFweLPPXiUiA4EeU2yU7c8LVcqw5AbajovWTWMt5hKQARKglEQwCjPpXvjiBfvmTnaXJwivkGkT8BDha8k303DNbFB-ECes0d4=@protonmail.com>
	<93649df9-27ab-abaf-00f3-da6c528344cc@purse.io>
	<yKuDn4LKCA9-olIsgnJO7VqN3RE9BgZWEzLu6-k4wG30iHYDIJa6wjTBoYXU0zWCRyNCEvBb4xQw7qp0BDGBCR4zccI6kJ_4AnlsB9vc9L4=@protonmail.com>
	<7b0d0bff-d898-10ff-5fdb-c982b82054a1@purse.io>
	<V_DCmFvCt-jxYr-fmfwy9JvDApaJWrBz9_QESEs85FkBjiq579w8SOSex7I_aJyL_cN2idZ3SuR9qt0Eer456GD6eV7BAlq0k9Em5JIOPmM=@protonmail.com>
Message-ID: <9e42533b-aac6-1438-068b-d02a13db53f5@purse.io>

On 10/15/19 8:50 AM, Joachim Str?mbergson wrote:

>>>>> [...] to generate much longer chain with superslow timestamp increase (~5 blocks in 1 second) without increasing difficulty (i.e. staying at min. diff.). [...]
>>>>> In that case, it would take about 7 minutes of block time seconds for
>>>>> the next retarget period, every 2016 blocks, and the difficulty would
>>>>> adjust. The difficulty would adjust in that case as if 2 weeks of blocks
>>>>> had been mined in 7 minutes. For the difficulty to remain the same the
>>>>> time between blocks needs to be 10 minutes.
>>>>> This calculation does not apply under time warp attack. You can fake timestamps of all blocks except for those relevant to the retarget calculation. Those are only the first and the last block in the 2016 block window.
>> This must be in reference to the non-overlapping difficulty calculation
>> and off-by-one bug?
> Indeed.
>
Yeah, limiting the width of the chain would not be effective unless the
timewarp off-by-one bug is resolved ? the height can be extended instead.

Rate limiting based on chainwork would slow down a timewarped low work
header chain. There would be a maximum rate at which the headers could
be sent. It would be around 32KB/s. It would take about a month to send
100GB.


From falke.marco at gmail.com  Thu Oct 17 13:23:39 2019
From: falke.marco at gmail.com (Marco Falke)
Date: Thu, 17 Oct 2019 09:23:39 -0400
Subject: [bitcoin-dev] BIPable-idea: Consistent and better definition of
 the term 'address'
In-Reply-To: <a5c28fd3-4ffd-94f8-f80c-c55c202a3f2c@emilengler.com>
References: <58e44347-6eee-a0c3-3b8a-965c7450780e@emilengler.com>
	<6fe67006-7131-a861-61fa-65392d5be069@riseup.net>
	<20824fa5-e3fa-8d4e-1678-4c2048b49b6b@emilengler.com>
	<CALJw2w6FeQDpiFZ9+j-tmho_HMFCyi-0wLrGqze9jD5iSLfuVw@mail.gmail.com>
	<ceb671aa-c85f-47d3-9084-aa66005640a9@gmail.com>
	<a5c28fd3-4ffd-94f8-f80c-c55c202a3f2c@emilengler.com>
Message-ID: <CAK51vgCpvLTMXwdrgSNaOMFAu25QuHkuXpwsaqK+92cq8NZiow@mail.gmail.com>

I also like the "bitcoin invoice address" term by Chris. Invoice is a
common term and easily translatable into other languages.

Marco

From andreas at schildbach.de  Thu Oct 17 19:38:53 2019
From: andreas at schildbach.de (Andreas Schildbach)
Date: Thu, 17 Oct 2019 21:38:53 +0200
Subject: [bitcoin-dev] Removal of reject network messages from Bitcoin
	Core (BIP61)
In-Reply-To: <CAFmfg2u3cLwG4h=tSF1+ho__1n2n4xyBGH+mwQgVYE9c_s+EMw@mail.gmail.com>
References: <CAK51vgDO2Tg38XbW0pqAnO3ETJ_qf8owRsUYsTXmrf7H2yGZtw@mail.gmail.com>
	<CAFmfg2u3cLwG4h=tSF1+ho__1n2n4xyBGH+mwQgVYE9c_s+EMw@mail.gmail.com>
Message-ID: <qoag0d$65ad$1@blaine.gmane.org>

On 16/10/2019 18.43, John Newbery via bitcoin-dev wrote:

> Following discussion on this mailing list, support for BIP 61 REJECT
> messages was not removed from Bitcoin Core in V0.19. The behaviour in
> that upcoming release is that REJECT messages are disabled by default
> and can be enabled using the `-enablebip61` command line option.

Is there a NODE_* bit we can use to pick peers that support this
(useful!) feature?



From me at emilengler.com  Thu Oct 17 19:28:44 2019
From: me at emilengler.com (Emil Engler)
Date: Thu, 17 Oct 2019 21:28:44 +0200
Subject: [bitcoin-dev] BIPable-idea: Consistent and better definition of
 the term 'address'
In-Reply-To: <CAK51vgCpvLTMXwdrgSNaOMFAu25QuHkuXpwsaqK+92cq8NZiow@mail.gmail.com>
References: <58e44347-6eee-a0c3-3b8a-965c7450780e@emilengler.com>
	<6fe67006-7131-a861-61fa-65392d5be069@riseup.net>
	<20824fa5-e3fa-8d4e-1678-4c2048b49b6b@emilengler.com>
	<CALJw2w6FeQDpiFZ9+j-tmho_HMFCyi-0wLrGqze9jD5iSLfuVw@mail.gmail.com>
	<ceb671aa-c85f-47d3-9084-aa66005640a9@gmail.com>
	<a5c28fd3-4ffd-94f8-f80c-c55c202a3f2c@emilengler.com>
	<CAK51vgCpvLTMXwdrgSNaOMFAu25QuHkuXpwsaqK+92cq8NZiow@mail.gmail.com>
Message-ID: <cb8885d2-7e9b-5701-0cce-adaa7662e306@emilengler.com>

As the idea got positive feedback, I've written the BIP.
The draft is available here:
https://github.com/bitcoin/bips/pull/856

Am 17.10.19 um 15:23 schrieb Marco Falke via bitcoin-dev:
> I also like the "bitcoin invoice address" term by Chris. Invoice is a
> common term and easily translatable into other languages.
> 
> Marco
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: pEpkey.asc
Type: application/pgp-keys
Size: 3147 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191017/0b2934c5/attachment-0001.bin>

From eric at voskuil.org  Thu Oct 17 20:16:47 2019
From: eric at voskuil.org (Eric Voskuil)
Date: Thu, 17 Oct 2019 13:16:47 -0700
Subject: [bitcoin-dev] Removal of reject network messages from Bitcoin
	Core (BIP61)
In-Reply-To: <qoag0d$65ad$1@blaine.gmane.org>
References: <qoag0d$65ad$1@blaine.gmane.org>
Message-ID: <5C03EE5A-B29E-44BC-903F-7094E504580D@voskuil.org>

As this is a P2P protocol change it should be exposed as a version increment (and a BIP), not just as a conditional service. If the intent is to retain this protocol indefinitely, exposing it conditionally, then a service bit would make sense, but it remains a protocol change.

BIP61 is explicit:

?All implementations of the P2P protocol version 70,002 and later should support the reject message.?

e

> On Oct 17, 2019, at 12:54, Andreas Schildbach via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
> 
> ?On 16/10/2019 18.43, John Newbery via bitcoin-dev wrote:
> 
>> Following discussion on this mailing list, support for BIP 61 REJECT
>> messages was not removed from Bitcoin Core in V0.19. The behaviour in
>> that upcoming release is that REJECT messages are disabled by default
>> and can be enabled using the `-enablebip61` command line option.
> 
> Is there a NODE_* bit we can use to pick peers that support this
> (useful!) feature?
> 
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From me at emilengler.com  Thu Oct 17 20:30:41 2019
From: me at emilengler.com (Emil Engler)
Date: Thu, 17 Oct 2019 22:30:41 +0200
Subject: [bitcoin-dev] Co-Author for ''Redefinition of the term address"
Message-ID: <5e6239b8-b989-e777-ca42-10fb70d0c553@emilengler.com>

Hi, I need a co-author for this BIP:
https://github.com/bitcoin/bips/pull/856

See the reasons mentioned by Marco Falke
-------------- next part --------------
A non-text attachment was scrubbed...
Name: pEpkey.asc
Type: application/pgp-keys
Size: 3147 bytes
Desc: not available
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191017/94c19d3c/attachment-0001.bin>

From john at johnnewbery.com  Fri Oct 18 20:53:23 2019
From: john at johnnewbery.com (John Newbery)
Date: Fri, 18 Oct 2019 16:53:23 -0400
Subject: [bitcoin-dev] Removal of reject network messages from Bitcoin
 Core (BIP61)
In-Reply-To: <CAFmfg2u3cLwG4h=tSF1+ho__1n2n4xyBGH+mwQgVYE9c_s+EMw@mail.gmail.com>
References: <CAK51vgDO2Tg38XbW0pqAnO3ETJ_qf8owRsUYsTXmrf7H2yGZtw@mail.gmail.com>
	<CAFmfg2u3cLwG4h=tSF1+ho__1n2n4xyBGH+mwQgVYE9c_s+EMw@mail.gmail.com>
Message-ID: <CAFmfg2sU=ur7NdzU6r9bGC=xZqcX7WA7ux-r8QOwNa+FKhS4Pw@mail.gmail.com>

> Is there a NODE_* bit we can use to pick peers that support this (useful!)
feature?

No. BIP 61 has no mechanism for advertising that a node will send REJECT
messages.

On Wed, Oct 16, 2019 at 12:43 PM John Newbery <john at johnnewbery.com> wrote:

> Following discussion on this mailing list, support for BIP 61 REJECT
> messages was not removed from Bitcoin Core in V0.19. The behaviour in that
> upcoming release is that REJECT messages are disabled by default and can be
> enabled using the `-enablebip61` command line option.
>
> Support for REJECT messages will be removed entirely in Bitcoin Core
> V0.20, expected for release in mid 2020. The PR to remove support was
> merged into Bitcoin Core's master branch this week.
>
> Adoption of new Bitcoin Core versions across reachable nodes generally
> takes several months. https://bitnodes.earn.com/dashboard/?days=365 shows
> that although v0.18 was released in May 2019, there are still several
> hundred reachable nodes on V0.17, V0.16, V0.15 and earlier software.
> Software that currently use REJECT messages from public nodes for
> troubleshooting issues therefore have plenty of time to transition to one
> of the methods listed by Marco in the email above.
>
> John
>
> On Tue, Mar 5, 2019 at 10:28 PM Marco Falke via bitcoin-dev <
> bitcoin-dev at lists.linuxfoundation.org> wrote:
>
>> Bitcoin Core may send "reject" messages as response to "tx", "block" or
>> "version" messages from a network peer when the message could not be
>> accepted.
>>
>> This feature is toggled by the `-enablebip61` command line option and has
>> been
>> disabled by default since Bitcoin Core version 0.18.0 (not yet released
>> as of
>> time of writing). Nodes on the network can not generally be trusted to
>> send
>> valid ("reject") messages, so this should only ever be used when
>> connected to a
>> trusted node. At this time, I am not aware of any software that requires
>> this
>> feature, and I would like to remove if from Bitcoin Core to make the
>> codebase
>> slimmer, easier to understand and maintain. Let us know if your
>> application
>> relies on this feature and you can not use any of the recommended
>> alternatives:
>>
>> * Testing or debugging of implementations of the Bitcoin P2P network
>> protocol
>>   should be done by inspecting the log messages that are produced by a
>> recent
>>   version of Bitcoin Core. Bitcoin Core logs debug messages
>>   (`-debug=<category>`) to a stream (`-printtoconsole`) or to a file
>>   (`-debuglogfile=<debug.log>`).
>>
>> * Testing the validity of a block can be achieved by specific RPCs:
>>   - `submitblock`
>>   - `getblocktemplate` with `'mode'` set to `'proposal'` for blocks with
>>     potentially invalid POW
>>
>> * Testing the validity of a transaction can be achieved by specific RPCs:
>>   - `sendrawtransaction`
>>   - `testmempoolaccept`
>>
>> * Wallets should not use the absence of "reject" messages to indicate a
>>   transaction has propagated the network, nor should wallets use "reject"
>>   messages to set transaction fees. Wallets should rather use fee
>> estimation
>>   to determine transaction fees and set replace-by-fee if desired. Thus,
>> they
>>   could wait until the transaction has confirmed (taking into account the
>> fee
>>   target they set (compare the RPC `estimatesmartfee`)) or listen for the
>>   transaction announcement by other network peers to check for
>> propagation.
>>
>> I propose to remove "reject" messages from Bitcoin Core 0.19.0 unless
>> there are
>> valid concerns about its removal.
>>
>> Marco
>> _______________________________________________
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191018/947d583d/attachment-0001.html>

From dave at dtrt.org  Fri Oct 18 22:45:35 2019
From: dave at dtrt.org (David A. Harding)
Date: Fri, 18 Oct 2019 12:45:35 -1000
Subject: [bitcoin-dev] Removal of reject network messages from Bitcoin
 Core (BIP61)
In-Reply-To: <5C03EE5A-B29E-44BC-903F-7094E504580D@voskuil.org>
References: <qoag0d$65ad$1@blaine.gmane.org>
	<5C03EE5A-B29E-44BC-903F-7094E504580D@voskuil.org>
Message-ID: <20191018224535.wy6f55grpfk2sncq@ganymede>

On Thu, Oct 17, 2019 at 01:16:47PM -0700, Eric Voskuil via bitcoin-dev wrote:
> As this is a P2P protocol change it should be exposed as a version
> increment (and a BIP) [...]
>
> BIP61 is explicit:
> 
> ?All implementations of the P2P protocol version 70,002 and later
> should support the reject message.?

I don't think a new BIP or a version number increment is necessary.

1. "Should support" isn't the same as "must support".  See
   https://tools.ietf.org/html/rfc2119 ; by that reading,
   implementations with protocol versions above 70,002 are not required
   to support the reject message.

2. If you don't implement a BIP, as Bitcoin Core explicitly doesn't any
   more for BIP61[1], you're not bound by its conditions.

-Dave

[1] https://github.com/bitcoin/bitcoin/blob/master/doc/bips.md  "BIP61
[...] Support was removed in v0.20.0"

From lucash.dev at gmail.com  Fri Oct 18 22:01:54 2019
From: lucash.dev at gmail.com (Lucas H)
Date: Fri, 18 Oct 2019 15:01:54 -0700
Subject: [bitcoin-dev] Trustless hash-price insurance contracts
Message-ID: <CAHa=hJVuOdSyPhzu+cGHumw-94yAfxPE_1CkMYRBqxPsyjYBKA@mail.gmail.com>

Hi,

This is my first post to this list -- even though I did some tiny
contributions to bitcoin core I feel quite a beginner -- so if my idea is
stupid, already known, or too off-topic, just let me know.

TL;DR: a trustless contract that guarantees minimum profitability of a
mining operation -- in case Bitcoin/hash price goes too low. It can be
trustless bc we can use the assumption that the price of hashing is low to
unlock funds.

The problem:

A miner invests in new mining equipment, but if the hash-rate goes up too
much (the price he is paid for a hash goes down by too much) he will have a
loss.

Solution: trustless hash-price insurance contract (or can we call it an
option to sell hashes at a given price?)

An insurer who believes that it's unlikely the price of a hash will go down
a lot negotiates a contract with the miner implemented as a Bitcoin
transaction:

Inputs: a deposit from the insurer and a premium payment by the miner
Output1: simply the premium payment to the insurer
Output2 -- that's the actual insurance
  There are three OR'ed conditions for paying it:
  A. After expiry date (in blocks) insurer can spend
  B. Both miner and insurer can spend at any time by mutual agreement
  C. Before expiry, miner can spend by providing **a pre-image that
produces a hash within certain difficulty constraints**

The thing that makes it a hash-price insurance (or option, pardon my lack
of precise financial jargon), is that if hashing becomes cheap enough, it
becomes profitable to spend resources finding a suitable pre-image, rather
than mining Bitcoin.
Of course, both parties can reach an agreement that doesn't require
actually spending these resources -- so the miner can still mine Bitcoin
and compensate for the lower-than-expected reward with part of the
insurance deposit.
If the price doesn't go down enough, the miner just mines Bitcoin and the
insurer gets his deposit back.
It's basically an instrument for guaranteeing a minimum profitability of
the mining operation.

Implementation issues: unfortunately we can't do arithmetic comparison with
long integers >32bit in the script, so implementation of the difficulty
requirement needs to be hacky. I think we can use the hashes of one or more
pre-images with a given short length, and the miner has to provide the
exact pre-images. The pre-images are chosen by the insurer, and we would
need a "honesty" deposit or other mechanism to punish the insurer if he
chooses a hash that doesn't correspond to any short-length pre-image. I'm
not sure about this implementation though, maybe we actually need new
opcodes.

What do you guys think?
Thanks for reading it all! Hope it was worth your time!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191018/02afb5ce/attachment.html>

From somber.night at protonmail.com  Sun Oct 20 00:29:25 2019
From: somber.night at protonmail.com (SomberNight)
Date: Sun, 20 Oct 2019 00:29:25 +0000
Subject: [bitcoin-dev] Draft BIP for SNICKER
Message-ID: <YwZ3vq20LFvpx-nKn1RJjcRHwYTAVCC0v0EyD0y6zVMlQtKXUFNAaEk_QE2dzYDU6z2eK0S0TDXRPfl1_y93RgDjdCGboOgjcERBTLUPHao=@protonmail.com>

Hi all,

waxwing, ThomasV, and I recently had a discussion about implementing SNICKER in Electrum; specifically the "Receiver" role. To me, SNICKER is an interesting proposal, due to the non-interactivity and because it seems it would be easy to implement the "Receiver" role in a light wallet. If enough users are using wallets that implement the "Receiver" role, even if full nodes and specialised scripts are needed to run SNICKER as a "Proposer", then coinjoins via SNICKER could become fairly frequent on-chain, benefitting the whole ecosystem indirectly by breaking common chain-analysis assumptions even further.

The BIP (draft) describes how the Receiver can deterministically find all his outputs and reconstruct all corresponding private keys, just from the seed words and the blockchain.
However what is not explicitly pointed out, and what I would like to point out in this mail, is that SNICKER breaks watch-only functionality.

See "Receiver actions" > "Storage of Keys" section ("Re-derive from blockchain history"). [0]

Specifically, the output address in the SNICKER transaction that pays to the "Receiver", is constructed from the pubkey `P_A + cG`, where `P_A` is a pubkey of "Receiver" (typically a leaf pubkey along some BIP32 path), and `c` is a tweak. This tweak was constructed such that `c = ECDH(Q, P_A)`, where `Q` is a pubkey of the "Proposer" that appears in the witness of the SNICKER tx.

As the referenced section [0] explains, the "Receiver" can restore from seed, and assuming he knows he needs to do extra scanning steps (e.g. via a seed version that signals SNICKER support), he can find and regain access to his SNICKER outputs. However, to calculate `c` he needs access to his private keys, as it is the ECDH of one of the Receiver's pubkeys and one of the Proposer's pubkeys.

This means the proposed scheme is fundamentally incompatible with watch-only wallets.
Nowadays many users expect being able to watch their addresses from an unsecure machine, or to be able to offline sign transactions. In the case of Electrum specifically, Electrum Personal Server (EPS) is also using xpubs to function. We've been exposing users to xpubs since the initial BIP32 implementation (and even predating BIP32, in the legacy Electrum HD scheme, there were already "master public keys").

It would seem that if we implemented SNICKER, users would have to make a choice, most likely during wallet creation time, whether they want to be able to use xpubs or to potentially participate in SNICKER coinjoins as a "Receiver" (and then encode the choice in the seed version). This choice seems rather difficult to communicate to users. Further, if SNICKER is not supported by the default choice then it is less likely to take off and hence less useful for the user; OTOH if xpubs are not supported by the default choice then existing user expectations are broken.

(Note that I am using a loosened definition of xpub here. The pubkeys in SNICKER tx output scripts are not along any BIP32 derivation. The point here is whether they could somehow be identified deterministically without access to secret material.)

Unfortunately it is not clear how the SNICKER scheme could be adjusted to "fix" this. Note that `c` needs to be known exactly by the two coinjoin-participants and no-one else; otherwise the anonymity set (of 2) is broken as:
- which SNICKER output corresponds to the tweaked public key and hence to the Receiver, can then be identified (as soon as the output is spent and the pubkey appears on-chain), and
- using subset-sum analysis the inputs and the outputs can be linked
SNICKER assumes almost no communication between the two parties, so it seems difficult to find a sufficient construction for `c` such that it can be recreated by the Receiver if he only has an xpub (and access to the blockchain) as all pubkeys from the xpub that the Proposer would have access to are already public information visible on-chain.

ghost43


[0] https://gist.github.com/AdamISZ/2c13fb5819bd469ca318156e2cf25d79#Storage_of_Keys



> Hello list,
> Here is a link for a draft of a BIP for a different type of CoinJoin I've named 'SNICKER' = Simple Non-Interactive Coinjoin with Keys for Encryption Reused.
>
> https://gist.github.com/AdamISZ/2c13fb5819bd469ca318156e2cf25d79
>
> Purpose of writing this as a BIP:
> There was some discussion on the Wasabi repo about this recently (https://github.com/zkSNACKs/Meta/issues/67) and it prompted me to do something I should have done way back when I came up with the idea in late '17: write a technical specification, because one of the main attractive points about this is that it isn't a hugely difficult task for a wallet developer to implement (especially: Receiver side), and it would only really have value if wallet developers did indeed implement it. To be specific, it requires ECDH (which is already available in libsecp256k1 anyway) and ECIES which is pretty easy to do (just ecdh and hmac, kinda).
>
> Plenty of uncertainty on the specs, in particular the specification for transactions, e.g. see 'Partially signed transactions' subsection, point 3). Also perhaps the encryption specs. I went with the exact algo already used by Electrum here, but it could be considered dubious (CBC).
>
> Thanks for any feedback.
>
> Adam Gibson / waxwing


From eric at voskuil.org  Sun Oct 20 05:03:09 2019
From: eric at voskuil.org (Eric Voskuil)
Date: Sun, 20 Oct 2019 01:03:09 -0400
Subject: [bitcoin-dev] Trustless hash-price insurance contracts
In-Reply-To: <CAHa=hJVuOdSyPhzu+cGHumw-94yAfxPE_1CkMYRBqxPsyjYBKA@mail.gmail.com>
References: <CAHa=hJVuOdSyPhzu+cGHumw-94yAfxPE_1CkMYRBqxPsyjYBKA@mail.gmail.com>
Message-ID: <ED2B169F-9548-4EBF-AEA0-270EBA4A4502@voskuil.org>

Hi Lucas,

I would question the assumption inherent in the problem statement. Setting aside variance discount, proximity premium, and questions of relative efficiency, as these are presumably already considered by the miner upon the purchase of new equipment, it?s not clear why a loss is assumed in the case of subsequently increasing hash rate. 

The assumption of increasing hash rate implies an expectation of increasing return on investment.  There are certainly speculative errors, but a loss on new equipment implies *all miners* are operating at a loss, which is not a sustainable situation.

If any miner is profitable it is the miner with the new equipment, and if he is not, hash rate will drop until he is. This drop is most likely to be precipitated by older equipment going offline.

Best,
Eric

> On Oct 20, 2019, at 00:31, Lucas H via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
> 
> ?
> Hi,
> 
> This is my first post to this list -- even though I did some tiny contributions to bitcoin core I feel quite a beginner -- so if my idea is stupid, already known, or too off-topic, just let me know.
> 
> TL;DR: a trustless contract that guarantees minimum profitability of a mining operation -- in case Bitcoin/hash price goes too low. It can be trustless bc we can use the assumption that the price of hashing is low to unlock funds.
> 
> The problem:
> 
> A miner invests in new mining equipment, but if the hash-rate goes up too much (the price he is paid for a hash goes down by too much) he will have a loss.
> 
> Solution: trustless hash-price insurance contract (or can we call it an option to sell hashes at a given price?)
> 
> An insurer who believes that it's unlikely the price of a hash will go down a lot negotiates a contract with the miner implemented as a Bitcoin transaction:
> 
> Inputs: a deposit from the insurer and a premium payment by the miner
> Output1: simply the premium payment to the insurer
> Output2 -- that's the actual insurance
>   There are three OR'ed conditions for paying it:
>   A. After expiry date (in blocks) insurer can spend
>   B. Both miner and insurer can spend at any time by mutual agreement
>   C. Before expiry, miner can spend by providing **a pre-image that produces a hash within certain difficulty constraints**
> 
> The thing that makes it a hash-price insurance (or option, pardon my lack of precise financial jargon), is that if hashing becomes cheap enough, it becomes profitable to spend resources finding a suitable pre-image, rather than mining Bitcoin.
> Of course, both parties can reach an agreement that doesn't require actually spending these resources -- so the miner can still mine Bitcoin and compensate for the lower-than-expected reward with part of the insurance deposit.
> If the price doesn't go down enough, the miner just mines Bitcoin and the insurer gets his deposit back.
> It's basically an instrument for guaranteeing a minimum profitability of the mining operation.
> 
> Implementation issues: unfortunately we can't do arithmetic comparison with long integers >32bit in the script, so implementation of the difficulty requirement needs to be hacky. I think we can use the hashes of one or more pre-images with a given short length, and the miner has to provide the exact pre-images. The pre-images are chosen by the insurer, and we would need a "honesty" deposit or other mechanism to punish the insurer if he chooses a hash that doesn't correspond to any short-length pre-image. I'm not sure about this implementation though, maybe we actually need new opcodes.
> 
> What do you guys think?
> Thanks for reading it all! Hope it was worth your time!
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From eric at voskuil.org  Sun Oct 20 05:13:12 2019
From: eric at voskuil.org (Eric Voskuil)
Date: Sun, 20 Oct 2019 01:13:12 -0400
Subject: [bitcoin-dev] Removal of reject network messages from Bitcoin
	Core (BIP61)
In-Reply-To: <20191018224535.wy6f55grpfk2sncq@ganymede>
References: <20191018224535.wy6f55grpfk2sncq@ganymede>
Message-ID: <8C87E9AF-A064-42BB-AB9D-7477C10625BF@voskuil.org>

I agree, thanks.

FWIW I?ve never been a fan of the ?reject? message, or its implementation.

https://github.com/bitcoin/bips/wiki/Comments:BIP-0061

e

> On Oct 18, 2019, at 18:46, David A. Harding <dave at dtrt.org> wrote:
> 
> ?On Thu, Oct 17, 2019 at 01:16:47PM -0700, Eric Voskuil via bitcoin-dev wrote:
>> As this is a P2P protocol change it should be exposed as a version
>> increment (and a BIP) [...]
>> 
>> BIP61 is explicit:
>> 
>> ?All implementations of the P2P protocol version 70,002 and later
>> should support the reject message.?
> 
> I don't think a new BIP or a version number increment is necessary.
> 
> 1. "Should support" isn't the same as "must support".  See
>   https://tools.ietf.org/html/rfc2119 ; by that reading,
>   implementations with protocol versions above 70,002 are not required
>   to support the reject message.
> 
> 2. If you don't implement a BIP, as Bitcoin Core explicitly doesn't any
>   more for BIP61[1], you're not bound by its conditions.
> 
> -Dave
> 
> [1] https://github.com/bitcoin/bitcoin/blob/master/doc/bips.md  "BIP61
> [...] Support was removed in v0.20.0"
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191020/d2498549/attachment-0001.html>

From dave at dtrt.org  Mon Oct 21 00:06:08 2019
From: dave at dtrt.org (David A. Harding)
Date: Sun, 20 Oct 2019 14:06:08 -1000
Subject: [bitcoin-dev] Draft BIP for SNICKER
In-Reply-To: <YwZ3vq20LFvpx-nKn1RJjcRHwYTAVCC0v0EyD0y6zVMlQtKXUFNAaEk_QE2dzYDU6z2eK0S0TDXRPfl1_y93RgDjdCGboOgjcERBTLUPHao=@protonmail.com>
References: <YwZ3vq20LFvpx-nKn1RJjcRHwYTAVCC0v0EyD0y6zVMlQtKXUFNAaEk_QE2dzYDU6z2eK0S0TDXRPfl1_y93RgDjdCGboOgjcERBTLUPHao=@protonmail.com>
Message-ID: <20191021000608.ajvzjxh6phtuhydp@ganymede>

On Sun, Oct 20, 2019 at 12:29:25AM +0000, SomberNight via bitcoin-dev wrote:
> waxwing, ThomasV, and I recently had a discussion about implementing
> SNICKER in Electrum; specifically the "Receiver" role. 

That'd be awesome!

> As the referenced section [0] explains, the "Receiver" can restore
> from seed, and assuming he knows he needs to do extra scanning steps
> (e.g. via a seed version that signals SNICKER support), he can find
> and regain access to his SNICKER outputs. However, to calculate `c` he
> needs access to his private keys, as it is the ECDH of one of the
> Receiver's pubkeys and one of the Proposer's pubkeys.
> 
> This means the proposed scheme is fundamentally incompatible with
> watch-only wallets.
> 
> [0] https://gist.github.com/AdamISZ/2c13fb5819bd469ca318156e2cf25d79#Storage_of_Keys

Your logic seems correct for the watching half of the wallet, but I
think it's ok to consider requiring interaction with the cold wallet.
Let's look at the recovery procedure from the SNICKER documentation
that you kindly cited:

    1. Derive all regular addresses normally (doable watch-only for
    wallets using public BIP32 derivation)

    2. Find all transactions spending an output for each of those
    addresses.  Determine whether the spend looks like a SNICKER
    coinjoin (e.g. "two equal-[value] outputs").  (doable watch-only)
    
    3. "For each of those transactions, check, for each of the two equal
    sized outputs, whether one destination address can be regenerated
    from by taking c found in the method described above" (not doable
    watch only; requires private keys)

I'd expect the set of candidate transactions produced in step #2 to be
pretty small and probably with no false positives for users not
participating in SNICKER coinjoins or doing lots of payment batching.
That means, if any SNICKER candidates were found by a watch-only wallet,
they could be compactly bundled up and the user could be encouraged to
copy them to the corresponding cold wallet using the same means used for
PSBTs (e.g. USB drive, QR codes, etc).  You wouldn't even need the whole
transactions, just the BIP32 index of the user's key, the pubkey of the
suspected proposer, and a checksum of the resultant address.

The cold wallet could then perform step #3 using its private keys and
return a file/QRcode/whatever to the hot wallet telling it any shared
secrets it found.

This process may need to be repeated several times if an output created
by one SNICKER round is spent in a subsequent SNICKER round.  This can be
addressed by simply refusing to participate in chains of SNICKER
transactions or by refusing to participant in chains of SNICKERs more
than n long (requring a maximum n rounds of recovery).  It could also be
addressed by the watching-only wallet looking ahead at the block chain a
bit in order to grab SNICKER-like child and grandchild transactions of
our SNICKER candidates and sending them also to the cold wallet for
attempted shared secret recovery.

The SNICKER recovery process is, of course, only required for wallet
recovery and not normal wallet use, so I don't think a small amount of
round-trip communication between the hot wallet and the cold wallet is
too much to ask---especially since anyone using SNICKER with a
watching-only wallet must be regularly interacting with their cold
wallet anyway to sign the coinjoins.

-Dave

From eric at voskuil.org  Sun Oct 20 14:57:37 2019
From: eric at voskuil.org (Eric Voskuil)
Date: Sun, 20 Oct 2019 10:57:37 -0400
Subject: [bitcoin-dev] Trustless hash-price insurance contracts
In-Reply-To: <5vCX5TKGwYMITwx_jp3BuX9nDr_DIez5kDPkF9ATAh_XYrQ4Y2rUGNU7qEkcy54BIEuNLB8TyznoOVBypNRWu0mTnqX4_D1oNK6ZT2fudQA=@mathbot.com>
References: <5vCX5TKGwYMITwx_jp3BuX9nDr_DIez5kDPkF9ATAh_XYrQ4Y2rUGNU7qEkcy54BIEuNLB8TyznoOVBypNRWu0mTnqX4_D1oNK6ZT2fudQA=@mathbot.com>
Message-ID: <90A19D12-D964-41AE-A27F-AB99B66936D1@voskuil.org>


> On Oct 20, 2019, at 10:10, JW Weatherman <jw at mathbot.com> wrote:
> 
> ?I think the assumption is not that all miners are unprofitable, but that a single miner could make an investment that becomes unprofitable if the hash rate increases more than he expected.

This is a restatement of the assumption I questioned. Hash rate increase does not imply unprofitability. The new rig should be profitable.

What is being assumed is a hash rate increase *without* a proportional block reward value increase. In this case if the newest equipment is unprofitable, all miners are unprofitable.

> Depending on the cost of the offered insurance it would be prudent for a miner to decrease his potential loss by buying insurance for this possibility.
> 
> And the existence of attractive insurance contracts would lower the barrier to entry for new competitors in mining and this would increase bitcoins security.
> 
> -JW
> 
> 
> 
> 
> ??????? Original Message ???????
>> On Sunday, October 20, 2019 1:03 AM, Eric Voskuil via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:
>> 
>> Hi Lucas,
>> 
>> I would question the assumption inherent in the problem statement. Setting aside variance discount, proximity premium, and questions of relative efficiency, as these are presumably already considered by the miner upon the purchase of new equipment, it?s not clear why a loss is assumed in the case of subsequently increasing hash rate.
>> 
>> The assumption of increasing hash rate implies an expectation of increasing return on investment. There are certainly speculative errors, but a loss on new equipment implies all miners are operating at a loss, which is not a sustainable situation.
>> 
>> If any miner is profitable it is the miner with the new equipment, and if he is not, hash rate will drop until he is. This drop is most likely to be precipitated by older equipment going offline.
>> 
>> Best,
>> Eric
>> 
>>>> On Oct 20, 2019, at 00:31, Lucas H via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:
>>> Hi,
>>> This is my first post to this list -- even though I did some tiny contributions to bitcoin core I feel quite a beginner -- so if my idea is stupid, already known, or too off-topic, just let me know.
>>> TL;DR: a trustless contract that guarantees minimum profitability of a mining operation -- in case Bitcoin/hash price goes too low. It can be trustless bc we can use the assumption that the price of hashing is low to unlock funds.
>>> The problem:
>>> A miner invests in new mining equipment, but if the hash-rate goes up too much (the price he is paid for a hash goes down by too much) he will have a loss.
>>> Solution: trustless hash-price insurance contract (or can we call it an option to sell hashes at a given price?)
>>> An insurer who believes that it's unlikely the price of a hash will go down a lot negotiates a contract with the miner implemented as a Bitcoin transaction:
>>> Inputs: a deposit from the insurer and a premium payment by the miner
>>> Output1: simply the premium payment to the insurer
>>> Output2 -- that's the actual insurance
>>> There are three OR'ed conditions for paying it:
>>> A. After expiry date (in blocks) insurer can spend
>>> B. Both miner and insurer can spend at any time by mutual agreement
>>> C. Before expiry, miner can spend by providing a pre-image that produces a hash within certain difficulty constraints
>>> The thing that makes it a hash-price insurance (or option, pardon my lack of precise financial jargon), is that if hashing becomes cheap enough, it becomes profitable to spend resources finding a suitable pre-image, rather than mining Bitcoin.
>>> Of course, both parties can reach an agreement that doesn't require actually spending these resources -- so the miner can still mine Bitcoin and compensate for the lower-than-expected reward with part of the insurance deposit.
>>> If the price doesn't go down enough, the miner just mines Bitcoin and the insurer gets his deposit back.
>>> It's basically an instrument for guaranteeing a minimum profitability of the mining operation.
>>> Implementation issues: unfortunately we can't do arithmetic comparison with long integers >32bit in the script, so implementation of the difficulty requirement needs to be hacky. I think we can use the hashes of one or more pre-images with a given short length, and the miner has to provide the exact pre-images. The pre-images are chosen by the insurer, and we would need a "honesty" deposit or other mechanism to punish the insurer if he chooses a hash that doesn't correspond to any short-length pre-image. I'm not sure about this implementation though, maybe we actually need new opcodes.
>>> What do you guys think?
>>> Thanks for reading it all! Hope it was worth your time!
>>> 
>>> bitcoin-dev mailing list
>>> bitcoin-dev at lists.linuxfoundation.org
>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>> 
>> bitcoin-dev mailing list
>> bitcoin-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 
> 

From eric at voskuil.org  Sun Oct 20 16:16:57 2019
From: eric at voskuil.org (Eric Voskuil)
Date: Sun, 20 Oct 2019 12:16:57 -0400
Subject: [bitcoin-dev] Trustless hash-price insurance contracts
In-Reply-To: <E8cgzgpxhsuivizcjW9yAo2lsv8LmR2kXjBvqm0bA4i0HXa11o-X2ntYXcmjlZ5iCOCS6OzNze-RHIZkqCLHMjtBTan4VnxskFmC-8NX850=@mathbot.com>
References: <E8cgzgpxhsuivizcjW9yAo2lsv8LmR2kXjBvqm0bA4i0HXa11o-X2ntYXcmjlZ5iCOCS6OzNze-RHIZkqCLHMjtBTan4VnxskFmC-8NX850=@mathbot.com>
Message-ID: <C54E6D4A-7E74-4169-8D92-1CCD633A6DE3@voskuil.org>

So we are talking about a miner insuring against his own inefficiency.

Furthermore a disproportionate increase in hash rate is based on the expectation of higher future return (investment leads returns). So the insurance could end up paying out against realized profit.

Generally speaking, insuring investment is a zero sum game.

e

> On Oct 20, 2019, at 12:10, JW Weatherman <jw at mathbot.com> wrote:
> 
> ?Oh, I see your point.
> 
> However the insurance contract would protect the miner even in that case. A miner with great confidence that he is running optimal hardware and has optimal electricity and labor costs probably wouldn't be interested in purchasing insurance for a high price, but if it was cheap enough it would still be worth it. And any potential new entrants on the edge of jumping in would enter when they otherwise would not have because of the decreased costs (decreased risk).
> 
> An analogy would be car insurance. If you are an excellent driver you wouldn't be willing to spend a ton of money to protect your car in the event of an accident, but if it is cheap enough you would. And there may be people that are unwilling to take the risk of a damaged car that refrain from becoming drivers until insurance allows them to lower the worst case scenario of a damaged car.
> 
> -JW
> 
> 
> 
> 
> ??????? Original Message ???????
>> On Sunday, October 20, 2019 10:57 AM, Eric Voskuil <eric at voskuil.org> wrote:
>> 
>> 
>> 
>>>> On Oct 20, 2019, at 10:10, JW Weatherman jw at mathbot.com wrote:
>>> I think the assumption is not that all miners are unprofitable, but that a single miner could make an investment that becomes unprofitable if the hash rate increases more than he expected.
>> 
>> This is a restatement of the assumption I questioned. Hash rate increase does not imply unprofitability. The new rig should be profitable.
>> 
>> What is being assumed is a hash rate increase without a proportional block reward value increase. In this case if the newest equipment is unprofitable, all miners are unprofitable.
>> 
>>> Depending on the cost of the offered insurance it would be prudent for a miner to decrease his potential loss by buying insurance for this possibility.
>>> And the existence of attractive insurance contracts would lower the barrier to entry for new competitors in mining and this would increase bitcoins security.
>>> -JW
>>> ??????? Original Message ???????
>>> 
>>>> On Sunday, October 20, 2019 1:03 AM, Eric Voskuil via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:
>>>> Hi Lucas,
>>>> I would question the assumption inherent in the problem statement. Setting aside variance discount, proximity premium, and questions of relative efficiency, as these are presumably already considered by the miner upon the purchase of new equipment, it?s not clear why a loss is assumed in the case of subsequently increasing hash rate.
>>>> The assumption of increasing hash rate implies an expectation of increasing return on investment. There are certainly speculative errors, but a loss on new equipment implies all miners are operating at a loss, which is not a sustainable situation.
>>>> If any miner is profitable it is the miner with the new equipment, and if he is not, hash rate will drop until he is. This drop is most likely to be precipitated by older equipment going offline.
>>>> Best,
>>>> Eric
>>>> 
>>>>>> On Oct 20, 2019, at 00:31, Lucas H via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:
>>>>>> Hi,
>>>>>> This is my first post to this list -- even though I did some tiny contributions to bitcoin core I feel quite a beginner -- so if my idea is stupid, already known, or too off-topic, just let me know.
>>>>>> TL;DR: a trustless contract that guarantees minimum profitability of a mining operation -- in case Bitcoin/hash price goes too low. It can be trustless bc we can use the assumption that the price of hashing is low to unlock funds.
>>>>>> The problem:
>>>>>> A miner invests in new mining equipment, but if the hash-rate goes up too much (the price he is paid for a hash goes down by too much) he will have a loss.
>>>>>> Solution: trustless hash-price insurance contract (or can we call it an option to sell hashes at a given price?)
>>>>>> An insurer who believes that it's unlikely the price of a hash will go down a lot negotiates a contract with the miner implemented as a Bitcoin transaction:
>>>>>> Inputs: a deposit from the insurer and a premium payment by the miner
>>>>>> Output1: simply the premium payment to the insurer
>>>>>> Output2 -- that's the actual insurance
>>>>>> There are three OR'ed conditions for paying it:
>>>>>> A. After expiry date (in blocks) insurer can spend
>>>>>> B. Both miner and insurer can spend at any time by mutual agreement
>>>>>> C. Before expiry, miner can spend by providing a pre-image that produces a hash within certain difficulty constraints
>>>>>> The thing that makes it a hash-price insurance (or option, pardon my lack of precise financial jargon), is that if hashing becomes cheap enough, it becomes profitable to spend resources finding a suitable pre-image, rather than mining Bitcoin.
>>>>>> Of course, both parties can reach an agreement that doesn't require actually spending these resources -- so the miner can still mine Bitcoin and compensate for the lower-than-expected reward with part of the insurance deposit.
>>>>>> If the price doesn't go down enough, the miner just mines Bitcoin and the insurer gets his deposit back.
>>>>>> It's basically an instrument for guaranteeing a minimum profitability of the mining operation.
>>>>>> Implementation issues: unfortunately we can't do arithmetic comparison with long integers >32bit in the script, so implementation of the difficulty requirement needs to be hacky. I think we can use the hashes of one or more pre-images with a given short length, and the miner has to provide the exact pre-images. The pre-images are chosen by the insurer, and we would need a "honesty" deposit or other mechanism to punish the insurer if he chooses a hash that doesn't correspond to any short-length pre-image. I'm not sure about this implementation though, maybe we actually need new opcodes.
>>>>>> What do you guys think?
>>>>>> Thanks for reading it all! Hope it was worth your time!
>>>>> 
>>>>> bitcoin-dev mailing list
>>>>> bitcoin-dev at lists.linuxfoundation.org
>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>> 
>>>> bitcoin-dev mailing list
>>>> bitcoin-dev at lists.linuxfoundation.org
>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 
> 

From eric at voskuil.org  Sun Oct 20 20:17:03 2019
From: eric at voskuil.org (Eric Voskuil)
Date: Sun, 20 Oct 2019 16:17:03 -0400
Subject: [bitcoin-dev] Trustless hash-price insurance contracts
In-Reply-To: <CAHa=hJVLp5=ao1p5kooE=4Fo9_jh6WVtG1uF0T_vAP7r=9zjww@mail.gmail.com>
References: <CAHa=hJVLp5=ao1p5kooE=4Fo9_jh6WVtG1uF0T_vAP7r=9zjww@mail.gmail.com>
Message-ID: <9C06A36D-C0A7-4C15-ACDB-63FF3572B079@voskuil.org>

Hi Lucas,

This can all be inferred from the problem statement. In other words this doesn?t change the assumptions behind my comments. However this is an unsupportable assumption:

?Difficulty would only go down in this case at the end of life of these equipment, if there isn't a new wave of even more efficient equipment being adopted before that.?

Operating at a loss would only be justifiable in the case of expected future returns, not due to sunk costs.

e

> On Oct 20, 2019, at 15:46, Lucas H <lucash.dev at gmail.com> wrote:
> 
> ?
> Hi, guys.
> 
> Thanks a lot for taking the time to read and discuss my post.
> 
> I definitely wasn't clear enough about the problem statement -- so let me try to clarify my thinking.
> 
> First, the main uncertainty the miner is trying to protect against isn't the inefficiency of his new equipment, but how much new mining equipment is being deployed world-wide, which he can't know in advance (as the system is permissionless).
> 
> Second, there are two different metrics that can mean "profitable" that I think are getting confused (probably my fault for lack of using the right terms).
> 
> - Let's call it "operational profitability", and use "P" to denote it, where P = [bitcoin earned]/time - [operational cost of running equipment]/time.
>    Obviously if P < 0, the miner will just shut down his equipment.
> - Return on investment (ROI). A positive ROI requires not just that P > 0, but that it is enough to compensate for the initial investment of buying or building the equipment. As long as P > 0, a miner will keep his equipment running, even at a negative ROI, as the alternative would be an even worse negative ROI. Sure he can sell it, but however buys it will also keep it running, otherwise the equipment is worthless.
> 
> The instrument I describe above protects against the scenario where P > 0, but ROI < 0.
> (it's possible it could be useful in some cases to protect against P < 0, but that's not my main motivator and isn't an assumption)
> 
> If too many miners are deploying too much new equipment at the same time, it's possible that your ROI becomes negative, while nobody shuts down their equipment and the difficulty still keeps going up. In fact, it is possible for all miners to have negative ROI for a while without a reduction in difficulty. Difficulty would only go down in this case at the end of life of these equipment, if there isn't a new wave of even more efficient equipment
> being adopted before that.
> 
> Let's see a simplified scenario in which the insurance becomes useful. This is just one example, and other scenarios could also work.
> 
> - Bitcoin price relatively constant, that is, it's not the main driver of P during this period.
> - Approximately constant block rewards.
> - New equipment comes to market with much higher efficiency than all old equipment. So the old stock of old equipment becomes irrelevant after a short while. 
> - All miners decide to deploy new equipment, but none knows how much the others are deploying, or when, or at what price or P. 
> - Let's just assume P>0 for all miners using the new equipment.
> - Let's assume every unit of the new equipment runs at the same maximum hashrate it's capable of.
> 
> Let's say miner A buys Na units of the new equipment and the total number deployed by all miners is N.
> 
> A's share of the block rewards will be Na / N. 
> 
> If N is much higher than A's initial estimate, his ROI might well become negative, and the insurance would help him prevent a loss.
> 
> Hope this makes the problem a bit clearer.
> 
> Thanks!
> @lucash-dev
> 
>> On Sun, Oct 20, 2019 at 9:16 AM Eric Voskuil <eric at voskuil.org> wrote:
>> So we are talking about a miner insuring against his own inefficiency.
>> 
>> Furthermore a disproportionate increase in hash rate is based on the expectation of higher future return (investment leads returns). So the insurance could end up paying out against realized profit.
>> 
>> Generally speaking, insuring investment is a zero sum game.
>> 
>> e
>> 
>> > On Oct 20, 2019, at 12:10, JW Weatherman <jw at mathbot.com> wrote:
>> > 
>> > ?Oh, I see your point.
>> > 
>> > However the insurance contract would protect the miner even in that case. A miner with great confidence that he is running optimal hardware and has optimal electricity and labor costs probably wouldn't be interested in purchasing insurance for a high price, but if it was cheap enough it would still be worth it. And any potential new entrants on the edge of jumping in would enter when they otherwise would not have because of the decreased costs (decreased risk).
>> > 
>> > An analogy would be car insurance. If you are an excellent driver you wouldn't be willing to spend a ton of money to protect your car in the event of an accident, but if it is cheap enough you would. And there may be people that are unwilling to take the risk of a damaged car that refrain from becoming drivers until insurance allows them to lower the worst case scenario of a damaged car.
>> > 
>> > -JW
>> > 
>> > 
>> > 
>> > 
>> > ??????? Original Message ???????
>> >> On Sunday, October 20, 2019 10:57 AM, Eric Voskuil <eric at voskuil.org> wrote:
>> >> 
>> >> 
>> >> 
>> >>>> On Oct 20, 2019, at 10:10, JW Weatherman jw at mathbot.com wrote:
>> >>> I think the assumption is not that all miners are unprofitable, but that a single miner could make an investment that becomes unprofitable if the hash rate increases more than he expected.
>> >> 
>> >> This is a restatement of the assumption I questioned. Hash rate increase does not imply unprofitability. The new rig should be profitable.
>> >> 
>> >> What is being assumed is a hash rate increase without a proportional block reward value increase. In this case if the newest equipment is unprofitable, all miners are unprofitable.
>> >> 
>> >>> Depending on the cost of the offered insurance it would be prudent for a miner to decrease his potential loss by buying insurance for this possibility.
>> >>> And the existence of attractive insurance contracts would lower the barrier to entry for new competitors in mining and this would increase bitcoins security.
>> >>> -JW
>> >>> ??????? Original Message ???????
>> >>> 
>> >>>> On Sunday, October 20, 2019 1:03 AM, Eric Voskuil via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:
>> >>>> Hi Lucas,
>> >>>> I would question the assumption inherent in the problem statement. Setting aside variance discount, proximity premium, and questions of relative efficiency, as these are presumably already considered by the miner upon the purchase of new equipment, it?s not clear why a loss is assumed in the case of subsequently increasing hash rate.
>> >>>> The assumption of increasing hash rate implies an expectation of increasing return on investment. There are certainly speculative errors, but a loss on new equipment implies all miners are operating at a loss, which is not a sustainable situation.
>> >>>> If any miner is profitable it is the miner with the new equipment, and if he is not, hash rate will drop until he is. This drop is most likely to be precipitated by older equipment going offline.
>> >>>> Best,
>> >>>> Eric
>> >>>> 
>> >>>>>> On Oct 20, 2019, at 00:31, Lucas H via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:
>> >>>>>> Hi,
>> >>>>>> This is my first post to this list -- even though I did some tiny contributions to bitcoin core I feel quite a beginner -- so if my idea is stupid, already known, or too off-topic, just let me know.
>> >>>>>> TL;DR: a trustless contract that guarantees minimum profitability of a mining operation -- in case Bitcoin/hash price goes too low. It can be trustless bc we can use the assumption that the price of hashing is low to unlock funds.
>> >>>>>> The problem:
>> >>>>>> A miner invests in new mining equipment, but if the hash-rate goes up too much (the price he is paid for a hash goes down by too much) he will have a loss.
>> >>>>>> Solution: trustless hash-price insurance contract (or can we call it an option to sell hashes at a given price?)
>> >>>>>> An insurer who believes that it's unlikely the price of a hash will go down a lot negotiates a contract with the miner implemented as a Bitcoin transaction:
>> >>>>>> Inputs: a deposit from the insurer and a premium payment by the miner
>> >>>>>> Output1: simply the premium payment to the insurer
>> >>>>>> Output2 -- that's the actual insurance
>> >>>>>> There are three OR'ed conditions for paying it:
>> >>>>>> A. After expiry date (in blocks) insurer can spend
>> >>>>>> B. Both miner and insurer can spend at any time by mutual agreement
>> >>>>>> C. Before expiry, miner can spend by providing a pre-image that produces a hash within certain difficulty constraints
>> >>>>>> The thing that makes it a hash-price insurance (or option, pardon my lack of precise financial jargon), is that if hashing becomes cheap enough, it becomes profitable to spend resources finding a suitable pre-image, rather than mining Bitcoin.
>> >>>>>> Of course, both parties can reach an agreement that doesn't require actually spending these resources -- so the miner can still mine Bitcoin and compensate for the lower-than-expected reward with part of the insurance deposit.
>> >>>>>> If the price doesn't go down enough, the miner just mines Bitcoin and the insurer gets his deposit back.
>> >>>>>> It's basically an instrument for guaranteeing a minimum profitability of the mining operation.
>> >>>>>> Implementation issues: unfortunately we can't do arithmetic comparison with long integers >32bit in the script, so implementation of the difficulty requirement needs to be hacky. I think we can use the hashes of one or more pre-images with a given short length, and the miner has to provide the exact pre-images. The pre-images are chosen by the insurer, and we would need a "honesty" deposit or other mechanism to punish the insurer if he chooses a hash that doesn't correspond to any short-length pre-image. I'm not sure about this implementation though, maybe we actually need new opcodes.
>> >>>>>> What do you guys think?
>> >>>>>> Thanks for reading it all! Hope it was worth your time!
>> >>>>> 
>> >>>>> bitcoin-dev mailing list
>> >>>>> bitcoin-dev at lists.linuxfoundation.org
>> >>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>> >>>> 
>> >>>> bitcoin-dev mailing list
>> >>>> bitcoin-dev at lists.linuxfoundation.org
>> >>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>> > 
>> > 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191020/1a643cc1/attachment-0001.html>

From eric at voskuil.org  Mon Oct 21 03:34:12 2019
From: eric at voskuil.org (Eric Voskuil)
Date: Sun, 20 Oct 2019 20:34:12 -0700
Subject: [bitcoin-dev] Trustless hash-price insurance contracts
Message-ID: <DA5B30F2-4553-4A8F-BB37-F8F3DC67A260@voskuil.org>

Hi Lucas,

You are assuming that all miners operate at equal efficiency. The least efficient miners are expected to drop offline first. Even with identical hardware and operational efficiency, the necessary variance discount and proximity premium create a profitability spread. The relation between hash rate and reward value is not only predictable, it is easily observed.

There is an error in your sold hardware scenario. When equipment is sold at a loss, the remaining operating miners have a reduced capital cost, which means, despite higher hash rate, miners are profitable. The least efficient miners have written down their expected losses and hash rate becomes consistent with market returns despite being higher.

With respect to the contract, I don?t yet see this working, but there are several gaps and I don?t want to make assumptions. More detailed specification would be helpful. Even a full scenario with numbers and justifications would be something.

e

> On Oct 20, 2019, at 14:33, Lucas H <lucash.dev at gmail.com> wrote:
> 
> Sorry, Eric, but I think you're completely missing the point.
> 
> It has nothing to do with sunken cost -- but the fact that the mining equipment is good for nothing else other than performing hashing operations.
> As long as someone can get paid more than they spend to keep the equipment running, i.e. P>0,  it will keep running.
> Your argument only makes sense in an ASIC-free world.
> 
> Let's assume you decide to just shut down your whole operation. In that scenario, it doesn't make sense *not* to sell your equipment, even at a loss. Just destroying it makes no economic sense: your loss would be much worse. So you'll sell it -- at a loss -- to someone who will buy it at a price that will make *their* ROI>0 for keeping the equipment running --  and the equipment *will* be again running, and *will* keep the hashrate high. Only consequence of you shutting down your operation is you taking a loss. 
> 
> Even if you sell it to someone who will run it exactly as efficiently as you, or even at lower efficiency (as long as P>0), they'll just pay less for the equipment than you did, their ROI will be >0 and you'll bear the loss. No drop in hashrate.
> 
> Hashrate can only respond to mining being unprofitable in the sense "P" -- not in the sense "ROI". But a miner can still go bankrupt even if P>0.
> 
> Please note that none of the above breaks the economic assumptions of the protocol. The problem I'm talking about isn't a problem in the protocol, but a problem for miners -- and it's the same as in many kinds of economic activity.
> 
> Consider investing in building an oil refinery -- if the price of the refined products get lower than expected to pay for the capital, but still high enough to pay for operating costs, you'd rather keep it running (or sell to someone who will keep it running) than just sell the parts as scrap metal. In that case you might want to protect yourself against the price of the refined products going too low.
> 
> Of course miners can (and maybe already do) hedge against these scenarios using other kinds of instruments -- most likely facilitated by a trusted 3rd party. I'm just interested in the possibility of a new, trustless instrument.
> 
> *Anyway* I'm far more interested in the technical feasibility of the contract, given the economic assumptions, than it's economic practicality in the present.
> 
> 
> 
> 
> 
>>> On Sun, Oct 20, 2019 at 1:17 PM Eric Voskuil <eric at voskuil.org> wrote:
>>> Hi Lucas,
>>> 
>>> This can all be inferred from the problem statement. In other words this doesn?t change the assumptions behind my comments. However this is an unsupportable assumption:
>>> 
>>> ?Difficulty would only go down in this case at the end of life of these equipment, if there isn't a new wave of even more efficient equipment being adopted before that.?
>>> 
>>> Operating at a loss would only be justifiable in the case of expected future returns, not due to sunk costs.
>>> 
>>> e
>>> 
>>>> On Oct 20, 2019, at 15:46, Lucas H <lucash.dev at gmail.com> wrote:
>>>> 
>>> ?
>>> Hi, guys.
>>> 
>>> Thanks a lot for taking the time to read and discuss my post.
>>> 
>>> I definitely wasn't clear enough about the problem statement -- so let me try to clarify my thinking.
>>> 
>>> First, the main uncertainty the miner is trying to protect against isn't the inefficiency of his new equipment, but how much new mining equipment is being deployed world-wide, which he can't know in advance (as the system is permissionless).
>>> 
>>> Second, there are two different metrics that can mean "profitable" that I think are getting confused (probably my fault for lack of using the right terms).
>>> 
>>> - Let's call it "operational profitability", and use "P" to denote it, where P = [bitcoin earned]/time - [operational cost of running equipment]/time.
>>>    Obviously if P < 0, the miner will just shut down his equipment.
>>> - Return on investment (ROI). A positive ROI requires not just that P > 0, but that it is enough to compensate for the initial investment of buying or building the equipment. As long as P > 0, a miner will keep his equipment running, even at a negative ROI, as the alternative would be an even worse negative ROI. Sure he can sell it, but however buys it will also keep it running, otherwise the equipment is worthless.
>>> 
>>> The instrument I describe above protects against the scenario where P > 0, but ROI < 0.
>>> (it's possible it could be useful in some cases to protect against P < 0, but that's not my main motivator and isn't an assumption)
>>> 
>>> If too many miners are deploying too much new equipment at the same time, it's possible that your ROI becomes negative, while nobody shuts down their equipment and the difficulty still keeps going up. In fact, it is possible for all miners to have negative ROI for a while without a reduction in difficulty. Difficulty would only go down in this case at the end of life of these equipment, if there isn't a new wave of even more efficient equipment
>>> being adopted before that.
>>> 
>>> Let's see a simplified scenario in which the insurance becomes useful. This is just one example, and other scenarios could also work.
>>> 
>>> - Bitcoin price relatively constant, that is, it's not the main driver of P during this period.
>>> - Approximately constant block rewards.
>>> - New equipment comes to market with much higher efficiency than all old equipment. So the old stock of old equipment becomes irrelevant after a short while. 
>>> - All miners decide to deploy new equipment, but none knows how much the others are deploying, or when, or at what price or P. 
>>> - Let's just assume P>0 for all miners using the new equipment.
>>> - Let's assume every unit of the new equipment runs at the same maximum hashrate it's capable of.
>>> 
>>> Let's say miner A buys Na units of the new equipment and the total number deployed by all miners is N.
>>> 
>>> A's share of the block rewards will be Na / N. 
>>> 
>>> If N is much higher than A's initial estimate, his ROI might well become negative, and the insurance would help him prevent a loss.
>>> 
>>> Hope this makes the problem a bit clearer.
>>> 
>>> Thanks!
>>> @lucash-dev
>>> 
>>>> On Sun, Oct 20, 2019 at 9:16 AM Eric Voskuil <eric at voskuil.org> wrote:
>>>> So we are talking about a miner insuring against his own inefficiency.
>>>> 
>>>> Furthermore a disproportionate increase in hash rate is based on the expectation of higher future return (investment leads returns). So the insurance could end up paying out against realized profit.
>>>> 
>>>> Generally speaking, insuring investment is a zero sum game.
>>>> 
>>>> e
>>>> 
>>>> > On Oct 20, 2019, at 12:10, JW Weatherman <jw at mathbot.com> wrote:
>>>> > 
>>>> > ?Oh, I see your point.
>>>> > 
>>>> > However the insurance contract would protect the miner even in that case. A miner with great confidence that he is running optimal hardware and has optimal electricity and labor costs probably wouldn't be interested in purchasing insurance for a high price, but if it was cheap enough it would still be worth it. And any potential new entrants on the edge of jumping in would enter when they otherwise would not have because of the decreased costs (decreased risk).
>>>> > 
>>>> > An analogy would be car insurance. If you are an excellent driver you wouldn't be willing to spend a ton of money to protect your car in the event of an accident, but if it is cheap enough you would. And there may be people that are unwilling to take the risk of a damaged car that refrain from becoming drivers until insurance allows them to lower the worst case scenario of a damaged car.
>>>> > 
>>>> > -JW
>>>> > 
>>>> > 
>>>> > 
>>>> > 
>>>> > ??????? Original Message ???????
>>>> >> On Sunday, October 20, 2019 10:57 AM, Eric Voskuil <eric at voskuil.org> wrote:
>>>> >> 
>>>> >> 
>>>> >> 
>>>> >>>> On Oct 20, 2019, at 10:10, JW Weatherman jw at mathbot.com wrote:
>>>> >>> I think the assumption is not that all miners are unprofitable, but that a single miner could make an investment that becomes unprofitable if the hash rate increases more than he expected.
>>>> >> 
>>>> >> This is a restatement of the assumption I questioned. Hash rate increase does not imply unprofitability. The new rig should be profitable.
>>>> >> 
>>>> >> What is being assumed is a hash rate increase without a proportional block reward value increase. In this case if the newest equipment is unprofitable, all miners are unprofitable.
>>>> >> 
>>>> >>> Depending on the cost of the offered insurance it would be prudent for a miner to decrease his potential loss by buying insurance for this possibility.
>>>> >>> And the existence of attractive insurance contracts would lower the barrier to entry for new competitors in mining and this would increase bitcoins security.
>>>> >>> -JW
>>>> >>> ??????? Original Message ???????
>>>> >>> 
>>>> >>>> On Sunday, October 20, 2019 1:03 AM, Eric Voskuil via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:
>>>> >>>> Hi Lucas,
>>>> >>>> I would question the assumption inherent in the problem statement. Setting aside variance discount, proximity premium, and questions of relative efficiency, as these are presumably already considered by the miner upon the purchase of new equipment, it?s not clear why a loss is assumed in the case of subsequently increasing hash rate.
>>>> >>>> The assumption of increasing hash rate implies an expectation of increasing return on investment. There are certainly speculative errors, but a loss on new equipment implies all miners are operating at a loss, which is not a sustainable situation.
>>>> >>>> If any miner is profitable it is the miner with the new equipment, and if he is not, hash rate will drop until he is. This drop is most likely to be precipitated by older equipment going offline.
>>>> >>>> Best,
>>>> >>>> Eric
>>>> >>>> 
>>>> >>>>>> On Oct 20, 2019, at 00:31, Lucas H via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:
>>>> >>>>>> Hi,
>>>> >>>>>> This is my first post to this list -- even though I did some tiny contributions to bitcoin core I feel quite a beginner -- so if my idea is stupid, already known, or too off-topic, just let me know.
>>>> >>>>>> TL;DR: a trustless contract that guarantees minimum profitability of a mining operation -- in case Bitcoin/hash price goes too low. It can be trustless bc we can use the assumption that the price of hashing is low to unlock funds.
>>>> >>>>>> The problem:
>>>> >>>>>> A miner invests in new mining equipment, but if the hash-rate goes up too much (the price he is paid for a hash goes down by too much) he will have a loss.
>>>> >>>>>> Solution: trustless hash-price insurance contract (or can we call it an option to sell hashes at a given price?)
>>>> >>>>>> An insurer who believes that it's unlikely the price of a hash will go down a lot negotiates a contract with the miner implemented as a Bitcoin transaction:
>>>> >>>>>> Inputs: a deposit from the insurer and a premium payment by the miner
>>>> >>>>>> Output1: simply the premium payment to the insurer
>>>> >>>>>> Output2 -- that's the actual insurance
>>>> >>>>>> There are three OR'ed conditions for paying it:
>>>> >>>>>> A. After expiry date (in blocks) insurer can spend
>>>> >>>>>> B. Both miner and insurer can spend at any time by mutual agreement
>>>> >>>>>> C. Before expiry, miner can spend by providing a pre-image that produces a hash within certain difficulty constraints
>>>> >>>>>> The thing that makes it a hash-price insurance (or option, pardon my lack of precise financial jargon), is that if hashing becomes cheap enough, it becomes profitable to spend resources finding a suitable pre-image, rather than mining Bitcoin.
>>>> >>>>>> Of course, both parties can reach an agreement that doesn't require actually spending these resources -- so the miner can still mine Bitcoin and compensate for the lower-than-expected reward with part of the insurance deposit.
>>>> >>>>>> If the price doesn't go down enough, the miner just mines Bitcoin and the insurer gets his deposit back.
>>>> >>>>>> It's basically an instrument for guaranteeing a minimum profitability of the mining operation.
>>>> >>>>>> Implementation issues: unfortunately we can't do arithmetic comparison with long integers >32bit in the script, so implementation of the difficulty requirement needs to be hacky. I think we can use the hashes of one or more pre-images with a given short length, and the miner has to provide the exact pre-images. The pre-images are chosen by the insurer, and we would need a "honesty" deposit or other mechanism to punish the insurer if he chooses a hash that doesn't correspond to any short-length pre-image. I'm not sure about this implementation though, maybe we actually need new opcodes.
>>>> >>>>>> What do you guys think?
>>>> >>>>>> Thanks for reading it all! Hope it was worth your time!
>>>> >>>>> 
>>>> >>>>> bitcoin-dev mailing list
>>>> >>>>> bitcoin-dev at lists.linuxfoundation.org
>>>> >>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>> >>>> 
>>>> >>>> bitcoin-dev mailing list
>>>> >>>> bitcoin-dev at lists.linuxfoundation.org
>>>> >>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>>>> > 
>>>> >
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191020/5b027844/attachment-0001.html>

From jw at mathbot.com  Sun Oct 20 14:10:55 2019
From: jw at mathbot.com (JW Weatherman)
Date: Sun, 20 Oct 2019 14:10:55 +0000
Subject: [bitcoin-dev] Trustless hash-price insurance contracts
In-Reply-To: <ED2B169F-9548-4EBF-AEA0-270EBA4A4502@voskuil.org>
References: <CAHa=hJVuOdSyPhzu+cGHumw-94yAfxPE_1CkMYRBqxPsyjYBKA@mail.gmail.com>
	<ED2B169F-9548-4EBF-AEA0-270EBA4A4502@voskuil.org>
Message-ID: <5vCX5TKGwYMITwx_jp3BuX9nDr_DIez5kDPkF9ATAh_XYrQ4Y2rUGNU7qEkcy54BIEuNLB8TyznoOVBypNRWu0mTnqX4_D1oNK6ZT2fudQA=@mathbot.com>

I think the assumption is not that all miners are unprofitable, but that a single miner could make an investment that becomes unprofitable if the hash rate increases more than he expected.

Depending on the cost of the offered insurance it would be prudent for a miner to decrease his potential loss by buying insurance for this possibility.

And the existence of attractive insurance contracts would lower the barrier to entry for new competitors in mining and this would increase bitcoins security.

-JW




??????? Original Message ???????
On Sunday, October 20, 2019 1:03 AM, Eric Voskuil via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:

> Hi Lucas,
>
> I would question the assumption inherent in the problem statement. Setting aside variance discount, proximity premium, and questions of relative efficiency, as these are presumably already considered by the miner upon the purchase of new equipment, it?s not clear why a loss is assumed in the case of subsequently increasing hash rate.
>
> The assumption of increasing hash rate implies an expectation of increasing return on investment. There are certainly speculative errors, but a loss on new equipment implies all miners are operating at a loss, which is not a sustainable situation.
>
> If any miner is profitable it is the miner with the new equipment, and if he is not, hash rate will drop until he is. This drop is most likely to be precipitated by older equipment going offline.
>
> Best,
> Eric
>
> > On Oct 20, 2019, at 00:31, Lucas H via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:
> > Hi,
> > This is my first post to this list -- even though I did some tiny contributions to bitcoin core I feel quite a beginner -- so if my idea is stupid, already known, or too off-topic, just let me know.
> > TL;DR: a trustless contract that guarantees minimum profitability of a mining operation -- in case Bitcoin/hash price goes too low. It can be trustless bc we can use the assumption that the price of hashing is low to unlock funds.
> > The problem:
> > A miner invests in new mining equipment, but if the hash-rate goes up too much (the price he is paid for a hash goes down by too much) he will have a loss.
> > Solution: trustless hash-price insurance contract (or can we call it an option to sell hashes at a given price?)
> > An insurer who believes that it's unlikely the price of a hash will go down a lot negotiates a contract with the miner implemented as a Bitcoin transaction:
> > Inputs: a deposit from the insurer and a premium payment by the miner
> > Output1: simply the premium payment to the insurer
> > Output2 -- that's the actual insurance
> > There are three OR'ed conditions for paying it:
> > A. After expiry date (in blocks) insurer can spend
> > B. Both miner and insurer can spend at any time by mutual agreement
> > C. Before expiry, miner can spend by providing a pre-image that produces a hash within certain difficulty constraints
> > The thing that makes it a hash-price insurance (or option, pardon my lack of precise financial jargon), is that if hashing becomes cheap enough, it becomes profitable to spend resources finding a suitable pre-image, rather than mining Bitcoin.
> > Of course, both parties can reach an agreement that doesn't require actually spending these resources -- so the miner can still mine Bitcoin and compensate for the lower-than-expected reward with part of the insurance deposit.
> > If the price doesn't go down enough, the miner just mines Bitcoin and the insurer gets his deposit back.
> > It's basically an instrument for guaranteeing a minimum profitability of the mining operation.
> > Implementation issues: unfortunately we can't do arithmetic comparison with long integers >32bit in the script, so implementation of the difficulty requirement needs to be hacky. I think we can use the hashes of one or more pre-images with a given short length, and the miner has to provide the exact pre-images. The pre-images are chosen by the insurer, and we would need a "honesty" deposit or other mechanism to punish the insurer if he chooses a hash that doesn't correspond to any short-length pre-image. I'm not sure about this implementation though, maybe we actually need new opcodes.
> > What do you guys think?
> > Thanks for reading it all! Hope it was worth your time!
> >
> > bitcoin-dev mailing list
> > bitcoin-dev at lists.linuxfoundation.org
> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev



From jw at mathbot.com  Sun Oct 20 16:10:53 2019
From: jw at mathbot.com (JW Weatherman)
Date: Sun, 20 Oct 2019 16:10:53 +0000
Subject: [bitcoin-dev] Trustless hash-price insurance contracts
In-Reply-To: <90A19D12-D964-41AE-A27F-AB99B66936D1@voskuil.org>
References: <5vCX5TKGwYMITwx_jp3BuX9nDr_DIez5kDPkF9ATAh_XYrQ4Y2rUGNU7qEkcy54BIEuNLB8TyznoOVBypNRWu0mTnqX4_D1oNK6ZT2fudQA=@mathbot.com>
	<90A19D12-D964-41AE-A27F-AB99B66936D1@voskuil.org>
Message-ID: <E8cgzgpxhsuivizcjW9yAo2lsv8LmR2kXjBvqm0bA4i0HXa11o-X2ntYXcmjlZ5iCOCS6OzNze-RHIZkqCLHMjtBTan4VnxskFmC-8NX850=@mathbot.com>

Oh, I see your point.

However the insurance contract would protect the miner even in that case. A miner with great confidence that he is running optimal hardware and has optimal electricity and labor costs probably wouldn't be interested in purchasing insurance for a high price, but if it was cheap enough it would still be worth it. And any potential new entrants on the edge of jumping in would enter when they otherwise would not have because of the decreased costs (decreased risk).

An analogy would be car insurance. If you are an excellent driver you wouldn't be willing to spend a ton of money to protect your car in the event of an accident, but if it is cheap enough you would. And there may be people that are unwilling to take the risk of a damaged car that refrain from becoming drivers until insurance allows them to lower the worst case scenario of a damaged car.

-JW




??????? Original Message ???????
On Sunday, October 20, 2019 10:57 AM, Eric Voskuil <eric at voskuil.org> wrote:

>
>
> > On Oct 20, 2019, at 10:10, JW Weatherman jw at mathbot.com wrote:
> > I think the assumption is not that all miners are unprofitable, but that a single miner could make an investment that becomes unprofitable if the hash rate increases more than he expected.
>
> This is a restatement of the assumption I questioned. Hash rate increase does not imply unprofitability. The new rig should be profitable.
>
> What is being assumed is a hash rate increase without a proportional block reward value increase. In this case if the newest equipment is unprofitable, all miners are unprofitable.
>
> > Depending on the cost of the offered insurance it would be prudent for a miner to decrease his potential loss by buying insurance for this possibility.
> > And the existence of attractive insurance contracts would lower the barrier to entry for new competitors in mining and this would increase bitcoins security.
> > -JW
> > ??????? Original Message ???????
> >
> > > On Sunday, October 20, 2019 1:03 AM, Eric Voskuil via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:
> > > Hi Lucas,
> > > I would question the assumption inherent in the problem statement. Setting aside variance discount, proximity premium, and questions of relative efficiency, as these are presumably already considered by the miner upon the purchase of new equipment, it?s not clear why a loss is assumed in the case of subsequently increasing hash rate.
> > > The assumption of increasing hash rate implies an expectation of increasing return on investment. There are certainly speculative errors, but a loss on new equipment implies all miners are operating at a loss, which is not a sustainable situation.
> > > If any miner is profitable it is the miner with the new equipment, and if he is not, hash rate will drop until he is. This drop is most likely to be precipitated by older equipment going offline.
> > > Best,
> > > Eric
> > >
> > > > > On Oct 20, 2019, at 00:31, Lucas H via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:
> > > > > Hi,
> > > > > This is my first post to this list -- even though I did some tiny contributions to bitcoin core I feel quite a beginner -- so if my idea is stupid, already known, or too off-topic, just let me know.
> > > > > TL;DR: a trustless contract that guarantees minimum profitability of a mining operation -- in case Bitcoin/hash price goes too low. It can be trustless bc we can use the assumption that the price of hashing is low to unlock funds.
> > > > > The problem:
> > > > > A miner invests in new mining equipment, but if the hash-rate goes up too much (the price he is paid for a hash goes down by too much) he will have a loss.
> > > > > Solution: trustless hash-price insurance contract (or can we call it an option to sell hashes at a given price?)
> > > > > An insurer who believes that it's unlikely the price of a hash will go down a lot negotiates a contract with the miner implemented as a Bitcoin transaction:
> > > > > Inputs: a deposit from the insurer and a premium payment by the miner
> > > > > Output1: simply the premium payment to the insurer
> > > > > Output2 -- that's the actual insurance
> > > > > There are three OR'ed conditions for paying it:
> > > > > A. After expiry date (in blocks) insurer can spend
> > > > > B. Both miner and insurer can spend at any time by mutual agreement
> > > > > C. Before expiry, miner can spend by providing a pre-image that produces a hash within certain difficulty constraints
> > > > > The thing that makes it a hash-price insurance (or option, pardon my lack of precise financial jargon), is that if hashing becomes cheap enough, it becomes profitable to spend resources finding a suitable pre-image, rather than mining Bitcoin.
> > > > > Of course, both parties can reach an agreement that doesn't require actually spending these resources -- so the miner can still mine Bitcoin and compensate for the lower-than-expected reward with part of the insurance deposit.
> > > > > If the price doesn't go down enough, the miner just mines Bitcoin and the insurer gets his deposit back.
> > > > > It's basically an instrument for guaranteeing a minimum profitability of the mining operation.
> > > > > Implementation issues: unfortunately we can't do arithmetic comparison with long integers >32bit in the script, so implementation of the difficulty requirement needs to be hacky. I think we can use the hashes of one or more pre-images with a given short length, and the miner has to provide the exact pre-images. The pre-images are chosen by the insurer, and we would need a "honesty" deposit or other mechanism to punish the insurer if he chooses a hash that doesn't correspond to any short-length pre-image. I'm not sure about this implementation though, maybe we actually need new opcodes.
> > > > > What do you guys think?
> > > > > Thanks for reading it all! Hope it was worth your time!
> > > >
> > > > bitcoin-dev mailing list
> > > > bitcoin-dev at lists.linuxfoundation.org
> > > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> > >
> > > bitcoin-dev mailing list
> > > bitcoin-dev at lists.linuxfoundation.org
> > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev



From lucash.dev at gmail.com  Sun Oct 20 19:45:49 2019
From: lucash.dev at gmail.com (Lucas H)
Date: Sun, 20 Oct 2019 12:45:49 -0700
Subject: [bitcoin-dev] Trustless hash-price insurance contracts
In-Reply-To: <C54E6D4A-7E74-4169-8D92-1CCD633A6DE3@voskuil.org>
References: <E8cgzgpxhsuivizcjW9yAo2lsv8LmR2kXjBvqm0bA4i0HXa11o-X2ntYXcmjlZ5iCOCS6OzNze-RHIZkqCLHMjtBTan4VnxskFmC-8NX850=@mathbot.com>
	<C54E6D4A-7E74-4169-8D92-1CCD633A6DE3@voskuil.org>
Message-ID: <CAHa=hJVLp5=ao1p5kooE=4Fo9_jh6WVtG1uF0T_vAP7r=9zjww@mail.gmail.com>

Hi, guys.

Thanks a lot for taking the time to read and discuss my post.

I definitely wasn't clear enough about the problem statement -- so let me
try to clarify my thinking.

First, the main uncertainty the miner is trying to protect against isn't
the inefficiency of his new equipment, but how much new mining equipment is
being deployed world-wide, which he can't know in advance (as the system is
permissionless).

Second, there are two different metrics that can mean "profitable" that I
think are getting confused (probably my fault for lack of using the right
terms).

- Let's call it "operational profitability", and use "P" to denote it,
where P = [bitcoin earned]/time - [operational cost of running
equipment]/time.
   Obviously if P < 0, the miner will just shut down his equipment.
- Return on investment (ROI). A positive ROI requires not just that P > 0,
but that it is enough to compensate for the initial investment of buying or
building the equipment. As long as P > 0, a miner will keep his equipment
running, even at a negative ROI, as the alternative would be an even worse
negative ROI. Sure he can sell it, but however buys it will also keep it
running, otherwise the equipment is worthless.

The instrument I describe above protects against the scenario where P > 0,
but ROI < 0.
(it's possible it could be useful in some cases to protect against P < 0,
but that's not my main motivator and isn't an assumption)

If too many miners are deploying too much new equipment at the same time,
it's possible that your ROI becomes negative, while nobody shuts down their
equipment and the difficulty still keeps going up. In fact, it is possible
for all miners to have negative ROI for a while without a reduction in
difficulty. Difficulty would only go down in this case at the end of life
of these equipment, if there isn't a new wave of even more efficient
equipment
being adopted before that.

Let's see a simplified scenario in which the insurance becomes useful. This
is just one example, and other scenarios could also work.

- Bitcoin price relatively constant, that is, it's not the main driver of P
during this period.
- Approximately constant block rewards.
- New equipment comes to market with much higher efficiency than all old
equipment. So the old stock of old equipment becomes irrelevant after a
short while.
- All miners decide to deploy new equipment, but none knows how much the
others are deploying, or when, or at what price or P.
- Let's just assume P>0 for all miners using the new equipment.
- Let's assume every unit of the new equipment runs at the same maximum
hashrate it's capable of.

Let's say miner A buys Na units of the new equipment and the total number
deployed by all miners is N.

A's share of the block rewards will be Na / N.

If N is much higher than A's initial estimate, his ROI might well become
negative, and the insurance would help him prevent a loss.

Hope this makes the problem a bit clearer.

Thanks!
@lucash-dev

On Sun, Oct 20, 2019 at 9:16 AM Eric Voskuil <eric at voskuil.org> wrote:

> So we are talking about a miner insuring against his own inefficiency.
>
> Furthermore a disproportionate increase in hash rate is based on the
> expectation of higher future return (investment leads returns). So the
> insurance could end up paying out against realized profit.
>
> Generally speaking, insuring investment is a zero sum game.
>
> e
>
> > On Oct 20, 2019, at 12:10, JW Weatherman <jw at mathbot.com> wrote:
> >
> > ?Oh, I see your point.
> >
> > However the insurance contract would protect the miner even in that
> case. A miner with great confidence that he is running optimal hardware and
> has optimal electricity and labor costs probably wouldn't be interested in
> purchasing insurance for a high price, but if it was cheap enough it would
> still be worth it. And any potential new entrants on the edge of jumping in
> would enter when they otherwise would not have because of the decreased
> costs (decreased risk).
> >
> > An analogy would be car insurance. If you are an excellent driver you
> wouldn't be willing to spend a ton of money to protect your car in the
> event of an accident, but if it is cheap enough you would. And there may be
> people that are unwilling to take the risk of a damaged car that refrain
> from becoming drivers until insurance allows them to lower the worst case
> scenario of a damaged car.
> >
> > -JW
> >
> >
> >
> >
> > ??????? Original Message ???????
> >> On Sunday, October 20, 2019 10:57 AM, Eric Voskuil <eric at voskuil.org>
> wrote:
> >>
> >>
> >>
> >>>> On Oct 20, 2019, at 10:10, JW Weatherman jw at mathbot.com wrote:
> >>> I think the assumption is not that all miners are unprofitable, but
> that a single miner could make an investment that becomes unprofitable if
> the hash rate increases more than he expected.
> >>
> >> This is a restatement of the assumption I questioned. Hash rate
> increase does not imply unprofitability. The new rig should be profitable.
> >>
> >> What is being assumed is a hash rate increase without a proportional
> block reward value increase. In this case if the newest equipment is
> unprofitable, all miners are unprofitable.
> >>
> >>> Depending on the cost of the offered insurance it would be prudent for
> a miner to decrease his potential loss by buying insurance for this
> possibility.
> >>> And the existence of attractive insurance contracts would lower the
> barrier to entry for new competitors in mining and this would increase
> bitcoins security.
> >>> -JW
> >>> ??????? Original Message ???????
> >>>
> >>>> On Sunday, October 20, 2019 1:03 AM, Eric Voskuil via bitcoin-dev
> bitcoin-dev at lists.linuxfoundation.org wrote:
> >>>> Hi Lucas,
> >>>> I would question the assumption inherent in the problem statement.
> Setting aside variance discount, proximity premium, and questions of
> relative efficiency, as these are presumably already considered by the
> miner upon the purchase of new equipment, it?s not clear why a loss is
> assumed in the case of subsequently increasing hash rate.
> >>>> The assumption of increasing hash rate implies an expectation of
> increasing return on investment. There are certainly speculative errors,
> but a loss on new equipment implies all miners are operating at a loss,
> which is not a sustainable situation.
> >>>> If any miner is profitable it is the miner with the new equipment,
> and if he is not, hash rate will drop until he is. This drop is most likely
> to be precipitated by older equipment going offline.
> >>>> Best,
> >>>> Eric
> >>>>
> >>>>>> On Oct 20, 2019, at 00:31, Lucas H via bitcoin-dev
> bitcoin-dev at lists.linuxfoundation.org wrote:
> >>>>>> Hi,
> >>>>>> This is my first post to this list -- even though I did some tiny
> contributions to bitcoin core I feel quite a beginner -- so if my idea is
> stupid, already known, or too off-topic, just let me know.
> >>>>>> TL;DR: a trustless contract that guarantees minimum profitability
> of a mining operation -- in case Bitcoin/hash price goes too low. It can be
> trustless bc we can use the assumption that the price of hashing is low to
> unlock funds.
> >>>>>> The problem:
> >>>>>> A miner invests in new mining equipment, but if the hash-rate goes
> up too much (the price he is paid for a hash goes down by too much) he will
> have a loss.
> >>>>>> Solution: trustless hash-price insurance contract (or can we call
> it an option to sell hashes at a given price?)
> >>>>>> An insurer who believes that it's unlikely the price of a hash will
> go down a lot negotiates a contract with the miner implemented as a Bitcoin
> transaction:
> >>>>>> Inputs: a deposit from the insurer and a premium payment by the
> miner
> >>>>>> Output1: simply the premium payment to the insurer
> >>>>>> Output2 -- that's the actual insurance
> >>>>>> There are three OR'ed conditions for paying it:
> >>>>>> A. After expiry date (in blocks) insurer can spend
> >>>>>> B. Both miner and insurer can spend at any time by mutual agreement
> >>>>>> C. Before expiry, miner can spend by providing a pre-image that
> produces a hash within certain difficulty constraints
> >>>>>> The thing that makes it a hash-price insurance (or option, pardon
> my lack of precise financial jargon), is that if hashing becomes cheap
> enough, it becomes profitable to spend resources finding a suitable
> pre-image, rather than mining Bitcoin.
> >>>>>> Of course, both parties can reach an agreement that doesn't require
> actually spending these resources -- so the miner can still mine Bitcoin
> and compensate for the lower-than-expected reward with part of the
> insurance deposit.
> >>>>>> If the price doesn't go down enough, the miner just mines Bitcoin
> and the insurer gets his deposit back.
> >>>>>> It's basically an instrument for guaranteeing a minimum
> profitability of the mining operation.
> >>>>>> Implementation issues: unfortunately we can't do arithmetic
> comparison with long integers >32bit in the script, so implementation of
> the difficulty requirement needs to be hacky. I think we can use the hashes
> of one or more pre-images with a given short length, and the miner has to
> provide the exact pre-images. The pre-images are chosen by the insurer, and
> we would need a "honesty" deposit or other mechanism to punish the insurer
> if he chooses a hash that doesn't correspond to any short-length pre-image.
> I'm not sure about this implementation though, maybe we actually need new
> opcodes.
> >>>>>> What do you guys think?
> >>>>>> Thanks for reading it all! Hope it was worth your time!
> >>>>>
> >>>>> bitcoin-dev mailing list
> >>>>> bitcoin-dev at lists.linuxfoundation.org
> >>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> >>>>
> >>>> bitcoin-dev mailing list
> >>>> bitcoin-dev at lists.linuxfoundation.org
> >>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191020/46e65991/attachment-0001.html>

From lucash.dev at gmail.com  Sun Oct 20 21:33:24 2019
From: lucash.dev at gmail.com (Lucas H)
Date: Sun, 20 Oct 2019 14:33:24 -0700
Subject: [bitcoin-dev] Trustless hash-price insurance contracts
In-Reply-To: <9C06A36D-C0A7-4C15-ACDB-63FF3572B079@voskuil.org>
References: <CAHa=hJVLp5=ao1p5kooE=4Fo9_jh6WVtG1uF0T_vAP7r=9zjww@mail.gmail.com>
	<9C06A36D-C0A7-4C15-ACDB-63FF3572B079@voskuil.org>
Message-ID: <CAHa=hJVG9J2uzb84Be2OoSG1j3S4cWzkps8hXYmL7o-hVmEhgw@mail.gmail.com>

Sorry, Eric, but I think you're completely missing the point.

It has nothing to do with sunken cost -- but the fact that the mining
equipment is good for nothing else other than performing hashing operations.
As long as someone can get paid more than they spend to keep the equipment
running, i.e. P>0,  it will keep running.
Your argument only makes sense in an ASIC-free world.

Let's assume you decide to just shut down your whole operation. In that
scenario, it doesn't make sense *not* to sell your equipment, even at a
loss. Just destroying it makes no economic sense: your loss would be much
worse. So you'll sell it -- at a loss -- to someone who will buy it at a
price that will make *their* ROI>0 for keeping the equipment running --
and the equipment *will* be again running, and *will* keep the hashrate
high. Only consequence of you shutting down your operation is you taking a
loss.

Even if you sell it to someone who will run it exactly as efficiently as
you, or even at lower efficiency (as long as P>0), they'll just pay less
for the equipment than you did, their ROI will be >0 and you'll bear the
loss. No drop in hashrate.

Hashrate can only respond to mining being unprofitable in the sense "P" --
not in the sense "ROI". But a miner can still go bankrupt even if P>0.

Please note that none of the above breaks the economic assumptions of the
protocol. The problem I'm talking about isn't a problem in the protocol,
but a problem for miners -- and it's the same as in many kinds of economic
activity.

Consider investing in building an oil refinery -- if the price of the
refined products get lower than expected to pay for the capital, but still
high enough to pay for operating costs, you'd rather keep it running (or
sell to someone who will keep it running) than just sell the parts as scrap
metal. In that case you might want to protect yourself against the price of
the refined products going too low.

Of course miners can (and maybe already do) hedge against these scenarios
using other kinds of instruments -- most likely facilitated by a trusted
3rd party. I'm just interested in the possibility of a new, trustless
instrument.

*Anyway* I'm far more interested in the technical feasibility of the
contract, given the economic assumptions, than it's economic practicality
in the present.





On Sun, Oct 20, 2019 at 1:17 PM Eric Voskuil <eric at voskuil.org> wrote:

> Hi Lucas,
>
> This can all be inferred from the problem statement. In other words this
> doesn?t change the assumptions behind my comments. However this is an
> unsupportable assumption:
>
> ?Difficulty would only go down in this case at the end of life of these
> equipment, if there isn't a new wave of even more efficient equipment being
> adopted before that.?
>
> Operating at a loss would only be justifiable in the case of expected
> future returns, not due to sunk costs.
>
> e
>
> On Oct 20, 2019, at 15:46, Lucas H <lucash.dev at gmail.com> wrote:
>
> ?
> Hi, guys.
>
> Thanks a lot for taking the time to read and discuss my post.
>
> I definitely wasn't clear enough about the problem statement -- so let me
> try to clarify my thinking.
>
> First, the main uncertainty the miner is trying to protect against isn't
> the inefficiency of his new equipment, but how much new mining equipment is
> being deployed world-wide, which he can't know in advance (as the system is
> permissionless).
>
> Second, there are two different metrics that can mean "profitable" that I
> think are getting confused (probably my fault for lack of using the right
> terms).
>
> - Let's call it "operational profitability", and use "P" to denote it,
> where P = [bitcoin earned]/time - [operational cost of running
> equipment]/time.
>    Obviously if P < 0, the miner will just shut down his equipment.
> - Return on investment (ROI). A positive ROI requires not just that P > 0,
> but that it is enough to compensate for the initial investment of buying or
> building the equipment. As long as P > 0, a miner will keep his equipment
> running, even at a negative ROI, as the alternative would be an even worse
> negative ROI. Sure he can sell it, but however buys it will also keep it
> running, otherwise the equipment is worthless.
>
> The instrument I describe above protects against the scenario where P > 0,
> but ROI < 0.
> (it's possible it could be useful in some cases to protect against P < 0,
> but that's not my main motivator and isn't an assumption)
>
> If too many miners are deploying too much new equipment at the same time,
> it's possible that your ROI becomes negative, while nobody shuts down their
> equipment and the difficulty still keeps going up. In fact, it is possible
> for all miners to have negative ROI for a while without a reduction in
> difficulty. Difficulty would only go down in this case at the end of life
> of these equipment, if there isn't a new wave of even more efficient
> equipment
> being adopted before that.
>
> Let's see a simplified scenario in which the insurance becomes useful.
> This is just one example, and other scenarios could also work.
>
> - Bitcoin price relatively constant, that is, it's not the main driver of
> P during this period.
> - Approximately constant block rewards.
> - New equipment comes to market with much higher efficiency than all old
> equipment. So the old stock of old equipment becomes irrelevant after a
> short while.
> - All miners decide to deploy new equipment, but none knows how much the
> others are deploying, or when, or at what price or P.
> - Let's just assume P>0 for all miners using the new equipment.
> - Let's assume every unit of the new equipment runs at the same maximum
> hashrate it's capable of.
>
> Let's say miner A buys Na units of the new equipment and the total number
> deployed by all miners is N.
>
> A's share of the block rewards will be Na / N.
>
> If N is much higher than A's initial estimate, his ROI might well become
> negative, and the insurance would help him prevent a loss.
>
> Hope this makes the problem a bit clearer.
>
> Thanks!
> @lucash-dev
>
> On Sun, Oct 20, 2019 at 9:16 AM Eric Voskuil <eric at voskuil.org> wrote:
>
>> So we are talking about a miner insuring against his own inefficiency.
>>
>> Furthermore a disproportionate increase in hash rate is based on the
>> expectation of higher future return (investment leads returns). So the
>> insurance could end up paying out against realized profit.
>>
>> Generally speaking, insuring investment is a zero sum game.
>>
>> e
>>
>> > On Oct 20, 2019, at 12:10, JW Weatherman <jw at mathbot.com> wrote:
>> >
>> > ?Oh, I see your point.
>> >
>> > However the insurance contract would protect the miner even in that
>> case. A miner with great confidence that he is running optimal hardware and
>> has optimal electricity and labor costs probably wouldn't be interested in
>> purchasing insurance for a high price, but if it was cheap enough it would
>> still be worth it. And any potential new entrants on the edge of jumping in
>> would enter when they otherwise would not have because of the decreased
>> costs (decreased risk).
>> >
>> > An analogy would be car insurance. If you are an excellent driver you
>> wouldn't be willing to spend a ton of money to protect your car in the
>> event of an accident, but if it is cheap enough you would. And there may be
>> people that are unwilling to take the risk of a damaged car that refrain
>> from becoming drivers until insurance allows them to lower the worst case
>> scenario of a damaged car.
>> >
>> > -JW
>> >
>> >
>> >
>> >
>> > ??????? Original Message ???????
>> >> On Sunday, October 20, 2019 10:57 AM, Eric Voskuil <eric at voskuil.org>
>> wrote:
>> >>
>> >>
>> >>
>> >>>> On Oct 20, 2019, at 10:10, JW Weatherman jw at mathbot.com wrote:
>> >>> I think the assumption is not that all miners are unprofitable, but
>> that a single miner could make an investment that becomes unprofitable if
>> the hash rate increases more than he expected.
>> >>
>> >> This is a restatement of the assumption I questioned. Hash rate
>> increase does not imply unprofitability. The new rig should be profitable.
>> >>
>> >> What is being assumed is a hash rate increase without a proportional
>> block reward value increase. In this case if the newest equipment is
>> unprofitable, all miners are unprofitable.
>> >>
>> >>> Depending on the cost of the offered insurance it would be prudent
>> for a miner to decrease his potential loss by buying insurance for this
>> possibility.
>> >>> And the existence of attractive insurance contracts would lower the
>> barrier to entry for new competitors in mining and this would increase
>> bitcoins security.
>> >>> -JW
>> >>> ??????? Original Message ???????
>> >>>
>> >>>> On Sunday, October 20, 2019 1:03 AM, Eric Voskuil via bitcoin-dev
>> bitcoin-dev at lists.linuxfoundation.org wrote:
>> >>>> Hi Lucas,
>> >>>> I would question the assumption inherent in the problem statement.
>> Setting aside variance discount, proximity premium, and questions of
>> relative efficiency, as these are presumably already considered by the
>> miner upon the purchase of new equipment, it?s not clear why a loss is
>> assumed in the case of subsequently increasing hash rate.
>> >>>> The assumption of increasing hash rate implies an expectation of
>> increasing return on investment. There are certainly speculative errors,
>> but a loss on new equipment implies all miners are operating at a loss,
>> which is not a sustainable situation.
>> >>>> If any miner is profitable it is the miner with the new equipment,
>> and if he is not, hash rate will drop until he is. This drop is most likely
>> to be precipitated by older equipment going offline.
>> >>>> Best,
>> >>>> Eric
>> >>>>
>> >>>>>> On Oct 20, 2019, at 00:31, Lucas H via bitcoin-dev
>> bitcoin-dev at lists.linuxfoundation.org wrote:
>> >>>>>> Hi,
>> >>>>>> This is my first post to this list -- even though I did some tiny
>> contributions to bitcoin core I feel quite a beginner -- so if my idea is
>> stupid, already known, or too off-topic, just let me know.
>> >>>>>> TL;DR: a trustless contract that guarantees minimum profitability
>> of a mining operation -- in case Bitcoin/hash price goes too low. It can be
>> trustless bc we can use the assumption that the price of hashing is low to
>> unlock funds.
>> >>>>>> The problem:
>> >>>>>> A miner invests in new mining equipment, but if the hash-rate goes
>> up too much (the price he is paid for a hash goes down by too much) he will
>> have a loss.
>> >>>>>> Solution: trustless hash-price insurance contract (or can we call
>> it an option to sell hashes at a given price?)
>> >>>>>> An insurer who believes that it's unlikely the price of a hash
>> will go down a lot negotiates a contract with the miner implemented as a
>> Bitcoin transaction:
>> >>>>>> Inputs: a deposit from the insurer and a premium payment by the
>> miner
>> >>>>>> Output1: simply the premium payment to the insurer
>> >>>>>> Output2 -- that's the actual insurance
>> >>>>>> There are three OR'ed conditions for paying it:
>> >>>>>> A. After expiry date (in blocks) insurer can spend
>> >>>>>> B. Both miner and insurer can spend at any time by mutual agreement
>> >>>>>> C. Before expiry, miner can spend by providing a pre-image that
>> produces a hash within certain difficulty constraints
>> >>>>>> The thing that makes it a hash-price insurance (or option, pardon
>> my lack of precise financial jargon), is that if hashing becomes cheap
>> enough, it becomes profitable to spend resources finding a suitable
>> pre-image, rather than mining Bitcoin.
>> >>>>>> Of course, both parties can reach an agreement that doesn't
>> require actually spending these resources -- so the miner can still mine
>> Bitcoin and compensate for the lower-than-expected reward with part of the
>> insurance deposit.
>> >>>>>> If the price doesn't go down enough, the miner just mines Bitcoin
>> and the insurer gets his deposit back.
>> >>>>>> It's basically an instrument for guaranteeing a minimum
>> profitability of the mining operation.
>> >>>>>> Implementation issues: unfortunately we can't do arithmetic
>> comparison with long integers >32bit in the script, so implementation of
>> the difficulty requirement needs to be hacky. I think we can use the hashes
>> of one or more pre-images with a given short length, and the miner has to
>> provide the exact pre-images. The pre-images are chosen by the insurer, and
>> we would need a "honesty" deposit or other mechanism to punish the insurer
>> if he chooses a hash that doesn't correspond to any short-length pre-image.
>> I'm not sure about this implementation though, maybe we actually need new
>> opcodes.
>> >>>>>> What do you guys think?
>> >>>>>> Thanks for reading it all! Hope it was worth your time!
>> >>>>>
>> >>>>> bitcoin-dev mailing list
>> >>>>> bitcoin-dev at lists.linuxfoundation.org
>> >>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>> >>>>
>> >>>> bitcoin-dev mailing list
>> >>>> bitcoin-dev at lists.linuxfoundation.org
>> >>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>> >
>> >
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191020/556d1bf2/attachment-0001.html>

From andreas at schildbach.de  Mon Oct 21 08:44:16 2019
From: andreas at schildbach.de (Andreas Schildbach)
Date: Mon, 21 Oct 2019 10:44:16 +0200
Subject: [bitcoin-dev] Removal of reject network messages from Bitcoin
	Core (BIP61)
In-Reply-To: <CAFmfg2sU=ur7NdzU6r9bGC=xZqcX7WA7ux-r8QOwNa+FKhS4Pw@mail.gmail.com>
References: <CAK51vgDO2Tg38XbW0pqAnO3ETJ_qf8owRsUYsTXmrf7H2yGZtw@mail.gmail.com>
	<CAFmfg2u3cLwG4h=tSF1+ho__1n2n4xyBGH+mwQgVYE9c_s+EMw@mail.gmail.com>
	<CAFmfg2sU=ur7NdzU6r9bGC=xZqcX7WA7ux-r8QOwNa+FKhS4Pw@mail.gmail.com>
Message-ID: <qojr51$7pnq$1@blaine.gmane.org>

I guess then the best way to discover nodes that have reject messages
enabled is connecting/disconnecting to random nodes and send them
invalid transactions and keep the ones which reply with a reject message.


On 18/10/2019 22.53, John Newbery via bitcoin-dev wrote:
>>?Is there a NODE_* bit we can use to pick peers that support this
> (useful!) feature?
> 
> No. BIP 61 has no mechanism for advertising that a node will send REJECT
> messages.
> 
> On Wed, Oct 16, 2019 at 12:43 PM John Newbery <john at johnnewbery.com
> <mailto:john at johnnewbery.com>> wrote:
> 
>     Following discussion on this mailing list, support for BIP 61 REJECT
>     messages was not removed from Bitcoin Core in V0.19. The behaviour
>     in that upcoming release is that REJECT messages are disabled by
>     default and can be enabled using the `-enablebip61` command line option.
> 
>     Support for REJECT messages will be removed entirely in Bitcoin Core
>     V0.20, expected for release in mid 2020. The PR to remove support
>     was merged into Bitcoin Core's master branch this week.
> 
>     Adoption of new Bitcoin Core versions across reachable nodes
>     generally takes several months.
>     https://bitnodes.earn.com/dashboard/?days=365 shows that although
>     v0.18 was released in May 2019, there are still several hundred
>     reachable nodes on V0.17, V0.16, V0.15 and earlier software.
>     Software that currently use REJECT messages from public nodes for
>     troubleshooting issues therefore have plenty of time to transition
>     to one of the methods listed by Marco in the email above.
> 
>     John
> 
>     On Tue, Mar 5, 2019 at 10:28 PM Marco Falke via bitcoin-dev
>     <bitcoin-dev at lists.linuxfoundation.org
>     <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:
> 
>         Bitcoin Core may send "reject" messages as response to "tx",
>         "block" or
>         "version" messages from a network peer when the message could
>         not be accepted.
> 
>         This feature is toggled by the `-enablebip61` command line
>         option and has been
>         disabled by default since Bitcoin Core version 0.18.0 (not yet
>         released as of
>         time of writing). Nodes on the network can not generally be
>         trusted to send
>         valid ("reject") messages, so this should only ever be used when
>         connected to a
>         trusted node. At this time, I am not aware of any software that
>         requires this
>         feature, and I would like to remove if from Bitcoin Core to make
>         the codebase
>         slimmer, easier to understand and maintain. Let us know if your
>         application
>         relies on this feature and you can not use any of the
>         recommended alternatives:
> 
>         * Testing or debugging of implementations of the Bitcoin P2P
>         network protocol
>         ? should be done by inspecting the log messages that are
>         produced by a recent
>         ? version of Bitcoin Core. Bitcoin Core logs debug messages
>         ? (`-debug=<category>`) to a stream (`-printtoconsole`) or to a file
>         ? (`-debuglogfile=<debug.log>`).
> 
>         * Testing the validity of a block can be achieved by specific RPCs:
>         ? - `submitblock`
>         ? - `getblocktemplate` with `'mode'` set to `'proposal'` for
>         blocks with
>         ? ? potentially invalid POW
> 
>         * Testing the validity of a transaction can be achieved by
>         specific RPCs:
>         ? - `sendrawtransaction`
>         ? - `testmempoolaccept`
> 
>         * Wallets should not use the absence of "reject" messages to
>         indicate a
>         ? transaction has propagated the network, nor should wallets use
>         "reject"
>         ? messages to set transaction fees. Wallets should rather use
>         fee estimation
>         ? to determine transaction fees and set replace-by-fee if
>         desired. Thus, they
>         ? could wait until the transaction has confirmed (taking into
>         account the fee
>         ? target they set (compare the RPC `estimatesmartfee`)) or
>         listen for the
>         ? transaction announcement by other network peers to check for
>         propagation.
> 
>         I propose to remove "reject" messages from Bitcoin Core 0.19.0
>         unless there are
>         valid concerns about its removal.
> 
>         Marco
>         _______________________________________________
>         bitcoin-dev mailing list
>         bitcoin-dev at lists.linuxfoundation.org
>         <mailto:bitcoin-dev at lists.linuxfoundation.org>
>         https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 
> 
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
> 



From riccardo.casatta at gmail.com  Mon Oct 21 11:00:26 2019
From: riccardo.casatta at gmail.com (Riccardo Casatta)
Date: Mon, 21 Oct 2019 13:00:26 +0200
Subject: [bitcoin-dev] Draft BIP for SNICKER
In-Reply-To: <20191021000608.ajvzjxh6phtuhydp@ganymede>
References: <YwZ3vq20LFvpx-nKn1RJjcRHwYTAVCC0v0EyD0y6zVMlQtKXUFNAaEk_QE2dzYDU6z2eK0S0TDXRPfl1_y93RgDjdCGboOgjcERBTLUPHao=@protonmail.com>
	<20191021000608.ajvzjxh6phtuhydp@ganymede>
Message-ID: <CADabwBCnTj1_9M7oibhTj0JAj3O=i+ejHPZMDOUpyneJAuGGcw@mail.gmail.com>

The "Receiver" could immediately create a tx that spend the coinjoin
outputs to bip32 keys,
The hard part is that he had to delay the broadcast otherwise he loose
privacy

Il giorno lun 21 ott 2019 alle ore 02:08 David A. Harding via bitcoin-dev <
bitcoin-dev at lists.linuxfoundation.org> ha scritto:

> On Sun, Oct 20, 2019 at 12:29:25AM +0000, SomberNight via bitcoin-dev
> wrote:
> > waxwing, ThomasV, and I recently had a discussion about implementing
> > SNICKER in Electrum; specifically the "Receiver" role.
>
> That'd be awesome!
>
> > As the referenced section [0] explains, the "Receiver" can restore
> > from seed, and assuming he knows he needs to do extra scanning steps
> > (e.g. via a seed version that signals SNICKER support), he can find
> > and regain access to his SNICKER outputs. However, to calculate `c` he
> > needs access to his private keys, as it is the ECDH of one of the
> > Receiver's pubkeys and one of the Proposer's pubkeys.
> >
> > This means the proposed scheme is fundamentally incompatible with
> > watch-only wallets.
> >
> > [0]
> https://gist.github.com/AdamISZ/2c13fb5819bd469ca318156e2cf25d79#Storage_of_Keys
>
> Your logic seems correct for the watching half of the wallet, but I
> think it's ok to consider requiring interaction with the cold wallet.
> Let's look at the recovery procedure from the SNICKER documentation
> that you kindly cited:
>
>     1. Derive all regular addresses normally (doable watch-only for
>     wallets using public BIP32 derivation)
>
>     2. Find all transactions spending an output for each of those
>     addresses.  Determine whether the spend looks like a SNICKER
>     coinjoin (e.g. "two equal-[value] outputs").  (doable watch-only)
>
>     3. "For each of those transactions, check, for each of the two equal
>     sized outputs, whether one destination address can be regenerated
>     from by taking c found in the method described above" (not doable
>     watch only; requires private keys)
>
> I'd expect the set of candidate transactions produced in step #2 to be
> pretty small and probably with no false positives for users not
> participating in SNICKER coinjoins or doing lots of payment batching.
> That means, if any SNICKER candidates were found by a watch-only wallet,
> they could be compactly bundled up and the user could be encouraged to
> copy them to the corresponding cold wallet using the same means used for
> PSBTs (e.g. USB drive, QR codes, etc).  You wouldn't even need the whole
> transactions, just the BIP32 index of the user's key, the pubkey of the
> suspected proposer, and a checksum of the resultant address.
>
> The cold wallet could then perform step #3 using its private keys and
> return a file/QRcode/whatever to the hot wallet telling it any shared
> secrets it found.
>
> This process may need to be repeated several times if an output created
> by one SNICKER round is spent in a subsequent SNICKER round.  This can be
> addressed by simply refusing to participate in chains of SNICKER
> transactions or by refusing to participant in chains of SNICKERs more
> than n long (requring a maximum n rounds of recovery).  It could also be
> addressed by the watching-only wallet looking ahead at the block chain a
> bit in order to grab SNICKER-like child and grandchild transactions of
> our SNICKER candidates and sending them also to the cold wallet for
> attempted shared secret recovery.
>
> The SNICKER recovery process is, of course, only required for wallet
> recovery and not normal wallet use, so I don't think a small amount of
> round-trip communication between the hot wallet and the cold wallet is
> too much to ask---especially since anyone using SNICKER with a
> watching-only wallet must be regularly interacting with their cold
> wallet anyway to sign the coinjoins.
>
> -Dave
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
>


-- 
Riccardo Casatta - @RCasatta <https://twitter.com/RCasatta>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191021/51924e75/attachment.html>

From somber.night at protonmail.com  Mon Oct 21 15:04:59 2019
From: somber.night at protonmail.com (SomberNight)
Date: Mon, 21 Oct 2019 15:04:59 +0000
Subject: [bitcoin-dev] Draft BIP for SNICKER
In-Reply-To: <20191021000608.ajvzjxh6phtuhydp@ganymede>
References: <YwZ3vq20LFvpx-nKn1RJjcRHwYTAVCC0v0EyD0y6zVMlQtKXUFNAaEk_QE2dzYDU6z2eK0S0TDXRPfl1_y93RgDjdCGboOgjcERBTLUPHao=@protonmail.com>
	<20191021000608.ajvzjxh6phtuhydp@ganymede>
Message-ID: <clOIQUf5e2vT3KqKplQwrS5MgB8ptPDSQWkpOMGoAE3rS90i7y-8mNRmcecfVJwiYePhNYAfFlBYsOKqvavm4yVI-zEfo8pnG6AY_fiyMXs=@protonmail.com>

> The SNICKER recovery process is, of course, only required for wallet
recovery and not normal wallet use, so I don't think a small amount of
round-trip communication between the hot wallet and the cold wallet is
too much to ask---especially since anyone using SNICKER with a
watching-only wallet must be regularly interacting with their cold
wallet anyway to sign the coinjoins.

What you described only considers the "initial setup" of a watch-only wallet. There are many usecases for watch-only wallets. There doesn't even necessarily need to be any offline-signing involved. For example, consider a user who has a hot wallet on their laptop with xprv; and wants to watch their addresses using an xpub from their mobile. Or consider giving an xpub to an accountant. Or giving an xpub to your Electrum Personal Server (which is how it works).

Note that all these usecases require "on-going" discovery of addresses, and so they would break.

ghost43

(ps: Apologies Dave for the double-email; forgot to cc list originally)

From AdamISZ at protonmail.com  Tue Oct 22 13:21:00 2019
From: AdamISZ at protonmail.com (AdamISZ)
Date: Tue, 22 Oct 2019 13:21:00 +0000
Subject: [bitcoin-dev] Draft BIP for SNICKER
In-Reply-To: <clOIQUf5e2vT3KqKplQwrS5MgB8ptPDSQWkpOMGoAE3rS90i7y-8mNRmcecfVJwiYePhNYAfFlBYsOKqvavm4yVI-zEfo8pnG6AY_fiyMXs=@protonmail.com>
References: <YwZ3vq20LFvpx-nKn1RJjcRHwYTAVCC0v0EyD0y6zVMlQtKXUFNAaEk_QE2dzYDU6z2eK0S0TDXRPfl1_y93RgDjdCGboOgjcERBTLUPHao=@protonmail.com>
	<20191021000608.ajvzjxh6phtuhydp@ganymede>
	<clOIQUf5e2vT3KqKplQwrS5MgB8ptPDSQWkpOMGoAE3rS90i7y-8mNRmcecfVJwiYePhNYAfFlBYsOKqvavm4yVI-zEfo8pnG6AY_fiyMXs=@protonmail.com>
Message-ID: <mq_HOhcWf2T7ik9Em3nb5VCePi5cV17Wf_c8qS5zWwXh0vnJVzBO_q6Nl8RQBJysBOhZC2rjAw3hbq2tHIoEyTKE8QQaJgF9LpgpcP0Nl8g=@protonmail.com>

Just to chime in on these points:

My discussions with ghost43 and ThomasV led me to the same conclusion, at least in general, for the whole watch-only issue:

It's necessary that the key tweak (`c` as per draft BIP) be known by Proposer (because has to add it to transaction before signing) and Receiver (to check ownership), but must not be known by anyone else (else Coinjoin function fails), hence it can't be publically derivable in any way but must require information secret to the two parties. This can be a pure random sent along with the encrypted proposal (the original concept), or based on such, or implicit via ECDH (arubi's suggestion, now in the draft, requiring each party to access their own secret key). So I reached the same conclusion: the classic watch-only use case of monitoring a wallet in real time with no privkey access is incompatible with this.

It's worth mentioning a nuance, however: distinguish two requirements: (1) to recover from zero information and (2) to monitor in real time as new SNICKER transactions arrive.

For (2) it's interesting to observe that the tweak `c` is not a money-controlling secret; it's only a privacy-controlling secret. If you imagined two wallets, one hot and one cold, with the second tracking the first but having a lower security requirement because cold, then the `c` values could be sent along from the hot to the cold, as they are created, without changing the cold's security model as they are not money-controlling private keys. They should still be encrypted of course, but that's largely a technical detail, if they were exposed it would only break the effect of the coinjoin outputs being indistinguishable.

For (1) the above does not apply; for there, we don't have anyone telling us what `c` values to look for, we have to somehow rederive, and to do that we need key access, so it reverts to the discussion above about whether it might be possible to interact with the cold wallet 'manually' so to speak.

To be clear, I don't think either of the above paragraphs describe things that are particularly likely to be implemented, but the hot/cold monitoring is at least feasible, if there were enough desire for it.

At the higher level, how important is this? I guess it just depends; there are similar problems (not identical, and perhaps more addressable?) in Lightning; importing keys is generally non-trivial; one can always sweep non-standard keys back into the HD tree, but clearly that is not really a solution in general; one can mark out wallets/seeds of this type as distinct; not all wallets need to have watch-only (phone wallets? small wallets? lower security?) one can prioritise spends of these coins. Etc.

Some more general comments:

Note Elichai's comment on the draft (repeated here for local convenience: https://gist.github.com/AdamISZ/2c13fb5819bd469ca318156e2cf25d79#gistcomment-3014924) about AES-GCM vs AES-CBC, any thoughts?

I didn't discuss the security of the construction for a Receiver from a Proposer who should after all be assumed to be an attacker (except, I emphasised that PSBT parsing could be sensitive on this point); I hope it's clear to everyone that the construction Q = P + cG is only controllable by the owner of the discrete log of P (trivial reduction: if an attacker who knows c, can find the private key q of Q, he can derive the private key p of P as q - c, thus he is an ECDLP cracker).

Thanks for all the comments so far, it's been very useful.

AdamISZ/waxwing/Adam Gibson

Sent with ProtonMail Secure Email.

??????? Original Message ???????
On Monday, October 21, 2019 4:04 PM, SomberNight via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:

> > The SNICKER recovery process is, of course, only required for wallet
>
> recovery and not normal wallet use, so I don't think a small amount of
> round-trip communication between the hot wallet and the cold wallet is
> too much to ask---especially since anyone using SNICKER with a
> watching-only wallet must be regularly interacting with their cold
> wallet anyway to sign the coinjoins.
>
> What you described only considers the "initial setup" of a watch-only wallet. There are many usecases for watch-only wallets. There doesn't even necessarily need to be any offline-signing involved. For example, consider a user who has a hot wallet on their laptop with xprv; and wants to watch their addresses using an xpub from their mobile. Or consider giving an xpub to an accountant. Or giving an xpub to your Electrum Personal Server (which is how it works).
>
> Note that all these usecases require "on-going" discovery of addresses, and so they would break.
>
> ghost43
>
> (ps: Apologies Dave for the double-email; forgot to cc list originally)
>
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev



From steven.j.lee at gmail.com  Wed Oct 23 17:44:49 2019
From: steven.j.lee at gmail.com (Steve Lee)
Date: Wed, 23 Oct 2019 10:44:49 -0700
Subject: [bitcoin-dev] Sign up for Taproot BIP review by October 30
Message-ID: <CABu3BAePQVzGX=1ugTBqwzvbB6jEKC4LGG-Mwpki5iyTvs5d9Q@mail.gmail.com>

Hello everyone,

The schnorr/taproot/tapscript BIPs are ready for review at this point, and
we want to get as much in-depth review from as broad a range of people as
we can before we go further on implementation/deployment. Reviewing the
BIPs is hard in two ways: not many people are familiar with reviewing BIPs
in the first place, and there are a lot of concepts involved in the three
BIPs for people to get their heads around.

This is a proposal <https://github.com/ajtowns/taproot-review> for a
structured review period. The idea is that participants will be given some
guidance/structure for going through the BIPs, and at the end should be
able to either describe issues with the BIP drafts that warrant changes, or
be confident that they?ve examined the proposals thoroughly enough to give
an ?ACK? that the drafts should be formalised and move forwards into
implementation/deployment phases.

Benefits of participating:
* Deeply understand schnorr and taproot
* Be a stakeholder in Bitcoin consensus development
* Support/safeguard decentralisation of Bitcoin protocol development
* Have fun!

If you are interested in participating, please sign up here
<https://forms.gle/stnJvHQHREdkaxkS8 >.

Special thanks to AJ Towns for doing most of the heavy lifting to get this
going!

Steve
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191023/e864c58e/attachment.html>

From naumenko.gs at gmail.com  Wed Oct 23 21:52:49 2019
From: naumenko.gs at gmail.com (Gleb Naumenko)
Date: Wed, 23 Oct 2019 17:52:49 -0400
Subject: [bitcoin-dev] Signaling support for addr relay (revision #1)
In-Reply-To: <fd174eb5-3699-4eed-b11e-e0370da63598@Spark>
References: <fd174eb5-3699-4eed-b11e-e0370da63598@Spark>
Message-ID: <485c6620-4927-4f44-b060-a3be1c31f135@Spark>

Hi,

### Introduction

I was recently looking into AddrMan and I realized that unlike with blocks (BIP152) and transactions (a node can opt-out via various mechanisms such as blocks-only or block-only-relay), address relay is under-specified.

For example, we had a discussion [1] on whether SPV nodes store/relay IP addresses. While it seems they don?t do it currently in practice, in some cases they should if they want to be secure and reliable.

### Motivation

This change would decouple addr relay considerations from light/full node/block-relay-only.
This would also allow us to easier analyze (in a scientific sense, not in a spying sense) and adjust address relay, which currently seems to have understudied properties and guarantees.
In practice, this may allow more efficient address relay (fewer messages and less time to relay a new address across all nodes) both immediately and potentially long-term.

### Solution

I want to suggest making explicit whether a node promises to participate in address relay by a) forwarding unsolicited messages (I work on a somewhat related issue in this PR [2]) , and, b) responding to GETADDR.

In my opinion, these 2 signals (a and b) should be viewed independently.

Obviously, these signals should not be relied upon and future protocol changes should assume they may represent lies.
However, explicitly opting-out of relay addresses will help to improve non-adversarial address relay.

### Implementation

I see 2 ways to implement this:
- 2 new service bits
- per-link-direction negotiation: e.g., use BIP-155 (a new message sendaddrv2 is discussed here [3] and can be used to signal this)

Both of them can allow decoupling addr relay from node type, but they do have different trade-offs.

#### Service bits

Having service bits makes sense only if nodes are going to make peering decisions based on it. (everything else might be achieved without introducing a service bit). It is not clear to me whether this makes sense in this context.

The fundamental problem with service bits is that they make a uniform ?promise? for all connections to a given node. E.g., if node X announces NODE_ADDR_FORWARD, all nodes in the network expect node X to forward addresses. (If the ?promise? is not strong, then additional negotiation is required anyway, so service bits do not solve the problem).

It?s worth keeping in mind that all of the honest reachable full nodes nodes DO relay addresses, and we already won?t connect to those nodes which don?t (light clients). Service bits won?t help here because the problem of connecting to non-addr-relaying full nodes does not exist.
Maybe, if we think that a large fraction of reachable nodes might start completely disabling addr relay to all in the future, then it makes sense to have this service bit, to prevent nodes from accidentally connecting to these peers only and not learning addrs.

Intuitively, it?s also easier to shoot in the leg with the deployment of service bits (might make it easier for attacker to accumulate connections comparing to the case of victims choosing their peers uniformly at random without considering new service bit).

#### Per-link-direction negotiation

This approach does not have the shortcomings mentioned above.

In addition, I think that having more flexibility (Per-link-direction negotiation) is better for the future of the protocol, where some nodes might want to opt-out of addr relay for a subset of their links.
(A node might want to opt-out from addr relay for particular links due to privacy reasons because addr-relay currently leaks information and maybe we shouldn?t relay transactions through the same links).

And I think this future is much more likely to happen than a future where a significant fraction of reachable nodes disable addr relay to *everyone* and need to announce this to the network. Also, even if needed, this can be done with per-link-direction negotiation too, and handled by the peers accordingly.

Per-link-direction negotiation also allows to decouple the behaviour from inbound/outbound type of connection (currently we do not respond to GETADDR from outbound). This logic seems not fundamental to me, but rather a temporary heuristic to prevent attacks, which might be changed in future.

### Conclusion

I think the solution fundamentally depends on the answer to:
?Do we believe that some of the future security advices for node operators would be to disable address relay to all (or most) of the links?.

If yes, I think we should use service bits.
If no, I think we should use per-link-direction negotiation.

If the answer will change, we can also add a service bit later.

Anyway, according to the current considerations I explained in this email, I?d suggest extending BIP-155 with per-link-direction negotiation, but I?m interested in the opinion of the community.

### References

1. Bitcoin core dev IRC meeting (http://www.erisian.com.au/bitcoin-core-dev/log-2019-10-17.html)
2. p2p: Avoid forwarding ADDR messages to SPV nodes (https://github.com/bitcoin/bitcoin/pull/17194)
3. BIP 155: addrv2 BIP proposal (https://github.com/bitcoin/bips/pull/766)

? gleb
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191023/55c63626/attachment.html>

From johanth at gmail.com  Thu Oct 24 13:49:09 2019
From: johanth at gmail.com (=?UTF-8?Q?Johan_Tor=C3=A5s_Halseth?=)
Date: Thu, 24 Oct 2019 15:49:09 +0200
Subject: [bitcoin-dev] [Lightning-dev] CPFP Carve-Out for Fee-Prediction
 Issues in Contracting Applications (eg Lightning)
In-Reply-To: <87zhr0gvw0.fsf@rustcorp.com.au>
References: <c3f68b73-84c6-7428-4bf6-b47802141392@mattcorallo.com>
	<878t163qzi.fsf@rustcorp.com.au>
	<725fc55a-6263-a9fc-74a5-1017cb1cc885@mattcorallo.com>
	<87wonfem03.fsf@rustcorp.com.au>
	<D072562F-5AD0-4B38-94D1-A0AEF04C3DEB@mattcorallo.com>
	<87zhr0gvw0.fsf@rustcorp.com.au>
Message-ID: <CAD3i26AjhQ9VkCo_5y8aqZ_8YvSqKP2MCkdRv8YunjAhmmXz=Q@mail.gmail.com>

Reviving this old thread now that the recently released RC for bitcoind
0.19 includes the above mentioned carve-out rule.

In an attempt to pave the way for more robust CPFP of on-chain contracts
(Lightning commitment transactions), the carve-out rule was added in
https://github.com/bitcoin/bitcoin/pull/15681. However, having worked on an
implementation of a new commitment format for utilizing the Bring Your Own
Fees strategy using CPFP, I?m wondering if the special case rule should
have been relaxed a bit, to avoid the need for adding a 1 CSV to all
outputs (in case of Lightning this means HTLC scripts would need to be
changed to add the CSV delay).

Instead, what about letting the rule be

The last transaction which is added to a package of dependent
transactions in the mempool must:
  * Have no more than one unconfirmed parent.

This would of course allow adding a large transaction to each output of the
unconfirmed parent, which in effect would allow an attacker to exceed the
MAX_PACKAGE_VIRTUAL_SIZE limit in some cases. However, is this a problem
with the current mempool acceptance code in bitcoind? I would imagine
evicting transactions based on feerate when the max mempool size is met
handles this, but I?m asking since it seems like there has been several
changes to the acceptance code and eviction policy since the limit was
first introduced.

- Johan


On Wed, Feb 13, 2019 at 6:57 AM Rusty Russell <rusty at rustcorp.com.au> wrote:

> Matt Corallo <lf-lists at mattcorallo.com> writes:
> >>> Thus, even if you imagine a steady-state mempool growth, unless the
> >>> "near the top of the mempool" criteria is "near the top of the next
> >>> block" (which is obviously *not* incentive-compatible)
> >>
> >> I was defining "top of mempool" as "in the first 4 MSipa", ie. next
> >> block, and assumed you'd only allow RBF if the old package wasn't in the
> >> top and the replacement would be.  That seems incentive compatible; more
> >> than the current scheme?
> >
> > My point was, because of block time variance, even that criteria doesn't
> hold up. If you assume a steady flow of new transactions and one or two
> blocks come in "late", suddenly "top 4MWeight" isn't likely to get
> confirmed until a few blocks come in "early". Given block variance within a
> 12 block window, this is a relatively likely scenario.
>
> [ Digging through old mail. ]
>
> Doesn't really matter.  Lightning close algorithm would be:
>
> 1.  Give bitcoind unileratal close.
> 2.  Ask bitcoind what current expidited fee is (or survey your mempool).
> 3.  Give bitcoind child "push" tx at that total feerate.
> 4.  If next block doesn't contain unilateral close tx, goto 2.
>
> In this case, if you allow a simpified RBF where 'you can replace if
> 1. feerate is higher, 2. new tx is in first 4Msipa of mempool, 3. old tx
> isnt',
> it works.
>
> It allows someone 100k of free tx spam, sure.  But it's simple.
>
> We could further restrict it by marking the unilateral close somehow to
> say "gonna be pushed" and further limiting the child tx weight (say,
> 5kSipa?) in that case.
>
> Cheers,
> Rusty.
> _______________________________________________
> Lightning-dev mailing list
> Lightning-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191024/065e650f/attachment.html>

From joachimstr at protonmail.com  Thu Oct 24 08:01:49 2019
From: joachimstr at protonmail.com (=?UTF-8?Q?Joachim_Str=C3=B6mbergson?=)
Date: Thu, 24 Oct 2019 08:01:49 +0000
Subject: [bitcoin-dev] Signaling support for addr relay (revision #1)
In-Reply-To: <485c6620-4927-4f44-b060-a3be1c31f135@Spark>
References: <fd174eb5-3699-4eed-b11e-e0370da63598@Spark>
	<485c6620-4927-4f44-b060-a3be1c31f135@Spark>
Message-ID: <1k3NxcpF9ShapWuKBA705zVwn1s0zgDOJywNrzIgBOetBsO75i8t9eO1TD0f9giGT74jJxtMjf25gj8T7Dj63CrooStdeT9-r2XLruXdHqo=@protonmail.com>

> Anyway, according to the current considerations I explained in this email, I?d suggest extending BIP-155 with per-link-direction negotiation, but I?m interested in the opinion of the community.

I don't have a strong opinion here but intuitively, it seems to me that per-link variant makes better sense as I currently can't imagine the future requirement for completely opting out.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191024/d08d6f66/attachment.html>

From erik at q32.com  Thu Oct 24 15:34:14 2019
From: erik at q32.com (Erik Aronesty)
Date: Thu, 24 Oct 2019 11:34:14 -0400
Subject: [bitcoin-dev] Transition to post-quantum
In-Reply-To: <1518738259.3550.170.camel@mmci.uni-saarland.de>
References: <CAFEpHQHP7XXBYUP6CF1OeYoBpj0UwK+qpYG-14_zQZDX4Md7UA@mail.gmail.com>
	<1518450650.7829.87.camel@mmci.uni-saarland.de>
	<CAFEpHQHYdE3m2GUtN=ijvtYUudwtcG52rRxzH66VFbgO1KEihw@mail.gmail.com>
	<1518504374.9878.24.camel@mmci.uni-saarland.de>
	<882306fa-3ea0-99bd-61c6-f646d27c2ab6@gmail.com>
	<1518710367.3550.111.camel@mmci.uni-saarland.de>
	<CAAt2M1-JtmcMH2WCx5T5U8B-B+Ob61WPSvX4JoOcWzYFCLSTmw@mail.gmail.com>
	<1518731861.3550.131.camel@mmci.uni-saarland.de>
	<CAAt2M1-0-c1-OC0g0_6aBueR8wU+ipPw4U_zSLkdoh3K79PWsw@mail.gmail.com>
	<1518738259.3550.170.camel@mmci.uni-saarland.de>
Message-ID: <CAJowKgL9-1WxEHxNdwO36vN7JcqiO7HWH=7T2hBBfxLX2nY4xg@mail.gmail.com>

- It would be hard to prove you have access to an x that can produce
H(g^x) in a way that doesn't expose g^x and isn't one of those slow,
interactive bit-encryption algorithms.

- Instead a simple scheme would publish a transaction to the
blockchain that lists:
     - pre-quantum signature
     - hash of post-quantum address

- Any future transactions would require both the pre *and*
post-quantum signatures.

That scheme would need to be implemented sufficient number of years
before quantum became a pressing issue, but it's super simple,
spam-proof (requires fees), and flexible enough that it can change as
post-quantum addressing improves.

Imagine there are 2 quantum addressing schemes in order of discovery.

1. Soft-fork 1 accepts the first scheme and people begin publishing
PRE/POST upgrades.
2. Discovery is made that shows a second scheme has smaller
transactions and faster validation.
3. Soft-fork 2 refuses to accept upgrades to the first scheme in
transactions beyond a certain block number in order to improve
performance.






On Thu, Feb 15, 2018 at 6:44 PM Tim Ruffing via bitcoin-dev
<bitcoin-dev at lists.linuxfoundation.org> wrote:
>
> On Thu, 2018-02-15 at 23:44 +0100, Natanael wrote:
> > If your argument is that we publish the full transaction minus the
> > public key and signatures, just committing to it, and then revealing
> > that later (which means an attacker can't modify the transaction in
> > advance in a way that produces a valid transaction);
>
> Almost. Actually we reveal the entire transaction later.
>
> >
> > [...] while *NOT* allowing expiration makes it a trivial DoS target.
> >
> > Anybody can flood the miners with invalid transaction commitments. No
> > miner can ever prune invalid commitments until a valid transaction is
> > finalized which conflicts with the invalid commitments. You can't
> > even rate limit it safely.
>
> Yes, that's certainly true. I mentioned that issue already.
>
> You can rate limit this: The only thing I see is that one can require
> transaction fees even for commitments. That's super annoying, because
> you need a second (PQ-)UTXO just to commit. But it's not impossible.
>
> You can call this impractical and this may well be true. But what will
> be most practical in the future depends on many parameters that are
> totally unclear at the moment, e.g., the efficiency of zero-knowledge
> proof systems. Who knows?
>
> If you would like to use zero-knowledge proofs to recover an UTXO with
> an P2PKH address, you need to prove in zero-knowledge that you know
> some secret key x such that H(g^x)=addr. That seems plausible. But
> P2PKH is by far the simplest example. For arbitrary scripts, this can
> become pretty complex and nasty, even if our proof systems and machines
> are fast enough.
>
> _______________________________________________
> bitcoin-dev mailing list
> bitcoin-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev

From lf-lists at mattcorallo.com  Thu Oct 24 21:25:14 2019
From: lf-lists at mattcorallo.com (Matt Corallo)
Date: Thu, 24 Oct 2019 21:25:14 +0000
Subject: [bitcoin-dev] [Lightning-dev] CPFP Carve-Out for Fee-Prediction
 Issues in Contracting Applications (eg Lightning)
In-Reply-To: <CAD3i26AjhQ9VkCo_5y8aqZ_8YvSqKP2MCkdRv8YunjAhmmXz=Q@mail.gmail.com>
References: <c3f68b73-84c6-7428-4bf6-b47802141392@mattcorallo.com>
	<878t163qzi.fsf@rustcorp.com.au>
	<725fc55a-6263-a9fc-74a5-1017cb1cc885@mattcorallo.com>
	<87wonfem03.fsf@rustcorp.com.au>
	<D072562F-5AD0-4B38-94D1-A0AEF04C3DEB@mattcorallo.com>
	<87zhr0gvw0.fsf@rustcorp.com.au>
	<CAD3i26AjhQ9VkCo_5y8aqZ_8YvSqKP2MCkdRv8YunjAhmmXz=Q@mail.gmail.com>
Message-ID: <83915e8a-f49a-8233-0389-934c189f770c@mattcorallo.com>

I may be missing something, but I'm not sure how this changes anything?

If you have a commitment transaction, you always need at least, and
exactly, one non-CSV output per party. The fact that there is a size
limitation on the transaction that spends for carve-out purposes only
effects how many other inputs/outputs you can add, but somehow I doubt
its ever going to be a large enough number to matter.

Matt

On 10/24/19 1:49 PM, Johan Tor?s Halseth wrote:
> Reviving this old thread now that the recently released RC for bitcoind
> 0.19 includes the above mentioned carve-out rule.
> 
> In an attempt to pave the way for more robust CPFP of on-chain contracts
> (Lightning commitment transactions), the carve-out rule was added in
> https://github.com/bitcoin/bitcoin/pull/15681. However, having worked on
> an implementation of a new commitment format for utilizing the Bring
> Your Own Fees strategy using CPFP, I?m wondering if the special case
> rule should have been relaxed a bit, to avoid the need for adding a 1
> CSV to all outputs (in case of Lightning this means HTLC scripts would
> need to be changed to add the CSV delay).
> 
> Instead, what about letting the rule be
> 
> The last transaction which is added to a package of dependent
> transactions in the mempool must:
> ? * Have no more than one unconfirmed parent.
> 
> This would of course allow adding a large transaction to each output of
> the unconfirmed parent, which in effect would allow an attacker to
> exceed the MAX_PACKAGE_VIRTUAL_SIZE limit in some cases. However, is
> this a problem with the current mempool acceptance code in bitcoind? I
> would imagine evicting transactions based on feerate when the max
> mempool size is met handles this, but I?m asking since it seems like
> there has been several changes to the acceptance code and eviction
> policy since the limit was first introduced.
> 
> - Johan
> 
> 
> On Wed, Feb 13, 2019 at 6:57 AM Rusty Russell <rusty at rustcorp.com.au
> <mailto:rusty at rustcorp.com.au>> wrote:
> 
>     Matt Corallo <lf-lists at mattcorallo.com
>     <mailto:lf-lists at mattcorallo.com>> writes:
>     >>> Thus, even if you imagine a steady-state mempool growth, unless the
>     >>> "near the top of the mempool" criteria is "near the top of the next
>     >>> block" (which is obviously *not* incentive-compatible)
>     >>
>     >> I was defining "top of mempool" as "in the first 4 MSipa", ie. next
>     >> block, and assumed you'd only allow RBF if the old package wasn't
>     in the
>     >> top and the replacement would be.? That seems incentive
>     compatible; more
>     >> than the current scheme?
>     >
>     > My point was, because of block time variance, even that criteria
>     doesn't hold up. If you assume a steady flow of new transactions and
>     one or two blocks come in "late", suddenly "top 4MWeight" isn't
>     likely to get confirmed until a few blocks come in "early". Given
>     block variance within a 12 block window, this is a relatively likely
>     scenario.
> 
>     [ Digging through old mail. ]
> 
>     Doesn't really matter.? Lightning close algorithm would be:
> 
>     1.? Give bitcoind unileratal close.
>     2.? Ask bitcoind what current expidited fee is (or survey your mempool).
>     3.? Give bitcoind child "push" tx at that total feerate.
>     4.? If next block doesn't contain unilateral close tx, goto 2.
> 
>     In this case, if you allow a simpified RBF where 'you can replace if
>     1. feerate is higher, 2. new tx is in first 4Msipa of mempool, 3.
>     old tx isnt',
>     it works.
> 
>     It allows someone 100k of free tx spam, sure.? But it's simple.
> 
>     We could further restrict it by marking the unilateral close somehow to
>     say "gonna be pushed" and further limiting the child tx weight (say,
>     5kSipa?) in that case.
> 
>     Cheers,
>     Rusty.
>     _______________________________________________
>     Lightning-dev mailing list
>     Lightning-dev at lists.linuxfoundation.org
>     <mailto:Lightning-dev at lists.linuxfoundation.org>
>     https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev
> 

From lf-lists at mattcorallo.com  Fri Oct 25 17:30:41 2019
From: lf-lists at mattcorallo.com (Matt Corallo)
Date: Fri, 25 Oct 2019 07:30:41 -1000
Subject: [bitcoin-dev] [Lightning-dev] CPFP Carve-Out for Fee-Prediction
	Issues in Contracting Applications (eg Lightning)
In-Reply-To: <CAD3i26Cf+QpbFXh63NiMig9eceeKaezZwk89A_h_76S_XKkQ9g@mail.gmail.com>
References: <CAD3i26Cf+QpbFXh63NiMig9eceeKaezZwk89A_h_76S_XKkQ9g@mail.gmail.com>
Message-ID: <6728FF51-E378-4AED-99BA-ECB83688AA9C@mattcorallo.com>

I don?te see how? Let?s imagine Party A has two spendable outputs, now they stuff the package size on one of their spendable outlets until it is right at the limit, add one more on their other output (to meet the Carve-Out), and now Party B can?t do anything.

> On Oct 24, 2019, at 21:05, Johan Tor?s Halseth <johanth at gmail.com> wrote:
> 
> ?
> It essentially changes the rule to always allow CPFP-ing the commitment as long as there is an output available without any descendants. It changes the commitment from "you always need at least, and exactly, one non-CSV output per party. " to "you always need at least one non-CSV output per party. "
> 
> I realize these limits are there for a reason though, but I'm wondering if could relax them. Also now that jeremyrubin has expressed problems with the current mempool limits.
> 
>> On Thu, Oct 24, 2019 at 11:25 PM Matt Corallo <lf-lists at mattcorallo.com> wrote:
>> I may be missing something, but I'm not sure how this changes anything?
>> 
>> If you have a commitment transaction, you always need at least, and
>> exactly, one non-CSV output per party. The fact that there is a size
>> limitation on the transaction that spends for carve-out purposes only
>> effects how many other inputs/outputs you can add, but somehow I doubt
>> its ever going to be a large enough number to matter.
>> 
>> Matt
>> 
>> On 10/24/19 1:49 PM, Johan Tor?s Halseth wrote:
>> > Reviving this old thread now that the recently released RC for bitcoind
>> > 0.19 includes the above mentioned carve-out rule.
>> > 
>> > In an attempt to pave the way for more robust CPFP of on-chain contracts
>> > (Lightning commitment transactions), the carve-out rule was added in
>> > https://github.com/bitcoin/bitcoin/pull/15681. However, having worked on
>> > an implementation of a new commitment format for utilizing the Bring
>> > Your Own Fees strategy using CPFP, I?m wondering if the special case
>> > rule should have been relaxed a bit, to avoid the need for adding a 1
>> > CSV to all outputs (in case of Lightning this means HTLC scripts would
>> > need to be changed to add the CSV delay).
>> > 
>> > Instead, what about letting the rule be
>> > 
>> > The last transaction which is added to a package of dependent
>> > transactions in the mempool must:
>> >   * Have no more than one unconfirmed parent.
>> > 
>> > This would of course allow adding a large transaction to each output of
>> > the unconfirmed parent, which in effect would allow an attacker to
>> > exceed the MAX_PACKAGE_VIRTUAL_SIZE limit in some cases. However, is
>> > this a problem with the current mempool acceptance code in bitcoind? I
>> > would imagine evicting transactions based on feerate when the max
>> > mempool size is met handles this, but I?m asking since it seems like
>> > there has been several changes to the acceptance code and eviction
>> > policy since the limit was first introduced.
>> > 
>> > - Johan
>> > 
>> > 
>> > On Wed, Feb 13, 2019 at 6:57 AM Rusty Russell <rusty at rustcorp.com.au
>> > <mailto:rusty at rustcorp.com.au>> wrote:
>> > 
>> >     Matt Corallo <lf-lists at mattcorallo.com
>> >     <mailto:lf-lists at mattcorallo.com>> writes:
>> >     >>> Thus, even if you imagine a steady-state mempool growth, unless the
>> >     >>> "near the top of the mempool" criteria is "near the top of the next
>> >     >>> block" (which is obviously *not* incentive-compatible)
>> >     >>
>> >     >> I was defining "top of mempool" as "in the first 4 MSipa", ie. next
>> >     >> block, and assumed you'd only allow RBF if the old package wasn't
>> >     in the
>> >     >> top and the replacement would be.  That seems incentive
>> >     compatible; more
>> >     >> than the current scheme?
>> >     >
>> >     > My point was, because of block time variance, even that criteria
>> >     doesn't hold up. If you assume a steady flow of new transactions and
>> >     one or two blocks come in "late", suddenly "top 4MWeight" isn't
>> >     likely to get confirmed until a few blocks come in "early". Given
>> >     block variance within a 12 block window, this is a relatively likely
>> >     scenario.
>> > 
>> >     [ Digging through old mail. ]
>> > 
>> >     Doesn't really matter.  Lightning close algorithm would be:
>> > 
>> >     1.  Give bitcoind unileratal close.
>> >     2.  Ask bitcoind what current expidited fee is (or survey your mempool).
>> >     3.  Give bitcoind child "push" tx at that total feerate.
>> >     4.  If next block doesn't contain unilateral close tx, goto 2.
>> > 
>> >     In this case, if you allow a simpified RBF where 'you can replace if
>> >     1. feerate is higher, 2. new tx is in first 4Msipa of mempool, 3.
>> >     old tx isnt',
>> >     it works.
>> > 
>> >     It allows someone 100k of free tx spam, sure.  But it's simple.
>> > 
>> >     We could further restrict it by marking the unilateral close somehow to
>> >     say "gonna be pushed" and further limiting the child tx weight (say,
>> >     5kSipa?) in that case.
>> > 
>> >     Cheers,
>> >     Rusty.
>> >     _______________________________________________
>> >     Lightning-dev mailing list
>> >     Lightning-dev at lists.linuxfoundation.org
>> >     <mailto:Lightning-dev at lists.linuxfoundation.org>
>> >     https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev
>> > 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191025/2221408c/attachment.html>

From jlrubin at mit.edu  Sun Oct 27 19:13:09 2019
From: jlrubin at mit.edu (Jeremy)
Date: Sun, 27 Oct 2019 12:13:09 -0700
Subject: [bitcoin-dev] [Lightning-dev] CPFP Carve-Out for Fee-Prediction
 Issues in Contracting Applications (eg Lightning)
In-Reply-To: <6728FF51-E378-4AED-99BA-ECB83688AA9C@mattcorallo.com>
References: <CAD3i26Cf+QpbFXh63NiMig9eceeKaezZwk89A_h_76S_XKkQ9g@mail.gmail.com>
	<6728FF51-E378-4AED-99BA-ECB83688AA9C@mattcorallo.com>
Message-ID: <CAD5xwhhK4xexDe=aBv78BsK5BvE=4S0OcqeXYHVAfN3wTOr51Q@mail.gmail.com>

Johan,

The issues with mempool limits for OP_SECURETHEBAG are related, but have
distinct solutions.

There are two main categories of mempool issues at stake. One is relay
cost, the other is mempool walking.

In terms of relay cost, if an ancestor can be replaced, it will invalidate
all it's children, meaning that no one paid for that broadcasting. This can
be fixed by appropriately assessing Replace By Fee update fees to
encapsulate all descendants, but there are some tricky edge cases that make
this non-obvious to do.

The other issue is walking the mempool -- many of the algorithms we use in
the mempool can be N log N or N^2 in the number of descendants. (simple
example: an input chain of length N to a fan out of N outputs that are all
spent, is O(N^2) to look up ancestors per-child, unless we're caching).

The other sort of walking issue is where the indegree or outdegree for a
transaction is high. Then when we are computing descendants or ancestors we
will need to visit it multiple times. To avoid re-expanding a node, we
currently cache it with a set. This uses O(N) extra memory and makes O(N
Log N) (we use std::set not unordered_set) comparisons.

I just opened a PR which should help with some of the walking issues by
allowing us to cheaply cache which nodes we've visited on a run. It makes a
lot of previously O(N log N) stuff O(N) and doesn't allocate as much new
memory. See: https://github.com/bitcoin/bitcoin/pull/17268.


Now, for OP_SECURETHEBAG we want a particular property that is very
different from with lightning htlcs (as is). We want that an unlimited
number of child OP_SECURETHEBAG txns may extend from a confirmed
OP_SECURETHEBAG, and then at the leaf nodes, we want the same rule as
lightning (one dangling unconfirmed to permit channels).

OP_SECURETHEBAG can help with the LN issue by putting all HTLCS into a tree
where they are individualized leaf nodes with a preceding CSV. Then, the
above fix would ensure each HTLC always has time to close properly as they
would have individualized lockpoints. This is desirable for some additional
reasons and not for others, but it should "work".



--
@JeremyRubin <https://twitter.com/JeremyRubin>
<https://twitter.com/JeremyRubin>


On Fri, Oct 25, 2019 at 10:31 AM Matt Corallo <lf-lists at mattcorallo.com>
wrote:

> I don?te see how? Let?s imagine Party A has two spendable outputs, now
> they stuff the package size on one of their spendable outlets until it is
> right at the limit, add one more on their other output (to meet the
> Carve-Out), and now Party B can?t do anything.
>
> On Oct 24, 2019, at 21:05, Johan Tor?s Halseth <johanth at gmail.com> wrote:
>
> ?
> It essentially changes the rule to always allow CPFP-ing the commitment as
> long as there is an output available without any descendants. It changes
> the commitment from "you always need at least, and exactly, one non-CSV
> output per party. " to "you always need at least one non-CSV output per
> party. "
>
> I realize these limits are there for a reason though, but I'm wondering if
> could relax them. Also now that jeremyrubin has expressed problems with the
> current mempool limits.
>
> On Thu, Oct 24, 2019 at 11:25 PM Matt Corallo <lf-lists at mattcorallo.com>
> wrote:
>
>> I may be missing something, but I'm not sure how this changes anything?
>>
>> If you have a commitment transaction, you always need at least, and
>> exactly, one non-CSV output per party. The fact that there is a size
>> limitation on the transaction that spends for carve-out purposes only
>> effects how many other inputs/outputs you can add, but somehow I doubt
>> its ever going to be a large enough number to matter.
>>
>> Matt
>>
>> On 10/24/19 1:49 PM, Johan Tor?s Halseth wrote:
>> > Reviving this old thread now that the recently released RC for bitcoind
>> > 0.19 includes the above mentioned carve-out rule.
>> >
>> > In an attempt to pave the way for more robust CPFP of on-chain contracts
>> > (Lightning commitment transactions), the carve-out rule was added in
>> > https://github.com/bitcoin/bitcoin/pull/15681. However, having worked
>> on
>> > an implementation of a new commitment format for utilizing the Bring
>> > Your Own Fees strategy using CPFP, I?m wondering if the special case
>> > rule should have been relaxed a bit, to avoid the need for adding a 1
>> > CSV to all outputs (in case of Lightning this means HTLC scripts would
>> > need to be changed to add the CSV delay).
>> >
>> > Instead, what about letting the rule be
>> >
>> > The last transaction which is added to a package of dependent
>> > transactions in the mempool must:
>> >   * Have no more than one unconfirmed parent.
>> >
>> > This would of course allow adding a large transaction to each output of
>> > the unconfirmed parent, which in effect would allow an attacker to
>> > exceed the MAX_PACKAGE_VIRTUAL_SIZE limit in some cases. However, is
>> > this a problem with the current mempool acceptance code in bitcoind? I
>> > would imagine evicting transactions based on feerate when the max
>> > mempool size is met handles this, but I?m asking since it seems like
>> > there has been several changes to the acceptance code and eviction
>> > policy since the limit was first introduced.
>> >
>> > - Johan
>> >
>> >
>> > On Wed, Feb 13, 2019 at 6:57 AM Rusty Russell <rusty at rustcorp.com.au
>> > <mailto:rusty at rustcorp.com.au>> wrote:
>> >
>> >     Matt Corallo <lf-lists at mattcorallo.com
>> >     <mailto:lf-lists at mattcorallo.com>> writes:
>> >     >>> Thus, even if you imagine a steady-state mempool growth, unless
>> the
>> >     >>> "near the top of the mempool" criteria is "near the top of the
>> next
>> >     >>> block" (which is obviously *not* incentive-compatible)
>> >     >>
>> >     >> I was defining "top of mempool" as "in the first 4 MSipa", ie.
>> next
>> >     >> block, and assumed you'd only allow RBF if the old package wasn't
>> >     in the
>> >     >> top and the replacement would be.  That seems incentive
>> >     compatible; more
>> >     >> than the current scheme?
>> >     >
>> >     > My point was, because of block time variance, even that criteria
>> >     doesn't hold up. If you assume a steady flow of new transactions and
>> >     one or two blocks come in "late", suddenly "top 4MWeight" isn't
>> >     likely to get confirmed until a few blocks come in "early". Given
>> >     block variance within a 12 block window, this is a relatively likely
>> >     scenario.
>> >
>> >     [ Digging through old mail. ]
>> >
>> >     Doesn't really matter.  Lightning close algorithm would be:
>> >
>> >     1.  Give bitcoind unileratal close.
>> >     2.  Ask bitcoind what current expidited fee is (or survey your
>> mempool).
>> >     3.  Give bitcoind child "push" tx at that total feerate.
>> >     4.  If next block doesn't contain unilateral close tx, goto 2.
>> >
>> >     In this case, if you allow a simpified RBF where 'you can replace if
>> >     1. feerate is higher, 2. new tx is in first 4Msipa of mempool, 3.
>> >     old tx isnt',
>> >     it works.
>> >
>> >     It allows someone 100k of free tx spam, sure.  But it's simple.
>> >
>> >     We could further restrict it by marking the unilateral close
>> somehow to
>> >     say "gonna be pushed" and further limiting the child tx weight (say,
>> >     5kSipa?) in that case.
>> >
>> >     Cheers,
>> >     Rusty.
>> >     _______________________________________________
>> >     Lightning-dev mailing list
>> >     Lightning-dev at lists.linuxfoundation.org
>> >     <mailto:Lightning-dev at lists.linuxfoundation.org>
>> >     https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev
>> >
>>
> _______________________________________________
> Lightning-dev mailing list
> Lightning-dev at lists.linuxfoundation.org
> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191027/184af597/attachment.html>

From dave at dtrt.org  Sun Oct 27 22:54:02 2019
From: dave at dtrt.org (David A. Harding)
Date: Sun, 27 Oct 2019 12:54:02 -1000
Subject: [bitcoin-dev] [Lightning-dev] CPFP Carve-Out for Fee-Prediction
 Issues in Contracting Applications (eg Lightning)
In-Reply-To: <CAD3i26AjhQ9VkCo_5y8aqZ_8YvSqKP2MCkdRv8YunjAhmmXz=Q@mail.gmail.com>
References: <c3f68b73-84c6-7428-4bf6-b47802141392@mattcorallo.com>
	<878t163qzi.fsf@rustcorp.com.au>
	<725fc55a-6263-a9fc-74a5-1017cb1cc885@mattcorallo.com>
	<87wonfem03.fsf@rustcorp.com.au>
	<D072562F-5AD0-4B38-94D1-A0AEF04C3DEB@mattcorallo.com>
	<87zhr0gvw0.fsf@rustcorp.com.au>
	<CAD3i26AjhQ9VkCo_5y8aqZ_8YvSqKP2MCkdRv8YunjAhmmXz=Q@mail.gmail.com>
Message-ID: <20191027225402.rfajp4j6itujq5av@ganymede>

On Thu, Oct 24, 2019 at 03:49:09PM +0200, Johan Tor?s Halseth wrote:
> [...] what about letting the rule be
> 
> The last transaction which is added to a package of dependent
> transactions in the mempool must:
>   * Have no more than one unconfirmed parent.
> [... subsequent email ...]
> I realize these limits are there for a reason though, but I'm wondering if
> we could relax them.

Johan,

I'm not sure any of the other replies to this thread addressed your
request for a reason behind the limits related to your proposal, so I
thought I'd point out that---subsequent to your posting here---a
document[1] was added to the Bitcoin Core developer wiki that I think
describes the risk of the approach you proposed:

> Free relay attack:
>
> - Create a low feerate transaction T.
>
> - Send zillions of child transactions that are slightly higher feerate
>   than T until mempool is full.
>
> - Create one small transaction with feerate just higher than T?s, and
>    watch T and all its children get evicted. Total fees in mempool drops
>    dramatically!
>
> - Attacker just relayed (say) 300MB of data across the whole network
>   but only pays small feerate on one small transaction.

The document goes on to describe at a high level how Bitcoin Core
attempts to mitigate this problem as well as other ways it tries to
optimize the mempool in order to maximize miner profit (and so ensure
that miners continue to use public transaction relay).

I hope that's helpful to you and to others in both understanding the
current state and in thinking about ways in which it might be improved.

-Dave

[1] https://github.com/bitcoin-core/bitcoin-devwiki/wiki/Mempool-and-mining
    Content adapted from slides by Suhas Daftuar, uploaded and formatted
    by Gregory Sanders and Marco Falke.


From johanth at gmail.com  Fri Oct 25 07:05:15 2019
From: johanth at gmail.com (=?UTF-8?Q?Johan_Tor=C3=A5s_Halseth?=)
Date: Fri, 25 Oct 2019 09:05:15 +0200
Subject: [bitcoin-dev] [Lightning-dev] CPFP Carve-Out for Fee-Prediction
 Issues in Contracting Applications (eg Lightning)
In-Reply-To: <83915e8a-f49a-8233-0389-934c189f770c@mattcorallo.com>
References: <c3f68b73-84c6-7428-4bf6-b47802141392@mattcorallo.com>
	<878t163qzi.fsf@rustcorp.com.au>
	<725fc55a-6263-a9fc-74a5-1017cb1cc885@mattcorallo.com>
	<87wonfem03.fsf@rustcorp.com.au>
	<D072562F-5AD0-4B38-94D1-A0AEF04C3DEB@mattcorallo.com>
	<87zhr0gvw0.fsf@rustcorp.com.au>
	<CAD3i26AjhQ9VkCo_5y8aqZ_8YvSqKP2MCkdRv8YunjAhmmXz=Q@mail.gmail.com>
	<83915e8a-f49a-8233-0389-934c189f770c@mattcorallo.com>
Message-ID: <CAD3i26Cf+QpbFXh63NiMig9eceeKaezZwk89A_h_76S_XKkQ9g@mail.gmail.com>

It essentially changes the rule to always allow CPFP-ing the commitment as
long as there is an output available without any descendants. It changes
the commitment from "you always need at least, and exactly, one non-CSV
output per party. " to "you always need at least one non-CSV output per
party. "

I realize these limits are there for a reason though, but I'm wondering if
could relax them. Also now that jeremyrubin has expressed problems with the
current mempool limits.

On Thu, Oct 24, 2019 at 11:25 PM Matt Corallo <lf-lists at mattcorallo.com>
wrote:

> I may be missing something, but I'm not sure how this changes anything?
>
> If you have a commitment transaction, you always need at least, and
> exactly, one non-CSV output per party. The fact that there is a size
> limitation on the transaction that spends for carve-out purposes only
> effects how many other inputs/outputs you can add, but somehow I doubt
> its ever going to be a large enough number to matter.
>
> Matt
>
> On 10/24/19 1:49 PM, Johan Tor?s Halseth wrote:
> > Reviving this old thread now that the recently released RC for bitcoind
> > 0.19 includes the above mentioned carve-out rule.
> >
> > In an attempt to pave the way for more robust CPFP of on-chain contracts
> > (Lightning commitment transactions), the carve-out rule was added in
> > https://github.com/bitcoin/bitcoin/pull/15681. However, having worked on
> > an implementation of a new commitment format for utilizing the Bring
> > Your Own Fees strategy using CPFP, I?m wondering if the special case
> > rule should have been relaxed a bit, to avoid the need for adding a 1
> > CSV to all outputs (in case of Lightning this means HTLC scripts would
> > need to be changed to add the CSV delay).
> >
> > Instead, what about letting the rule be
> >
> > The last transaction which is added to a package of dependent
> > transactions in the mempool must:
> >   * Have no more than one unconfirmed parent.
> >
> > This would of course allow adding a large transaction to each output of
> > the unconfirmed parent, which in effect would allow an attacker to
> > exceed the MAX_PACKAGE_VIRTUAL_SIZE limit in some cases. However, is
> > this a problem with the current mempool acceptance code in bitcoind? I
> > would imagine evicting transactions based on feerate when the max
> > mempool size is met handles this, but I?m asking since it seems like
> > there has been several changes to the acceptance code and eviction
> > policy since the limit was first introduced.
> >
> > - Johan
> >
> >
> > On Wed, Feb 13, 2019 at 6:57 AM Rusty Russell <rusty at rustcorp.com.au
> > <mailto:rusty at rustcorp.com.au>> wrote:
> >
> >     Matt Corallo <lf-lists at mattcorallo.com
> >     <mailto:lf-lists at mattcorallo.com>> writes:
> >     >>> Thus, even if you imagine a steady-state mempool growth, unless
> the
> >     >>> "near the top of the mempool" criteria is "near the top of the
> next
> >     >>> block" (which is obviously *not* incentive-compatible)
> >     >>
> >     >> I was defining "top of mempool" as "in the first 4 MSipa", ie.
> next
> >     >> block, and assumed you'd only allow RBF if the old package wasn't
> >     in the
> >     >> top and the replacement would be.  That seems incentive
> >     compatible; more
> >     >> than the current scheme?
> >     >
> >     > My point was, because of block time variance, even that criteria
> >     doesn't hold up. If you assume a steady flow of new transactions and
> >     one or two blocks come in "late", suddenly "top 4MWeight" isn't
> >     likely to get confirmed until a few blocks come in "early". Given
> >     block variance within a 12 block window, this is a relatively likely
> >     scenario.
> >
> >     [ Digging through old mail. ]
> >
> >     Doesn't really matter.  Lightning close algorithm would be:
> >
> >     1.  Give bitcoind unileratal close.
> >     2.  Ask bitcoind what current expidited fee is (or survey your
> mempool).
> >     3.  Give bitcoind child "push" tx at that total feerate.
> >     4.  If next block doesn't contain unilateral close tx, goto 2.
> >
> >     In this case, if you allow a simpified RBF where 'you can replace if
> >     1. feerate is higher, 2. new tx is in first 4Msipa of mempool, 3.
> >     old tx isnt',
> >     it works.
> >
> >     It allows someone 100k of free tx spam, sure.  But it's simple.
> >
> >     We could further restrict it by marking the unilateral close somehow
> to
> >     say "gonna be pushed" and further limiting the child tx weight (say,
> >     5kSipa?) in that case.
> >
> >     Cheers,
> >     Rusty.
> >     _______________________________________________
> >     Lightning-dev mailing list
> >     Lightning-dev at lists.linuxfoundation.org
> >     <mailto:Lightning-dev at lists.linuxfoundation.org>
> >     https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191025/58a3d7b8/attachment-0001.html>

From johanth at gmail.com  Mon Oct 28 09:45:39 2019
From: johanth at gmail.com (=?UTF-8?Q?Johan_Tor=C3=A5s_Halseth?=)
Date: Mon, 28 Oct 2019 10:45:39 +0100
Subject: [bitcoin-dev] [Lightning-dev] CPFP Carve-Out for Fee-Prediction
 Issues in Contracting Applications (eg Lightning)
In-Reply-To: <CAD5xwhhK4xexDe=aBv78BsK5BvE=4S0OcqeXYHVAfN3wTOr51Q@mail.gmail.com>
References: <CAD3i26Cf+QpbFXh63NiMig9eceeKaezZwk89A_h_76S_XKkQ9g@mail.gmail.com>
	<6728FF51-E378-4AED-99BA-ECB83688AA9C@mattcorallo.com>
	<CAD5xwhhK4xexDe=aBv78BsK5BvE=4S0OcqeXYHVAfN3wTOr51Q@mail.gmail.com>
Message-ID: <CAD3i26DTxnDwhQd+kfS609W==A8oFA8DVJfwMiPt6NSXqhqw4w@mail.gmail.com>

>
>
> I don?te see how? Let?s imagine Party A has two spendable outputs, now
> they stuff the package size on one of their spendable outlets until it is
> right at the limit, add one more on their other output (to meet the
> Carve-Out), and now Party B can?t do anything.

Matt: With the proposed change, party B would always be able to add a child
to its output, regardless of what games party A is playing.


Thanks for the explanation, Jeremy!


> In terms of relay cost, if an ancestor can be replaced, it will invalidate
> all it's children, meaning that no one paid for that broadcasting. This can
> be fixed by appropriately assessing Replace By Fee update fees to
> encapsulate all descendants, but there are some tricky edge cases that make
> this non-obvious to do.


Relay cost is the obvious problem with just naively removing all limits.
Relaxing the current rules by allowing to add a child to each output as
long as it has a single unconfirmed parent would still only allow free
relay of O(size of parent) extra data (which might not be that bad? Similar
to the carve-out rule we could put limits on the child size). This would be
enough for the current LN use case (increasing fee of commitment tx), but
not for OP_SECURETHEBAG I guess, as you need the tree of children, as you
mention.

I imagine walking the mempool wouldn't change much, as you would only have
one extra child per output. But here I'm just speculating, as I don't know
the code well enough know what the diff would look like.


> OP_SECURETHEBAG can help with the LN issue by putting all HTLCS into a
> tree where they are individualized leaf nodes with a preceding CSV. Then,
> the above fix would ensure each HTLC always has time to close properly as
> they would have individualized lockpoints. This is desirable for some
> additional reasons and not for others, but it should "work".


This is interesting for an LN commitment! You could really hide every
output of the commitment within OP_STB, which could either allow bypassing
the fee-pinning attack entirely (if the output cannot be spent unconfirmed)
or adding fees to the commitment using SIGHASH_SINGLE|ANYONECANPAY.

- Johan

On Sun, Oct 27, 2019 at 8:13 PM Jeremy <jlrubin at mit.edu> wrote:

> Johan,
>
> The issues with mempool limits for OP_SECURETHEBAG are related, but have
> distinct solutions.
>
> There are two main categories of mempool issues at stake. One is relay
> cost, the other is mempool walking.
>
> In terms of relay cost, if an ancestor can be replaced, it will invalidate
> all it's children, meaning that no one paid for that broadcasting. This can
> be fixed by appropriately assessing Replace By Fee update fees to
> encapsulate all descendants, but there are some tricky edge cases that make
> this non-obvious to do.
>
> The other issue is walking the mempool -- many of the algorithms we use in
> the mempool can be N log N or N^2 in the number of descendants. (simple
> example: an input chain of length N to a fan out of N outputs that are all
> spent, is O(N^2) to look up ancestors per-child, unless we're caching).
>
> The other sort of walking issue is where the indegree or outdegree for a
> transaction is high. Then when we are computing descendants or ancestors we
> will need to visit it multiple times. To avoid re-expanding a node, we
> currently cache it with a set. This uses O(N) extra memory and makes O(N
> Log N) (we use std::set not unordered_set) comparisons.
>
> I just opened a PR which should help with some of the walking issues by
> allowing us to cheaply cache which nodes we've visited on a run. It makes a
> lot of previously O(N log N) stuff O(N) and doesn't allocate as much new
> memory. See: https://github.com/bitcoin/bitcoin/pull/17268.
>
>
> Now, for OP_SECURETHEBAG we want a particular property that is very
> different from with lightning htlcs (as is). We want that an unlimited
> number of child OP_SECURETHEBAG txns may extend from a confirmed
> OP_SECURETHEBAG, and then at the leaf nodes, we want the same rule as
> lightning (one dangling unconfirmed to permit channels).
>
> OP_SECURETHEBAG can help with the LN issue by putting all HTLCS into a
> tree where they are individualized leaf nodes with a preceding CSV. Then,
> the above fix would ensure each HTLC always has time to close properly as
> they would have individualized lockpoints. This is desirable for some
> additional reasons and not for others, but it should "work".
>
>
>
> --
> @JeremyRubin <https://twitter.com/JeremyRubin>
> <https://twitter.com/JeremyRubin>
>
>
> On Fri, Oct 25, 2019 at 10:31 AM Matt Corallo <lf-lists at mattcorallo.com>
> wrote:
>
>> I don?te see how? Let?s imagine Party A has two spendable outputs, now
>> they stuff the package size on one of their spendable outlets until it is
>> right at the limit, add one more on their other output (to meet the
>> Carve-Out), and now Party B can?t do anything.
>>
>> On Oct 24, 2019, at 21:05, Johan Tor?s Halseth <johanth at gmail.com> wrote:
>>
>> ?
>> It essentially changes the rule to always allow CPFP-ing the commitment
>> as long as there is an output available without any descendants. It changes
>> the commitment from "you always need at least, and exactly, one non-CSV
>> output per party. " to "you always need at least one non-CSV output per
>> party. "
>>
>> I realize these limits are there for a reason though, but I'm wondering
>> if could relax them. Also now that jeremyrubin has expressed problems with
>> the current mempool limits.
>>
>> On Thu, Oct 24, 2019 at 11:25 PM Matt Corallo <lf-lists at mattcorallo.com>
>> wrote:
>>
>>> I may be missing something, but I'm not sure how this changes anything?
>>>
>>> If you have a commitment transaction, you always need at least, and
>>> exactly, one non-CSV output per party. The fact that there is a size
>>> limitation on the transaction that spends for carve-out purposes only
>>> effects how many other inputs/outputs you can add, but somehow I doubt
>>> its ever going to be a large enough number to matter.
>>>
>>> Matt
>>>
>>> On 10/24/19 1:49 PM, Johan Tor?s Halseth wrote:
>>> > Reviving this old thread now that the recently released RC for bitcoind
>>> > 0.19 includes the above mentioned carve-out rule.
>>> >
>>> > In an attempt to pave the way for more robust CPFP of on-chain
>>> contracts
>>> > (Lightning commitment transactions), the carve-out rule was added in
>>> > https://github.com/bitcoin/bitcoin/pull/15681. However, having worked
>>> on
>>> > an implementation of a new commitment format for utilizing the Bring
>>> > Your Own Fees strategy using CPFP, I?m wondering if the special case
>>> > rule should have been relaxed a bit, to avoid the need for adding a 1
>>> > CSV to all outputs (in case of Lightning this means HTLC scripts would
>>> > need to be changed to add the CSV delay).
>>> >
>>> > Instead, what about letting the rule be
>>> >
>>> > The last transaction which is added to a package of dependent
>>> > transactions in the mempool must:
>>> >   * Have no more than one unconfirmed parent.
>>> >
>>> > This would of course allow adding a large transaction to each output of
>>> > the unconfirmed parent, which in effect would allow an attacker to
>>> > exceed the MAX_PACKAGE_VIRTUAL_SIZE limit in some cases. However, is
>>> > this a problem with the current mempool acceptance code in bitcoind? I
>>> > would imagine evicting transactions based on feerate when the max
>>> > mempool size is met handles this, but I?m asking since it seems like
>>> > there has been several changes to the acceptance code and eviction
>>> > policy since the limit was first introduced.
>>> >
>>> > - Johan
>>> >
>>> >
>>> > On Wed, Feb 13, 2019 at 6:57 AM Rusty Russell <rusty at rustcorp.com.au
>>> > <mailto:rusty at rustcorp.com.au>> wrote:
>>> >
>>> >     Matt Corallo <lf-lists at mattcorallo.com
>>> >     <mailto:lf-lists at mattcorallo.com>> writes:
>>> >     >>> Thus, even if you imagine a steady-state mempool growth,
>>> unless the
>>> >     >>> "near the top of the mempool" criteria is "near the top of the
>>> next
>>> >     >>> block" (which is obviously *not* incentive-compatible)
>>> >     >>
>>> >     >> I was defining "top of mempool" as "in the first 4 MSipa", ie.
>>> next
>>> >     >> block, and assumed you'd only allow RBF if the old package
>>> wasn't
>>> >     in the
>>> >     >> top and the replacement would be.  That seems incentive
>>> >     compatible; more
>>> >     >> than the current scheme?
>>> >     >
>>> >     > My point was, because of block time variance, even that criteria
>>> >     doesn't hold up. If you assume a steady flow of new transactions
>>> and
>>> >     one or two blocks come in "late", suddenly "top 4MWeight" isn't
>>> >     likely to get confirmed until a few blocks come in "early". Given
>>> >     block variance within a 12 block window, this is a relatively
>>> likely
>>> >     scenario.
>>> >
>>> >     [ Digging through old mail. ]
>>> >
>>> >     Doesn't really matter.  Lightning close algorithm would be:
>>> >
>>> >     1.  Give bitcoind unileratal close.
>>> >     2.  Ask bitcoind what current expidited fee is (or survey your
>>> mempool).
>>> >     3.  Give bitcoind child "push" tx at that total feerate.
>>> >     4.  If next block doesn't contain unilateral close tx, goto 2.
>>> >
>>> >     In this case, if you allow a simpified RBF where 'you can replace
>>> if
>>> >     1. feerate is higher, 2. new tx is in first 4Msipa of mempool, 3.
>>> >     old tx isnt',
>>> >     it works.
>>> >
>>> >     It allows someone 100k of free tx spam, sure.  But it's simple.
>>> >
>>> >     We could further restrict it by marking the unilateral close
>>> somehow to
>>> >     say "gonna be pushed" and further limiting the child tx weight
>>> (say,
>>> >     5kSipa?) in that case.
>>> >
>>> >     Cheers,
>>> >     Rusty.
>>> >     _______________________________________________
>>> >     Lightning-dev mailing list
>>> >     Lightning-dev at lists.linuxfoundation.org
>>> >     <mailto:Lightning-dev at lists.linuxfoundation.org>
>>> >     https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev
>>> >
>>>
>> _______________________________________________
>> Lightning-dev mailing list
>> Lightning-dev at lists.linuxfoundation.org
>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191028/92777d7b/attachment-0001.html>

From vitteaymeric at gmail.com  Mon Oct 28 10:33:32 2019
From: vitteaymeric at gmail.com (Aymeric Vitte)
Date: Mon, 28 Oct 2019 11:33:32 +0100
Subject: [bitcoin-dev] Fwd: node-Tor is now open source in clear (and
	modular)
In-Reply-To: <f5037ad1-7f31-b8ab-844f-8a6c9a6344ce@gmail.com>
References: <f5037ad1-7f31-b8ab-844f-8a6c9a6344ce@gmail.com>
Message-ID: <70e77789-9fce-0d54-0aed-361035e79c1d@gmail.com>

FYI, javascript implementation of the Tor protocol on server side and
inside browsers

Not related directly to bitcoin-dev but might be of some use one day to
anonymize bitcoin apps (light wallets for example)


-------- Message transf?r? --------
Sujet?: 	node-Tor is now open source in clear (and modular)
Date?: 	Thu, 24 Oct 2019 18:02:42 +0200
De?: 	Aymeric Vitte <vitteaymeric at gmail.com>
Pour?: 	tor-talk at lists.torproject.org



Please see https://github.com/Ayms/node-Tor and http://peersm.com/peersm2

This is a javascript implementation of the Tor protocol on server side
(nodejs) and inside browsers, please note that it is not intended to add
nodes into the Tor network, neither to implement the Tor Browser
features, it is intended to build projects using the Tor protocol from
the browser and/or servers (most likely P2P projects), the Onion Proxy
and Onion Router functions are available directly inside the browser
which establishes circuits with other nodes understanding the Tor
protocol (so it's not a "dumb" proxy), but it can of course establish
circuits with the Tor network nodes (see
https://github.com/Ayms/node-Tor#test-configuration-and-use) and act as
a Tor node

It is financed by NLnet via EU Horizon 2020 Next Generation Internet
Privacy & Trust Enhancing Technologies, now open source under a MIT
license and we made it modular, it is fast (extensively tested when
video streaming was there, especially with bittorrent or ORDB concept)
and the total unminified code
(https://github.com/Ayms/node-Tor/blob/master/html/browser.js) is only 1
MB (so ~600 kB minified) which is quite small for what it does, this is
not a browser extension/module but pure js

Possible next steps are to implement elliptic crypto and connections via
WebRTC Snowflake (peersm2 above uses WebSockets a bit the way flashproxy
was working, ie implementing the ws interface on bridges side), as well
as integrating it with "Discover and move your coins by yourself"
(https://peersm.com/wallet) for anonymous blockchain search and
anonymous sending of transactions from the browser

-- 
Move your coins by yourself (browser version): https://peersm.com/wallet
Bitcoin transactions made simple: https://github.com/Ayms/bitcoin-transactions
Zcash wallets made simple: https://github.com/Ayms/zcash-wallets
Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets
Get the torrent dynamic blocklist: http://peersm.com/getblocklist
Check the 10 M passwords list: http://peersm.com/findmyass
Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org
Peersm : http://www.peersm.com
torrent-live: https://github.com/Ayms/torrent-live
node-Tor : https://www.github.com/Ayms/node-Tor
GitHub : https://www.github.com/Ayms

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191028/1c6957c0/attachment.html>

From dave at dtrt.org  Mon Oct 28 17:14:38 2019
From: dave at dtrt.org (David A. Harding)
Date: Mon, 28 Oct 2019 07:14:38 -1000
Subject: [bitcoin-dev] [Lightning-dev] CPFP Carve-Out for Fee-Prediction
 Issues in Contracting Applications (eg Lightning)
In-Reply-To: <CAD3i26DTxnDwhQd+kfS609W==A8oFA8DVJfwMiPt6NSXqhqw4w@mail.gmail.com>
References: <CAD3i26Cf+QpbFXh63NiMig9eceeKaezZwk89A_h_76S_XKkQ9g@mail.gmail.com>
	<6728FF51-E378-4AED-99BA-ECB83688AA9C@mattcorallo.com>
	<CAD5xwhhK4xexDe=aBv78BsK5BvE=4S0OcqeXYHVAfN3wTOr51Q@mail.gmail.com>
	<CAD3i26DTxnDwhQd+kfS609W==A8oFA8DVJfwMiPt6NSXqhqw4w@mail.gmail.com>
Message-ID: <20191028171416.7owxqblz3ttsvw5r@ganymede>

On Mon, Oct 28, 2019 at 10:45:39AM +0100, Johan Tor?s Halseth wrote:
> Relay cost is the obvious problem with just naively removing all limits.
> Relaxing the current rules by allowing to add a child to each output as
> long as it has a single unconfirmed parent would still only allow free
> relay of O(size of parent) extra data (which might not be that bad? Similar
> to the carve-out rule we could put limits on the child size). 

A parent transaction near the limit of 100,000 vbytes could have almost
10,000 outputs paying OP_TRUE (10 vbytes per output).  If the children
were limited to 10,000 vbytes each (the current max carve-out size),
that allows relaying 100 mega-vbytes or nearly 400 MB data size (larger
than the default maximum mempool size in Bitcoin Core).

As Matt noted in discussion on #lightning-dev about this issue, it's
possible to increase second-child carve-out to nth-child carve-out but
we'd need to be careful about choosing an appropriately low value for n.

For example, BOLT2 limits the number of HTLCs to 483 on each side of the
channel (so 966 + 2 outputs total), which means the worst case free
relay to support the current LN protocol would be approximately:

    (100000 + 968 * 10000) * 4 = ~39 MB

Even if the mempool was empty (as it sometimes is these days), it would
only cost an attacker about 1.5 BTC to fill it at the default minimum
relay feerate[1] so that they could execute this attack at the minimal
cost per iteration of paying for a few hundred or a few thousand vbytes
at slightly higher than the current mempool minimum fee.

Instead, with the existing rules (including second-child carve-out),
they'd have to iterate (39 MB / 400 kB = ~100) times more often to
achieve an equivalent waste of bandwidth, costing them proportionally
more in fees.

So, I think these rough numbers clearly back what Matt said about us
being able to raise the limits a bit if we need to, but that we have to
be careful not to raise them so far that attackers can make it
significantly more bandwidth expensive for people to run relaying full
nodes.

-Dave

[1] Several developers are working on lowering the default minimum in
Bitcoin Core, which would of course make this attack proportionally
cheaper.

From johanth at gmail.com  Wed Oct 30 07:22:53 2019
From: johanth at gmail.com (=?UTF-8?Q?Johan_Tor=C3=A5s_Halseth?=)
Date: Wed, 30 Oct 2019 08:22:53 +0100
Subject: [bitcoin-dev] [Lightning-dev] CPFP Carve-Out for Fee-Prediction
 Issues in Contracting Applications (eg Lightning)
In-Reply-To: <20191028171416.7owxqblz3ttsvw5r@ganymede>
References: <CAD3i26Cf+QpbFXh63NiMig9eceeKaezZwk89A_h_76S_XKkQ9g@mail.gmail.com>
	<6728FF51-E378-4AED-99BA-ECB83688AA9C@mattcorallo.com>
	<CAD5xwhhK4xexDe=aBv78BsK5BvE=4S0OcqeXYHVAfN3wTOr51Q@mail.gmail.com>
	<CAD3i26DTxnDwhQd+kfS609W==A8oFA8DVJfwMiPt6NSXqhqw4w@mail.gmail.com>
	<20191028171416.7owxqblz3ttsvw5r@ganymede>
Message-ID: <CAD3i26BkuxMyazGsQ1+ij25f3xhXUBpUTcOfgYKnY4fur5rGBw@mail.gmail.com>

On Mon, Oct 28, 2019 at 6:16 PM David A. Harding <dave at dtrt.org> wrote:

> A parent transaction near the limit of 100,000 vbytes could have almost
> 10,000 outputs paying OP_TRUE (10 vbytes per output).  If the children
> were limited to 10,000 vbytes each (the current max carve-out size),
> that allows relaying 100 mega-vbytes or nearly 400 MB data size (larger
> than the default maximum mempool size in Bitcoin Core).
>

Thanks, Dave, I wasn't aware the limits would allow this many outputs. And
as your calculation shows, this opens up the potential for free relay of
large amounts of data.

We could start special casing to only allow this for "LN commitment-like"
transactions, but this would be application specific changes, and your
calculation shows that even with the BOLT2 numbers there still exists cases
with a large number of children.

We are moving forward with adding a 1 block delay to all outputs to utilize
the current carve-out rule, and the changes aren't that bad. See Joost's
post in "[PATCH] First draft of option_simplfied_commitment"

- Johan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191030/bc2a90f3/attachment.html>

From ac89 at buffalo.edu  Mon Oct 28 14:12:49 2019
From: ac89 at buffalo.edu (Casciano, Anthony)
Date: Mon, 28 Oct 2019 14:12:49 +0000
Subject: [bitcoin-dev] Ring VS Schnorr signatures
In-Reply-To: <mailman.27.1572264008.20057.bitcoin-dev@lists.linuxfoundation.org>
References: <mailman.27.1572264008.20057.bitcoin-dev@lists.linuxfoundation.org>
Message-ID: <1572271969224.87029@buffalo.edu>

In the world of architectural trade-offs, specifically: confidentiality versus base layer performance, and
in the current regulatory environment, as global monetary affairs begin to get "more real," I'm now leaning towards greater confidentiality rather than my earlier preference for performance.

Do Ring sigs with Stealth addresses impede blockchain performance or do they mis-align with Bitcoin's longer term
dev roadmap?


~ TC

________________________________________
From: bitcoin-dev-bounces at lists.linuxfoundation.org <bitcoin-dev-bounces at lists.linuxfoundation.org> on behalf of bitcoin-dev-request at lists.linuxfoundation.org <bitcoin-dev-request at lists.linuxfoundation.org>
Sent: Monday, October 28, 2019 8:00 AM
To: bitcoin-dev at lists.linuxfoundation.org
Subject: bitcoin-dev Digest, Vol 53, Issue 41

Send bitcoin-dev mailing list submissions to
        bitcoin-dev at lists.linuxfoundation.org

To subscribe or unsubscribe via the World Wide Web, visit
        https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev
or, via email, send a message with subject or body 'help' to
        bitcoin-dev-request at lists.linuxfoundation.org

You can reach the person managing the list at
        bitcoin-dev-owner at lists.linuxfoundation.org

When replying, please edit your Subject line so it is more specific
than "Re: Contents of bitcoin-dev digest..."


Today's Topics:

   1. Fwd: node-Tor is now open source in clear (and    modular)
      (Aymeric Vitte)


----------------------------------------------------------------------

Message: 1
Date: Mon, 28 Oct 2019 11:33:32 +0100
From: Aymeric Vitte <vitteaymeric at gmail.com>
To: Bitcoin Dev <bitcoin-dev at lists.linuxfoundation.org>
Subject: [bitcoin-dev] Fwd: node-Tor is now open source in clear (and
        modular)
Message-ID: <70e77789-9fce-0d54-0aed-361035e79c1d at gmail.com>
Content-Type: text/plain; charset="utf-8"

FYI, javascript implementation of the Tor protocol on server side and
inside browsers

Not related directly to bitcoin-dev but might be of some use one day to
anonymize bitcoin apps (light wallets for example)


-------- Message transf?r? --------
Sujet?:         node-Tor is now open source in clear (and modular)
Date?:  Thu, 24 Oct 2019 18:02:42 +0200
De?:    Aymeric Vitte <vitteaymeric at gmail.com>
Pour?:  tor-talk at lists.torproject.org



Please see https://github.com/Ayms/node-Tor and http://peersm.com/peersm2

This is a javascript implementation of the Tor protocol on server side
(nodejs) and inside browsers, please note that it is not intended to add
nodes into the Tor network, neither to implement the Tor Browser
features, it is intended to build projects using the Tor protocol from
the browser and/or servers (most likely P2P projects), the Onion Proxy
and Onion Router functions are available directly inside the browser
which establishes circuits with other nodes understanding the Tor
protocol (so it's not a "dumb" proxy), but it can of course establish
circuits with the Tor network nodes (see
https://github.com/Ayms/node-Tor#test-configuration-and-use) and act as
a Tor node

It is financed by NLnet via EU Horizon 2020 Next Generation Internet
Privacy & Trust Enhancing Technologies, now open source under a MIT
license and we made it modular, it is fast (extensively tested when
video streaming was there, especially with bittorrent or ORDB concept)
and the total unminified code
(https://github.com/Ayms/node-Tor/blob/master/html/browser.js) is only 1
MB (so ~600 kB minified) which is quite small for what it does, this is
not a browser extension/module but pure js

Possible next steps are to implement elliptic crypto and connections via
WebRTC Snowflake (peersm2 above uses WebSockets a bit the way flashproxy
was working, ie implementing the ws interface on bridges side), as well
as integrating it with "Discover and move your coins by yourself"
(https://peersm.com/wallet) for anonymous blockchain search and
anonymous sending of transactions from the browser

--
Move your coins by yourself (browser version): https://peersm.com/wallet
Bitcoin transactions made simple: https://github.com/Ayms/bitcoin-transactions
Zcash wallets made simple: https://github.com/Ayms/zcash-wallets
Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets
Get the torrent dynamic blocklist: http://peersm.com/getblocklist
Check the 10 M passwords list: http://peersm.com/findmyass
Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org
Peersm : http://www.peersm.com
torrent-live: https://github.com/Ayms/torrent-live
node-Tor : https://www.github.com/Ayms/node-Tor
GitHub : https://www.github.com/Ayms

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191028/1c6957c0/attachment-0001.html>

------------------------------

_______________________________________________
bitcoin-dev mailing list
bitcoin-dev at lists.linuxfoundation.org
https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev


End of bitcoin-dev Digest, Vol 53, Issue 41
*******************************************

